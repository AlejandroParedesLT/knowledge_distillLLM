gpu-compute7
Tue Apr  8 06:44:47 AM EDT 2025
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2012 ./finetune.py --base-path . --model-path gpt2-xl --ckpt-name gpt2-xlarge --n-gpu 4 --data-dir ./processed_data/bugnet_python/full/gpt2/ --num-workers 0 --dev-num 1000 --lr 0.00005 --batch-size 2 --eval-batch-size 4 --gradient-accumulation-steps 1 --warmup-iters 0 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --epochs 10 --max-length 512 --max-prompt-length 256 --do-train --do-valid --eval-gen --save-interval 2000 --eval-interval 2000 --log-interval 4 --mid-log-num 10 --save ./results/gpt2/train/sft/gpt2-xlarge/ --seed 10 --seed-order 10 --deepspeed --deepspeed_config ./configs/deepspeed/ds_config_zero1_fp16.json --type lm --do-sample --top-k 0 --top-p 1.0 --temperature 1.0
PYTHONPATH=.
W0408 06:44:49.957000 121789 torch/distributed/run.py:792] 
W0408 06:44:49.957000 121789 torch/distributed/run.py:792] *****************************************
W0408 06:44:49.957000 121789 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0408 06:44:49.957000 121789 torch/distributed/run.py:792] *****************************************
[2025-04-08 06:44:56,050] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-08 06:44:56,093] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-08 06:44:56,213] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-08 06:44:56,271] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
using world size: 4
[2025-04-08 06:45:05,959] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-08 06:45:05,960] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
arguments:
  model_path ................... gpt2-xl
  ckpt_name .................... gpt2-xlarge
  model_type ................... gpt2
  teacher_model_type ........... None
  n_gpu ........................ 4
  n_nodes ...................... 1
  teacher_model_path ........... None
  teacher_ckpt_name ............ None
  teacher_model_fp16 ........... False
  model_parallel ............... False
  model_parallel_size .......... None
  no_value ..................... False
  dropout_path_rate ............ None
  dtype ........................ torch.float16
  type ......................... lm
  do_train ..................... True
  do_valid ..................... True
  do_eval ...................... False
  base_path .................... .
  load ......................... None
  save ......................... ./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
  log_interval ................. 4
  mid_log_num .................. 10
  save_interval ................ 2000
  eval_interval ................ 2000
  local_rank ................... 0
  save_additional_suffix ....... 
  save_rollout ................. False
  eb_sample_times .............. 3
  data_dir ..................... ./processed_data/bugnet_python/full/gpt2/
  processed_data_dir ........... None
  force_process ................ False
  force_process_demo ........... False
  data_process_workers ......... -1
  train_num .................... -1
  train_ratio .................. 1
  dev_num ...................... 1000
  dev_ratio .................... 1
  gen_num ...................... -1
  data_names ................... None
  prompt_type .................. None
  num_workers .................. 0
  max_prompt_length ............ 256
  min_prompt_length ............ 128
  json_data .................... False
  bin_data ..................... False
  txt_data ..................... False
  prompt_data_dir .............. None
  lm_data_dir .................. None
  eval_ppl ..................... False
  eval_rw ...................... False
  eval_gen ..................... True
  only_prompt .................. False
  batch_size ................... 2
  eval_batch_size .............. 4
  clip_grad .................... 1.0
  total_iters .................. None
  train_iters_per_epoch ........ -1
  max_length ................... 512
  seed ......................... 10
  seed_order ................... 10
  seed_data .................... 42
  seed_ppo ..................... 42
  seed_lm ...................... 7
  epochs ....................... 10
  training_epochs .............. 10000
  gradient_accumulation_steps .. 1
  gradient_checkpointing ....... False
  attn_dtype ................... None
  lr ........................... 5e-05
  lr_min ....................... 1e-07
  weight_decay ................. 0.01
  loss_scale ................... 65536
  kd_ratio ..................... None
  warmup_iters ................. 0
  lr_decay_iters ............... None
  lr_decay_style ............... cosine
  scheduler_name ............... constant_trm
  reward_scaling ............... None
  cliprange_reward ............. 1
  ppo_epochs ................... None
  num_rollouts ................. 256
  num_rollouts_per_device ...... None
  cliprange .................... 0.2
  chunk_size ................... None
  gamma ........................ 0.95
  length_norm .................. False
  single_step_reg .............. False
  teacher_mixed_alpha .......... None
  lm_coef ...................... 1
  top_k ........................ 0
  top_p ........................ 1.0
  do_sample .................... True
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  num_beams .................... 1
  temperature .................. 1.0
  peft ......................... None
  peft_lora_r .................. 8
  peft_lora_alpha .............. 32
  peft_lora_dropout ............ 0.1
  peft_name .................... None
  peft_path .................... None
  teacher_peft_name ............ None
  teacher_peft_path ............ None
  deepspeed .................... True
  deepspeed_config ............. ./configs/deepspeed/ds_config_zero1_fp16.json
  deepscale .................... False
  deepscale_config ............. None
  rank ......................... 0
  world_size ................... 4
[2025-04-08 06:45:06,750] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-08 06:45:06,918] [INFO] [comm.py:658:init_distributed] cdb=None
Tue Apr  8 06:45:06 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   48C    P0             31W /  250W |       4MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   42C    P0             39W /  250W |      20MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   40C    P0             39W /  250W |      26MiB /  32768MiB |      1%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   43C    P0             40W /  250W |      60MiB /  32768MiB |      1%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3        306MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3         56MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3         56MiB |
+-----------------------------------------------------------------------------------------+

[2025-04-08 06:45:06,925] [INFO] [comm.py:658:init_distributed] cdb=None
Tue Apr  8 06:45:06 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   48C    P0             31W /  250W |       4MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   42C    P0             41W /  250W |     310MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   40C    P0             39W /  250W |     310MiB /  32768MiB |      1%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   44C    P0             40W /  250W |     310MiB /  32768MiB |      1%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3        306MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3        306MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3        306MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 06:45:06 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   48C    P0             31W /  250W |       4MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   42C    P0             41W /  250W |     310MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   40C    P0             39W /  250W |     310MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   44C    P0             40W /  250W |     310MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3        306MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3        306MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3        306MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 06:45:07 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   48C    P0             31W /  250W |       4MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   42C    P0             41W /  250W |     310MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   40C    P0             39W /  250W |     310MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   44C    P0             40W /  250W |     310MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3        306MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3        306MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3        306MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 06:45:07 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   48C    P0             31W /  250W |       4MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   42C    P0             41W /  250W |     310MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   40C    P0             39W /  250W |     310MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   44C    P0             40W /  250W |     310MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3        306MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3        306MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3        306MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 06:45:07 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   48C    P0             31W /  250W |       4MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   42C    P0             41W /  250W |     310MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   40C    P0             39W /  250W |     310MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   44C    P0             40W /  250W |     310MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3        306MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3        306MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3        306MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 06:45:07 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   48C    P0             31W /  250W |       4MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   42C    P0             41W /  250W |     310MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   40C    P0             39W /  250W |     310MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   44C    P0             40W /  250W |     310MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3        306MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3        306MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3        306MiB |
+-----------------------------------------------------------------------------------------+

Probing Dataset
Probing end. Max data state 1, total length 1693
1693
Num LM instances: 1693
train num 1693
Probing Dataset
Probing end. Max data state 1, total length 596
596
Num LM instances: 596
Train iters per epoch 211
total_iters 2110
Tue Apr  8 06:45:07 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   48C    P0             31W /  250W |       4MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   42C    P0             41W /  250W |     310MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   40C    P0             39W /  250W |     310MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   44C    P0             40W /  250W |     310MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3        306MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3        306MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3        306MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 06:45:08 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   48C    P0             31W /  250W |       4MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   42C    P0             41W /  250W |     310MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   40C    P0             39W /  250W |     310MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   44C    P0             40W /  250W |     310MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3        306MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3        306MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3        306MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 06:45:08 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   48C    P0             31W /  250W |       4MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   43C    P0             41W /  250W |     310MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   40C    P0             39W /  250W |     310MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   44C    P0             40W /  250W |     310MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3        306MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3        306MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3        306MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 06:45:08 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   48C    P0             31W /  250W |       4MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   43C    P0             41W /  250W |     310MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   40C    P0             39W /  250W |     310MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   44C    P0             40W /  250W |     310MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3        306MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3        306MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3        306MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 06:45:09 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   48C    P0             31W /  250W |       4MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   43C    P0             41W /  250W |     310MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   40C    P0             39W /  250W |     310MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   44C    P0             40W /  250W |     310MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3        306MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3        306MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3        306MiB |
+-----------------------------------------------------------------------------------------+

 > number of parameters: 1557611200
Model load time: 5.726822137832642s
Optimizer = AdamW
[2025-04-08 06:45:14,468] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.4, git-hash=unknown, git-branch=unknown
[2025-04-08 06:45:14,473] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 4
[2025-04-08 06:45:14,601] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 4
[2025-04-08 06:45:14,773] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 4
[2025-04-08 06:45:15,423] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 4
[2025-04-08 06:45:16,862] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-04-08 06:45:16,865] [INFO] [logging.py:128:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-04-08 06:45:16,865] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-08 06:45:17,018] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2025-04-08 06:45:17,030] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2025-04-08 06:45:17,030] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 1 optimizer
[2025-04-08 06:45:17,031] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-08 06:45:17,031] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-08 06:45:17,031] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-08 06:45:17,031] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-08 06:45:43,411] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-08 06:45:43,413] [INFO] [utils.py:782:see_memory_usage] MA 7.3 GB         Max_MA 7.3 GB         CA 7.32 GB         Max_CA 7 GB 
[2025-04-08 06:45:43,413] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 19.09 GB, percent = 7.6%
Tue Apr  8 06:45:43 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   50C    P0             47W /  250W |    7950MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   43C    P0             46W /  250W |    9436MiB /  32768MiB |      1%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   41C    P0             44W /  250W |    7930MiB /  32768MiB |      3%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   44C    P0             40W /  250W |    7930MiB /  32768MiB |      1%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3       7946MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3       9432MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3       7926MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3       7926MiB |
+-----------------------------------------------------------------------------------------+

[2025-04-08 06:45:43,659] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-08 06:45:43,661] [INFO] [utils.py:782:see_memory_usage] MA 7.3 GB         Max_MA 8.75 GB         CA 8.77 GB         Max_CA 9 GB 
[2025-04-08 06:45:43,661] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 18.81 GB, percent = 7.5%
[2025-04-08 06:45:43,662] [INFO] [stage_1_and_2.py:550:__init__] optimizer state initialized
Tue Apr  8 06:45:43 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   50C    P0             47W /  250W |    9436MiB /  32768MiB |      1%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   43C    P0             46W /  250W |    9436MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   41C    P0             44W /  250W |    9436MiB /  32768MiB |      1%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   44C    P0             40W /  250W |    9436MiB /  32768MiB |      1%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3       9432MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3       9432MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3       9432MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3       9432MiB |
+-----------------------------------------------------------------------------------------+

[2025-04-08 06:45:43,868] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-08 06:45:43,869] [INFO] [utils.py:782:see_memory_usage] MA 7.3 GB         Max_MA 7.3 GB         CA 8.77 GB         Max_CA 9 GB 
[2025-04-08 06:45:43,870] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 18.8 GB, percent = 7.5%
[2025-04-08 06:45:43,876] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-08 06:45:43,876] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-04-08 06:45:43,877] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f323ca828f0>
[2025-04-08 06:45:43,877] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05, 5e-05], mom=[(0.9, 0.999), (0.9, 0.999)]
[2025-04-08 06:45:43,880] [INFO] [config.py:1001:print] DeepSpeedEngine configuration:
[2025-04-08 06:45:43,882] [INFO] [config.py:1005:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-08 06:45:43,882] [INFO] [config.py:1005:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-08 06:45:43,882] [INFO] [config.py:1005:print]   amp_enabled .................. False
[2025-04-08 06:45:43,883] [INFO] [config.py:1005:print]   amp_params ................... False
[2025-04-08 06:45:43,883] [INFO] [config.py:1005:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-08 06:45:43,884] [INFO] [config.py:1005:print]   bfloat16_enabled ............. False
[2025-04-08 06:45:43,884] [INFO] [config.py:1005:print]   bfloat16_immediate_grad_update  False
[2025-04-08 06:45:43,884] [INFO] [config.py:1005:print]   checkpoint_parallel_write_pipeline  False
[2025-04-08 06:45:43,884] [INFO] [config.py:1005:print]   checkpoint_tag_validation_enabled  True
[2025-04-08 06:45:43,884] [INFO] [config.py:1005:print]   checkpoint_tag_validation_fail  False
[2025-04-08 06:45:43,885] [INFO] [config.py:1005:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f3250d51ab0>
[2025-04-08 06:45:43,885] [INFO] [config.py:1005:print]   communication_data_type ...... None
[2025-04-08 06:45:43,885] [INFO] [config.py:1005:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-08 06:45:43,885] [INFO] [config.py:1005:print]   curriculum_enabled_legacy .... False
[2025-04-08 06:45:43,886] [INFO] [config.py:1005:print]   curriculum_params_legacy ..... False
[2025-04-08 06:45:43,886] [INFO] [config.py:1005:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-08 06:45:43,886] [INFO] [config.py:1005:print]   data_efficiency_enabled ...... False
[2025-04-08 06:45:43,886] [INFO] [config.py:1005:print]   dataloader_drop_last ......... False
[2025-04-08 06:45:43,886] [INFO] [config.py:1005:print]   disable_allgather ............ False
[2025-04-08 06:45:43,887] [INFO] [config.py:1005:print]   dump_state ................... False
[2025-04-08 06:45:43,887] [INFO] [config.py:1005:print]   dynamic_loss_scale_args ...... None
[2025-04-08 06:45:43,887] [INFO] [config.py:1005:print]   eigenvalue_enabled ........... False
[2025-04-08 06:45:43,887] [INFO] [config.py:1005:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-08 06:45:43,887] [INFO] [config.py:1005:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-08 06:45:43,888] [INFO] [config.py:1005:print]   eigenvalue_layer_num ......... 0
[2025-04-08 06:45:43,888] [INFO] [config.py:1005:print]   eigenvalue_max_iter .......... 100
[2025-04-08 06:45:43,888] [INFO] [config.py:1005:print]   eigenvalue_stability ......... 1e-06
[2025-04-08 06:45:43,888] [INFO] [config.py:1005:print]   eigenvalue_tol ............... 0.01
[2025-04-08 06:45:43,888] [INFO] [config.py:1005:print]   eigenvalue_verbose ........... False
[2025-04-08 06:45:43,889] [INFO] [config.py:1005:print]   elasticity_enabled ........... False
[2025-04-08 06:45:43,889] [INFO] [config.py:1005:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-08 06:45:43,889] [INFO] [config.py:1005:print]   fp16_auto_cast ............... None
[2025-04-08 06:45:43,889] [INFO] [config.py:1005:print]   fp16_enabled ................. False
[2025-04-08 06:45:43,889] [INFO] [config.py:1005:print]   fp16_master_weights_and_gradients  False
[2025-04-08 06:45:43,890] [INFO] [config.py:1005:print]   global_rank .................. 0
[2025-04-08 06:45:43,890] [INFO] [config.py:1005:print]   grad_accum_dtype ............. None
[2025-04-08 06:45:43,890] [INFO] [config.py:1005:print]   gradient_accumulation_steps .. 1
[2025-04-08 06:45:43,890] [INFO] [config.py:1005:print]   gradient_clipping ............ 1.0
[2025-04-08 06:45:43,890] [INFO] [config.py:1005:print]   gradient_predivide_factor .... 1.0
[2025-04-08 06:45:43,891] [INFO] [config.py:1005:print]   graph_harvesting ............. False
[2025-04-08 06:45:43,891] [INFO] [config.py:1005:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-08 06:45:43,891] [INFO] [config.py:1005:print]   initial_dynamic_scale ........ 65536
[2025-04-08 06:45:43,891] [INFO] [config.py:1005:print]   load_universal_checkpoint .... False
[2025-04-08 06:45:43,892] [INFO] [config.py:1005:print]   loss_scale ................... 0
[2025-04-08 06:45:43,892] [INFO] [config.py:1005:print]   memory_breakdown ............. False
[2025-04-08 06:45:43,892] [INFO] [config.py:1005:print]   mics_hierarchial_params_gather  False
[2025-04-08 06:45:43,892] [INFO] [config.py:1005:print]   mics_shard_size .............. -1
[2025-04-08 06:45:43,892] [INFO] [config.py:1005:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-08 06:45:43,893] [INFO] [config.py:1005:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-08 06:45:43,893] [INFO] [config.py:1005:print]   optimizer_legacy_fusion ...... False
[2025-04-08 06:45:43,893] [INFO] [config.py:1005:print]   optimizer_name ............... None
[2025-04-08 06:45:43,893] [INFO] [config.py:1005:print]   optimizer_params ............. None
[2025-04-08 06:45:43,894] [INFO] [config.py:1005:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-08 06:45:43,894] [INFO] [config.py:1005:print]   pld_enabled .................. False
[2025-04-08 06:45:43,894] [INFO] [config.py:1005:print]   pld_params ................... False
[2025-04-08 06:45:43,894] [INFO] [config.py:1005:print]   prescale_gradients ........... False
[2025-04-08 06:45:43,894] [INFO] [config.py:1005:print]   scheduler_name ............... None
[2025-04-08 06:45:43,894] [INFO] [config.py:1005:print]   scheduler_params ............. None
[2025-04-08 06:45:43,895] [INFO] [config.py:1005:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-08 06:45:43,895] [INFO] [config.py:1005:print]   sparse_attention ............. None
[2025-04-08 06:45:43,895] [INFO] [config.py:1005:print]   sparse_gradients_enabled ..... False
[2025-04-08 06:45:43,895] [INFO] [config.py:1005:print]   steps_per_print .............. 10000000
[2025-04-08 06:45:43,895] [INFO] [config.py:1005:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-08 06:45:43,896] [INFO] [config.py:1005:print]   timers_config ................ enabled=True synchronized=True
[2025-04-08 06:45:43,896] [INFO] [config.py:1005:print]   train_batch_size ............. 8
[2025-04-08 06:45:43,896] [INFO] [config.py:1005:print]   train_micro_batch_size_per_gpu  2
[2025-04-08 06:45:43,896] [INFO] [config.py:1005:print]   use_data_before_expert_parallel_  False
[2025-04-08 06:45:43,897] [INFO] [config.py:1005:print]   use_node_local_storage ....... False
[2025-04-08 06:45:43,897] [INFO] [config.py:1005:print]   wall_clock_breakdown ......... False
[2025-04-08 06:45:43,897] [INFO] [config.py:1005:print]   weight_quantization_config ... None
[2025-04-08 06:45:43,897] [INFO] [config.py:1005:print]   world_size ................... 4
[2025-04-08 06:45:43,897] [INFO] [config.py:1005:print]   zero_allow_untested_optimizer  True
[2025-04-08 06:45:43,898] [INFO] [config.py:1005:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-08 06:45:43,898] [INFO] [config.py:1005:print]   zero_enabled ................. True
[2025-04-08 06:45:43,898] [INFO] [config.py:1005:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-08 06:45:43,898] [INFO] [config.py:1005:print]   zero_optimization_stage ...... 1
[2025-04-08 06:45:43,912] [INFO] [config.py:991:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 2, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 1
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false, 
    "gradient_clipping": 1.0, 
    "steps_per_print": 1.000000e+07
}
Model mem
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   7475 MiB |   7475 MiB |  15037 MiB |   7562 MiB |
|       from large pool |   7426 MiB |   7426 MiB |  14983 MiB |   7557 MiB |
|       from small pool |     48 MiB |     48 MiB |     54 MiB |      5 MiB |
|---------------------------------------------------------------------------|
| Active memory         |   7475 MiB |   7475 MiB |  15037 MiB |   7562 MiB |
|       from large pool |   7426 MiB |   7426 MiB |  14983 MiB |   7557 MiB |
|       from small pool |     48 MiB |     48 MiB |     54 MiB |      5 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |   7475 MiB |   7475 MiB |  14902 MiB |   7427 MiB |
|       from large pool |   7426 MiB |   7426 MiB |  14848 MiB |   7422 MiB |
|       from small pool |     48 MiB |     48 MiB |     53 MiB |      4 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   8982 MiB |   8982 MiB |  15072 MiB |   6090 MiB |
|       from large pool |   8930 MiB |   8930 MiB |  15018 MiB |   6088 MiB |
|       from small pool |     52 MiB |     52 MiB |     54 MiB |      2 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  21198 KiB |  21198 KiB | 563427 KiB | 542229 KiB |
|       from large pool |  18104 KiB |  18104 KiB | 527487 KiB | 509382 KiB |
|       from small pool |   3093 KiB |   3093 KiB |  35940 KiB |  32847 KiB |
|---------------------------------------------------------------------------|
| Allocations           |     100    |     100    |    1262    |    1162    |
|       from large pool |       3    |       3    |     198    |     195    |
|       from small pool |      97    |      97    |    1064    |     967    |
|---------------------------------------------------------------------------|
| Active allocs         |     100    |     100    |    1262    |    1162    |
|       from large pool |       3    |       3    |     198    |     195    |
|       from small pool |      97    |      97    |    1064    |     967    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      30    |      30    |     201    |     171    |
|       from large pool |       4    |       4    |     174    |     170    |
|       from small pool |      26    |      26    |      27    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |     231    |     226    |
|       from large pool |       2    |       2    |      53    |      51    |
|       from small pool |       3    |       3    |     178    |     175    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

Start Fine-tuning
Tue Apr  8 06:45:43 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   50C    P0             47W /  250W |    9436MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   43C    P0             46W /  250W |    9436MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   41C    P0             44W /  250W |    9436MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   44C    P0             40W /  250W |    9436MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3       9432MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3       9432MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3       9432MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3       9432MiB |
+-----------------------------------------------------------------------------------------+
Tue Apr  8 06:45:43 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   50C    P0             47W /  250W |    9436MiB /  32768MiB |      1%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   43C    P0             46W /  250W |    9436MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   41C    P0             44W /  250W |    9436MiB /  32768MiB |      1%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   44C    P0             40W /  250W |    9436MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3       9432MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3       9432MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3       9432MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3       9432MiB |
+-----------------------------------------------------------------------------------------+


Tue Apr  8 06:45:44 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   50C    P0             47W /  250W |    9436MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   44C    P0             46W /  250W |    9436MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   41C    P0             44W /  250W |    9436MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   44C    P0             40W /  250W |    9436MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3       9432MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3       9434MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3       9432MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3       9432MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 06:45:44 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   50C    P0             47W /  250W |    9436MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   44C    P0             46W /  250W |    9436MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   40C    P0             44W /  250W |    9436MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   44C    P0             40W /  250W |    9436MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3       9432MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3       9436MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3       9432MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3       9432MiB |
+-----------------------------------------------------------------------------------------+

Start Fine-tuning
Tue Apr  8 06:45:44 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   50C    P0             47W /  250W |    9436MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   43C    P0            100W /  250W |    9450MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   41C    P0             44W /  250W |    9436MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   44C    P0             40W /  250W |    9436MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3       9432MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3       9446MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3       9432MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3       9432MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 06:45:44 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   50C    P0             47W /  250W |    9436MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   48C    P0            231W /  250W |   10644MiB /  32768MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   41C    P0             44W /  250W |    9436MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   45C    P0            153W /  250W |    9450MiB /  32768MiB |      4%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3       9432MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      10640MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3       9432MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3       9446MiB |
+-----------------------------------------------------------------------------------------+

dp size 4
0/63
1/63
2/63
3/63
4/63
5/63
6/63
7/63
8/63
9/63
10/63
11/63
12/63
13/63
14/63
15/63
Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]Evaluating:   2%|         | 1/63 [00:32<33:22, 32.30s/it]Evaluating:   3%|         | 2/63 [01:04<32:32, 32.00s/it]Evaluating:   5%|         | 3/63 [01:35<31:54, 31.90s/it]Evaluating:   6%|         | 4/63 [02:07<31:20, 31.87s/it]Evaluating:   8%|         | 5/63 [02:39<30:47, 31.85s/it]Evaluating:  10%|         | 6/63 [03:11<30:13, 31.82s/it]Evaluating:  11%|         | 7/63 [03:43<29:41, 31.81s/it]Evaluating:  13%|        | 8/63 [04:14<29:08, 31.79s/it]Evaluating:  14%|        | 9/63 [04:46<28:36, 31.78s/it]Evaluating:  16%|        | 10/63 [05:18<28:04, 31.79s/it]Evaluating:  17%|        | 11/63 [05:50<27:33, 31.79s/it]Evaluating:  19%|        | 12/63 [06:21<27:01, 31.79s/it]Evaluating:  21%|        | 13/63 [06:53<26:29, 31.79s/it]Evaluating:  22%|       | 14/63 [07:25<25:59, 31.82s/it]Evaluating:  24%|       | 15/63 [07:57<25:28, 31.84s/it]Evaluating:  25%|     16/63
17/63
18/63
19/63
20/63
21/63
22/63
23/63
24/63
25/63
26/63
27/63
28/63
29/63
30/63
  | 16/63 [08:29<24:57, 31.86s/it]Evaluating:  27%|       | 17/63 [09:00<24:19, 31.72s/it]Evaluating:  29%|       | 18/63 [09:32<23:49, 31.76s/it]Evaluating:  30%|       | 19/63 [10:04<23:17, 31.77s/it]Evaluating:  32%|      | 20/63 [10:36<22:46, 31.78s/it]Evaluating:  33%|      | 21/63 [11:08<22:15, 31.80s/it]Evaluating:  35%|      | 22/63 [11:40<21:45, 31.83s/it]Evaluating:  37%|      | 23/63 [12:11<21:13, 31.85s/it]Evaluating:  38%|      | 24/63 [12:43<20:41, 31.83s/it]Evaluating:  40%|      | 25/63 [13:15<20:10, 31.85s/it]Evaluating:  41%|     | 26/63 [13:47<19:38, 31.84s/it]Evaluating:  43%|     | 27/63 [14:19<19:06, 31.86s/it]Evaluating:  44%|     | 28/63 [14:51<18:35, 31.86s/it]Evaluating:  46%|     | 29/63 [15:22<18:02, 31.85s/it]Evaluating:  48%|     | 30/63 [15:54<17:30, 31.85s/it]Evaluating:  49%| 31/63
32/63
33/63
34/63
35/63
36/63
Distributed index stop interation. Idx: 597 Total_length: 596
Distributed index stop interation. Idx: 596 Total_length: 596
    | 31/63 [16:26<16:59, 31.85s/it]Evaluating:  51%|     | 32/63 [16:58<16:26, 31.84s/it]Evaluating:  52%|    | 33/63 [17:30<15:56, 31.88s/it]Evaluating:  54%|    | 34/63 [18:02<15:23, 31.85s/it]Evaluating:  56%|    | 35/63 [18:34<14:51, 31.84s/it]Evaluating:  57%|    | 36/63 [19:05<14:19, 31.85s/it]Evaluating:  59%|    | 37/63 [19:37<13:47, 31.84s/it]Evaluating:  59%|    | 37/63 [19:37<13:47, 31.83s/it]
Distributed index stop interation. Idx: 598 Total_length: 596Distributed index stop interation. Idx: 599 Total_length: 596

Tue Apr  8 07:05:23 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   57C    P0             46W /  250W |   12762MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   49C    P0             48W /  250W |   12762MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   46C    P0             46W /  250W |   12762MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   50C    P0             41W /  250W |   12762MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      12758MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      12758MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      12758MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      12758MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 07:05:23 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   57C    P0             46W /  250W |   12762MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   49C    P0             48W /  250W |   12762MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   46C    P0             46W /  250W |   12762MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   50C    P0             41W /  250W |   12762MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      12758MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      12758MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      12758MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      12758MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 07:05:23 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   57C    P0             46W /  250W |   12762MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   49C    P0             48W /  250W |   12762MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   46C    P0             46W /  250W |   12762MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   50C    P0             41W /  250W |   12762MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      12758MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      12758MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      12758MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      12758MiB |
+-----------------------------------------------------------------------------------------+

./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1/eval/0
dev | avg_loss: 2.03891194188917 | {'exact_match': 0.0, 'rougeL': 5.5521}
Tue Apr  8 07:05:28 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   56C    P0             45W /  250W |   12762MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   49C    P0             49W /  250W |   29496MiB /  32768MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   46C    P0             47W /  250W |   29496MiB /  32768MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   50C    P0             48W /  250W |   29466MiB /  32768MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      12758MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29492MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29492MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      29462MiB |
+-----------------------------------------------------------------------------------------+

train | epoch   0 | Iter:      1/  2110 | global iter:      1/  2110 | loss: 2.0232 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale:     1.0000 | micro time: 3.949 | step time: 0.000
train | epoch   0 | Iter:      2/  2110 | global iter:      2/  2110 | loss: 1.9101 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale:     1.0000 | micro time: 4.003 | step time: 0.000
train | epoch   0 | Iter:      3/  2110 | global iter:      3/  2110 | loss: 1.3849 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   0 | Iter:      4/  2110 | global iter:      4/  2110 | loss: 0.8023 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:      4/  2110 | global iter:      4/  2110 | loss: 1.5301 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale:     1.0000 | micro time: 3.775 | step time: 3.880
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:      5/  2110 | global iter:      5/  2110 | loss: 1.1277 | ds_loss: 0.0000 | lr: 4.9999e-05 | scale:     1.0000 | micro time: 3.780 | step time: 0.000
train | epoch   0 | Iter:      6/  2110 | global iter:      6/  2110 | loss: 0.8473 | ds_loss: 0.0000 | lr: 4.9999e-05 | scale:     1.0000 | micro time: 3.773 | step time: 0.000
train | epoch   0 | Iter:      7/  2110 | global iter:      7/  2110 | loss: 0.7411 | ds_loss: 0.0000 | lr: 4.9999e-05 | scale:     1.0000 | micro time: 3.772 | step time: 0.000
train | epoch   0 | Iter:      8/  2110 | global iter:      8/  2110 | loss: 0.7180 | ds_loss: 0.0000 | lr: 4.9998e-05 | scale:     1.0000 | micro time: 3.814 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:      8/  2110 | global iter:      8/  2110 | loss: 0.8585 | ds_loss: 0.0000 | lr: 4.9998e-05 | scale:     1.0000 | micro time: 3.814 | step time: 3.785
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:      9/  2110 | global iter:      9/  2110 | loss: 0.8600 | ds_loss: 0.0000 | lr: 4.9998e-05 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   0 | Iter:     10/  2110 | global iter:     10/  2110 | loss: 0.6155 | ds_loss: 0.0000 | lr: 4.9997e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   0 | Iter:     11/  2110 | global iter:     11/  2110 | loss: 0.7508 | ds_loss: 0.0000 | lr: 4.9997e-05 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
train | epoch   0 | Iter:     12/  2110 | global iter:     12/  2110 | loss: 0.9159 | ds_loss: 0.0000 | lr: 4.9996e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:     12/  2110 | global iter:     12/  2110 | loss: 0.7855 | ds_loss: 0.0000 | lr: 4.9996e-05 | scale:     1.0000 | micro time: 3.787 | step time: 3.792
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:     13/  2110 | global iter:     13/  2110 | loss: 0.4739 | ds_loss: 0.0000 | lr: 4.9995e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   0 | Iter:     14/  2110 | global iter:     14/  2110 | loss: 0.6273 | ds_loss: 0.0000 | lr: 4.9995e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   0 | Iter:     15/  2110 | global iter:     15/  2110 | loss: 0.5971 | ds_loss: 0.0000 | lr: 4.9994e-05 | scale:     1.0000 | micro time: 3.814 | step time: 0.000
train | epoch   0 | Iter:     16/  2110 | global iter:     16/  2110 | loss: 0.5109 | ds_loss: 0.0000 | lr: 4.9993e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:     16/  2110 | global iter:     16/  2110 | loss: 0.5523 | ds_loss: 0.0000 | lr: 4.9993e-05 | scale:     1.0000 | micro time: 3.798 | step time: 3.802
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:     17/  2110 | global iter:     17/  2110 | loss: 0.6713 | ds_loss: 0.0000 | lr: 4.9992e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   0 | Iter:     18/  2110 | global iter:     18/  2110 | loss: 0.5478 | ds_loss: 0.0000 | lr: 4.9991e-05 | scale:     1.0000 | micro time: 3.830 | step time: 0.000
train | epoch   0 | Iter:     19/  2110 | global iter:     19/  2110 | loss: 0.4256 | ds_loss: 0.0000 | lr: 4.9990e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   0 | Iter:     20/  2110 | global iter:     20/  2110 | loss: 0.6864 | ds_loss: 0.0000 | lr: 4.9989e-05 | scale:     1.0000 | micro time: 3.820 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:     20/  2110 | global iter:     20/  2110 | loss: 0.5828 | ds_loss: 0.0000 | lr: 4.9989e-05 | scale:     1.0000 | micro time: 3.820 | step time: 3.805
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:     21/  2110 | global iter:     21/  2110 | loss: 0.5810 | ds_loss: 0.0000 | lr: 4.9988e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   0 | Iter:     22/  2110 | global iter:     22/  2110 | loss: 0.6359 | ds_loss: 0.0000 | lr: 4.9987e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   0 | Iter:     23/  2110 | global iter:     23/  2110 | loss: 0.4597 | ds_loss: 0.0000 | lr: 4.9985e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   0 | Iter:     24/  2110 | global iter:     24/  2110 | loss: 0.5465 | ds_loss: 0.0000 | lr: 4.9984e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:     24/  2110 | global iter:     24/  2110 | loss: 0.5558 | ds_loss: 0.0000 | lr: 4.9984e-05 | scale:     1.0000 | micro time: 3.802 | step time: 3.795
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:     25/  2110 | global iter:     25/  2110 | loss: 0.5335 | ds_loss: 0.0000 | lr: 4.9983e-05 | scale:     1.0000 | micro time: 3.813 | step time: 0.000
train | epoch   0 | Iter:     26/  2110 | global iter:     26/  2110 | loss: 0.7395 | ds_loss: 0.0000 | lr: 4.9981e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   0 | Iter:     27/  2110 | global iter:     27/  2110 | loss: 0.6164 | ds_loss: 0.0000 | lr: 4.9980e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   0 | Iter:     28/  2110 | global iter:     28/  2110 | loss: 0.4819 | ds_loss: 0.0000 | lr: 4.9978e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:     28/  2110 | global iter:     28/  2110 | loss: 0.5928 | ds_loss: 0.0000 | lr: 4.9978e-05 | scale:     1.0000 | micro time: 3.818 | step time: 3.806
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:     29/  2110 | global iter:     29/  2110 | loss: 0.5740 | ds_loss: 0.0000 | lr: 4.9977e-05 | scale:     1.0000 | micro time: 3.805 | step time: 0.000
train | epoch   0 | Iter:     30/  2110 | global iter:     30/  2110 | loss: 0.9354 | ds_loss: 0.0000 | lr: 4.9975e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   0 | Iter:     31/  2110 | global iter:     31/  2110 | loss: 0.4414 | ds_loss: 0.0000 | lr: 4.9973e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   0 | Iter:     32/  2110 | global iter:     32/  2110 | loss: 0.4265 | ds_loss: 0.0000 | lr: 4.9972e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:     32/  2110 | global iter:     32/  2110 | loss: 0.5943 | ds_loss: 0.0000 | lr: 4.9972e-05 | scale:     1.0000 | micro time: 3.801 | step time: 3.796
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:     33/  2110 | global iter:     33/  2110 | loss: 0.5770 | ds_loss: 0.0000 | lr: 4.9970e-05 | scale:     1.0000 | micro time: 3.784 | step time: 0.000
train | epoch   0 | Iter:     34/  2110 | global iter:     34/  2110 | loss: 0.4388 | ds_loss: 0.0000 | lr: 4.9968e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   0 | Iter:     35/  2110 | global iter:     35/  2110 | loss: 0.5238 | ds_loss: 0.0000 | lr: 4.9966e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
train | epoch   0 | Iter:     36/  2110 | global iter:     36/  2110 | loss: 0.2554 | ds_loss: 0.0000 | lr: 4.9964e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:     36/  2110 | global iter:     36/  2110 | loss: 0.4488 | ds_loss: 0.0000 | lr: 4.9964e-05 | scale:     1.0000 | micro time: 3.786 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:     37/  2110 | global iter:     37/  2110 | loss: 0.5486 | ds_loss: 0.0000 | lr: 4.9962e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   0 | Iter:     38/  2110 | global iter:     38/  2110 | loss: 0.5241 | ds_loss: 0.0000 | lr: 4.9960e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   0 | Iter:     39/  2110 | global iter:     39/  2110 | loss: 0.5704 | ds_loss: 0.0000 | lr: 4.9958e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   0 | Iter:     40/  2110 | global iter:     40/  2110 | loss: 0.6824 | ds_loss: 0.0000 | lr: 4.9956e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:     40/  2110 | global iter:     40/  2110 | loss: 0.5814 | ds_loss: 0.0000 | lr: 4.9956e-05 | scale:     1.0000 | micro time: 3.801 | step time: 3.792
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:     41/  2110 | global iter:     41/  2110 | loss: 0.5528 | ds_loss: 0.0000 | lr: 4.9954e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   0 | Iter:     42/  2110 | global iter:     42/  2110 | loss: 0.5408 | ds_loss: 0.0000 | lr: 4.9951e-05 | scale:     1.0000 | micro time: 3.797 | step time: 0.000
train | epoch   0 | Iter:     43/  2110 | global iter:     43/  2110 | loss: 0.5058 | ds_loss: 0.0000 | lr: 4.9949e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
train | epoch   0 | Iter:     44/  2110 | global iter:     44/  2110 | loss: 0.6884 | ds_loss: 0.0000 | lr: 4.9946e-05 | scale:     1.0000 | micro time: 3.772 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:     44/  2110 | global iter:     44/  2110 | loss: 0.5720 | ds_loss: 0.0000 | lr: 4.9946e-05 | scale:     1.0000 | micro time: 3.772 | step time: 3.796
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:     45/  2110 | global iter:     45/  2110 | loss: 0.4935 | ds_loss: 0.0000 | lr: 4.9944e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   0 | Iter:     46/  2110 | global iter:     46/  2110 | loss: 0.6546 | ds_loss: 0.0000 | lr: 4.9942e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   0 | Iter:     47/  2110 | global iter:     47/  2110 | loss: 0.3524 | ds_loss: 0.0000 | lr: 4.9939e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
train | epoch   0 | Iter:     48/  2110 | global iter:     48/  2110 | loss: 0.3745 | ds_loss: 0.0000 | lr: 4.9936e-05 | scale:     1.0000 | micro time: 3.800 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:     48/  2110 | global iter:     48/  2110 | loss: 0.4687 | ds_loss: 0.0000 | lr: 4.9936e-05 | scale:     1.0000 | micro time: 3.800 | step time: 3.802
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:     49/  2110 | global iter:     49/  2110 | loss: 0.6201 | ds_loss: 0.0000 | lr: 4.9934e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   0 | Iter:     50/  2110 | global iter:     50/  2110 | loss: 0.7857 | ds_loss: 0.0000 | lr: 4.9931e-05 | scale:     1.0000 | micro time: 3.780 | step time: 0.000
train | epoch   0 | Iter:     51/  2110 | global iter:     51/  2110 | loss: 0.6775 | ds_loss: 0.0000 | lr: 4.9928e-05 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
train | epoch   0 | Iter:     52/  2110 | global iter:     52/  2110 | loss: 0.3561 | ds_loss: 0.0000 | lr: 4.9925e-05 | scale:     1.0000 | micro time: 3.780 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:     52/  2110 | global iter:     52/  2110 | loss: 0.6098 | ds_loss: 0.0000 | lr: 4.9925e-05 | scale:     1.0000 | micro time: 3.780 | step time: 3.784
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:     53/  2110 | global iter:     53/  2110 | loss: 0.4021 | ds_loss: 0.0000 | lr: 4.9922e-05 | scale:     1.0000 | micro time: 3.797 | step time: 0.000
train | epoch   0 | Iter:     54/  2110 | global iter:     54/  2110 | loss: 0.6291 | ds_loss: 0.0000 | lr: 4.9919e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   0 | Iter:     55/  2110 | global iter:     55/  2110 | loss: 0.3994 | ds_loss: 0.0000 | lr: 4.9916e-05 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   0 | Iter:     56/  2110 | global iter:     56/  2110 | loss: 0.3725 | ds_loss: 0.0000 | lr: 4.9913e-05 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:     56/  2110 | global iter:     56/  2110 | loss: 0.4508 | ds_loss: 0.0000 | lr: 4.9913e-05 | scale:     1.0000 | micro time: 3.799 | step time: 3.796
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:     57/  2110 | global iter:     57/  2110 | loss: 0.4344 | ds_loss: 0.0000 | lr: 4.9910e-05 | scale:     1.0000 | micro time: 3.796 | step time: 0.000
train | epoch   0 | Iter:     58/  2110 | global iter:     58/  2110 | loss: 0.4213 | ds_loss: 0.0000 | lr: 4.9907e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   0 | Iter:     59/  2110 | global iter:     59/  2110 | loss: 0.5361 | ds_loss: 0.0000 | lr: 4.9904e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   0 | Iter:     60/  2110 | global iter:     60/  2110 | loss: 0.6872 | ds_loss: 0.0000 | lr: 4.9901e-05 | scale:     1.0000 | micro time: 3.792 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:     60/  2110 | global iter:     60/  2110 | loss: 0.5197 | ds_loss: 0.0000 | lr: 4.9901e-05 | scale:     1.0000 | micro time: 3.792 | step time: 3.793
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:     61/  2110 | global iter:     61/  2110 | loss: 0.5084 | ds_loss: 0.0000 | lr: 4.9897e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   0 | Iter:     62/  2110 | global iter:     62/  2110 | loss: 0.4918 | ds_loss: 0.0000 | lr: 4.9894e-05 | scale:     1.0000 | micro time: 3.780 | step time: 0.000
train | epoch   0 | Iter:     63/  2110 | global iter:     63/  2110 | loss: 0.5146 | ds_loss: 0.0000 | lr: 4.9890e-05 | scale:     1.0000 | micro time: 3.809 | step time: 0.000
train | epoch   0 | Iter:     64/  2110 | global iter:     64/  2110 | loss: 0.7004 | ds_loss: 0.0000 | lr: 4.9887e-05 | scale:     1.0000 | micro time: 3.792 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:     64/  2110 | global iter:     64/  2110 | loss: 0.5538 | ds_loss: 0.0000 | lr: 4.9887e-05 | scale:     1.0000 | micro time: 3.792 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:     65/  2110 | global iter:     65/  2110 | loss: 0.4619 | ds_loss: 0.0000 | lr: 4.9883e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   0 | Iter:     66/  2110 | global iter:     66/  2110 | loss: 0.4661 | ds_loss: 0.0000 | lr: 4.9880e-05 | scale:     1.0000 | micro time: 3.776 | step time: 0.000
train | epoch   0 | Iter:     67/  2110 | global iter:     67/  2110 | loss: 0.5543 | ds_loss: 0.0000 | lr: 4.9876e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   0 | Iter:     68/  2110 | global iter:     68/  2110 | loss: 0.5613 | ds_loss: 0.0000 | lr: 4.9872e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:     68/  2110 | global iter:     68/  2110 | loss: 0.5109 | ds_loss: 0.0000 | lr: 4.9872e-05 | scale:     1.0000 | micro time: 3.810 | step time: 3.792
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:     69/  2110 | global iter:     69/  2110 | loss: 0.3342 | ds_loss: 0.0000 | lr: 4.9868e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   0 | Iter:     70/  2110 | global iter:     70/  2110 | loss: 0.8064 | ds_loss: 0.0000 | lr: 4.9865e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   0 | Iter:     71/  2110 | global iter:     71/  2110 | loss: 0.3925 | ds_loss: 0.0000 | lr: 4.9861e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   0 | Iter:     72/  2110 | global iter:     72/  2110 | loss: 0.6796 | ds_loss: 0.0000 | lr: 4.9857e-05 | scale:     1.0000 | micro time: 3.812 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:     72/  2110 | global iter:     72/  2110 | loss: 0.5532 | ds_loss: 0.0000 | lr: 4.9857e-05 | scale:     1.0000 | micro time: 3.812 | step time: 3.790
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:     73/  2110 | global iter:     73/  2110 | loss: 0.6398 | ds_loss: 0.0000 | lr: 4.9853e-05 | scale:     1.0000 | micro time: 3.767 | step time: 0.000
train | epoch   0 | Iter:     74/  2110 | global iter:     74/  2110 | loss: 0.5050 | ds_loss: 0.0000 | lr: 4.9849e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
train | epoch   0 | Iter:     75/  2110 | global iter:     75/  2110 | loss: 0.5963 | ds_loss: 0.0000 | lr: 4.9845e-05 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
train | epoch   0 | Iter:     76/  2110 | global iter:     76/  2110 | loss: 0.6568 | ds_loss: 0.0000 | lr: 4.9840e-05 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:     76/  2110 | global iter:     76/  2110 | loss: 0.5995 | ds_loss: 0.0000 | lr: 4.9840e-05 | scale:     1.0000 | micro time: 3.803 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:     77/  2110 | global iter:     77/  2110 | loss: 0.5418 | ds_loss: 0.0000 | lr: 4.9836e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   0 | Iter:     78/  2110 | global iter:     78/  2110 | loss: 0.2854 | ds_loss: 0.0000 | lr: 4.9832e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   0 | Iter:     79/  2110 | global iter:     79/  2110 | loss: 0.4517 | ds_loss: 0.0000 | lr: 4.9828e-05 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
train | epoch   0 | Iter:     80/  2110 | global iter:     80/  2110 | loss: 0.4818 | ds_loss: 0.0000 | lr: 4.9823e-05 | scale:     1.0000 | micro time: 3.784 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:     80/  2110 | global iter:     80/  2110 | loss: 0.4402 | ds_loss: 0.0000 | lr: 4.9823e-05 | scale:     1.0000 | micro time: 3.784 | step time: 3.790
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:     81/  2110 | global iter:     81/  2110 | loss: 0.6010 | ds_loss: 0.0000 | lr: 4.9819e-05 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
train | epoch   0 | Iter:     82/  2110 | global iter:     82/  2110 | loss: 0.3528 | ds_loss: 0.0000 | lr: 4.9814e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   0 | Iter:     83/  2110 | global iter:     83/  2110 | loss: 0.3326 | ds_loss: 0.0000 | lr: 4.9810e-05 | scale:     1.0000 | micro time: 3.830 | step time: 0.000
train | epoch   0 | Iter:     84/  2110 | global iter:     84/  2110 | loss: 0.2859 | ds_loss: 0.0000 | lr: 4.9805e-05 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:     84/  2110 | global iter:     84/  2110 | loss: 0.3931 | ds_loss: 0.0000 | lr: 4.9805e-05 | scale:     1.0000 | micro time: 3.778 | step time: 3.804
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:     85/  2110 | global iter:     85/  2110 | loss: 0.4564 | ds_loss: 0.0000 | lr: 4.9800e-05 | scale:     1.0000 | micro time: 3.813 | step time: 0.000
train | epoch   0 | Iter:     86/  2110 | global iter:     86/  2110 | loss: 0.4285 | ds_loss: 0.0000 | lr: 4.9796e-05 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   0 | Iter:     87/  2110 | global iter:     87/  2110 | loss: 0.3590 | ds_loss: 0.0000 | lr: 4.9791e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   0 | Iter:     88/  2110 | global iter:     88/  2110 | loss: 0.3251 | ds_loss: 0.0000 | lr: 4.9786e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:     88/  2110 | global iter:     88/  2110 | loss: 0.3923 | ds_loss: 0.0000 | lr: 4.9786e-05 | scale:     1.0000 | micro time: 3.802 | step time: 3.796
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:     89/  2110 | global iter:     89/  2110 | loss: 0.5909 | ds_loss: 0.0000 | lr: 4.9781e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   0 | Iter:     90/  2110 | global iter:     90/  2110 | loss: 0.3982 | ds_loss: 0.0000 | lr: 4.9776e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   0 | Iter:     91/  2110 | global iter:     91/  2110 | loss: 0.4780 | ds_loss: 0.0000 | lr: 4.9771e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   0 | Iter:     92/  2110 | global iter:     92/  2110 | loss: 0.5154 | ds_loss: 0.0000 | lr: 4.9766e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:     92/  2110 | global iter:     92/  2110 | loss: 0.4956 | ds_loss: 0.0000 | lr: 4.9766e-05 | scale:     1.0000 | micro time: 3.795 | step time: 3.800
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:     93/  2110 | global iter:     93/  2110 | loss: 0.6267 | ds_loss: 0.0000 | lr: 4.9761e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   0 | Iter:     94/  2110 | global iter:     94/  2110 | loss: 0.7624 | ds_loss: 0.0000 | lr: 4.9756e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   0 | Iter:     95/  2110 | global iter:     95/  2110 | loss: 0.3189 | ds_loss: 0.0000 | lr: 4.9751e-05 | scale:     1.0000 | micro time: 3.813 | step time: 0.000
train | epoch   0 | Iter:     96/  2110 | global iter:     96/  2110 | loss: 0.4939 | ds_loss: 0.0000 | lr: 4.9746e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:     96/  2110 | global iter:     96/  2110 | loss: 0.5505 | ds_loss: 0.0000 | lr: 4.9746e-05 | scale:     1.0000 | micro time: 3.782 | step time: 3.799
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:     97/  2110 | global iter:     97/  2110 | loss: 0.4170 | ds_loss: 0.0000 | lr: 4.9740e-05 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   0 | Iter:     98/  2110 | global iter:     98/  2110 | loss: 0.7417 | ds_loss: 0.0000 | lr: 4.9735e-05 | scale:     1.0000 | micro time: 3.780 | step time: 0.000
train | epoch   0 | Iter:     99/  2110 | global iter:     99/  2110 | loss: 0.3997 | ds_loss: 0.0000 | lr: 4.9729e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   0 | Iter:    100/  2110 | global iter:    100/  2110 | loss: 0.4519 | ds_loss: 0.0000 | lr: 4.9724e-05 | scale:     1.0000 | micro time: 3.816 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    100/  2110 | global iter:    100/  2110 | loss: 0.5026 | ds_loss: 0.0000 | lr: 4.9724e-05 | scale:     1.0000 | micro time: 3.816 | step time: 3.801
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    101/  2110 | global iter:    101/  2110 | loss: 0.5146 | ds_loss: 0.0000 | lr: 4.9718e-05 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
train | epoch   0 | Iter:    102/  2110 | global iter:    102/  2110 | loss: 0.4222 | ds_loss: 0.0000 | lr: 4.9713e-05 | scale:     1.0000 | micro time: 3.780 | step time: 0.000
train | epoch   0 | Iter:    103/  2110 | global iter:    103/  2110 | loss: 0.4086 | ds_loss: 0.0000 | lr: 4.9707e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
train | epoch   0 | Iter:    104/  2110 | global iter:    104/  2110 | loss: 0.7172 | ds_loss: 0.0000 | lr: 4.9701e-05 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    104/  2110 | global iter:    104/  2110 | loss: 0.5157 | ds_loss: 0.0000 | lr: 4.9701e-05 | scale:     1.0000 | micro time: 3.779 | step time: 3.792
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    105/  2110 | global iter:    105/  2110 | loss: 0.3177 | ds_loss: 0.0000 | lr: 4.9696e-05 | scale:     1.0000 | micro time: 3.792 | step time: 0.000
train | epoch   0 | Iter:    106/  2110 | global iter:    106/  2110 | loss: 0.6234 | ds_loss: 0.0000 | lr: 4.9690e-05 | scale:     1.0000 | micro time: 3.770 | step time: 0.000
train | epoch   0 | Iter:    107/  2110 | global iter:    107/  2110 | loss: 0.5131 | ds_loss: 0.0000 | lr: 4.9684e-05 | scale:     1.0000 | micro time: 3.811 | step time: 0.000
train | epoch   0 | Iter:    108/  2110 | global iter:    108/  2110 | loss: 0.3631 | ds_loss: 0.0000 | lr: 4.9678e-05 | scale:     1.0000 | micro time: 3.815 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    108/  2110 | global iter:    108/  2110 | loss: 0.4543 | ds_loss: 0.0000 | lr: 4.9678e-05 | scale:     1.0000 | micro time: 3.815 | step time: 3.797
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    109/  2110 | global iter:    109/  2110 | loss: 0.5890 | ds_loss: 0.0000 | lr: 4.9672e-05 | scale:     1.0000 | micro time: 3.777 | step time: 0.000
train | epoch   0 | Iter:    110/  2110 | global iter:    110/  2110 | loss: 0.3718 | ds_loss: 0.0000 | lr: 4.9666e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   0 | Iter:    111/  2110 | global iter:    111/  2110 | loss: 0.4845 | ds_loss: 0.0000 | lr: 4.9660e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   0 | Iter:    112/  2110 | global iter:    112/  2110 | loss: 0.3552 | ds_loss: 0.0000 | lr: 4.9654e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    112/  2110 | global iter:    112/  2110 | loss: 0.4501 | ds_loss: 0.0000 | lr: 4.9654e-05 | scale:     1.0000 | micro time: 3.810 | step time: 3.796
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    113/  2110 | global iter:    113/  2110 | loss: 0.4513 | ds_loss: 0.0000 | lr: 4.9648e-05 | scale:     1.0000 | micro time: 3.807 | step time: 0.000
train | epoch   0 | Iter:    114/  2110 | global iter:    114/  2110 | loss: 0.4502 | ds_loss: 0.0000 | lr: 4.9641e-05 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   0 | Iter:    115/  2110 | global iter:    115/  2110 | loss: 0.5150 | ds_loss: 0.0000 | lr: 4.9635e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   0 | Iter:    116/  2110 | global iter:    116/  2110 | loss: 0.4854 | ds_loss: 0.0000 | lr: 4.9629e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    116/  2110 | global iter:    116/  2110 | loss: 0.4755 | ds_loss: 0.0000 | lr: 4.9629e-05 | scale:     1.0000 | micro time: 3.790 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    117/  2110 | global iter:    117/  2110 | loss: 0.3961 | ds_loss: 0.0000 | lr: 4.9622e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   0 | Iter:    118/  2110 | global iter:    118/  2110 | loss: 0.4390 | ds_loss: 0.0000 | lr: 4.9616e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   0 | Iter:    119/  2110 | global iter:    119/  2110 | loss: 0.3231 | ds_loss: 0.0000 | lr: 4.9609e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   0 | Iter:    120/  2110 | global iter:    120/  2110 | loss: 0.4931 | ds_loss: 0.0000 | lr: 4.9603e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    120/  2110 | global iter:    120/  2110 | loss: 0.4128 | ds_loss: 0.0000 | lr: 4.9603e-05 | scale:     1.0000 | micro time: 3.810 | step time: 3.801
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    121/  2110 | global iter:    121/  2110 | loss: 0.2315 | ds_loss: 0.0000 | lr: 4.9596e-05 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   0 | Iter:    122/  2110 | global iter:    122/  2110 | loss: 0.4804 | ds_loss: 0.0000 | lr: 4.9590e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   0 | Iter:    123/  2110 | global iter:    123/  2110 | loss: 0.2681 | ds_loss: 0.0000 | lr: 4.9583e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   0 | Iter:    124/  2110 | global iter:    124/  2110 | loss: 0.6141 | ds_loss: 0.0000 | lr: 4.9576e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    124/  2110 | global iter:    124/  2110 | loss: 0.3985 | ds_loss: 0.0000 | lr: 4.9576e-05 | scale:     1.0000 | micro time: 3.798 | step time: 3.795
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    125/  2110 | global iter:    125/  2110 | loss: 0.2629 | ds_loss: 0.0000 | lr: 4.9569e-05 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   0 | Iter:    126/  2110 | global iter:    126/  2110 | loss: 0.3921 | ds_loss: 0.0000 | lr: 4.9562e-05 | scale:     1.0000 | micro time: 3.817 | step time: 0.000
train | epoch   0 | Iter:    127/  2110 | global iter:    127/  2110 | loss: 0.3179 | ds_loss: 0.0000 | lr: 4.9555e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   0 | Iter:    128/  2110 | global iter:    128/  2110 | loss: 0.4902 | ds_loss: 0.0000 | lr: 4.9548e-05 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    128/  2110 | global iter:    128/  2110 | loss: 0.3658 | ds_loss: 0.0000 | lr: 4.9548e-05 | scale:     1.0000 | micro time: 3.793 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    129/  2110 | global iter:    129/  2110 | loss: 0.5165 | ds_loss: 0.0000 | lr: 4.9541e-05 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
train | epoch   0 | Iter:    130/  2110 | global iter:    130/  2110 | loss: 0.3360 | ds_loss: 0.0000 | lr: 4.9534e-05 | scale:     1.0000 | micro time: 3.767 | step time: 0.000
train | epoch   0 | Iter:    131/  2110 | global iter:    131/  2110 | loss: 0.4550 | ds_loss: 0.0000 | lr: 4.9527e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   0 | Iter:    132/  2110 | global iter:    132/  2110 | loss: 0.3828 | ds_loss: 0.0000 | lr: 4.9520e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    132/  2110 | global iter:    132/  2110 | loss: 0.4226 | ds_loss: 0.0000 | lr: 4.9520e-05 | scale:     1.0000 | micro time: 3.798 | step time: 3.789
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    133/  2110 | global iter:    133/  2110 | loss: 0.2579 | ds_loss: 0.0000 | lr: 4.9512e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   0 | Iter:    134/  2110 | global iter:    134/  2110 | loss: 0.3595 | ds_loss: 0.0000 | lr: 4.9505e-05 | scale:     1.0000 | micro time: 3.777 | step time: 0.000
train | epoch   0 | Iter:    135/  2110 | global iter:    135/  2110 | loss: 0.4143 | ds_loss: 0.0000 | lr: 4.9498e-05 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   0 | Iter:    136/  2110 | global iter:    136/  2110 | loss: 0.2515 | ds_loss: 0.0000 | lr: 4.9490e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    136/  2110 | global iter:    136/  2110 | loss: 0.3208 | ds_loss: 0.0000 | lr: 4.9490e-05 | scale:     1.0000 | micro time: 3.801 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    137/  2110 | global iter:    137/  2110 | loss: 0.4391 | ds_loss: 0.0000 | lr: 4.9483e-05 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   0 | Iter:    138/  2110 | global iter:    138/  2110 | loss: 0.5047 | ds_loss: 0.0000 | lr: 4.9475e-05 | scale:     1.0000 | micro time: 3.800 | step time: 0.000
train | epoch   0 | Iter:    139/  2110 | global iter:    139/  2110 | loss: 0.3728 | ds_loss: 0.0000 | lr: 4.9468e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   0 | Iter:    140/  2110 | global iter:    140/  2110 | loss: 0.7008 | ds_loss: 0.0000 | lr: 4.9460e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    140/  2110 | global iter:    140/  2110 | loss: 0.5044 | ds_loss: 0.0000 | lr: 4.9460e-05 | scale:     1.0000 | micro time: 3.794 | step time: 3.792
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    141/  2110 | global iter:    141/  2110 | loss: 0.4404 | ds_loss: 0.0000 | lr: 4.9452e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   0 | Iter:    142/  2110 | global iter:    142/  2110 | loss: 0.3987 | ds_loss: 0.0000 | lr: 4.9444e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   0 | Iter:    143/  2110 | global iter:    143/  2110 | loss: 0.4188 | ds_loss: 0.0000 | lr: 4.9437e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   0 | Iter:    144/  2110 | global iter:    144/  2110 | loss: 0.5495 | ds_loss: 0.0000 | lr: 4.9429e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    144/  2110 | global iter:    144/  2110 | loss: 0.4518 | ds_loss: 0.0000 | lr: 4.9429e-05 | scale:     1.0000 | micro time: 3.790 | step time: 3.798
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    145/  2110 | global iter:    145/  2110 | loss: 0.6094 | ds_loss: 0.0000 | lr: 4.9421e-05 | scale:     1.0000 | micro time: 3.811 | step time: 0.000
train | epoch   0 | Iter:    146/  2110 | global iter:    146/  2110 | loss: 0.5735 | ds_loss: 0.0000 | lr: 4.9413e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   0 | Iter:    147/  2110 | global iter:    147/  2110 | loss: 0.4240 | ds_loss: 0.0000 | lr: 4.9405e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   0 | Iter:    148/  2110 | global iter:    148/  2110 | loss: 0.3323 | ds_loss: 0.0000 | lr: 4.9397e-05 | scale:     1.0000 | micro time: 3.812 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    148/  2110 | global iter:    148/  2110 | loss: 0.4848 | ds_loss: 0.0000 | lr: 4.9397e-05 | scale:     1.0000 | micro time: 3.812 | step time: 3.798
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    149/  2110 | global iter:    149/  2110 | loss: 0.3762 | ds_loss: 0.0000 | lr: 4.9389e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   0 | Iter:    150/  2110 | global iter:    150/  2110 | loss: 0.5096 | ds_loss: 0.0000 | lr: 4.9380e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
train | epoch   0 | Iter:    151/  2110 | global iter:    151/  2110 | loss: 0.3033 | ds_loss: 0.0000 | lr: 4.9372e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   0 | Iter:    152/  2110 | global iter:    152/  2110 | loss: 0.5840 | ds_loss: 0.0000 | lr: 4.9364e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    152/  2110 | global iter:    152/  2110 | loss: 0.4433 | ds_loss: 0.0000 | lr: 4.9364e-05 | scale:     1.0000 | micro time: 3.798 | step time: 3.801
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    153/  2110 | global iter:    153/  2110 | loss: 0.5471 | ds_loss: 0.0000 | lr: 4.9355e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   0 | Iter:    154/  2110 | global iter:    154/  2110 | loss: 0.8424 | ds_loss: 0.0000 | lr: 4.9347e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   0 | Iter:    155/  2110 | global iter:    155/  2110 | loss: 0.2058 | ds_loss: 0.0000 | lr: 4.9339e-05 | scale:     1.0000 | micro time: 3.822 | step time: 0.000
train | epoch   0 | Iter:    156/  2110 | global iter:    156/  2110 | loss: 0.2995 | ds_loss: 0.0000 | lr: 4.9330e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    156/  2110 | global iter:    156/  2110 | loss: 0.4737 | ds_loss: 0.0000 | lr: 4.9330e-05 | scale:     1.0000 | micro time: 3.785 | step time: 3.799
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    157/  2110 | global iter:    157/  2110 | loss: 0.2681 | ds_loss: 0.0000 | lr: 4.9321e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   0 | Iter:    158/  2110 | global iter:    158/  2110 | loss: 0.4783 | ds_loss: 0.0000 | lr: 4.9313e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   0 | Iter:    159/  2110 | global iter:    159/  2110 | loss: 0.2563 | ds_loss: 0.0000 | lr: 4.9304e-05 | scale:     1.0000 | micro time: 3.767 | step time: 0.000
train | epoch   0 | Iter:    160/  2110 | global iter:    160/  2110 | loss: 0.3525 | ds_loss: 0.0000 | lr: 4.9295e-05 | scale:     1.0000 | micro time: 3.814 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    160/  2110 | global iter:    160/  2110 | loss: 0.3388 | ds_loss: 0.0000 | lr: 4.9295e-05 | scale:     1.0000 | micro time: 3.814 | step time: 3.793
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    161/  2110 | global iter:    161/  2110 | loss: 0.6122 | ds_loss: 0.0000 | lr: 4.9287e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   0 | Iter:    162/  2110 | global iter:    162/  2110 | loss: 0.8092 | ds_loss: 0.0000 | lr: 4.9278e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   0 | Iter:    163/  2110 | global iter:    163/  2110 | loss: 0.3950 | ds_loss: 0.0000 | lr: 4.9269e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   0 | Iter:    164/  2110 | global iter:    164/  2110 | loss: 0.4498 | ds_loss: 0.0000 | lr: 4.9260e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    164/  2110 | global iter:    164/  2110 | loss: 0.5665 | ds_loss: 0.0000 | lr: 4.9260e-05 | scale:     1.0000 | micro time: 3.802 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    165/  2110 | global iter:    165/  2110 | loss: 0.5085 | ds_loss: 0.0000 | lr: 4.9251e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   0 | Iter:    166/  2110 | global iter:    166/  2110 | loss: 0.7121 | ds_loss: 0.0000 | lr: 4.9242e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   0 | Iter:    167/  2110 | global iter:    167/  2110 | loss: 0.5334 | ds_loss: 0.0000 | lr: 4.9233e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   0 | Iter:    168/  2110 | global iter:    168/  2110 | loss: 0.3815 | ds_loss: 0.0000 | lr: 4.9224e-05 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    168/  2110 | global iter:    168/  2110 | loss: 0.5339 | ds_loss: 0.0000 | lr: 4.9224e-05 | scale:     1.0000 | micro time: 3.803 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    169/  2110 | global iter:    169/  2110 | loss: 0.4530 | ds_loss: 0.0000 | lr: 4.9214e-05 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
train | epoch   0 | Iter:    170/  2110 | global iter:    170/  2110 | loss: 0.5362 | ds_loss: 0.0000 | lr: 4.9205e-05 | scale:     1.0000 | micro time: 3.774 | step time: 0.000
train | epoch   0 | Iter:    171/  2110 | global iter:    171/  2110 | loss: 0.3109 | ds_loss: 0.0000 | lr: 4.9196e-05 | scale:     1.0000 | micro time: 3.814 | step time: 0.000
train | epoch   0 | Iter:    172/  2110 | global iter:    172/  2110 | loss: 0.3528 | ds_loss: 0.0000 | lr: 4.9186e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    172/  2110 | global iter:    172/  2110 | loss: 0.4132 | ds_loss: 0.0000 | lr: 4.9186e-05 | scale:     1.0000 | micro time: 3.786 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    173/  2110 | global iter:    173/  2110 | loss: 0.5171 | ds_loss: 0.0000 | lr: 4.9177e-05 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
train | epoch   0 | Iter:    174/  2110 | global iter:    174/  2110 | loss: 0.4818 | ds_loss: 0.0000 | lr: 4.9167e-05 | scale:     1.0000 | micro time: 3.811 | step time: 0.000
train | epoch   0 | Iter:    175/  2110 | global iter:    175/  2110 | loss: 0.6258 | ds_loss: 0.0000 | lr: 4.9158e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   0 | Iter:    176/  2110 | global iter:    176/  2110 | loss: 0.5505 | ds_loss: 0.0000 | lr: 4.9148e-05 | scale:     1.0000 | micro time: 3.815 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    176/  2110 | global iter:    176/  2110 | loss: 0.5438 | ds_loss: 0.0000 | lr: 4.9148e-05 | scale:     1.0000 | micro time: 3.815 | step time: 3.801
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    177/  2110 | global iter:    177/  2110 | loss: 0.4627 | ds_loss: 0.0000 | lr: 4.9139e-05 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   0 | Iter:    178/  2110 | global iter:    178/  2110 | loss: 0.3780 | ds_loss: 0.0000 | lr: 4.9129e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   0 | Iter:    179/  2110 | global iter:    179/  2110 | loss: 0.5529 | ds_loss: 0.0000 | lr: 4.9119e-05 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   0 | Iter:    180/  2110 | global iter:    180/  2110 | loss: 0.4594 | ds_loss: 0.0000 | lr: 4.9109e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    180/  2110 | global iter:    180/  2110 | loss: 0.4632 | ds_loss: 0.0000 | lr: 4.9109e-05 | scale:     1.0000 | micro time: 3.794 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    181/  2110 | global iter:    181/  2110 | loss: 0.4629 | ds_loss: 0.0000 | lr: 4.9099e-05 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   0 | Iter:    182/  2110 | global iter:    182/  2110 | loss: 0.3360 | ds_loss: 0.0000 | lr: 4.9090e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   0 | Iter:    183/  2110 | global iter:    183/  2110 | loss: 0.4776 | ds_loss: 0.0000 | lr: 4.9080e-05 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   0 | Iter:    184/  2110 | global iter:    184/  2110 | loss: 0.5400 | ds_loss: 0.0000 | lr: 4.9070e-05 | scale:     1.0000 | micro time: 3.800 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    184/  2110 | global iter:    184/  2110 | loss: 0.4541 | ds_loss: 0.0000 | lr: 4.9070e-05 | scale:     1.0000 | micro time: 3.800 | step time: 3.792
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    185/  2110 | global iter:    185/  2110 | loss: 0.5873 | ds_loss: 0.0000 | lr: 4.9059e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   0 | Iter:    186/  2110 | global iter:    186/  2110 | loss: 0.5025 | ds_loss: 0.0000 | lr: 4.9049e-05 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   0 | Iter:    187/  2110 | global iter:    187/  2110 | loss: 0.5348 | ds_loss: 0.0000 | lr: 4.9039e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   0 | Iter:    188/  2110 | global iter:    188/  2110 | loss: 0.2356 | ds_loss: 0.0000 | lr: 4.9029e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    188/  2110 | global iter:    188/  2110 | loss: 0.4651 | ds_loss: 0.0000 | lr: 4.9029e-05 | scale:     1.0000 | micro time: 3.787 | step time: 3.790
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    189/  2110 | global iter:    189/  2110 | loss: 0.5011 | ds_loss: 0.0000 | lr: 4.9019e-05 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   0 | Iter:    190/  2110 | global iter:    190/  2110 | loss: 0.6279 | ds_loss: 0.0000 | lr: 4.9008e-05 | scale:     1.0000 | micro time: 3.792 | step time: 0.000
train | epoch   0 | Iter:    191/  2110 | global iter:    191/  2110 | loss: 0.6294 | ds_loss: 0.0000 | lr: 4.8998e-05 | scale:     1.0000 | micro time: 3.776 | step time: 0.000
train | epoch   0 | Iter:    192/  2110 | global iter:    192/  2110 | loss: 0.4196 | ds_loss: 0.0000 | lr: 4.8987e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    192/  2110 | global iter:    192/  2110 | loss: 0.5445 | ds_loss: 0.0000 | lr: 4.8987e-05 | scale:     1.0000 | micro time: 3.798 | step time: 3.787
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    193/  2110 | global iter:    193/  2110 | loss: 0.4332 | ds_loss: 0.0000 | lr: 4.8977e-05 | scale:     1.0000 | micro time: 3.784 | step time: 0.000
train | epoch   0 | Iter:    194/  2110 | global iter:    194/  2110 | loss: 0.3267 | ds_loss: 0.0000 | lr: 4.8966e-05 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   0 | Iter:    195/  2110 | global iter:    195/  2110 | loss: 0.5758 | ds_loss: 0.0000 | lr: 4.8956e-05 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   0 | Iter:    196/  2110 | global iter:    196/  2110 | loss: 0.3991 | ds_loss: 0.0000 | lr: 4.8945e-05 | scale:     1.0000 | micro time: 3.800 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    196/  2110 | global iter:    196/  2110 | loss: 0.4337 | ds_loss: 0.0000 | lr: 4.8945e-05 | scale:     1.0000 | micro time: 3.800 | step time: 3.786
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    197/  2110 | global iter:    197/  2110 | loss: 0.3916 | ds_loss: 0.0000 | lr: 4.8934e-05 | scale:     1.0000 | micro time: 3.766 | step time: 0.000
train | epoch   0 | Iter:    198/  2110 | global iter:    198/  2110 | loss: 0.4605 | ds_loss: 0.0000 | lr: 4.8924e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   0 | Iter:    199/  2110 | global iter:    199/  2110 | loss: 0.5290 | ds_loss: 0.0000 | lr: 4.8913e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   0 | Iter:    200/  2110 | global iter:    200/  2110 | loss: 0.5414 | ds_loss: 0.0000 | lr: 4.8902e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    200/  2110 | global iter:    200/  2110 | loss: 0.4806 | ds_loss: 0.0000 | lr: 4.8902e-05 | scale:     1.0000 | micro time: 3.795 | step time: 3.786
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    201/  2110 | global iter:    201/  2110 | loss: 0.4172 | ds_loss: 0.0000 | lr: 4.8891e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   0 | Iter:    202/  2110 | global iter:    202/  2110 | loss: 0.2355 | ds_loss: 0.0000 | lr: 4.8880e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   0 | Iter:    203/  2110 | global iter:    203/  2110 | loss: 0.2792 | ds_loss: 0.0000 | lr: 4.8869e-05 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   0 | Iter:    204/  2110 | global iter:    204/  2110 | loss: 0.6694 | ds_loss: 0.0000 | lr: 4.8858e-05 | scale:     1.0000 | micro time: 3.804 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    204/  2110 | global iter:    204/  2110 | loss: 0.4003 | ds_loss: 0.0000 | lr: 4.8858e-05 | scale:     1.0000 | micro time: 3.804 | step time: 3.792
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    205/  2110 | global iter:    205/  2110 | loss: 0.3647 | ds_loss: 0.0000 | lr: 4.8847e-05 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   0 | Iter:    206/  2110 | global iter:    206/  2110 | loss: 0.2784 | ds_loss: 0.0000 | lr: 4.8836e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   0 | Iter:    207/  2110 | global iter:    207/  2110 | loss: 0.2731 | ds_loss: 0.0000 | lr: 4.8824e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   0 | Iter:    208/  2110 | global iter:    208/  2110 | loss: 0.4950 | ds_loss: 0.0000 | lr: 4.8813e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    208/  2110 | global iter:    208/  2110 | loss: 0.3528 | ds_loss: 0.0000 | lr: 4.8813e-05 | scale:     1.0000 | micro time: 3.794 | step time: 3.785
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    209/  2110 | global iter:    209/  2110 | loss: 0.6766 | ds_loss: 0.0000 | lr: 4.8802e-05 | scale:     1.0000 | micro time: 3.796 | step time: 0.000
train | epoch   0 | Iter:    210/  2110 | global iter:    210/  2110 | loss: 0.3003 | ds_loss: 0.0000 | lr: 4.8790e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   0 | Iter:    211/  2110 | global iter:    211/  2110 | loss: 0.5913 | ds_loss: 0.0000 | lr: 4.8779e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   0 | Iter:    212/  2110 | global iter:    212/  2110 | loss: 0.2042 | ds_loss: 0.0000 | lr: 4.8767e-05 | scale:     1.0000 | micro time: 3.361 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    212/  2110 | global iter:    212/  2110 | loss: 0.4431 | ds_loss: 0.0000 | lr: 4.8767e-05 | scale:     1.0000 | micro time: 3.361 | step time: 3.689
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
Tue Apr  8 07:18:54 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   63C    P0             55W /  250W |   29814MiB /  32768MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   49C    P0             48W /  250W |   29848MiB /  32768MiB |     81%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   54C    P0             49W /  250W |   28312MiB /  32768MiB |     82%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 07:18:54 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   63C    P0             55W /  250W |   29814MiB /  32768MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   49C    P0             48W /  250W |   29848MiB /  32768MiB |     81%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   54C    P0             49W /  250W |   28312MiB /  32768MiB |     82%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 07:18:54 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   63C    P0             55W /  250W |   29814MiB /  32768MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   49C    P0             48W /  250W |   29848MiB /  32768MiB |     81%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   54C    P0             49W /  250W |   28312MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 07:18:54 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   63C    P0             55W /  250W |   29814MiB /  32768MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |     14%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   49C    P0             48W /  250W |   29848MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   54C    P0             49W /  250W |   28312MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

train | epoch   1 | Iter:    213/  2110 | global iter:    213/  2110 | loss: 0.3362 | ds_loss: 0.0000 | lr: 4.8756e-05 | scale:     1.0000 | micro time: 4.262 | step time: 0.000
train | epoch   1 | Iter:    214/  2110 | global iter:    214/  2110 | loss: 0.5306 | ds_loss: 0.0000 | lr: 4.8744e-05 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
train | epoch   1 | Iter:    215/  2110 | global iter:    215/  2110 | loss: 0.5102 | ds_loss: 0.0000 | lr: 4.8733e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   1 | Iter:    216/  2110 | global iter:    216/  2110 | loss: 0.1902 | ds_loss: 0.0000 | lr: 4.8721e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    216/  2110 | global iter:    216/  2110 | loss: 0.3918 | ds_loss: 0.0000 | lr: 4.8721e-05 | scale:     1.0000 | micro time: 3.806 | step time: 3.914
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    217/  2110 | global iter:    217/  2110 | loss: 0.2709 | ds_loss: 0.0000 | lr: 4.8709e-05 | scale:     1.0000 | micro time: 3.817 | step time: 0.000
train | epoch   1 | Iter:    218/  2110 | global iter:    218/  2110 | loss: 0.4525 | ds_loss: 0.0000 | lr: 4.8697e-05 | scale:     1.0000 | micro time: 3.807 | step time: 0.000
train | epoch   1 | Iter:    219/  2110 | global iter:    219/  2110 | loss: 0.3077 | ds_loss: 0.0000 | lr: 4.8685e-05 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   1 | Iter:    220/  2110 | global iter:    220/  2110 | loss: 0.3342 | ds_loss: 0.0000 | lr: 4.8673e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    220/  2110 | global iter:    220/  2110 | loss: 0.3413 | ds_loss: 0.0000 | lr: 4.8673e-05 | scale:     1.0000 | micro time: 3.782 | step time: 3.801
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    221/  2110 | global iter:    221/  2110 | loss: 0.5939 | ds_loss: 0.0000 | lr: 4.8661e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   1 | Iter:    222/  2110 | global iter:    222/  2110 | loss: 0.3979 | ds_loss: 0.0000 | lr: 4.8649e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   1 | Iter:    223/  2110 | global iter:    223/  2110 | loss: 0.3182 | ds_loss: 0.0000 | lr: 4.8637e-05 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   1 | Iter:    224/  2110 | global iter:    224/  2110 | loss: 0.2134 | ds_loss: 0.0000 | lr: 4.8625e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    224/  2110 | global iter:    224/  2110 | loss: 0.3808 | ds_loss: 0.0000 | lr: 4.8625e-05 | scale:     1.0000 | micro time: 3.786 | step time: 3.797
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    225/  2110 | global iter:    225/  2110 | loss: 0.2579 | ds_loss: 0.0000 | lr: 4.8613e-05 | scale:     1.0000 | micro time: 3.831 | step time: 0.000
train | epoch   1 | Iter:    226/  2110 | global iter:    226/  2110 | loss: 0.2691 | ds_loss: 0.0000 | lr: 4.8601e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   1 | Iter:    227/  2110 | global iter:    227/  2110 | loss: 0.2202 | ds_loss: 0.0000 | lr: 4.8588e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   1 | Iter:    228/  2110 | global iter:    228/  2110 | loss: 0.3672 | ds_loss: 0.0000 | lr: 4.8576e-05 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    228/  2110 | global iter:    228/  2110 | loss: 0.2786 | ds_loss: 0.0000 | lr: 4.8576e-05 | scale:     1.0000 | micro time: 3.775 | step time: 3.803
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    229/  2110 | global iter:    229/  2110 | loss: 0.1654 | ds_loss: 0.0000 | lr: 4.8564e-05 | scale:     1.0000 | micro time: 3.797 | step time: 0.000
train | epoch   1 | Iter:    230/  2110 | global iter:    230/  2110 | loss: 0.2835 | ds_loss: 0.0000 | lr: 4.8551e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   1 | Iter:    231/  2110 | global iter:    231/  2110 | loss: 0.2517 | ds_loss: 0.0000 | lr: 4.8539e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   1 | Iter:    232/  2110 | global iter:    232/  2110 | loss: 0.2046 | ds_loss: 0.0000 | lr: 4.8526e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    232/  2110 | global iter:    232/  2110 | loss: 0.2263 | ds_loss: 0.0000 | lr: 4.8526e-05 | scale:     1.0000 | micro time: 3.787 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    233/  2110 | global iter:    233/  2110 | loss: 0.2474 | ds_loss: 0.0000 | lr: 4.8514e-05 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   1 | Iter:    234/  2110 | global iter:    234/  2110 | loss: 0.1428 | ds_loss: 0.0000 | lr: 4.8501e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   1 | Iter:    235/  2110 | global iter:    235/  2110 | loss: 0.3771 | ds_loss: 0.0000 | lr: 4.8488e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
train | epoch   1 | Iter:    236/  2110 | global iter:    236/  2110 | loss: 0.2079 | ds_loss: 0.0000 | lr: 4.8476e-05 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    236/  2110 | global iter:    236/  2110 | loss: 0.2438 | ds_loss: 0.0000 | lr: 4.8476e-05 | scale:     1.0000 | micro time: 3.775 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    237/  2110 | global iter:    237/  2110 | loss: 0.3833 | ds_loss: 0.0000 | lr: 4.8463e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   1 | Iter:    238/  2110 | global iter:    238/  2110 | loss: 0.2245 | ds_loss: 0.0000 | lr: 4.8450e-05 | scale:     1.0000 | micro time: 3.774 | step time: 0.000
train | epoch   1 | Iter:    239/  2110 | global iter:    239/  2110 | loss: 0.2396 | ds_loss: 0.0000 | lr: 4.8437e-05 | scale:     1.0000 | micro time: 3.815 | step time: 0.000
train | epoch   1 | Iter:    240/  2110 | global iter:    240/  2110 | loss: 0.3182 | ds_loss: 0.0000 | lr: 4.8424e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    240/  2110 | global iter:    240/  2110 | loss: 0.2914 | ds_loss: 0.0000 | lr: 4.8424e-05 | scale:     1.0000 | micro time: 3.802 | step time: 3.795
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    241/  2110 | global iter:    241/  2110 | loss: 0.3196 | ds_loss: 0.0000 | lr: 4.8411e-05 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   1 | Iter:    242/  2110 | global iter:    242/  2110 | loss: 0.4572 | ds_loss: 0.0000 | lr: 4.8398e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   1 | Iter:    243/  2110 | global iter:    243/  2110 | loss: 0.4864 | ds_loss: 0.0000 | lr: 4.8385e-05 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   1 | Iter:    244/  2110 | global iter:    244/  2110 | loss: 0.3574 | ds_loss: 0.0000 | lr: 4.8372e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    244/  2110 | global iter:    244/  2110 | loss: 0.4052 | ds_loss: 0.0000 | lr: 4.8372e-05 | scale:     1.0000 | micro time: 3.795 | step time: 3.795
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    245/  2110 | global iter:    245/  2110 | loss: 0.5054 | ds_loss: 0.0000 | lr: 4.8358e-05 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   1 | Iter:    246/  2110 | global iter:    246/  2110 | loss: 0.3296 | ds_loss: 0.0000 | lr: 4.8345e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   1 | Iter:    247/  2110 | global iter:    247/  2110 | loss: 0.3348 | ds_loss: 0.0000 | lr: 4.8332e-05 | scale:     1.0000 | micro time: 3.772 | step time: 0.000
train | epoch   1 | Iter:    248/  2110 | global iter:    248/  2110 | loss: 0.3279 | ds_loss: 0.0000 | lr: 4.8318e-05 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    248/  2110 | global iter:    248/  2110 | loss: 0.3744 | ds_loss: 0.0000 | lr: 4.8318e-05 | scale:     1.0000 | micro time: 3.793 | step time: 3.785
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    249/  2110 | global iter:    249/  2110 | loss: 0.3045 | ds_loss: 0.0000 | lr: 4.8305e-05 | scale:     1.0000 | micro time: 3.784 | step time: 0.000
train | epoch   1 | Iter:    250/  2110 | global iter:    250/  2110 | loss: 0.2446 | ds_loss: 0.0000 | lr: 4.8291e-05 | scale:     1.0000 | micro time: 3.759 | step time: 0.000
train | epoch   1 | Iter:    251/  2110 | global iter:    251/  2110 | loss: 0.2007 | ds_loss: 0.0000 | lr: 4.8278e-05 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
train | epoch   1 | Iter:    252/  2110 | global iter:    252/  2110 | loss: 0.2415 | ds_loss: 0.0000 | lr: 4.8264e-05 | scale:     1.0000 | micro time: 3.792 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    252/  2110 | global iter:    252/  2110 | loss: 0.2478 | ds_loss: 0.0000 | lr: 4.8264e-05 | scale:     1.0000 | micro time: 3.792 | step time: 3.781
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    253/  2110 | global iter:    253/  2110 | loss: 0.1364 | ds_loss: 0.0000 | lr: 4.8251e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   1 | Iter:    254/  2110 | global iter:    254/  2110 | loss: 0.3144 | ds_loss: 0.0000 | lr: 4.8237e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   1 | Iter:    255/  2110 | global iter:    255/  2110 | loss: 0.4123 | ds_loss: 0.0000 | lr: 4.8223e-05 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   1 | Iter:    256/  2110 | global iter:    256/  2110 | loss: 0.4306 | ds_loss: 0.0000 | lr: 4.8209e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    256/  2110 | global iter:    256/  2110 | loss: 0.3234 | ds_loss: 0.0000 | lr: 4.8209e-05 | scale:     1.0000 | micro time: 3.818 | step time: 3.795
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    257/  2110 | global iter:    257/  2110 | loss: 0.1979 | ds_loss: 0.0000 | lr: 4.8196e-05 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
train | epoch   1 | Iter:    258/  2110 | global iter:    258/  2110 | loss: 0.3908 | ds_loss: 0.0000 | lr: 4.8182e-05 | scale:     1.0000 | micro time: 3.814 | step time: 0.000
train | epoch   1 | Iter:    259/  2110 | global iter:    259/  2110 | loss: 0.2600 | ds_loss: 0.0000 | lr: 4.8168e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   1 | Iter:    260/  2110 | global iter:    260/  2110 | loss: 0.2927 | ds_loss: 0.0000 | lr: 4.8154e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    260/  2110 | global iter:    260/  2110 | loss: 0.2854 | ds_loss: 0.0000 | lr: 4.8154e-05 | scale:     1.0000 | micro time: 3.790 | step time: 3.795
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    261/  2110 | global iter:    261/  2110 | loss: 0.2166 | ds_loss: 0.0000 | lr: 4.8140e-05 | scale:     1.0000 | micro time: 3.765 | step time: 0.000
train | epoch   1 | Iter:    262/  2110 | global iter:    262/  2110 | loss: 0.3527 | ds_loss: 0.0000 | lr: 4.8126e-05 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   1 | Iter:    263/  2110 | global iter:    263/  2110 | loss: 0.2663 | ds_loss: 0.0000 | lr: 4.8111e-05 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   1 | Iter:    264/  2110 | global iter:    264/  2110 | loss: 0.1833 | ds_loss: 0.0000 | lr: 4.8097e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    264/  2110 | global iter:    264/  2110 | loss: 0.2547 | ds_loss: 0.0000 | lr: 4.8097e-05 | scale:     1.0000 | micro time: 3.802 | step time: 3.785
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    265/  2110 | global iter:    265/  2110 | loss: 0.3675 | ds_loss: 0.0000 | lr: 4.8083e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   1 | Iter:    266/  2110 | global iter:    266/  2110 | loss: 0.4306 | ds_loss: 0.0000 | lr: 4.8069e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   1 | Iter:    267/  2110 | global iter:    267/  2110 | loss: 0.2764 | ds_loss: 0.0000 | lr: 4.8054e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   1 | Iter:    268/  2110 | global iter:    268/  2110 | loss: 0.4075 | ds_loss: 0.0000 | lr: 4.8040e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    268/  2110 | global iter:    268/  2110 | loss: 0.3705 | ds_loss: 0.0000 | lr: 4.8040e-05 | scale:     1.0000 | micro time: 3.794 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    269/  2110 | global iter:    269/  2110 | loss: 0.3927 | ds_loss: 0.0000 | lr: 4.8025e-05 | scale:     1.0000 | micro time: 3.808 | step time: 0.000
train | epoch   1 | Iter:    270/  2110 | global iter:    270/  2110 | loss: 0.2442 | ds_loss: 0.0000 | lr: 4.8011e-05 | scale:     1.0000 | micro time: 3.815 | step time: 0.000
train | epoch   1 | Iter:    271/  2110 | global iter:    271/  2110 | loss: 0.3225 | ds_loss: 0.0000 | lr: 4.7996e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   1 | Iter:    272/  2110 | global iter:    272/  2110 | loss: 0.1592 | ds_loss: 0.0000 | lr: 4.7982e-05 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    272/  2110 | global iter:    272/  2110 | loss: 0.2797 | ds_loss: 0.0000 | lr: 4.7982e-05 | scale:     1.0000 | micro time: 3.799 | step time: 3.808
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    273/  2110 | global iter:    273/  2110 | loss: 0.2274 | ds_loss: 0.0000 | lr: 4.7967e-05 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   1 | Iter:    274/  2110 | global iter:    274/  2110 | loss: 0.2575 | ds_loss: 0.0000 | lr: 4.7952e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   1 | Iter:    275/  2110 | global iter:    275/  2110 | loss: 0.2933 | ds_loss: 0.0000 | lr: 4.7938e-05 | scale:     1.0000 | micro time: 3.800 | step time: 0.000
train | epoch   1 | Iter:    276/  2110 | global iter:    276/  2110 | loss: 0.3028 | ds_loss: 0.0000 | lr: 4.7923e-05 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    276/  2110 | global iter:    276/  2110 | loss: 0.2702 | ds_loss: 0.0000 | lr: 4.7923e-05 | scale:     1.0000 | micro time: 3.793 | step time: 3.799
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    277/  2110 | global iter:    277/  2110 | loss: 0.3876 | ds_loss: 0.0000 | lr: 4.7908e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   1 | Iter:    278/  2110 | global iter:    278/  2110 | loss: 0.3690 | ds_loss: 0.0000 | lr: 4.7893e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   1 | Iter:    279/  2110 | global iter:    279/  2110 | loss: 0.1408 | ds_loss: 0.0000 | lr: 4.7878e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   1 | Iter:    280/  2110 | global iter:    280/  2110 | loss: 0.3035 | ds_loss: 0.0000 | lr: 4.7863e-05 | scale:     1.0000 | micro time: 3.776 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    280/  2110 | global iter:    280/  2110 | loss: 0.3002 | ds_loss: 0.0000 | lr: 4.7863e-05 | scale:     1.0000 | micro time: 3.776 | step time: 3.786
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    281/  2110 | global iter:    281/  2110 | loss: 0.2038 | ds_loss: 0.0000 | lr: 4.7848e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   1 | Iter:    282/  2110 | global iter:    282/  2110 | loss: 0.2463 | ds_loss: 0.0000 | lr: 4.7833e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   1 | Iter:    283/  2110 | global iter:    283/  2110 | loss: 0.3147 | ds_loss: 0.0000 | lr: 4.7818e-05 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   1 | Iter:    284/  2110 | global iter:    284/  2110 | loss: 0.3843 | ds_loss: 0.0000 | lr: 4.7802e-05 | scale:     1.0000 | micro time: 3.838 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    284/  2110 | global iter:    284/  2110 | loss: 0.2873 | ds_loss: 0.0000 | lr: 4.7802e-05 | scale:     1.0000 | micro time: 3.838 | step time: 3.804
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    285/  2110 | global iter:    285/  2110 | loss: 0.3190 | ds_loss: 0.0000 | lr: 4.7787e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   1 | Iter:    286/  2110 | global iter:    286/  2110 | loss: 0.2058 | ds_loss: 0.0000 | lr: 4.7772e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
train | epoch   1 | Iter:    287/  2110 | global iter:    287/  2110 | loss: 0.2627 | ds_loss: 0.0000 | lr: 4.7757e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   1 | Iter:    288/  2110 | global iter:    288/  2110 | loss: 0.4029 | ds_loss: 0.0000 | lr: 4.7741e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    288/  2110 | global iter:    288/  2110 | loss: 0.2976 | ds_loss: 0.0000 | lr: 4.7741e-05 | scale:     1.0000 | micro time: 3.802 | step time: 3.798
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    289/  2110 | global iter:    289/  2110 | loss: 0.4540 | ds_loss: 0.0000 | lr: 4.7726e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   1 | Iter:    290/  2110 | global iter:    290/  2110 | loss: 0.1978 | ds_loss: 0.0000 | lr: 4.7710e-05 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
train | epoch   1 | Iter:    291/  2110 | global iter:    291/  2110 | loss: 0.3690 | ds_loss: 0.0000 | lr: 4.7695e-05 | scale:     1.0000 | micro time: 3.830 | step time: 0.000
train | epoch   1 | Iter:    292/  2110 | global iter:    292/  2110 | loss: 0.3507 | ds_loss: 0.0000 | lr: 4.7679e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    292/  2110 | global iter:    292/  2110 | loss: 0.3429 | ds_loss: 0.0000 | lr: 4.7679e-05 | scale:     1.0000 | micro time: 3.790 | step time: 3.798
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    293/  2110 | global iter:    293/  2110 | loss: 0.3936 | ds_loss: 0.0000 | lr: 4.7663e-05 | scale:     1.0000 | micro time: 3.769 | step time: 0.000
train | epoch   1 | Iter:    294/  2110 | global iter:    294/  2110 | loss: 0.3151 | ds_loss: 0.0000 | lr: 4.7648e-05 | scale:     1.0000 | micro time: 3.822 | step time: 0.000
train | epoch   1 | Iter:    295/  2110 | global iter:    295/  2110 | loss: 0.2103 | ds_loss: 0.0000 | lr: 4.7632e-05 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
train | epoch   1 | Iter:    296/  2110 | global iter:    296/  2110 | loss: 0.3551 | ds_loss: 0.0000 | lr: 4.7616e-05 | scale:     1.0000 | micro time: 3.800 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    296/  2110 | global iter:    296/  2110 | loss: 0.3185 | ds_loss: 0.0000 | lr: 4.7616e-05 | scale:     1.0000 | micro time: 3.800 | step time: 3.795
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    297/  2110 | global iter:    297/  2110 | loss: 0.3810 | ds_loss: 0.0000 | lr: 4.7600e-05 | scale:     1.0000 | micro time: 3.812 | step time: 0.000
train | epoch   1 | Iter:    298/  2110 | global iter:    298/  2110 | loss: 0.2770 | ds_loss: 0.0000 | lr: 4.7584e-05 | scale:     1.0000 | micro time: 3.770 | step time: 0.000
train | epoch   1 | Iter:    299/  2110 | global iter:    299/  2110 | loss: 0.3577 | ds_loss: 0.0000 | lr: 4.7568e-05 | scale:     1.0000 | micro time: 3.822 | step time: 0.000
train | epoch   1 | Iter:    300/  2110 | global iter:    300/  2110 | loss: 0.1636 | ds_loss: 0.0000 | lr: 4.7552e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    300/  2110 | global iter:    300/  2110 | loss: 0.2948 | ds_loss: 0.0000 | lr: 4.7552e-05 | scale:     1.0000 | micro time: 3.790 | step time: 3.799
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    301/  2110 | global iter:    301/  2110 | loss: 0.3693 | ds_loss: 0.0000 | lr: 4.7536e-05 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
train | epoch   1 | Iter:    302/  2110 | global iter:    302/  2110 | loss: 0.2889 | ds_loss: 0.0000 | lr: 4.7520e-05 | scale:     1.0000 | micro time: 3.805 | step time: 0.000
train | epoch   1 | Iter:    303/  2110 | global iter:    303/  2110 | loss: 0.2308 | ds_loss: 0.0000 | lr: 4.7504e-05 | scale:     1.0000 | micro time: 3.796 | step time: 0.000
train | epoch   1 | Iter:    304/  2110 | global iter:    304/  2110 | loss: 0.4884 | ds_loss: 0.0000 | lr: 4.7488e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    304/  2110 | global iter:    304/  2110 | loss: 0.3443 | ds_loss: 0.0000 | lr: 4.7488e-05 | scale:     1.0000 | micro time: 3.802 | step time: 3.798
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    305/  2110 | global iter:    305/  2110 | loss: 0.4653 | ds_loss: 0.0000 | lr: 4.7471e-05 | scale:     1.0000 | micro time: 3.772 | step time: 0.000
train | epoch   1 | Iter:    306/  2110 | global iter:    306/  2110 | loss: 0.4143 | ds_loss: 0.0000 | lr: 4.7455e-05 | scale:     1.0000 | micro time: 3.797 | step time: 0.000
train | epoch   1 | Iter:    307/  2110 | global iter:    307/  2110 | loss: 0.3598 | ds_loss: 0.0000 | lr: 4.7439e-05 | scale:     1.0000 | micro time: 3.796 | step time: 0.000
train | epoch   1 | Iter:    308/  2110 | global iter:    308/  2110 | loss: 0.4002 | ds_loss: 0.0000 | lr: 4.7422e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    308/  2110 | global iter:    308/  2110 | loss: 0.4099 | ds_loss: 0.0000 | lr: 4.7422e-05 | scale:     1.0000 | micro time: 3.806 | step time: 3.793
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    309/  2110 | global iter:    309/  2110 | loss: 0.2262 | ds_loss: 0.0000 | lr: 4.7406e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   1 | Iter:    310/  2110 | global iter:    310/  2110 | loss: 0.2840 | ds_loss: 0.0000 | lr: 4.7389e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   1 | Iter:    311/  2110 | global iter:    311/  2110 | loss: 0.1522 | ds_loss: 0.0000 | lr: 4.7373e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   1 | Iter:    312/  2110 | global iter:    312/  2110 | loss: 0.1682 | ds_loss: 0.0000 | lr: 4.7356e-05 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    312/  2110 | global iter:    312/  2110 | loss: 0.2077 | ds_loss: 0.0000 | lr: 4.7356e-05 | scale:     1.0000 | micro time: 3.781 | step time: 3.787
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    313/  2110 | global iter:    313/  2110 | loss: 0.2393 | ds_loss: 0.0000 | lr: 4.7339e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   1 | Iter:    314/  2110 | global iter:    314/  2110 | loss: 0.2307 | ds_loss: 0.0000 | lr: 4.7323e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   1 | Iter:    315/  2110 | global iter:    315/  2110 | loss: 0.5807 | ds_loss: 0.0000 | lr: 4.7306e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   1 | Iter:    316/  2110 | global iter:    316/  2110 | loss: 0.3958 | ds_loss: 0.0000 | lr: 4.7289e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    316/  2110 | global iter:    316/  2110 | loss: 0.3616 | ds_loss: 0.0000 | lr: 4.7289e-05 | scale:     1.0000 | micro time: 3.786 | step time: 3.789
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    317/  2110 | global iter:    317/  2110 | loss: 0.3519 | ds_loss: 0.0000 | lr: 4.7272e-05 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   1 | Iter:    318/  2110 | global iter:    318/  2110 | loss: 0.1644 | ds_loss: 0.0000 | lr: 4.7255e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   1 | Iter:    319/  2110 | global iter:    319/  2110 | loss: 0.1467 | ds_loss: 0.0000 | lr: 4.7238e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   1 | Iter:    320/  2110 | global iter:    320/  2110 | loss: 0.2850 | ds_loss: 0.0000 | lr: 4.7221e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    320/  2110 | global iter:    320/  2110 | loss: 0.2370 | ds_loss: 0.0000 | lr: 4.7221e-05 | scale:     1.0000 | micro time: 3.790 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    321/  2110 | global iter:    321/  2110 | loss: 0.2850 | ds_loss: 0.0000 | lr: 4.7204e-05 | scale:     1.0000 | micro time: 3.772 | step time: 0.000
train | epoch   1 | Iter:    322/  2110 | global iter:    322/  2110 | loss: 0.3291 | ds_loss: 0.0000 | lr: 4.7187e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   1 | Iter:    323/  2110 | global iter:    323/  2110 | loss: 0.3969 | ds_loss: 0.0000 | lr: 4.7170e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   1 | Iter:    324/  2110 | global iter:    324/  2110 | loss: 0.3378 | ds_loss: 0.0000 | lr: 4.7153e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    324/  2110 | global iter:    324/  2110 | loss: 0.3372 | ds_loss: 0.0000 | lr: 4.7153e-05 | scale:     1.0000 | micro time: 3.771 | step time: 3.787
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    325/  2110 | global iter:    325/  2110 | loss: 0.4471 | ds_loss: 0.0000 | lr: 4.7135e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   1 | Iter:    326/  2110 | global iter:    326/  2110 | loss: 0.4680 | ds_loss: 0.0000 | lr: 4.7118e-05 | scale:     1.0000 | micro time: 3.804 | step time: 0.000
train | epoch   1 | Iter:    327/  2110 | global iter:    327/  2110 | loss: 0.2103 | ds_loss: 0.0000 | lr: 4.7101e-05 | scale:     1.0000 | micro time: 3.784 | step time: 0.000
train | epoch   1 | Iter:    328/  2110 | global iter:    328/  2110 | loss: 0.1932 | ds_loss: 0.0000 | lr: 4.7083e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    328/  2110 | global iter:    328/  2110 | loss: 0.3297 | ds_loss: 0.0000 | lr: 4.7083e-05 | scale:     1.0000 | micro time: 3.786 | step time: 3.790
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    329/  2110 | global iter:    329/  2110 | loss: 0.1547 | ds_loss: 0.0000 | lr: 4.7066e-05 | scale:     1.0000 | micro time: 3.804 | step time: 0.000
train | epoch   1 | Iter:    330/  2110 | global iter:    330/  2110 | loss: 0.1543 | ds_loss: 0.0000 | lr: 4.7048e-05 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   1 | Iter:    331/  2110 | global iter:    331/  2110 | loss: 0.2998 | ds_loss: 0.0000 | lr: 4.7031e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   1 | Iter:    332/  2110 | global iter:    332/  2110 | loss: 0.3535 | ds_loss: 0.0000 | lr: 4.7013e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    332/  2110 | global iter:    332/  2110 | loss: 0.2406 | ds_loss: 0.0000 | lr: 4.7013e-05 | scale:     1.0000 | micro time: 3.818 | step time: 3.796
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    333/  2110 | global iter:    333/  2110 | loss: 0.3434 | ds_loss: 0.0000 | lr: 4.6996e-05 | scale:     1.0000 | micro time: 3.816 | step time: 0.000
train | epoch   1 | Iter:    334/  2110 | global iter:    334/  2110 | loss: 0.3152 | ds_loss: 0.0000 | lr: 4.6978e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   1 | Iter:    335/  2110 | global iter:    335/  2110 | loss: 0.2953 | ds_loss: 0.0000 | lr: 4.6960e-05 | scale:     1.0000 | micro time: 3.811 | step time: 0.000
train | epoch   1 | Iter:    336/  2110 | global iter:    336/  2110 | loss: 0.3172 | ds_loss: 0.0000 | lr: 4.6942e-05 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    336/  2110 | global iter:    336/  2110 | loss: 0.3178 | ds_loss: 0.0000 | lr: 4.6942e-05 | scale:     1.0000 | micro time: 3.781 | step time: 3.799
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    337/  2110 | global iter:    337/  2110 | loss: 0.2002 | ds_loss: 0.0000 | lr: 4.6925e-05 | scale:     1.0000 | micro time: 3.773 | step time: 0.000
train | epoch   1 | Iter:    338/  2110 | global iter:    338/  2110 | loss: 0.2025 | ds_loss: 0.0000 | lr: 4.6907e-05 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
train | epoch   1 | Iter:    339/  2110 | global iter:    339/  2110 | loss: 0.4071 | ds_loss: 0.0000 | lr: 4.6889e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   1 | Iter:    340/  2110 | global iter:    340/  2110 | loss: 0.2471 | ds_loss: 0.0000 | lr: 4.6871e-05 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    340/  2110 | global iter:    340/  2110 | loss: 0.2642 | ds_loss: 0.0000 | lr: 4.6871e-05 | scale:     1.0000 | micro time: 3.789 | step time: 3.788
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    341/  2110 | global iter:    341/  2110 | loss: 0.1791 | ds_loss: 0.0000 | lr: 4.6853e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   1 | Iter:    342/  2110 | global iter:    342/  2110 | loss: 0.2565 | ds_loss: 0.0000 | lr: 4.6835e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   1 | Iter:    343/  2110 | global iter:    343/  2110 | loss: 0.3317 | ds_loss: 0.0000 | lr: 4.6817e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   1 | Iter:    344/  2110 | global iter:    344/  2110 | loss: 0.3185 | ds_loss: 0.0000 | lr: 4.6798e-05 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    344/  2110 | global iter:    344/  2110 | loss: 0.2715 | ds_loss: 0.0000 | lr: 4.6798e-05 | scale:     1.0000 | micro time: 3.789 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    345/  2110 | global iter:    345/  2110 | loss: 0.3154 | ds_loss: 0.0000 | lr: 4.6780e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   1 | Iter:    346/  2110 | global iter:    346/  2110 | loss: 0.4773 | ds_loss: 0.0000 | lr: 4.6762e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   1 | Iter:    347/  2110 | global iter:    347/  2110 | loss: 0.2803 | ds_loss: 0.0000 | lr: 4.6743e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   1 | Iter:    348/  2110 | global iter:    348/  2110 | loss: 0.4315 | ds_loss: 0.0000 | lr: 4.6725e-05 | scale:     1.0000 | micro time: 3.797 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    348/  2110 | global iter:    348/  2110 | loss: 0.3761 | ds_loss: 0.0000 | lr: 4.6725e-05 | scale:     1.0000 | micro time: 3.797 | step time: 3.792
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    349/  2110 | global iter:    349/  2110 | loss: 0.2837 | ds_loss: 0.0000 | lr: 4.6707e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   1 | Iter:    350/  2110 | global iter:    350/  2110 | loss: 0.3857 | ds_loss: 0.0000 | lr: 4.6688e-05 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   1 | Iter:    351/  2110 | global iter:    351/  2110 | loss: 0.2740 | ds_loss: 0.0000 | lr: 4.6670e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   1 | Iter:    352/  2110 | global iter:    352/  2110 | loss: 0.3434 | ds_loss: 0.0000 | lr: 4.6651e-05 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    352/  2110 | global iter:    352/  2110 | loss: 0.3217 | ds_loss: 0.0000 | lr: 4.6651e-05 | scale:     1.0000 | micro time: 3.799 | step time: 3.793
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    353/  2110 | global iter:    353/  2110 | loss: 0.1963 | ds_loss: 0.0000 | lr: 4.6633e-05 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   1 | Iter:    354/  2110 | global iter:    354/  2110 | loss: 0.3521 | ds_loss: 0.0000 | lr: 4.6614e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   1 | Iter:    355/  2110 | global iter:    355/  2110 | loss: 0.4862 | ds_loss: 0.0000 | lr: 4.6595e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   1 | Iter:    356/  2110 | global iter:    356/  2110 | loss: 0.3801 | ds_loss: 0.0000 | lr: 4.6576e-05 | scale:     1.0000 | micro time: 3.823 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    356/  2110 | global iter:    356/  2110 | loss: 0.3537 | ds_loss: 0.0000 | lr: 4.6576e-05 | scale:     1.0000 | micro time: 3.823 | step time: 3.806
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    357/  2110 | global iter:    357/  2110 | loss: 0.4030 | ds_loss: 0.0000 | lr: 4.6558e-05 | scale:     1.0000 | micro time: 3.825 | step time: 0.000
train | epoch   1 | Iter:    358/  2110 | global iter:    358/  2110 | loss: 0.4540 | ds_loss: 0.0000 | lr: 4.6539e-05 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   1 | Iter:    359/  2110 | global iter:    359/  2110 | loss: 0.3013 | ds_loss: 0.0000 | lr: 4.6520e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   1 | Iter:    360/  2110 | global iter:    360/  2110 | loss: 0.3281 | ds_loss: 0.0000 | lr: 4.6501e-05 | scale:     1.0000 | micro time: 3.767 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    360/  2110 | global iter:    360/  2110 | loss: 0.3716 | ds_loss: 0.0000 | lr: 4.6501e-05 | scale:     1.0000 | micro time: 3.767 | step time: 3.792
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    361/  2110 | global iter:    361/  2110 | loss: 0.3770 | ds_loss: 0.0000 | lr: 4.6482e-05 | scale:     1.0000 | micro time: 3.816 | step time: 0.000
train | epoch   1 | Iter:    362/  2110 | global iter:    362/  2110 | loss: 0.2784 | ds_loss: 0.0000 | lr: 4.6463e-05 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   1 | Iter:    363/  2110 | global iter:    363/  2110 | loss: 0.2396 | ds_loss: 0.0000 | lr: 4.6444e-05 | scale:     1.0000 | micro time: 3.814 | step time: 0.000
train | epoch   1 | Iter:    364/  2110 | global iter:    364/  2110 | loss: 0.3796 | ds_loss: 0.0000 | lr: 4.6425e-05 | scale:     1.0000 | micro time: 3.762 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    364/  2110 | global iter:    364/  2110 | loss: 0.3187 | ds_loss: 0.0000 | lr: 4.6425e-05 | scale:     1.0000 | micro time: 3.762 | step time: 3.793
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    365/  2110 | global iter:    365/  2110 | loss: 0.1425 | ds_loss: 0.0000 | lr: 4.6405e-05 | scale:     1.0000 | micro time: 3.774 | step time: 0.000
train | epoch   1 | Iter:    366/  2110 | global iter:    366/  2110 | loss: 0.3184 | ds_loss: 0.0000 | lr: 4.6386e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   1 | Iter:    367/  2110 | global iter:    367/  2110 | loss: 0.1882 | ds_loss: 0.0000 | lr: 4.6367e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   1 | Iter:    368/  2110 | global iter:    368/  2110 | loss: 0.2314 | ds_loss: 0.0000 | lr: 4.6348e-05 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    368/  2110 | global iter:    368/  2110 | loss: 0.2201 | ds_loss: 0.0000 | lr: 4.6348e-05 | scale:     1.0000 | micro time: 3.779 | step time: 3.786
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    369/  2110 | global iter:    369/  2110 | loss: 0.4373 | ds_loss: 0.0000 | lr: 4.6328e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   1 | Iter:    370/  2110 | global iter:    370/  2110 | loss: 0.2130 | ds_loss: 0.0000 | lr: 4.6309e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   1 | Iter:    371/  2110 | global iter:    371/  2110 | loss: 0.2994 | ds_loss: 0.0000 | lr: 4.6289e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   1 | Iter:    372/  2110 | global iter:    372/  2110 | loss: 0.2277 | ds_loss: 0.0000 | lr: 4.6270e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    372/  2110 | global iter:    372/  2110 | loss: 0.2943 | ds_loss: 0.0000 | lr: 4.6270e-05 | scale:     1.0000 | micro time: 3.791 | step time: 3.786
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    373/  2110 | global iter:    373/  2110 | loss: 0.2681 | ds_loss: 0.0000 | lr: 4.6250e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   1 | Iter:    374/  2110 | global iter:    374/  2110 | loss: 0.4796 | ds_loss: 0.0000 | lr: 4.6231e-05 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   1 | Iter:    375/  2110 | global iter:    375/  2110 | loss: 0.3888 | ds_loss: 0.0000 | lr: 4.6211e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   1 | Iter:    376/  2110 | global iter:    376/  2110 | loss: 0.1989 | ds_loss: 0.0000 | lr: 4.6191e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    376/  2110 | global iter:    376/  2110 | loss: 0.3339 | ds_loss: 0.0000 | lr: 4.6191e-05 | scale:     1.0000 | micro time: 3.790 | step time: 3.789
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    377/  2110 | global iter:    377/  2110 | loss: 0.2249 | ds_loss: 0.0000 | lr: 4.6172e-05 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
train | epoch   1 | Iter:    378/  2110 | global iter:    378/  2110 | loss: 0.2885 | ds_loss: 0.0000 | lr: 4.6152e-05 | scale:     1.0000 | micro time: 3.770 | step time: 0.000
train | epoch   1 | Iter:    379/  2110 | global iter:    379/  2110 | loss: 0.2429 | ds_loss: 0.0000 | lr: 4.6132e-05 | scale:     1.0000 | micro time: 3.811 | step time: 0.000
train | epoch   1 | Iter:    380/  2110 | global iter:    380/  2110 | loss: 0.2125 | ds_loss: 0.0000 | lr: 4.6112e-05 | scale:     1.0000 | micro time: 3.772 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    380/  2110 | global iter:    380/  2110 | loss: 0.2422 | ds_loss: 0.0000 | lr: 4.6112e-05 | scale:     1.0000 | micro time: 3.772 | step time: 3.785
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    381/  2110 | global iter:    381/  2110 | loss: 0.3895 | ds_loss: 0.0000 | lr: 4.6092e-05 | scale:     1.0000 | micro time: 3.797 | step time: 0.000
train | epoch   1 | Iter:    382/  2110 | global iter:    382/  2110 | loss: 0.2958 | ds_loss: 0.0000 | lr: 4.6072e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   1 | Iter:    383/  2110 | global iter:    383/  2110 | loss: 0.3401 | ds_loss: 0.0000 | lr: 4.6052e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   1 | Iter:    384/  2110 | global iter:    384/  2110 | loss: 0.3015 | ds_loss: 0.0000 | lr: 4.6032e-05 | scale:     1.0000 | micro time: 3.780 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    384/  2110 | global iter:    384/  2110 | loss: 0.3317 | ds_loss: 0.0000 | lr: 4.6032e-05 | scale:     1.0000 | micro time: 3.780 | step time: 3.793
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    385/  2110 | global iter:    385/  2110 | loss: 0.6132 | ds_loss: 0.0000 | lr: 4.6012e-05 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   1 | Iter:    386/  2110 | global iter:    386/  2110 | loss: 0.2584 | ds_loss: 0.0000 | lr: 4.5992e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   1 | Iter:    387/  2110 | global iter:    387/  2110 | loss: 0.3742 | ds_loss: 0.0000 | lr: 4.5971e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   1 | Iter:    388/  2110 | global iter:    388/  2110 | loss: 0.3824 | ds_loss: 0.0000 | lr: 4.5951e-05 | scale:     1.0000 | micro time: 3.770 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    388/  2110 | global iter:    388/  2110 | loss: 0.4070 | ds_loss: 0.0000 | lr: 4.5951e-05 | scale:     1.0000 | micro time: 3.770 | step time: 3.787
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    389/  2110 | global iter:    389/  2110 | loss: 0.3667 | ds_loss: 0.0000 | lr: 4.5931e-05 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
train | epoch   1 | Iter:    390/  2110 | global iter:    390/  2110 | loss: 0.4368 | ds_loss: 0.0000 | lr: 4.5911e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   1 | Iter:    391/  2110 | global iter:    391/  2110 | loss: 0.1865 | ds_loss: 0.0000 | lr: 4.5890e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   1 | Iter:    392/  2110 | global iter:    392/  2110 | loss: 0.3885 | ds_loss: 0.0000 | lr: 4.5870e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    392/  2110 | global iter:    392/  2110 | loss: 0.3446 | ds_loss: 0.0000 | lr: 4.5870e-05 | scale:     1.0000 | micro time: 3.798 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    393/  2110 | global iter:    393/  2110 | loss: 0.2938 | ds_loss: 0.0000 | lr: 4.5849e-05 | scale:     1.0000 | micro time: 3.792 | step time: 0.000
train | epoch   1 | Iter:    394/  2110 | global iter:    394/  2110 | loss: 0.2937 | ds_loss: 0.0000 | lr: 4.5829e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   1 | Iter:    395/  2110 | global iter:    395/  2110 | loss: 0.2692 | ds_loss: 0.0000 | lr: 4.5808e-05 | scale:     1.0000 | micro time: 3.812 | step time: 0.000
train | epoch   1 | Iter:    396/  2110 | global iter:    396/  2110 | loss: 0.3056 | ds_loss: 0.0000 | lr: 4.5787e-05 | scale:     1.0000 | micro time: 3.828 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    396/  2110 | global iter:    396/  2110 | loss: 0.2906 | ds_loss: 0.0000 | lr: 4.5787e-05 | scale:     1.0000 | micro time: 3.828 | step time: 3.809
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    397/  2110 | global iter:    397/  2110 | loss: 0.3115 | ds_loss: 0.0000 | lr: 4.5767e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   1 | Iter:    398/  2110 | global iter:    398/  2110 | loss: 0.2214 | ds_loss: 0.0000 | lr: 4.5746e-05 | scale:     1.0000 | micro time: 3.776 | step time: 0.000
train | epoch   1 | Iter:    399/  2110 | global iter:    399/  2110 | loss: 0.2441 | ds_loss: 0.0000 | lr: 4.5725e-05 | scale:     1.0000 | micro time: 3.822 | step time: 0.000
train | epoch   1 | Iter:    400/  2110 | global iter:    400/  2110 | loss: 0.1469 | ds_loss: 0.0000 | lr: 4.5704e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    400/  2110 | global iter:    400/  2110 | loss: 0.2310 | ds_loss: 0.0000 | lr: 4.5704e-05 | scale:     1.0000 | micro time: 3.785 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    401/  2110 | global iter:    401/  2110 | loss: 0.2738 | ds_loss: 0.0000 | lr: 4.5684e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   1 | Iter:    402/  2110 | global iter:    402/  2110 | loss: 0.2140 | ds_loss: 0.0000 | lr: 4.5663e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   1 | Iter:    403/  2110 | global iter:    403/  2110 | loss: 0.4269 | ds_loss: 0.0000 | lr: 4.5642e-05 | scale:     1.0000 | micro time: 3.811 | step time: 0.000
train | epoch   1 | Iter:    404/  2110 | global iter:    404/  2110 | loss: 0.3537 | ds_loss: 0.0000 | lr: 4.5621e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    404/  2110 | global iter:    404/  2110 | loss: 0.3171 | ds_loss: 0.0000 | lr: 4.5621e-05 | scale:     1.0000 | micro time: 3.810 | step time: 3.801
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    405/  2110 | global iter:    405/  2110 | loss: 0.2617 | ds_loss: 0.0000 | lr: 4.5600e-05 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   1 | Iter:    406/  2110 | global iter:    406/  2110 | loss: 0.3592 | ds_loss: 0.0000 | lr: 4.5579e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   1 | Iter:    407/  2110 | global iter:    407/  2110 | loss: 0.2469 | ds_loss: 0.0000 | lr: 4.5557e-05 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   1 | Iter:    408/  2110 | global iter:    408/  2110 | loss: 0.4011 | ds_loss: 0.0000 | lr: 4.5536e-05 | scale:     1.0000 | micro time: 3.826 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    408/  2110 | global iter:    408/  2110 | loss: 0.3172 | ds_loss: 0.0000 | lr: 4.5536e-05 | scale:     1.0000 | micro time: 3.826 | step time: 3.801
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    409/  2110 | global iter:    409/  2110 | loss: 0.3449 | ds_loss: 0.0000 | lr: 4.5515e-05 | scale:     1.0000 | micro time: 3.777 | step time: 0.000
train | epoch   1 | Iter:    410/  2110 | global iter:    410/  2110 | loss: 0.2367 | ds_loss: 0.0000 | lr: 4.5494e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   1 | Iter:    411/  2110 | global iter:    411/  2110 | loss: 0.3421 | ds_loss: 0.0000 | lr: 4.5472e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   1 | Iter:    412/  2110 | global iter:    412/  2110 | loss: 0.4635 | ds_loss: 0.0000 | lr: 4.5451e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    412/  2110 | global iter:    412/  2110 | loss: 0.3468 | ds_loss: 0.0000 | lr: 4.5451e-05 | scale:     1.0000 | micro time: 3.790 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    413/  2110 | global iter:    413/  2110 | loss: 0.3438 | ds_loss: 0.0000 | lr: 4.5430e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   1 | Iter:    414/  2110 | global iter:    414/  2110 | loss: 0.3322 | ds_loss: 0.0000 | lr: 4.5408e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   1 | Iter:    415/  2110 | global iter:    415/  2110 | loss: 0.4922 | ds_loss: 0.0000 | lr: 4.5387e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   1 | Iter:    416/  2110 | global iter:    416/  2110 | loss: 0.3338 | ds_loss: 0.0000 | lr: 4.5365e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    416/  2110 | global iter:    416/  2110 | loss: 0.3755 | ds_loss: 0.0000 | lr: 4.5365e-05 | scale:     1.0000 | micro time: 3.795 | step time: 3.796
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    417/  2110 | global iter:    417/  2110 | loss: 0.2057 | ds_loss: 0.0000 | lr: 4.5344e-05 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   1 | Iter:    418/  2110 | global iter:    418/  2110 | loss: 0.2401 | ds_loss: 0.0000 | lr: 4.5322e-05 | scale:     1.0000 | micro time: 3.822 | step time: 0.000
train | epoch   1 | Iter:    419/  2110 | global iter:    419/  2110 | loss: 0.4762 | ds_loss: 0.0000 | lr: 4.5300e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   1 | Iter:    420/  2110 | global iter:    420/  2110 | loss: 0.3748 | ds_loss: 0.0000 | lr: 4.5279e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    420/  2110 | global iter:    420/  2110 | loss: 0.3242 | ds_loss: 0.0000 | lr: 4.5279e-05 | scale:     1.0000 | micro time: 3.795 | step time: 3.797
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:    421/  2110 | global iter:    421/  2110 | loss: 0.2595 | ds_loss: 0.0000 | lr: 4.5257e-05 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
train | epoch   1 | Iter:    422/  2110 | global iter:    422/  2110 | loss: 0.2968 | ds_loss: 0.0000 | lr: 4.5235e-05 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   1 | Iter:    423/  2110 | global iter:    423/  2110 | loss: 0.4191 | ds_loss: 0.0000 | lr: 4.5213e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
train | epoch   1 | Iter:    424/  2110 | global iter:    424/  2110 | loss: 0.4641 | ds_loss: 0.0000 | lr: 4.5191e-05 | scale:     1.0000 | micro time: 3.327 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:    424/  2110 | global iter:    424/  2110 | loss: 0.3599 | ds_loss: 0.0000 | lr: 4.5191e-05 | scale:     1.0000 | micro time: 3.327 | step time: 3.684
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
Tue Apr  8 07:32:19 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   63C    P0             56W /  250W |   29814MiB /  32768MiB |     84%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |     66%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   50C    P0             48W /  250W |   29848MiB /  32768MiB |     70%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   54C    P0             49W /  250W |   28312MiB /  32768MiB |     78%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 07:32:19 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   63C    P0             56W /  250W |   29814MiB /  32768MiB |     84%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |     66%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   50C    P0             48W /  250W |   29848MiB /  32768MiB |     70%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   54C    P0             53W /  250W |   28312MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 07:32:19 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   63C    P0             56W /  250W |   29814MiB /  32768MiB |     84%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |     66%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   50C    P0             48W /  250W |   29848MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   54C    P0            190W /  250W |   28312MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 07:32:19 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   63C    P0             56W /  250W |   29814MiB /  32768MiB |     84%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |     66%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   50C    P0             48W /  250W |   29848MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   54C    P0            190W /  250W |   28312MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

train | epoch   2 | Iter:    425/  2110 | global iter:    425/  2110 | loss: 0.1258 | ds_loss: 0.0000 | lr: 4.5169e-05 | scale:     1.0000 | micro time: 3.978 | step time: 0.000
train | epoch   2 | Iter:    426/  2110 | global iter:    426/  2110 | loss: 0.1861 | ds_loss: 0.0000 | lr: 4.5147e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   2 | Iter:    427/  2110 | global iter:    427/  2110 | loss: 0.2745 | ds_loss: 0.0000 | lr: 4.5125e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   2 | Iter:    428/  2110 | global iter:    428/  2110 | loss: 0.1301 | ds_loss: 0.0000 | lr: 4.5103e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    428/  2110 | global iter:    428/  2110 | loss: 0.1791 | ds_loss: 0.0000 | lr: 4.5103e-05 | scale:     1.0000 | micro time: 3.795 | step time: 3.845
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    429/  2110 | global iter:    429/  2110 | loss: 0.2619 | ds_loss: 0.0000 | lr: 4.5081e-05 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
train | epoch   2 | Iter:    430/  2110 | global iter:    430/  2110 | loss: 0.1805 | ds_loss: 0.0000 | lr: 4.5059e-05 | scale:     1.0000 | micro time: 3.767 | step time: 0.000
train | epoch   2 | Iter:    431/  2110 | global iter:    431/  2110 | loss: 0.2365 | ds_loss: 0.0000 | lr: 4.5037e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   2 | Iter:    432/  2110 | global iter:    432/  2110 | loss: 0.2819 | ds_loss: 0.0000 | lr: 4.5014e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    432/  2110 | global iter:    432/  2110 | loss: 0.2402 | ds_loss: 0.0000 | lr: 4.5014e-05 | scale:     1.0000 | micro time: 3.802 | step time: 3.786
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    433/  2110 | global iter:    433/  2110 | loss: 0.1628 | ds_loss: 0.0000 | lr: 4.4992e-05 | scale:     1.0000 | micro time: 3.773 | step time: 0.000
train | epoch   2 | Iter:    434/  2110 | global iter:    434/  2110 | loss: 0.2805 | ds_loss: 0.0000 | lr: 4.4970e-05 | scale:     1.0000 | micro time: 3.814 | step time: 0.000
train | epoch   2 | Iter:    435/  2110 | global iter:    435/  2110 | loss: 0.1886 | ds_loss: 0.0000 | lr: 4.4947e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   2 | Iter:    436/  2110 | global iter:    436/  2110 | loss: 0.2017 | ds_loss: 0.0000 | lr: 4.4925e-05 | scale:     1.0000 | micro time: 3.774 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    436/  2110 | global iter:    436/  2110 | loss: 0.2084 | ds_loss: 0.0000 | lr: 4.4925e-05 | scale:     1.0000 | micro time: 3.774 | step time: 3.792
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    437/  2110 | global iter:    437/  2110 | loss: 0.1699 | ds_loss: 0.0000 | lr: 4.4902e-05 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   2 | Iter:    438/  2110 | global iter:    438/  2110 | loss: 0.1892 | ds_loss: 0.0000 | lr: 4.4880e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   2 | Iter:    439/  2110 | global iter:    439/  2110 | loss: 0.2044 | ds_loss: 0.0000 | lr: 4.4857e-05 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
train | epoch   2 | Iter:    440/  2110 | global iter:    440/  2110 | loss: 0.1336 | ds_loss: 0.0000 | lr: 4.4835e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    440/  2110 | global iter:    440/  2110 | loss: 0.1743 | ds_loss: 0.0000 | lr: 4.4835e-05 | scale:     1.0000 | micro time: 3.787 | step time: 3.789
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    441/  2110 | global iter:    441/  2110 | loss: 0.1972 | ds_loss: 0.0000 | lr: 4.4812e-05 | scale:     1.0000 | micro time: 3.768 | step time: 0.000
train | epoch   2 | Iter:    442/  2110 | global iter:    442/  2110 | loss: 0.1513 | ds_loss: 0.0000 | lr: 4.4789e-05 | scale:     1.0000 | micro time: 3.773 | step time: 0.000
train | epoch   2 | Iter:    443/  2110 | global iter:    443/  2110 | loss: 0.1503 | ds_loss: 0.0000 | lr: 4.4767e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   2 | Iter:    444/  2110 | global iter:    444/  2110 | loss: 0.2454 | ds_loss: 0.0000 | lr: 4.4744e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    444/  2110 | global iter:    444/  2110 | loss: 0.1861 | ds_loss: 0.0000 | lr: 4.4744e-05 | scale:     1.0000 | micro time: 3.801 | step time: 3.786
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    445/  2110 | global iter:    445/  2110 | loss: 0.2479 | ds_loss: 0.0000 | lr: 4.4721e-05 | scale:     1.0000 | micro time: 3.772 | step time: 0.000
train | epoch   2 | Iter:    446/  2110 | global iter:    446/  2110 | loss: 0.1746 | ds_loss: 0.0000 | lr: 4.4698e-05 | scale:     1.0000 | micro time: 3.792 | step time: 0.000
train | epoch   2 | Iter:    447/  2110 | global iter:    447/  2110 | loss: 0.1014 | ds_loss: 0.0000 | lr: 4.4675e-05 | scale:     1.0000 | micro time: 3.821 | step time: 0.000
train | epoch   2 | Iter:    448/  2110 | global iter:    448/  2110 | loss: 0.2183 | ds_loss: 0.0000 | lr: 4.4652e-05 | scale:     1.0000 | micro time: 3.767 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    448/  2110 | global iter:    448/  2110 | loss: 0.1855 | ds_loss: 0.0000 | lr: 4.4652e-05 | scale:     1.0000 | micro time: 3.767 | step time: 3.788
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    449/  2110 | global iter:    449/  2110 | loss: 0.1596 | ds_loss: 0.0000 | lr: 4.4629e-05 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   2 | Iter:    450/  2110 | global iter:    450/  2110 | loss: 0.1105 | ds_loss: 0.0000 | lr: 4.4606e-05 | scale:     1.0000 | micro time: 3.821 | step time: 0.000
train | epoch   2 | Iter:    451/  2110 | global iter:    451/  2110 | loss: 0.1594 | ds_loss: 0.0000 | lr: 4.4583e-05 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   2 | Iter:    452/  2110 | global iter:    452/  2110 | loss: 0.1250 | ds_loss: 0.0000 | lr: 4.4560e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    452/  2110 | global iter:    452/  2110 | loss: 0.1386 | ds_loss: 0.0000 | lr: 4.4560e-05 | scale:     1.0000 | micro time: 3.798 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    453/  2110 | global iter:    453/  2110 | loss: 0.1746 | ds_loss: 0.0000 | lr: 4.4537e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   2 | Iter:    454/  2110 | global iter:    454/  2110 | loss: 0.2338 | ds_loss: 0.0000 | lr: 4.4514e-05 | scale:     1.0000 | micro time: 3.763 | step time: 0.000
train | epoch   2 | Iter:    455/  2110 | global iter:    455/  2110 | loss: 0.2615 | ds_loss: 0.0000 | lr: 4.4490e-05 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   2 | Iter:    456/  2110 | global iter:    456/  2110 | loss: 0.1988 | ds_loss: 0.0000 | lr: 4.4467e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    456/  2110 | global iter:    456/  2110 | loss: 0.2172 | ds_loss: 0.0000 | lr: 4.4467e-05 | scale:     1.0000 | micro time: 3.810 | step time: 3.789
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    457/  2110 | global iter:    457/  2110 | loss: 0.1573 | ds_loss: 0.0000 | lr: 4.4444e-05 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
train | epoch   2 | Iter:    458/  2110 | global iter:    458/  2110 | loss: 0.3955 | ds_loss: 0.0000 | lr: 4.4420e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   2 | Iter:    459/  2110 | global iter:    459/  2110 | loss: 0.2450 | ds_loss: 0.0000 | lr: 4.4397e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   2 | Iter:    460/  2110 | global iter:    460/  2110 | loss: 0.1607 | ds_loss: 0.0000 | lr: 4.4373e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    460/  2110 | global iter:    460/  2110 | loss: 0.2396 | ds_loss: 0.0000 | lr: 4.4373e-05 | scale:     1.0000 | micro time: 3.790 | step time: 3.796
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    461/  2110 | global iter:    461/  2110 | loss: 0.2945 | ds_loss: 0.0000 | lr: 4.4350e-05 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
train | epoch   2 | Iter:    462/  2110 | global iter:    462/  2110 | loss: 0.1870 | ds_loss: 0.0000 | lr: 4.4326e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   2 | Iter:    463/  2110 | global iter:    463/  2110 | loss: 0.3104 | ds_loss: 0.0000 | lr: 4.4303e-05 | scale:     1.0000 | micro time: 3.825 | step time: 0.000
train | epoch   2 | Iter:    464/  2110 | global iter:    464/  2110 | loss: 0.1595 | ds_loss: 0.0000 | lr: 4.4279e-05 | scale:     1.0000 | micro time: 3.800 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    464/  2110 | global iter:    464/  2110 | loss: 0.2378 | ds_loss: 0.0000 | lr: 4.4279e-05 | scale:     1.0000 | micro time: 3.800 | step time: 3.801
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    465/  2110 | global iter:    465/  2110 | loss: 0.2577 | ds_loss: 0.0000 | lr: 4.4255e-05 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   2 | Iter:    466/  2110 | global iter:    466/  2110 | loss: 0.0809 | ds_loss: 0.0000 | lr: 4.4232e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   2 | Iter:    467/  2110 | global iter:    467/  2110 | loss: 0.1203 | ds_loss: 0.0000 | lr: 4.4208e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   2 | Iter:    468/  2110 | global iter:    468/  2110 | loss: 0.2312 | ds_loss: 0.0000 | lr: 4.4184e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    468/  2110 | global iter:    468/  2110 | loss: 0.1725 | ds_loss: 0.0000 | lr: 4.4184e-05 | scale:     1.0000 | micro time: 3.798 | step time: 3.798
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    469/  2110 | global iter:    469/  2110 | loss: 0.2522 | ds_loss: 0.0000 | lr: 4.4160e-05 | scale:     1.0000 | micro time: 3.817 | step time: 0.000
train | epoch   2 | Iter:    470/  2110 | global iter:    470/  2110 | loss: 0.2661 | ds_loss: 0.0000 | lr: 4.4136e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   2 | Iter:    471/  2110 | global iter:    471/  2110 | loss: 0.2815 | ds_loss: 0.0000 | lr: 4.4112e-05 | scale:     1.0000 | micro time: 3.822 | step time: 0.000
train | epoch   2 | Iter:    472/  2110 | global iter:    472/  2110 | loss: 0.3153 | ds_loss: 0.0000 | lr: 4.4088e-05 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    472/  2110 | global iter:    472/  2110 | loss: 0.2788 | ds_loss: 0.0000 | lr: 4.4088e-05 | scale:     1.0000 | micro time: 3.775 | step time: 3.800
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    473/  2110 | global iter:    473/  2110 | loss: 0.2125 | ds_loss: 0.0000 | lr: 4.4064e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   2 | Iter:    474/  2110 | global iter:    474/  2110 | loss: 0.2366 | ds_loss: 0.0000 | lr: 4.4040e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   2 | Iter:    475/  2110 | global iter:    475/  2110 | loss: 0.2605 | ds_loss: 0.0000 | lr: 4.4016e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   2 | Iter:    476/  2110 | global iter:    476/  2110 | loss: 0.2868 | ds_loss: 0.0000 | lr: 4.3992e-05 | scale:     1.0000 | micro time: 3.772 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    476/  2110 | global iter:    476/  2110 | loss: 0.2491 | ds_loss: 0.0000 | lr: 4.3992e-05 | scale:     1.0000 | micro time: 3.772 | step time: 3.792
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    477/  2110 | global iter:    477/  2110 | loss: 0.2310 | ds_loss: 0.0000 | lr: 4.3968e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   2 | Iter:    478/  2110 | global iter:    478/  2110 | loss: 0.3575 | ds_loss: 0.0000 | lr: 4.3943e-05 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
train | epoch   2 | Iter:    479/  2110 | global iter:    479/  2110 | loss: 0.2337 | ds_loss: 0.0000 | lr: 4.3919e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   2 | Iter:    480/  2110 | global iter:    480/  2110 | loss: 0.2623 | ds_loss: 0.0000 | lr: 4.3895e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    480/  2110 | global iter:    480/  2110 | loss: 0.2711 | ds_loss: 0.0000 | lr: 4.3895e-05 | scale:     1.0000 | micro time: 3.786 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    481/  2110 | global iter:    481/  2110 | loss: 0.1857 | ds_loss: 0.0000 | lr: 4.3871e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   2 | Iter:    482/  2110 | global iter:    482/  2110 | loss: 0.1384 | ds_loss: 0.0000 | lr: 4.3846e-05 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   2 | Iter:    483/  2110 | global iter:    483/  2110 | loss: 0.1620 | ds_loss: 0.0000 | lr: 4.3822e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   2 | Iter:    484/  2110 | global iter:    484/  2110 | loss: 0.1951 | ds_loss: 0.0000 | lr: 4.3797e-05 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    484/  2110 | global iter:    484/  2110 | loss: 0.1703 | ds_loss: 0.0000 | lr: 4.3797e-05 | scale:     1.0000 | micro time: 3.779 | step time: 3.788
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    485/  2110 | global iter:    485/  2110 | loss: 0.0978 | ds_loss: 0.0000 | lr: 4.3773e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   2 | Iter:    486/  2110 | global iter:    486/  2110 | loss: 0.2475 | ds_loss: 0.0000 | lr: 4.3748e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   2 | Iter:    487/  2110 | global iter:    487/  2110 | loss: 0.2477 | ds_loss: 0.0000 | lr: 4.3723e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   2 | Iter:    488/  2110 | global iter:    488/  2110 | loss: 0.1356 | ds_loss: 0.0000 | lr: 4.3699e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    488/  2110 | global iter:    488/  2110 | loss: 0.1822 | ds_loss: 0.0000 | lr: 4.3699e-05 | scale:     1.0000 | micro time: 3.787 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    489/  2110 | global iter:    489/  2110 | loss: 0.1169 | ds_loss: 0.0000 | lr: 4.3674e-05 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
train | epoch   2 | Iter:    490/  2110 | global iter:    490/  2110 | loss: 0.1762 | ds_loss: 0.0000 | lr: 4.3649e-05 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   2 | Iter:    491/  2110 | global iter:    491/  2110 | loss: 0.1647 | ds_loss: 0.0000 | lr: 4.3625e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   2 | Iter:    492/  2110 | global iter:    492/  2110 | loss: 0.3429 | ds_loss: 0.0000 | lr: 4.3600e-05 | scale:     1.0000 | micro time: 3.780 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    492/  2110 | global iter:    492/  2110 | loss: 0.2002 | ds_loss: 0.0000 | lr: 4.3600e-05 | scale:     1.0000 | micro time: 3.780 | step time: 3.786
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    493/  2110 | global iter:    493/  2110 | loss: 0.3629 | ds_loss: 0.0000 | lr: 4.3575e-05 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
train | epoch   2 | Iter:    494/  2110 | global iter:    494/  2110 | loss: 0.3183 | ds_loss: 0.0000 | lr: 4.3550e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   2 | Iter:    495/  2110 | global iter:    495/  2110 | loss: 0.2860 | ds_loss: 0.0000 | lr: 4.3525e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
train | epoch   2 | Iter:    496/  2110 | global iter:    496/  2110 | loss: 0.2200 | ds_loss: 0.0000 | lr: 4.3500e-05 | scale:     1.0000 | micro time: 3.770 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    496/  2110 | global iter:    496/  2110 | loss: 0.2968 | ds_loss: 0.0000 | lr: 4.3500e-05 | scale:     1.0000 | micro time: 3.770 | step time: 3.797
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    497/  2110 | global iter:    497/  2110 | loss: 0.2273 | ds_loss: 0.0000 | lr: 4.3475e-05 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   2 | Iter:    498/  2110 | global iter:    498/  2110 | loss: 0.1032 | ds_loss: 0.0000 | lr: 4.3450e-05 | scale:     1.0000 | micro time: 3.763 | step time: 0.000
train | epoch   2 | Iter:    499/  2110 | global iter:    499/  2110 | loss: 0.2731 | ds_loss: 0.0000 | lr: 4.3425e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   2 | Iter:    500/  2110 | global iter:    500/  2110 | loss: 0.0853 | ds_loss: 0.0000 | lr: 4.3400e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    500/  2110 | global iter:    500/  2110 | loss: 0.1723 | ds_loss: 0.0000 | lr: 4.3400e-05 | scale:     1.0000 | micro time: 3.795 | step time: 3.788
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    501/  2110 | global iter:    501/  2110 | loss: 0.2030 | ds_loss: 0.0000 | lr: 4.3374e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   2 | Iter:    502/  2110 | global iter:    502/  2110 | loss: 0.1413 | ds_loss: 0.0000 | lr: 4.3349e-05 | scale:     1.0000 | micro time: 3.814 | step time: 0.000
train | epoch   2 | Iter:    503/  2110 | global iter:    503/  2110 | loss: 0.1428 | ds_loss: 0.0000 | lr: 4.3324e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   2 | Iter:    504/  2110 | global iter:    504/  2110 | loss: 0.2272 | ds_loss: 0.0000 | lr: 4.3299e-05 | scale:     1.0000 | micro time: 3.767 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    504/  2110 | global iter:    504/  2110 | loss: 0.1786 | ds_loss: 0.0000 | lr: 4.3299e-05 | scale:     1.0000 | micro time: 3.767 | step time: 3.783
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    505/  2110 | global iter:    505/  2110 | loss: 0.1561 | ds_loss: 0.0000 | lr: 4.3273e-05 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
train | epoch   2 | Iter:    506/  2110 | global iter:    506/  2110 | loss: 0.1065 | ds_loss: 0.0000 | lr: 4.3248e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   2 | Iter:    507/  2110 | global iter:    507/  2110 | loss: 0.2195 | ds_loss: 0.0000 | lr: 4.3223e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   2 | Iter:    508/  2110 | global iter:    508/  2110 | loss: 0.1701 | ds_loss: 0.0000 | lr: 4.3197e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    508/  2110 | global iter:    508/  2110 | loss: 0.1630 | ds_loss: 0.0000 | lr: 4.3197e-05 | scale:     1.0000 | micro time: 3.810 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    509/  2110 | global iter:    509/  2110 | loss: 0.2900 | ds_loss: 0.0000 | lr: 4.3172e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   2 | Iter:    510/  2110 | global iter:    510/  2110 | loss: 0.1244 | ds_loss: 0.0000 | lr: 4.3146e-05 | scale:     1.0000 | micro time: 3.763 | step time: 0.000
train | epoch   2 | Iter:    511/  2110 | global iter:    511/  2110 | loss: 0.2509 | ds_loss: 0.0000 | lr: 4.3120e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   2 | Iter:    512/  2110 | global iter:    512/  2110 | loss: 0.2475 | ds_loss: 0.0000 | lr: 4.3095e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    512/  2110 | global iter:    512/  2110 | loss: 0.2282 | ds_loss: 0.0000 | lr: 4.3095e-05 | scale:     1.0000 | micro time: 3.806 | step time: 3.788
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    513/  2110 | global iter:    513/  2110 | loss: 0.0928 | ds_loss: 0.0000 | lr: 4.3069e-05 | scale:     1.0000 | micro time: 3.768 | step time: 0.000
train | epoch   2 | Iter:    514/  2110 | global iter:    514/  2110 | loss: 0.2461 | ds_loss: 0.0000 | lr: 4.3043e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   2 | Iter:    515/  2110 | global iter:    515/  2110 | loss: 0.1510 | ds_loss: 0.0000 | lr: 4.3018e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   2 | Iter:    516/  2110 | global iter:    516/  2110 | loss: 0.2037 | ds_loss: 0.0000 | lr: 4.2992e-05 | scale:     1.0000 | micro time: 3.814 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    516/  2110 | global iter:    516/  2110 | loss: 0.1734 | ds_loss: 0.0000 | lr: 4.2992e-05 | scale:     1.0000 | micro time: 3.814 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    517/  2110 | global iter:    517/  2110 | loss: 0.2557 | ds_loss: 0.0000 | lr: 4.2966e-05 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   2 | Iter:    518/  2110 | global iter:    518/  2110 | loss: 0.1737 | ds_loss: 0.0000 | lr: 4.2940e-05 | scale:     1.0000 | micro time: 3.822 | step time: 0.000
train | epoch   2 | Iter:    519/  2110 | global iter:    519/  2110 | loss: 0.3050 | ds_loss: 0.0000 | lr: 4.2914e-05 | scale:     1.0000 | micro time: 3.751 | step time: 0.000
train | epoch   2 | Iter:    520/  2110 | global iter:    520/  2110 | loss: 0.1305 | ds_loss: 0.0000 | lr: 4.2888e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    520/  2110 | global iter:    520/  2110 | loss: 0.2163 | ds_loss: 0.0000 | lr: 4.2888e-05 | scale:     1.0000 | micro time: 3.802 | step time: 3.789
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    521/  2110 | global iter:    521/  2110 | loss: 0.2119 | ds_loss: 0.0000 | lr: 4.2862e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   2 | Iter:    522/  2110 | global iter:    522/  2110 | loss: 0.2154 | ds_loss: 0.0000 | lr: 4.2836e-05 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
train | epoch   2 | Iter:    523/  2110 | global iter:    523/  2110 | loss: 0.2270 | ds_loss: 0.0000 | lr: 4.2810e-05 | scale:     1.0000 | micro time: 3.792 | step time: 0.000
train | epoch   2 | Iter:    524/  2110 | global iter:    524/  2110 | loss: 0.1753 | ds_loss: 0.0000 | lr: 4.2784e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    524/  2110 | global iter:    524/  2110 | loss: 0.2074 | ds_loss: 0.0000 | lr: 4.2784e-05 | scale:     1.0000 | micro time: 3.794 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    525/  2110 | global iter:    525/  2110 | loss: 0.2237 | ds_loss: 0.0000 | lr: 4.2758e-05 | scale:     1.0000 | micro time: 3.811 | step time: 0.000
train | epoch   2 | Iter:    526/  2110 | global iter:    526/  2110 | loss: 0.1331 | ds_loss: 0.0000 | lr: 4.2732e-05 | scale:     1.0000 | micro time: 3.796 | step time: 0.000
train | epoch   2 | Iter:    527/  2110 | global iter:    527/  2110 | loss: 0.1968 | ds_loss: 0.0000 | lr: 4.2705e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   2 | Iter:    528/  2110 | global iter:    528/  2110 | loss: 0.1884 | ds_loss: 0.0000 | lr: 4.2679e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    528/  2110 | global iter:    528/  2110 | loss: 0.1855 | ds_loss: 0.0000 | lr: 4.2679e-05 | scale:     1.0000 | micro time: 3.818 | step time: 3.804
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    529/  2110 | global iter:    529/  2110 | loss: 0.2664 | ds_loss: 0.0000 | lr: 4.2653e-05 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   2 | Iter:    530/  2110 | global iter:    530/  2110 | loss: 0.1833 | ds_loss: 0.0000 | lr: 4.2627e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   2 | Iter:    531/  2110 | global iter:    531/  2110 | loss: 0.1647 | ds_loss: 0.0000 | lr: 4.2600e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   2 | Iter:    532/  2110 | global iter:    532/  2110 | loss: 0.2841 | ds_loss: 0.0000 | lr: 4.2574e-05 | scale:     1.0000 | micro time: 3.814 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    532/  2110 | global iter:    532/  2110 | loss: 0.2246 | ds_loss: 0.0000 | lr: 4.2574e-05 | scale:     1.0000 | micro time: 3.814 | step time: 3.795
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    533/  2110 | global iter:    533/  2110 | loss: 0.2200 | ds_loss: 0.0000 | lr: 4.2547e-05 | scale:     1.0000 | micro time: 3.777 | step time: 0.000
train | epoch   2 | Iter:    534/  2110 | global iter:    534/  2110 | loss: 0.1805 | ds_loss: 0.0000 | lr: 4.2521e-05 | scale:     1.0000 | micro time: 3.822 | step time: 0.000
train | epoch   2 | Iter:    535/  2110 | global iter:    535/  2110 | loss: 0.1185 | ds_loss: 0.0000 | lr: 4.2494e-05 | scale:     1.0000 | micro time: 3.815 | step time: 0.000
train | epoch   2 | Iter:    536/  2110 | global iter:    536/  2110 | loss: 0.1980 | ds_loss: 0.0000 | lr: 4.2468e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    536/  2110 | global iter:    536/  2110 | loss: 0.1793 | ds_loss: 0.0000 | lr: 4.2468e-05 | scale:     1.0000 | micro time: 3.790 | step time: 3.801
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    537/  2110 | global iter:    537/  2110 | loss: 0.2252 | ds_loss: 0.0000 | lr: 4.2441e-05 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   2 | Iter:    538/  2110 | global iter:    538/  2110 | loss: 0.2100 | ds_loss: 0.0000 | lr: 4.2414e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   2 | Iter:    539/  2110 | global iter:    539/  2110 | loss: 0.1813 | ds_loss: 0.0000 | lr: 4.2388e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
train | epoch   2 | Iter:    540/  2110 | global iter:    540/  2110 | loss: 0.1780 | ds_loss: 0.0000 | lr: 4.2361e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    540/  2110 | global iter:    540/  2110 | loss: 0.1986 | ds_loss: 0.0000 | lr: 4.2361e-05 | scale:     1.0000 | micro time: 3.791 | step time: 3.795
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    541/  2110 | global iter:    541/  2110 | loss: 0.2442 | ds_loss: 0.0000 | lr: 4.2334e-05 | scale:     1.0000 | micro time: 3.813 | step time: 0.000
train | epoch   2 | Iter:    542/  2110 | global iter:    542/  2110 | loss: 0.3857 | ds_loss: 0.0000 | lr: 4.2307e-05 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   2 | Iter:    543/  2110 | global iter:    543/  2110 | loss: 0.2042 | ds_loss: 0.0000 | lr: 4.2281e-05 | scale:     1.0000 | micro time: 3.822 | step time: 0.000
train | epoch   2 | Iter:    544/  2110 | global iter:    544/  2110 | loss: 0.2506 | ds_loss: 0.0000 | lr: 4.2254e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    544/  2110 | global iter:    544/  2110 | loss: 0.2712 | ds_loss: 0.0000 | lr: 4.2254e-05 | scale:     1.0000 | micro time: 3.806 | step time: 3.810
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    545/  2110 | global iter:    545/  2110 | loss: 0.2196 | ds_loss: 0.0000 | lr: 4.2227e-05 | scale:     1.0000 | micro time: 3.792 | step time: 0.000
train | epoch   2 | Iter:    546/  2110 | global iter:    546/  2110 | loss: 0.2153 | ds_loss: 0.0000 | lr: 4.2200e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
train | epoch   2 | Iter:    547/  2110 | global iter:    547/  2110 | loss: 0.1788 | ds_loss: 0.0000 | lr: 4.2173e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   2 | Iter:    548/  2110 | global iter:    548/  2110 | loss: 0.1376 | ds_loss: 0.0000 | lr: 4.2146e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    548/  2110 | global iter:    548/  2110 | loss: 0.1878 | ds_loss: 0.0000 | lr: 4.2146e-05 | scale:     1.0000 | micro time: 3.806 | step time: 3.801
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    549/  2110 | global iter:    549/  2110 | loss: 0.2979 | ds_loss: 0.0000 | lr: 4.2119e-05 | scale:     1.0000 | micro time: 3.770 | step time: 0.000
train | epoch   2 | Iter:    550/  2110 | global iter:    550/  2110 | loss: 0.2540 | ds_loss: 0.0000 | lr: 4.2092e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   2 | Iter:    551/  2110 | global iter:    551/  2110 | loss: 0.3205 | ds_loss: 0.0000 | lr: 4.2064e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   2 | Iter:    552/  2110 | global iter:    552/  2110 | loss: 0.2004 | ds_loss: 0.0000 | lr: 4.2037e-05 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    552/  2110 | global iter:    552/  2110 | loss: 0.2682 | ds_loss: 0.0000 | lr: 4.2037e-05 | scale:     1.0000 | micro time: 3.783 | step time: 3.790
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    553/  2110 | global iter:    553/  2110 | loss: 0.1452 | ds_loss: 0.0000 | lr: 4.2010e-05 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   2 | Iter:    554/  2110 | global iter:    554/  2110 | loss: 0.1828 | ds_loss: 0.0000 | lr: 4.1983e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   2 | Iter:    555/  2110 | global iter:    555/  2110 | loss: 0.2374 | ds_loss: 0.0000 | lr: 4.1955e-05 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   2 | Iter:    556/  2110 | global iter:    556/  2110 | loss: 0.3011 | ds_loss: 0.0000 | lr: 4.1928e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    556/  2110 | global iter:    556/  2110 | loss: 0.2166 | ds_loss: 0.0000 | lr: 4.1928e-05 | scale:     1.0000 | micro time: 3.790 | step time: 3.784
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    557/  2110 | global iter:    557/  2110 | loss: 0.1265 | ds_loss: 0.0000 | lr: 4.1901e-05 | scale:     1.0000 | micro time: 3.763 | step time: 0.000
train | epoch   2 | Iter:    558/  2110 | global iter:    558/  2110 | loss: 0.1095 | ds_loss: 0.0000 | lr: 4.1873e-05 | scale:     1.0000 | micro time: 3.784 | step time: 0.000
train | epoch   2 | Iter:    559/  2110 | global iter:    559/  2110 | loss: 0.2875 | ds_loss: 0.0000 | lr: 4.1846e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   2 | Iter:    560/  2110 | global iter:    560/  2110 | loss: 0.1537 | ds_loss: 0.0000 | lr: 4.1818e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    560/  2110 | global iter:    560/  2110 | loss: 0.1693 | ds_loss: 0.0000 | lr: 4.1818e-05 | scale:     1.0000 | micro time: 3.798 | step time: 3.788
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    561/  2110 | global iter:    561/  2110 | loss: 0.1200 | ds_loss: 0.0000 | lr: 4.1791e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   2 | Iter:    562/  2110 | global iter:    562/  2110 | loss: 0.2046 | ds_loss: 0.0000 | lr: 4.1763e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   2 | Iter:    563/  2110 | global iter:    563/  2110 | loss: 0.3082 | ds_loss: 0.0000 | lr: 4.1736e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   2 | Iter:    564/  2110 | global iter:    564/  2110 | loss: 0.2507 | ds_loss: 0.0000 | lr: 4.1708e-05 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    564/  2110 | global iter:    564/  2110 | loss: 0.2209 | ds_loss: 0.0000 | lr: 4.1708e-05 | scale:     1.0000 | micro time: 3.775 | step time: 3.780
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    565/  2110 | global iter:    565/  2110 | loss: 0.1882 | ds_loss: 0.0000 | lr: 4.1680e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   2 | Iter:    566/  2110 | global iter:    566/  2110 | loss: 0.3040 | ds_loss: 0.0000 | lr: 4.1653e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   2 | Iter:    567/  2110 | global iter:    567/  2110 | loss: 0.2044 | ds_loss: 0.0000 | lr: 4.1625e-05 | scale:     1.0000 | micro time: 3.804 | step time: 0.000
train | epoch   2 | Iter:    568/  2110 | global iter:    568/  2110 | loss: 0.3329 | ds_loss: 0.0000 | lr: 4.1597e-05 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    568/  2110 | global iter:    568/  2110 | loss: 0.2574 | ds_loss: 0.0000 | lr: 4.1597e-05 | scale:     1.0000 | micro time: 3.793 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    569/  2110 | global iter:    569/  2110 | loss: 0.1834 | ds_loss: 0.0000 | lr: 4.1569e-05 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   2 | Iter:    570/  2110 | global iter:    570/  2110 | loss: 0.3017 | ds_loss: 0.0000 | lr: 4.1541e-05 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   2 | Iter:    571/  2110 | global iter:    571/  2110 | loss: 0.2253 | ds_loss: 0.0000 | lr: 4.1513e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   2 | Iter:    572/  2110 | global iter:    572/  2110 | loss: 0.3441 | ds_loss: 0.0000 | lr: 4.1486e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    572/  2110 | global iter:    572/  2110 | loss: 0.2637 | ds_loss: 0.0000 | lr: 4.1486e-05 | scale:     1.0000 | micro time: 3.806 | step time: 3.788
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    573/  2110 | global iter:    573/  2110 | loss: 0.1279 | ds_loss: 0.0000 | lr: 4.1458e-05 | scale:     1.0000 | micro time: 3.777 | step time: 0.000
train | epoch   2 | Iter:    574/  2110 | global iter:    574/  2110 | loss: 0.1933 | ds_loss: 0.0000 | lr: 4.1430e-05 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   2 | Iter:    575/  2110 | global iter:    575/  2110 | loss: 0.1938 | ds_loss: 0.0000 | lr: 4.1402e-05 | scale:     1.0000 | micro time: 3.796 | step time: 0.000
train | epoch   2 | Iter:    576/  2110 | global iter:    576/  2110 | loss: 0.1658 | ds_loss: 0.0000 | lr: 4.1373e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    576/  2110 | global iter:    576/  2110 | loss: 0.1702 | ds_loss: 0.0000 | lr: 4.1373e-05 | scale:     1.0000 | micro time: 3.782 | step time: 3.783
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    577/  2110 | global iter:    577/  2110 | loss: 0.2260 | ds_loss: 0.0000 | lr: 4.1345e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   2 | Iter:    578/  2110 | global iter:    578/  2110 | loss: 0.2146 | ds_loss: 0.0000 | lr: 4.1317e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   2 | Iter:    579/  2110 | global iter:    579/  2110 | loss: 0.2387 | ds_loss: 0.0000 | lr: 4.1289e-05 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
train | epoch   2 | Iter:    580/  2110 | global iter:    580/  2110 | loss: 0.1854 | ds_loss: 0.0000 | lr: 4.1261e-05 | scale:     1.0000 | micro time: 3.773 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    580/  2110 | global iter:    580/  2110 | loss: 0.2162 | ds_loss: 0.0000 | lr: 4.1261e-05 | scale:     1.0000 | micro time: 3.773 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    581/  2110 | global iter:    581/  2110 | loss: 0.1091 | ds_loss: 0.0000 | lr: 4.1233e-05 | scale:     1.0000 | micro time: 3.819 | step time: 0.000
train | epoch   2 | Iter:    582/  2110 | global iter:    582/  2110 | loss: 0.1666 | ds_loss: 0.0000 | lr: 4.1204e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   2 | Iter:    583/  2110 | global iter:    583/  2110 | loss: 0.2577 | ds_loss: 0.0000 | lr: 4.1176e-05 | scale:     1.0000 | micro time: 3.784 | step time: 0.000
train | epoch   2 | Iter:    584/  2110 | global iter:    584/  2110 | loss: 0.1718 | ds_loss: 0.0000 | lr: 4.1148e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    584/  2110 | global iter:    584/  2110 | loss: 0.1763 | ds_loss: 0.0000 | lr: 4.1148e-05 | scale:     1.0000 | micro time: 3.810 | step time: 3.799
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    585/  2110 | global iter:    585/  2110 | loss: 0.1573 | ds_loss: 0.0000 | lr: 4.1119e-05 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   2 | Iter:    586/  2110 | global iter:    586/  2110 | loss: 0.3004 | ds_loss: 0.0000 | lr: 4.1091e-05 | scale:     1.0000 | micro time: 3.822 | step time: 0.000
train | epoch   2 | Iter:    587/  2110 | global iter:    587/  2110 | loss: 0.2280 | ds_loss: 0.0000 | lr: 4.1062e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   2 | Iter:    588/  2110 | global iter:    588/  2110 | loss: 0.1804 | ds_loss: 0.0000 | lr: 4.1034e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    588/  2110 | global iter:    588/  2110 | loss: 0.2165 | ds_loss: 0.0000 | lr: 4.1034e-05 | scale:     1.0000 | micro time: 3.806 | step time: 3.799
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    589/  2110 | global iter:    589/  2110 | loss: 0.3566 | ds_loss: 0.0000 | lr: 4.1005e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   2 | Iter:    590/  2110 | global iter:    590/  2110 | loss: 0.2390 | ds_loss: 0.0000 | lr: 4.0977e-05 | scale:     1.0000 | micro time: 3.814 | step time: 0.000
train | epoch   2 | Iter:    591/  2110 | global iter:    591/  2110 | loss: 0.1916 | ds_loss: 0.0000 | lr: 4.0948e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   2 | Iter:    592/  2110 | global iter:    592/  2110 | loss: 0.1880 | ds_loss: 0.0000 | lr: 4.0919e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    592/  2110 | global iter:    592/  2110 | loss: 0.2438 | ds_loss: 0.0000 | lr: 4.0919e-05 | scale:     1.0000 | micro time: 3.798 | step time: 3.801
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    593/  2110 | global iter:    593/  2110 | loss: 0.3098 | ds_loss: 0.0000 | lr: 4.0891e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   2 | Iter:    594/  2110 | global iter:    594/  2110 | loss: 0.1314 | ds_loss: 0.0000 | lr: 4.0862e-05 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   2 | Iter:    595/  2110 | global iter:    595/  2110 | loss: 0.2492 | ds_loss: 0.0000 | lr: 4.0833e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   2 | Iter:    596/  2110 | global iter:    596/  2110 | loss: 0.1484 | ds_loss: 0.0000 | lr: 4.0804e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    596/  2110 | global iter:    596/  2110 | loss: 0.2097 | ds_loss: 0.0000 | lr: 4.0804e-05 | scale:     1.0000 | micro time: 3.801 | step time: 3.790
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    597/  2110 | global iter:    597/  2110 | loss: 0.2289 | ds_loss: 0.0000 | lr: 4.0776e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   2 | Iter:    598/  2110 | global iter:    598/  2110 | loss: 0.1558 | ds_loss: 0.0000 | lr: 4.0747e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   2 | Iter:    599/  2110 | global iter:    599/  2110 | loss: 0.1622 | ds_loss: 0.0000 | lr: 4.0718e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   2 | Iter:    600/  2110 | global iter:    600/  2110 | loss: 0.2189 | ds_loss: 0.0000 | lr: 4.0689e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    600/  2110 | global iter:    600/  2110 | loss: 0.1914 | ds_loss: 0.0000 | lr: 4.0689e-05 | scale:     1.0000 | micro time: 3.794 | step time: 3.793
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    601/  2110 | global iter:    601/  2110 | loss: 0.1931 | ds_loss: 0.0000 | lr: 4.0660e-05 | scale:     1.0000 | micro time: 3.774 | step time: 0.000
train | epoch   2 | Iter:    602/  2110 | global iter:    602/  2110 | loss: 0.0748 | ds_loss: 0.0000 | lr: 4.0631e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   2 | Iter:    603/  2110 | global iter:    603/  2110 | loss: 0.1594 | ds_loss: 0.0000 | lr: 4.0602e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   2 | Iter:    604/  2110 | global iter:    604/  2110 | loss: 0.2777 | ds_loss: 0.0000 | lr: 4.0573e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    604/  2110 | global iter:    604/  2110 | loss: 0.1762 | ds_loss: 0.0000 | lr: 4.0573e-05 | scale:     1.0000 | micro time: 3.802 | step time: 3.790
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    605/  2110 | global iter:    605/  2110 | loss: 0.1464 | ds_loss: 0.0000 | lr: 4.0544e-05 | scale:     1.0000 | micro time: 3.820 | step time: 0.000
train | epoch   2 | Iter:    606/  2110 | global iter:    606/  2110 | loss: 0.2414 | ds_loss: 0.0000 | lr: 4.0515e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   2 | Iter:    607/  2110 | global iter:    607/  2110 | loss: 0.1892 | ds_loss: 0.0000 | lr: 4.0485e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   2 | Iter:    608/  2110 | global iter:    608/  2110 | loss: 0.1829 | ds_loss: 0.0000 | lr: 4.0456e-05 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    608/  2110 | global iter:    608/  2110 | loss: 0.1900 | ds_loss: 0.0000 | lr: 4.0456e-05 | scale:     1.0000 | micro time: 3.793 | step time: 3.799
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    609/  2110 | global iter:    609/  2110 | loss: 0.1872 | ds_loss: 0.0000 | lr: 4.0427e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   2 | Iter:    610/  2110 | global iter:    610/  2110 | loss: 0.2263 | ds_loss: 0.0000 | lr: 4.0398e-05 | scale:     1.0000 | micro time: 3.805 | step time: 0.000
train | epoch   2 | Iter:    611/  2110 | global iter:    611/  2110 | loss: 0.1560 | ds_loss: 0.0000 | lr: 4.0368e-05 | scale:     1.0000 | micro time: 3.767 | step time: 0.000
train | epoch   2 | Iter:    612/  2110 | global iter:    612/  2110 | loss: 0.2882 | ds_loss: 0.0000 | lr: 4.0339e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    612/  2110 | global iter:    612/  2110 | loss: 0.2144 | ds_loss: 0.0000 | lr: 4.0339e-05 | scale:     1.0000 | micro time: 3.794 | step time: 3.787
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    613/  2110 | global iter:    613/  2110 | loss: 0.1744 | ds_loss: 0.0000 | lr: 4.0310e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   2 | Iter:    614/  2110 | global iter:    614/  2110 | loss: 0.1398 | ds_loss: 0.0000 | lr: 4.0280e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   2 | Iter:    615/  2110 | global iter:    615/  2110 | loss: 0.1741 | ds_loss: 0.0000 | lr: 4.0251e-05 | scale:     1.0000 | micro time: 3.814 | step time: 0.000
train | epoch   2 | Iter:    616/  2110 | global iter:    616/  2110 | loss: 0.1354 | ds_loss: 0.0000 | lr: 4.0221e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    616/  2110 | global iter:    616/  2110 | loss: 0.1559 | ds_loss: 0.0000 | lr: 4.0221e-05 | scale:     1.0000 | micro time: 3.782 | step time: 3.798
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    617/  2110 | global iter:    617/  2110 | loss: 0.1863 | ds_loss: 0.0000 | lr: 4.0192e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   2 | Iter:    618/  2110 | global iter:    618/  2110 | loss: 0.1173 | ds_loss: 0.0000 | lr: 4.0162e-05 | scale:     1.0000 | micro time: 3.754 | step time: 0.000
train | epoch   2 | Iter:    619/  2110 | global iter:    619/  2110 | loss: 0.2649 | ds_loss: 0.0000 | lr: 4.0133e-05 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   2 | Iter:    620/  2110 | global iter:    620/  2110 | loss: 0.2568 | ds_loss: 0.0000 | lr: 4.0103e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    620/  2110 | global iter:    620/  2110 | loss: 0.2063 | ds_loss: 0.0000 | lr: 4.0103e-05 | scale:     1.0000 | micro time: 3.802 | step time: 3.781
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    621/  2110 | global iter:    621/  2110 | loss: 0.3191 | ds_loss: 0.0000 | lr: 4.0074e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   2 | Iter:    622/  2110 | global iter:    622/  2110 | loss: 0.2560 | ds_loss: 0.0000 | lr: 4.0044e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   2 | Iter:    623/  2110 | global iter:    623/  2110 | loss: 0.1633 | ds_loss: 0.0000 | lr: 4.0014e-05 | scale:     1.0000 | micro time: 3.808 | step time: 0.000
train | epoch   2 | Iter:    624/  2110 | global iter:    624/  2110 | loss: 0.1732 | ds_loss: 0.0000 | lr: 3.9984e-05 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    624/  2110 | global iter:    624/  2110 | loss: 0.2279 | ds_loss: 0.0000 | lr: 3.9984e-05 | scale:     1.0000 | micro time: 3.789 | step time: 3.800
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    625/  2110 | global iter:    625/  2110 | loss: 0.1485 | ds_loss: 0.0000 | lr: 3.9955e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   2 | Iter:    626/  2110 | global iter:    626/  2110 | loss: 0.1346 | ds_loss: 0.0000 | lr: 3.9925e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   2 | Iter:    627/  2110 | global iter:    627/  2110 | loss: 0.2345 | ds_loss: 0.0000 | lr: 3.9895e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   2 | Iter:    628/  2110 | global iter:    628/  2110 | loss: 0.1821 | ds_loss: 0.0000 | lr: 3.9865e-05 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    628/  2110 | global iter:    628/  2110 | loss: 0.1749 | ds_loss: 0.0000 | lr: 3.9865e-05 | scale:     1.0000 | micro time: 3.789 | step time: 3.789
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    629/  2110 | global iter:    629/  2110 | loss: 0.2242 | ds_loss: 0.0000 | lr: 3.9835e-05 | scale:     1.0000 | micro time: 3.772 | step time: 0.000
train | epoch   2 | Iter:    630/  2110 | global iter:    630/  2110 | loss: 0.3050 | ds_loss: 0.0000 | lr: 3.9805e-05 | scale:     1.0000 | micro time: 3.833 | step time: 0.000
train | epoch   2 | Iter:    631/  2110 | global iter:    631/  2110 | loss: 0.2772 | ds_loss: 0.0000 | lr: 3.9775e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   2 | Iter:    632/  2110 | global iter:    632/  2110 | loss: 0.1651 | ds_loss: 0.0000 | lr: 3.9745e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    632/  2110 | global iter:    632/  2110 | loss: 0.2429 | ds_loss: 0.0000 | lr: 3.9745e-05 | scale:     1.0000 | micro time: 3.802 | step time: 3.799
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:    633/  2110 | global iter:    633/  2110 | loss: 0.1467 | ds_loss: 0.0000 | lr: 3.9715e-05 | scale:     1.0000 | micro time: 3.809 | step time: 0.000
train | epoch   2 | Iter:    634/  2110 | global iter:    634/  2110 | loss: 0.2561 | ds_loss: 0.0000 | lr: 3.9685e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   2 | Iter:    635/  2110 | global iter:    635/  2110 | loss: 0.3591 | ds_loss: 0.0000 | lr: 3.9655e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   2 | Iter:    636/  2110 | global iter:    636/  2110 | loss: 0.1771 | ds_loss: 0.0000 | lr: 3.9625e-05 | scale:     1.0000 | micro time: 3.334 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:    636/  2110 | global iter:    636/  2110 | loss: 0.2347 | ds_loss: 0.0000 | lr: 3.9625e-05 | scale:     1.0000 | micro time: 3.334 | step time: 3.679
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
Tue Apr  8 07:45:45 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   63C    P0             55W /  250W |   29814MiB /  32768MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |     50%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   49C    P0             48W /  250W |   29848MiB /  32768MiB |      8%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   54C    P0             49W /  250W |   28312MiB /  32768MiB |     83%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 07:45:45 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   63C    P0             55W /  250W |   29814MiB /  32768MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |     50%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   49C    P0             48W /  250W |   29848MiB /  32768MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   54C    P0             49W /  250W |   28312MiB /  32768MiB |     83%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 07:45:45 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   63C    P0             55W /  250W |   29814MiB /  32768MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |     50%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   49C    P0             48W /  250W |   29848MiB /  32768MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   54C    P0             49W /  250W |   28312MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 07:45:45 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   63C    P0             55W /  250W |   29814MiB /  32768MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |     50%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   49C    P0             49W /  250W |   29848MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   54C    P0             49W /  250W |   28312MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

train | epoch   3 | Iter:    637/  2110 | global iter:    637/  2110 | loss: 0.1036 | ds_loss: 0.0000 | lr: 3.9595e-05 | scale:     1.0000 | micro time: 4.210 | step time: 0.000
train | epoch   3 | Iter:    638/  2110 | global iter:    638/  2110 | loss: 0.1068 | ds_loss: 0.0000 | lr: 3.9565e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   3 | Iter:    639/  2110 | global iter:    639/  2110 | loss: 0.1200 | ds_loss: 0.0000 | lr: 3.9534e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   3 | Iter:    640/  2110 | global iter:    640/  2110 | loss: 0.1012 | ds_loss: 0.0000 | lr: 3.9504e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    640/  2110 | global iter:    640/  2110 | loss: 0.1079 | ds_loss: 0.0000 | lr: 3.9504e-05 | scale:     1.0000 | micro time: 3.802 | step time: 3.903
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    641/  2110 | global iter:    641/  2110 | loss: 0.2143 | ds_loss: 0.0000 | lr: 3.9474e-05 | scale:     1.0000 | micro time: 3.780 | step time: 0.000
train | epoch   3 | Iter:    642/  2110 | global iter:    642/  2110 | loss: 0.1308 | ds_loss: 0.0000 | lr: 3.9443e-05 | scale:     1.0000 | micro time: 3.774 | step time: 0.000
train | epoch   3 | Iter:    643/  2110 | global iter:    643/  2110 | loss: 0.1560 | ds_loss: 0.0000 | lr: 3.9413e-05 | scale:     1.0000 | micro time: 3.784 | step time: 0.000
train | epoch   3 | Iter:    644/  2110 | global iter:    644/  2110 | loss: 0.1090 | ds_loss: 0.0000 | lr: 3.9383e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    644/  2110 | global iter:    644/  2110 | loss: 0.1525 | ds_loss: 0.0000 | lr: 3.9383e-05 | scale:     1.0000 | micro time: 3.785 | step time: 3.781
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    645/  2110 | global iter:    645/  2110 | loss: 0.1470 | ds_loss: 0.0000 | lr: 3.9352e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   3 | Iter:    646/  2110 | global iter:    646/  2110 | loss: 0.1619 | ds_loss: 0.0000 | lr: 3.9322e-05 | scale:     1.0000 | micro time: 3.784 | step time: 0.000
train | epoch   3 | Iter:    647/  2110 | global iter:    647/  2110 | loss: 0.1184 | ds_loss: 0.0000 | lr: 3.9291e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   3 | Iter:    648/  2110 | global iter:    648/  2110 | loss: 0.0353 | ds_loss: 0.0000 | lr: 3.9261e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    648/  2110 | global iter:    648/  2110 | loss: 0.1156 | ds_loss: 0.0000 | lr: 3.9261e-05 | scale:     1.0000 | micro time: 3.802 | step time: 3.790
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    649/  2110 | global iter:    649/  2110 | loss: 0.1956 | ds_loss: 0.0000 | lr: 3.9230e-05 | scale:     1.0000 | micro time: 3.821 | step time: 0.000
train | epoch   3 | Iter:    650/  2110 | global iter:    650/  2110 | loss: 0.1725 | ds_loss: 0.0000 | lr: 3.9200e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   3 | Iter:    651/  2110 | global iter:    651/  2110 | loss: 0.1568 | ds_loss: 0.0000 | lr: 3.9169e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   3 | Iter:    652/  2110 | global iter:    652/  2110 | loss: 0.0842 | ds_loss: 0.0000 | lr: 3.9138e-05 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    652/  2110 | global iter:    652/  2110 | loss: 0.1523 | ds_loss: 0.0000 | lr: 3.9138e-05 | scale:     1.0000 | micro time: 3.778 | step time: 3.798
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    653/  2110 | global iter:    653/  2110 | loss: 0.1635 | ds_loss: 0.0000 | lr: 3.9108e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   3 | Iter:    654/  2110 | global iter:    654/  2110 | loss: 0.1255 | ds_loss: 0.0000 | lr: 3.9077e-05 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
train | epoch   3 | Iter:    655/  2110 | global iter:    655/  2110 | loss: 0.2069 | ds_loss: 0.0000 | lr: 3.9046e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   3 | Iter:    656/  2110 | global iter:    656/  2110 | loss: 0.0914 | ds_loss: 0.0000 | lr: 3.9016e-05 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    656/  2110 | global iter:    656/  2110 | loss: 0.1468 | ds_loss: 0.0000 | lr: 3.9016e-05 | scale:     1.0000 | micro time: 3.793 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    657/  2110 | global iter:    657/  2110 | loss: 0.0991 | ds_loss: 0.0000 | lr: 3.8985e-05 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   3 | Iter:    658/  2110 | global iter:    658/  2110 | loss: 0.1486 | ds_loss: 0.0000 | lr: 3.8954e-05 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
train | epoch   3 | Iter:    659/  2110 | global iter:    659/  2110 | loss: 0.2024 | ds_loss: 0.0000 | lr: 3.8923e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   3 | Iter:    660/  2110 | global iter:    660/  2110 | loss: 0.1038 | ds_loss: 0.0000 | lr: 3.8892e-05 | scale:     1.0000 | micro time: 3.814 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    660/  2110 | global iter:    660/  2110 | loss: 0.1385 | ds_loss: 0.0000 | lr: 3.8892e-05 | scale:     1.0000 | micro time: 3.814 | step time: 3.800
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    661/  2110 | global iter:    661/  2110 | loss: 0.1025 | ds_loss: 0.0000 | lr: 3.8861e-05 | scale:     1.0000 | micro time: 3.780 | step time: 0.000
train | epoch   3 | Iter:    662/  2110 | global iter:    662/  2110 | loss: 0.1645 | ds_loss: 0.0000 | lr: 3.8830e-05 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
train | epoch   3 | Iter:    663/  2110 | global iter:    663/  2110 | loss: 0.2041 | ds_loss: 0.0000 | lr: 3.8799e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   3 | Iter:    664/  2110 | global iter:    664/  2110 | loss: 0.2472 | ds_loss: 0.0000 | lr: 3.8768e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    664/  2110 | global iter:    664/  2110 | loss: 0.1796 | ds_loss: 0.0000 | lr: 3.8768e-05 | scale:     1.0000 | micro time: 3.810 | step time: 3.797
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    665/  2110 | global iter:    665/  2110 | loss: 0.0822 | ds_loss: 0.0000 | lr: 3.8737e-05 | scale:     1.0000 | micro time: 3.769 | step time: 0.000
train | epoch   3 | Iter:    666/  2110 | global iter:    666/  2110 | loss: 0.1588 | ds_loss: 0.0000 | lr: 3.8706e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   3 | Iter:    667/  2110 | global iter:    667/  2110 | loss: 0.1211 | ds_loss: 0.0000 | lr: 3.8675e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   3 | Iter:    668/  2110 | global iter:    668/  2110 | loss: 0.0665 | ds_loss: 0.0000 | lr: 3.8644e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    668/  2110 | global iter:    668/  2110 | loss: 0.1072 | ds_loss: 0.0000 | lr: 3.8644e-05 | scale:     1.0000 | micro time: 3.782 | step time: 3.786
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    669/  2110 | global iter:    669/  2110 | loss: 0.0761 | ds_loss: 0.0000 | lr: 3.8613e-05 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
train | epoch   3 | Iter:    670/  2110 | global iter:    670/  2110 | loss: 0.1818 | ds_loss: 0.0000 | lr: 3.8582e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   3 | Iter:    671/  2110 | global iter:    671/  2110 | loss: 0.1190 | ds_loss: 0.0000 | lr: 3.8550e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   3 | Iter:    672/  2110 | global iter:    672/  2110 | loss: 0.1590 | ds_loss: 0.0000 | lr: 3.8519e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    672/  2110 | global iter:    672/  2110 | loss: 0.1340 | ds_loss: 0.0000 | lr: 3.8519e-05 | scale:     1.0000 | micro time: 3.801 | step time: 3.799
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    673/  2110 | global iter:    673/  2110 | loss: 0.0860 | ds_loss: 0.0000 | lr: 3.8488e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   3 | Iter:    674/  2110 | global iter:    674/  2110 | loss: 0.1348 | ds_loss: 0.0000 | lr: 3.8456e-05 | scale:     1.0000 | micro time: 3.805 | step time: 0.000
train | epoch   3 | Iter:    675/  2110 | global iter:    675/  2110 | loss: 0.0937 | ds_loss: 0.0000 | lr: 3.8425e-05 | scale:     1.0000 | micro time: 3.822 | step time: 0.000
train | epoch   3 | Iter:    676/  2110 | global iter:    676/  2110 | loss: 0.1167 | ds_loss: 0.0000 | lr: 3.8394e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    676/  2110 | global iter:    676/  2110 | loss: 0.1078 | ds_loss: 0.0000 | lr: 3.8394e-05 | scale:     1.0000 | micro time: 3.794 | step time: 3.805
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    677/  2110 | global iter:    677/  2110 | loss: 0.1427 | ds_loss: 0.0000 | lr: 3.8362e-05 | scale:     1.0000 | micro time: 3.805 | step time: 0.000
train | epoch   3 | Iter:    678/  2110 | global iter:    678/  2110 | loss: 0.1609 | ds_loss: 0.0000 | lr: 3.8331e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   3 | Iter:    679/  2110 | global iter:    679/  2110 | loss: 0.0723 | ds_loss: 0.0000 | lr: 3.8299e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   3 | Iter:    680/  2110 | global iter:    680/  2110 | loss: 0.0796 | ds_loss: 0.0000 | lr: 3.8268e-05 | scale:     1.0000 | micro time: 3.767 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    680/  2110 | global iter:    680/  2110 | loss: 0.1139 | ds_loss: 0.0000 | lr: 3.8268e-05 | scale:     1.0000 | micro time: 3.767 | step time: 3.792
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    681/  2110 | global iter:    681/  2110 | loss: 0.1009 | ds_loss: 0.0000 | lr: 3.8236e-05 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
train | epoch   3 | Iter:    682/  2110 | global iter:    682/  2110 | loss: 0.1447 | ds_loss: 0.0000 | lr: 3.8205e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   3 | Iter:    683/  2110 | global iter:    683/  2110 | loss: 0.1150 | ds_loss: 0.0000 | lr: 3.8173e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   3 | Iter:    684/  2110 | global iter:    684/  2110 | loss: 0.0704 | ds_loss: 0.0000 | lr: 3.8142e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    684/  2110 | global iter:    684/  2110 | loss: 0.1077 | ds_loss: 0.0000 | lr: 3.8142e-05 | scale:     1.0000 | micro time: 3.790 | step time: 3.790
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    685/  2110 | global iter:    685/  2110 | loss: 0.1462 | ds_loss: 0.0000 | lr: 3.8110e-05 | scale:     1.0000 | micro time: 3.765 | step time: 0.000
train | epoch   3 | Iter:    686/  2110 | global iter:    686/  2110 | loss: 0.1559 | ds_loss: 0.0000 | lr: 3.8078e-05 | scale:     1.0000 | micro time: 3.814 | step time: 0.000
train | epoch   3 | Iter:    687/  2110 | global iter:    687/  2110 | loss: 0.1434 | ds_loss: 0.0000 | lr: 3.8047e-05 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   3 | Iter:    688/  2110 | global iter:    688/  2110 | loss: 0.1622 | ds_loss: 0.0000 | lr: 3.8015e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    688/  2110 | global iter:    688/  2110 | loss: 0.1519 | ds_loss: 0.0000 | lr: 3.8015e-05 | scale:     1.0000 | micro time: 3.806 | step time: 3.790
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    689/  2110 | global iter:    689/  2110 | loss: 0.0385 | ds_loss: 0.0000 | lr: 3.7983e-05 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
train | epoch   3 | Iter:    690/  2110 | global iter:    690/  2110 | loss: 0.0806 | ds_loss: 0.0000 | lr: 3.7951e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   3 | Iter:    691/  2110 | global iter:    691/  2110 | loss: 0.1047 | ds_loss: 0.0000 | lr: 3.7920e-05 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   3 | Iter:    692/  2110 | global iter:    692/  2110 | loss: 0.0942 | ds_loss: 0.0000 | lr: 3.7888e-05 | scale:     1.0000 | micro time: 3.811 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    692/  2110 | global iter:    692/  2110 | loss: 0.0795 | ds_loss: 0.0000 | lr: 3.7888e-05 | scale:     1.0000 | micro time: 3.811 | step time: 3.797
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    693/  2110 | global iter:    693/  2110 | loss: 0.2038 | ds_loss: 0.0000 | lr: 3.7856e-05 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   3 | Iter:    694/  2110 | global iter:    694/  2110 | loss: 0.1186 | ds_loss: 0.0000 | lr: 3.7824e-05 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   3 | Iter:    695/  2110 | global iter:    695/  2110 | loss: 0.0738 | ds_loss: 0.0000 | lr: 3.7792e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   3 | Iter:    696/  2110 | global iter:    696/  2110 | loss: 0.2084 | ds_loss: 0.0000 | lr: 3.7760e-05 | scale:     1.0000 | micro time: 3.769 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    696/  2110 | global iter:    696/  2110 | loss: 0.1512 | ds_loss: 0.0000 | lr: 3.7760e-05 | scale:     1.0000 | micro time: 3.769 | step time: 3.782
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    697/  2110 | global iter:    697/  2110 | loss: 0.1376 | ds_loss: 0.0000 | lr: 3.7728e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   3 | Iter:    698/  2110 | global iter:    698/  2110 | loss: 0.1461 | ds_loss: 0.0000 | lr: 3.7696e-05 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   3 | Iter:    699/  2110 | global iter:    699/  2110 | loss: 0.1169 | ds_loss: 0.0000 | lr: 3.7664e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   3 | Iter:    700/  2110 | global iter:    700/  2110 | loss: 0.1873 | ds_loss: 0.0000 | lr: 3.7632e-05 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    700/  2110 | global iter:    700/  2110 | loss: 0.1470 | ds_loss: 0.0000 | lr: 3.7632e-05 | scale:     1.0000 | micro time: 3.778 | step time: 3.787
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    701/  2110 | global iter:    701/  2110 | loss: 0.2083 | ds_loss: 0.0000 | lr: 3.7600e-05 | scale:     1.0000 | micro time: 3.800 | step time: 0.000
train | epoch   3 | Iter:    702/  2110 | global iter:    702/  2110 | loss: 0.1551 | ds_loss: 0.0000 | lr: 3.7568e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   3 | Iter:    703/  2110 | global iter:    703/  2110 | loss: 0.1696 | ds_loss: 0.0000 | lr: 3.7536e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
train | epoch   3 | Iter:    704/  2110 | global iter:    704/  2110 | loss: 0.0570 | ds_loss: 0.0000 | lr: 3.7504e-05 | scale:     1.0000 | micro time: 3.770 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    704/  2110 | global iter:    704/  2110 | loss: 0.1475 | ds_loss: 0.0000 | lr: 3.7504e-05 | scale:     1.0000 | micro time: 3.770 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    705/  2110 | global iter:    705/  2110 | loss: 0.1333 | ds_loss: 0.0000 | lr: 3.7471e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   3 | Iter:    706/  2110 | global iter:    706/  2110 | loss: 0.0732 | ds_loss: 0.0000 | lr: 3.7439e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   3 | Iter:    707/  2110 | global iter:    707/  2110 | loss: 0.1457 | ds_loss: 0.0000 | lr: 3.7407e-05 | scale:     1.0000 | micro time: 3.816 | step time: 0.000
train | epoch   3 | Iter:    708/  2110 | global iter:    708/  2110 | loss: 0.1093 | ds_loss: 0.0000 | lr: 3.7375e-05 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    708/  2110 | global iter:    708/  2110 | loss: 0.1154 | ds_loss: 0.0000 | lr: 3.7375e-05 | scale:     1.0000 | micro time: 3.781 | step time: 3.795
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    709/  2110 | global iter:    709/  2110 | loss: 0.1765 | ds_loss: 0.0000 | lr: 3.7342e-05 | scale:     1.0000 | micro time: 3.776 | step time: 0.000
train | epoch   3 | Iter:    710/  2110 | global iter:    710/  2110 | loss: 0.0957 | ds_loss: 0.0000 | lr: 3.7310e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   3 | Iter:    711/  2110 | global iter:    711/  2110 | loss: 0.2149 | ds_loss: 0.0000 | lr: 3.7278e-05 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   3 | Iter:    712/  2110 | global iter:    712/  2110 | loss: 0.2182 | ds_loss: 0.0000 | lr: 3.7245e-05 | scale:     1.0000 | micro time: 3.814 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    712/  2110 | global iter:    712/  2110 | loss: 0.1763 | ds_loss: 0.0000 | lr: 3.7245e-05 | scale:     1.0000 | micro time: 3.814 | step time: 3.792
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    713/  2110 | global iter:    713/  2110 | loss: 0.0851 | ds_loss: 0.0000 | lr: 3.7213e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   3 | Iter:    714/  2110 | global iter:    714/  2110 | loss: 0.1497 | ds_loss: 0.0000 | lr: 3.7180e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   3 | Iter:    715/  2110 | global iter:    715/  2110 | loss: 0.0855 | ds_loss: 0.0000 | lr: 3.7148e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   3 | Iter:    716/  2110 | global iter:    716/  2110 | loss: 0.1469 | ds_loss: 0.0000 | lr: 3.7115e-05 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    716/  2110 | global iter:    716/  2110 | loss: 0.1168 | ds_loss: 0.0000 | lr: 3.7115e-05 | scale:     1.0000 | micro time: 3.783 | step time: 3.793
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    717/  2110 | global iter:    717/  2110 | loss: 0.1063 | ds_loss: 0.0000 | lr: 3.7083e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   3 | Iter:    718/  2110 | global iter:    718/  2110 | loss: 0.0892 | ds_loss: 0.0000 | lr: 3.7050e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   3 | Iter:    719/  2110 | global iter:    719/  2110 | loss: 0.0892 | ds_loss: 0.0000 | lr: 3.7018e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   3 | Iter:    720/  2110 | global iter:    720/  2110 | loss: 0.1446 | ds_loss: 0.0000 | lr: 3.6985e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    720/  2110 | global iter:    720/  2110 | loss: 0.1073 | ds_loss: 0.0000 | lr: 3.6985e-05 | scale:     1.0000 | micro time: 3.802 | step time: 3.805
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    721/  2110 | global iter:    721/  2110 | loss: 0.1367 | ds_loss: 0.0000 | lr: 3.6952e-05 | scale:     1.0000 | micro time: 3.819 | step time: 0.000
train | epoch   3 | Iter:    722/  2110 | global iter:    722/  2110 | loss: 0.1035 | ds_loss: 0.0000 | lr: 3.6920e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
train | epoch   3 | Iter:    723/  2110 | global iter:    723/  2110 | loss: 0.0995 | ds_loss: 0.0000 | lr: 3.6887e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   3 | Iter:    724/  2110 | global iter:    724/  2110 | loss: 0.0908 | ds_loss: 0.0000 | lr: 3.6854e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    724/  2110 | global iter:    724/  2110 | loss: 0.1076 | ds_loss: 0.0000 | lr: 3.6854e-05 | scale:     1.0000 | micro time: 3.794 | step time: 3.808
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    725/  2110 | global iter:    725/  2110 | loss: 0.1135 | ds_loss: 0.0000 | lr: 3.6822e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   3 | Iter:    726/  2110 | global iter:    726/  2110 | loss: 0.1060 | ds_loss: 0.0000 | lr: 3.6789e-05 | scale:     1.0000 | micro time: 3.774 | step time: 0.000
train | epoch   3 | Iter:    727/  2110 | global iter:    727/  2110 | loss: 0.0678 | ds_loss: 0.0000 | lr: 3.6756e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   3 | Iter:    728/  2110 | global iter:    728/  2110 | loss: 0.1573 | ds_loss: 0.0000 | lr: 3.6723e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    728/  2110 | global iter:    728/  2110 | loss: 0.1112 | ds_loss: 0.0000 | lr: 3.6723e-05 | scale:     1.0000 | micro time: 3.798 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    729/  2110 | global iter:    729/  2110 | loss: 0.1048 | ds_loss: 0.0000 | lr: 3.6690e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   3 | Iter:    730/  2110 | global iter:    730/  2110 | loss: 0.1327 | ds_loss: 0.0000 | lr: 3.6657e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   3 | Iter:    731/  2110 | global iter:    731/  2110 | loss: 0.1077 | ds_loss: 0.0000 | lr: 3.6625e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
train | epoch   3 | Iter:    732/  2110 | global iter:    732/  2110 | loss: 0.1652 | ds_loss: 0.0000 | lr: 3.6592e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    732/  2110 | global iter:    732/  2110 | loss: 0.1276 | ds_loss: 0.0000 | lr: 3.6592e-05 | scale:     1.0000 | micro time: 3.771 | step time: 3.795
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    733/  2110 | global iter:    733/  2110 | loss: 0.1795 | ds_loss: 0.0000 | lr: 3.6559e-05 | scale:     1.0000 | micro time: 3.816 | step time: 0.000
train | epoch   3 | Iter:    734/  2110 | global iter:    734/  2110 | loss: 0.0749 | ds_loss: 0.0000 | lr: 3.6526e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   3 | Iter:    735/  2110 | global iter:    735/  2110 | loss: 0.1190 | ds_loss: 0.0000 | lr: 3.6493e-05 | scale:     1.0000 | micro time: 3.814 | step time: 0.000
train | epoch   3 | Iter:    736/  2110 | global iter:    736/  2110 | loss: 0.1552 | ds_loss: 0.0000 | lr: 3.6460e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    736/  2110 | global iter:    736/  2110 | loss: 0.1322 | ds_loss: 0.0000 | lr: 3.6460e-05 | scale:     1.0000 | micro time: 3.786 | step time: 3.802
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    737/  2110 | global iter:    737/  2110 | loss: 0.0858 | ds_loss: 0.0000 | lr: 3.6427e-05 | scale:     1.0000 | micro time: 3.773 | step time: 0.000
train | epoch   3 | Iter:    738/  2110 | global iter:    738/  2110 | loss: 0.2182 | ds_loss: 0.0000 | lr: 3.6394e-05 | scale:     1.0000 | micro time: 3.814 | step time: 0.000
train | epoch   3 | Iter:    739/  2110 | global iter:    739/  2110 | loss: 0.1624 | ds_loss: 0.0000 | lr: 3.6361e-05 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   3 | Iter:    740/  2110 | global iter:    740/  2110 | loss: 0.1374 | ds_loss: 0.0000 | lr: 3.6327e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    740/  2110 | global iter:    740/  2110 | loss: 0.1509 | ds_loss: 0.0000 | lr: 3.6327e-05 | scale:     1.0000 | micro time: 3.790 | step time: 3.790
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    741/  2110 | global iter:    741/  2110 | loss: 0.1398 | ds_loss: 0.0000 | lr: 3.6294e-05 | scale:     1.0000 | micro time: 3.832 | step time: 0.000
train | epoch   3 | Iter:    742/  2110 | global iter:    742/  2110 | loss: 0.1292 | ds_loss: 0.0000 | lr: 3.6261e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   3 | Iter:    743/  2110 | global iter:    743/  2110 | loss: 0.1588 | ds_loss: 0.0000 | lr: 3.6228e-05 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   3 | Iter:    744/  2110 | global iter:    744/  2110 | loss: 0.1107 | ds_loss: 0.0000 | lr: 3.6195e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    744/  2110 | global iter:    744/  2110 | loss: 0.1346 | ds_loss: 0.0000 | lr: 3.6195e-05 | scale:     1.0000 | micro time: 3.798 | step time: 3.800
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    745/  2110 | global iter:    745/  2110 | loss: 0.0591 | ds_loss: 0.0000 | lr: 3.6161e-05 | scale:     1.0000 | micro time: 3.773 | step time: 0.000
train | epoch   3 | Iter:    746/  2110 | global iter:    746/  2110 | loss: 0.1605 | ds_loss: 0.0000 | lr: 3.6128e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   3 | Iter:    747/  2110 | global iter:    747/  2110 | loss: 0.1175 | ds_loss: 0.0000 | lr: 3.6095e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   3 | Iter:    748/  2110 | global iter:    748/  2110 | loss: 0.1186 | ds_loss: 0.0000 | lr: 3.6061e-05 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    748/  2110 | global iter:    748/  2110 | loss: 0.1139 | ds_loss: 0.0000 | lr: 3.6061e-05 | scale:     1.0000 | micro time: 3.803 | step time: 3.787
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    749/  2110 | global iter:    749/  2110 | loss: 0.1893 | ds_loss: 0.0000 | lr: 3.6028e-05 | scale:     1.0000 | micro time: 3.807 | step time: 0.000
train | epoch   3 | Iter:    750/  2110 | global iter:    750/  2110 | loss: 0.0687 | ds_loss: 0.0000 | lr: 3.5995e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   3 | Iter:    751/  2110 | global iter:    751/  2110 | loss: 0.0784 | ds_loss: 0.0000 | lr: 3.5961e-05 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   3 | Iter:    752/  2110 | global iter:    752/  2110 | loss: 0.0628 | ds_loss: 0.0000 | lr: 3.5928e-05 | scale:     1.0000 | micro time: 3.800 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    752/  2110 | global iter:    752/  2110 | loss: 0.0998 | ds_loss: 0.0000 | lr: 3.5928e-05 | scale:     1.0000 | micro time: 3.800 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    753/  2110 | global iter:    753/  2110 | loss: 0.0629 | ds_loss: 0.0000 | lr: 3.5895e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   3 | Iter:    754/  2110 | global iter:    754/  2110 | loss: 0.2014 | ds_loss: 0.0000 | lr: 3.5861e-05 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   3 | Iter:    755/  2110 | global iter:    755/  2110 | loss: 0.1459 | ds_loss: 0.0000 | lr: 3.5828e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   3 | Iter:    756/  2110 | global iter:    756/  2110 | loss: 0.2019 | ds_loss: 0.0000 | lr: 3.5794e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    756/  2110 | global iter:    756/  2110 | loss: 0.1530 | ds_loss: 0.0000 | lr: 3.5794e-05 | scale:     1.0000 | micro time: 3.771 | step time: 3.784
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    757/  2110 | global iter:    757/  2110 | loss: 0.0971 | ds_loss: 0.0000 | lr: 3.5761e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   3 | Iter:    758/  2110 | global iter:    758/  2110 | loss: 0.1330 | ds_loss: 0.0000 | lr: 3.5727e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   3 | Iter:    759/  2110 | global iter:    759/  2110 | loss: 0.0851 | ds_loss: 0.0000 | lr: 3.5693e-05 | scale:     1.0000 | micro time: 3.780 | step time: 0.000
train | epoch   3 | Iter:    760/  2110 | global iter:    760/  2110 | loss: 0.1289 | ds_loss: 0.0000 | lr: 3.5660e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    760/  2110 | global iter:    760/  2110 | loss: 0.1110 | ds_loss: 0.0000 | lr: 3.5660e-05 | scale:     1.0000 | micro time: 3.791 | step time: 3.789
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    761/  2110 | global iter:    761/  2110 | loss: 0.0792 | ds_loss: 0.0000 | lr: 3.5626e-05 | scale:     1.0000 | micro time: 3.797 | step time: 0.000
train | epoch   3 | Iter:    762/  2110 | global iter:    762/  2110 | loss: 0.1242 | ds_loss: 0.0000 | lr: 3.5592e-05 | scale:     1.0000 | micro time: 3.767 | step time: 0.000
train | epoch   3 | Iter:    763/  2110 | global iter:    763/  2110 | loss: 0.1100 | ds_loss: 0.0000 | lr: 3.5559e-05 | scale:     1.0000 | micro time: 3.805 | step time: 0.000
train | epoch   3 | Iter:    764/  2110 | global iter:    764/  2110 | loss: 0.1370 | ds_loss: 0.0000 | lr: 3.5525e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    764/  2110 | global iter:    764/  2110 | loss: 0.1126 | ds_loss: 0.0000 | lr: 3.5525e-05 | scale:     1.0000 | micro time: 3.786 | step time: 3.789
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    765/  2110 | global iter:    765/  2110 | loss: 0.0911 | ds_loss: 0.0000 | lr: 3.5491e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   3 | Iter:    766/  2110 | global iter:    766/  2110 | loss: 0.1216 | ds_loss: 0.0000 | lr: 3.5458e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   3 | Iter:    767/  2110 | global iter:    767/  2110 | loss: 0.0812 | ds_loss: 0.0000 | lr: 3.5424e-05 | scale:     1.0000 | micro time: 3.774 | step time: 0.000
train | epoch   3 | Iter:    768/  2110 | global iter:    768/  2110 | loss: 0.1749 | ds_loss: 0.0000 | lr: 3.5390e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    768/  2110 | global iter:    768/  2110 | loss: 0.1172 | ds_loss: 0.0000 | lr: 3.5390e-05 | scale:     1.0000 | micro time: 3.801 | step time: 3.785
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    769/  2110 | global iter:    769/  2110 | loss: 0.0867 | ds_loss: 0.0000 | lr: 3.5356e-05 | scale:     1.0000 | micro time: 3.769 | step time: 0.000
train | epoch   3 | Iter:    770/  2110 | global iter:    770/  2110 | loss: 0.0647 | ds_loss: 0.0000 | lr: 3.5322e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   3 | Iter:    771/  2110 | global iter:    771/  2110 | loss: 0.1163 | ds_loss: 0.0000 | lr: 3.5288e-05 | scale:     1.0000 | micro time: 3.816 | step time: 0.000
train | epoch   3 | Iter:    772/  2110 | global iter:    772/  2110 | loss: 0.0649 | ds_loss: 0.0000 | lr: 3.5255e-05 | scale:     1.0000 | micro time: 3.784 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    772/  2110 | global iter:    772/  2110 | loss: 0.0831 | ds_loss: 0.0000 | lr: 3.5255e-05 | scale:     1.0000 | micro time: 3.784 | step time: 3.793
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    773/  2110 | global iter:    773/  2110 | loss: 0.0853 | ds_loss: 0.0000 | lr: 3.5221e-05 | scale:     1.0000 | micro time: 3.774 | step time: 0.000
train | epoch   3 | Iter:    774/  2110 | global iter:    774/  2110 | loss: 0.1970 | ds_loss: 0.0000 | lr: 3.5187e-05 | scale:     1.0000 | micro time: 3.800 | step time: 0.000
train | epoch   3 | Iter:    775/  2110 | global iter:    775/  2110 | loss: 0.0949 | ds_loss: 0.0000 | lr: 3.5153e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   3 | Iter:    776/  2110 | global iter:    776/  2110 | loss: 0.1828 | ds_loss: 0.0000 | lr: 3.5119e-05 | scale:     1.0000 | micro time: 3.770 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    776/  2110 | global iter:    776/  2110 | loss: 0.1400 | ds_loss: 0.0000 | lr: 3.5119e-05 | scale:     1.0000 | micro time: 3.770 | step time: 3.785
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    777/  2110 | global iter:    777/  2110 | loss: 0.0838 | ds_loss: 0.0000 | lr: 3.5085e-05 | scale:     1.0000 | micro time: 3.797 | step time: 0.000
train | epoch   3 | Iter:    778/  2110 | global iter:    778/  2110 | loss: 0.0957 | ds_loss: 0.0000 | lr: 3.5051e-05 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   3 | Iter:    779/  2110 | global iter:    779/  2110 | loss: 0.1324 | ds_loss: 0.0000 | lr: 3.5017e-05 | scale:     1.0000 | micro time: 3.809 | step time: 0.000
train | epoch   3 | Iter:    780/  2110 | global iter:    780/  2110 | loss: 0.1237 | ds_loss: 0.0000 | lr: 3.4983e-05 | scale:     1.0000 | micro time: 3.792 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    780/  2110 | global iter:    780/  2110 | loss: 0.1089 | ds_loss: 0.0000 | lr: 3.4983e-05 | scale:     1.0000 | micro time: 3.792 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    781/  2110 | global iter:    781/  2110 | loss: 0.1215 | ds_loss: 0.0000 | lr: 3.4949e-05 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   3 | Iter:    782/  2110 | global iter:    782/  2110 | loss: 0.1720 | ds_loss: 0.0000 | lr: 3.4915e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   3 | Iter:    783/  2110 | global iter:    783/  2110 | loss: 0.1388 | ds_loss: 0.0000 | lr: 3.4880e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   3 | Iter:    784/  2110 | global iter:    784/  2110 | loss: 0.1860 | ds_loss: 0.0000 | lr: 3.4846e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    784/  2110 | global iter:    784/  2110 | loss: 0.1546 | ds_loss: 0.0000 | lr: 3.4846e-05 | scale:     1.0000 | micro time: 3.798 | step time: 3.793
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    785/  2110 | global iter:    785/  2110 | loss: 0.2421 | ds_loss: 0.0000 | lr: 3.4812e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   3 | Iter:    786/  2110 | global iter:    786/  2110 | loss: 0.1959 | ds_loss: 0.0000 | lr: 3.4778e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   3 | Iter:    787/  2110 | global iter:    787/  2110 | loss: 0.1938 | ds_loss: 0.0000 | lr: 3.4744e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   3 | Iter:    788/  2110 | global iter:    788/  2110 | loss: 0.1387 | ds_loss: 0.0000 | lr: 3.4709e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    788/  2110 | global iter:    788/  2110 | loss: 0.1926 | ds_loss: 0.0000 | lr: 3.4709e-05 | scale:     1.0000 | micro time: 3.787 | step time: 3.793
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    789/  2110 | global iter:    789/  2110 | loss: 0.2191 | ds_loss: 0.0000 | lr: 3.4675e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   3 | Iter:    790/  2110 | global iter:    790/  2110 | loss: 0.1594 | ds_loss: 0.0000 | lr: 3.4641e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   3 | Iter:    791/  2110 | global iter:    791/  2110 | loss: 0.1064 | ds_loss: 0.0000 | lr: 3.4607e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   3 | Iter:    792/  2110 | global iter:    792/  2110 | loss: 0.1845 | ds_loss: 0.0000 | lr: 3.4572e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    792/  2110 | global iter:    792/  2110 | loss: 0.1673 | ds_loss: 0.0000 | lr: 3.4572e-05 | scale:     1.0000 | micro time: 3.802 | step time: 3.796
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    793/  2110 | global iter:    793/  2110 | loss: 0.1838 | ds_loss: 0.0000 | lr: 3.4538e-05 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
train | epoch   3 | Iter:    794/  2110 | global iter:    794/  2110 | loss: 0.2455 | ds_loss: 0.0000 | lr: 3.4503e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   3 | Iter:    795/  2110 | global iter:    795/  2110 | loss: 0.1037 | ds_loss: 0.0000 | lr: 3.4469e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   3 | Iter:    796/  2110 | global iter:    796/  2110 | loss: 0.0679 | ds_loss: 0.0000 | lr: 3.4435e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    796/  2110 | global iter:    796/  2110 | loss: 0.1502 | ds_loss: 0.0000 | lr: 3.4435e-05 | scale:     1.0000 | micro time: 3.786 | step time: 3.793
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    797/  2110 | global iter:    797/  2110 | loss: 0.1543 | ds_loss: 0.0000 | lr: 3.4400e-05 | scale:     1.0000 | micro time: 3.800 | step time: 0.000
train | epoch   3 | Iter:    798/  2110 | global iter:    798/  2110 | loss: 0.1338 | ds_loss: 0.0000 | lr: 3.4366e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   3 | Iter:    799/  2110 | global iter:    799/  2110 | loss: 0.1375 | ds_loss: 0.0000 | lr: 3.4331e-05 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   3 | Iter:    800/  2110 | global iter:    800/  2110 | loss: 0.1018 | ds_loss: 0.0000 | lr: 3.4297e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    800/  2110 | global iter:    800/  2110 | loss: 0.1318 | ds_loss: 0.0000 | lr: 3.4297e-05 | scale:     1.0000 | micro time: 3.794 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    801/  2110 | global iter:    801/  2110 | loss: 0.1557 | ds_loss: 0.0000 | lr: 3.4262e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   3 | Iter:    802/  2110 | global iter:    802/  2110 | loss: 0.1076 | ds_loss: 0.0000 | lr: 3.4228e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   3 | Iter:    803/  2110 | global iter:    803/  2110 | loss: 0.1297 | ds_loss: 0.0000 | lr: 3.4193e-05 | scale:     1.0000 | micro time: 3.820 | step time: 0.000
train | epoch   3 | Iter:    804/  2110 | global iter:    804/  2110 | loss: 0.1673 | ds_loss: 0.0000 | lr: 3.4159e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    804/  2110 | global iter:    804/  2110 | loss: 0.1401 | ds_loss: 0.0000 | lr: 3.4159e-05 | scale:     1.0000 | micro time: 3.785 | step time: 3.795
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    805/  2110 | global iter:    805/  2110 | loss: 0.1492 | ds_loss: 0.0000 | lr: 3.4124e-05 | scale:     1.0000 | micro time: 3.773 | step time: 0.000
train | epoch   3 | Iter:    806/  2110 | global iter:    806/  2110 | loss: 0.1008 | ds_loss: 0.0000 | lr: 3.4089e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   3 | Iter:    807/  2110 | global iter:    807/  2110 | loss: 0.0968 | ds_loss: 0.0000 | lr: 3.4055e-05 | scale:     1.0000 | micro time: 3.808 | step time: 0.000
train | epoch   3 | Iter:    808/  2110 | global iter:    808/  2110 | loss: 0.1288 | ds_loss: 0.0000 | lr: 3.4020e-05 | scale:     1.0000 | micro time: 3.792 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    808/  2110 | global iter:    808/  2110 | loss: 0.1189 | ds_loss: 0.0000 | lr: 3.4020e-05 | scale:     1.0000 | micro time: 3.792 | step time: 3.793
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    809/  2110 | global iter:    809/  2110 | loss: 0.0667 | ds_loss: 0.0000 | lr: 3.3985e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   3 | Iter:    810/  2110 | global iter:    810/  2110 | loss: 0.1127 | ds_loss: 0.0000 | lr: 3.3951e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   3 | Iter:    811/  2110 | global iter:    811/  2110 | loss: 0.0860 | ds_loss: 0.0000 | lr: 3.3916e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   3 | Iter:    812/  2110 | global iter:    812/  2110 | loss: 0.1067 | ds_loss: 0.0000 | lr: 3.3881e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    812/  2110 | global iter:    812/  2110 | loss: 0.0930 | ds_loss: 0.0000 | lr: 3.3881e-05 | scale:     1.0000 | micro time: 3.798 | step time: 3.785
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    813/  2110 | global iter:    813/  2110 | loss: 0.2293 | ds_loss: 0.0000 | lr: 3.3847e-05 | scale:     1.0000 | micro time: 3.800 | step time: 0.000
train | epoch   3 | Iter:    814/  2110 | global iter:    814/  2110 | loss: 0.2281 | ds_loss: 0.0000 | lr: 3.3812e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   3 | Iter:    815/  2110 | global iter:    815/  2110 | loss: 0.1487 | ds_loss: 0.0000 | lr: 3.3777e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   3 | Iter:    816/  2110 | global iter:    816/  2110 | loss: 0.0714 | ds_loss: 0.0000 | lr: 3.3742e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    816/  2110 | global iter:    816/  2110 | loss: 0.1694 | ds_loss: 0.0000 | lr: 3.3742e-05 | scale:     1.0000 | micro time: 3.798 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    817/  2110 | global iter:    817/  2110 | loss: 0.0720 | ds_loss: 0.0000 | lr: 3.3707e-05 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
train | epoch   3 | Iter:    818/  2110 | global iter:    818/  2110 | loss: 0.1317 | ds_loss: 0.0000 | lr: 3.3673e-05 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   3 | Iter:    819/  2110 | global iter:    819/  2110 | loss: 0.0716 | ds_loss: 0.0000 | lr: 3.3638e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   3 | Iter:    820/  2110 | global iter:    820/  2110 | loss: 0.0953 | ds_loss: 0.0000 | lr: 3.3603e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    820/  2110 | global iter:    820/  2110 | loss: 0.0927 | ds_loss: 0.0000 | lr: 3.3603e-05 | scale:     1.0000 | micro time: 3.795 | step time: 3.787
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    821/  2110 | global iter:    821/  2110 | loss: 0.1074 | ds_loss: 0.0000 | lr: 3.3568e-05 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   3 | Iter:    822/  2110 | global iter:    822/  2110 | loss: 0.1359 | ds_loss: 0.0000 | lr: 3.3533e-05 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
train | epoch   3 | Iter:    823/  2110 | global iter:    823/  2110 | loss: 0.1056 | ds_loss: 0.0000 | lr: 3.3498e-05 | scale:     1.0000 | micro time: 3.804 | step time: 0.000
train | epoch   3 | Iter:    824/  2110 | global iter:    824/  2110 | loss: 0.1762 | ds_loss: 0.0000 | lr: 3.3463e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    824/  2110 | global iter:    824/  2110 | loss: 0.1313 | ds_loss: 0.0000 | lr: 3.3463e-05 | scale:     1.0000 | micro time: 3.795 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    825/  2110 | global iter:    825/  2110 | loss: 0.2073 | ds_loss: 0.0000 | lr: 3.3428e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   3 | Iter:    826/  2110 | global iter:    826/  2110 | loss: 0.2086 | ds_loss: 0.0000 | lr: 3.3393e-05 | scale:     1.0000 | micro time: 3.797 | step time: 0.000
train | epoch   3 | Iter:    827/  2110 | global iter:    827/  2110 | loss: 0.1802 | ds_loss: 0.0000 | lr: 3.3358e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   3 | Iter:    828/  2110 | global iter:    828/  2110 | loss: 0.1098 | ds_loss: 0.0000 | lr: 3.3323e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    828/  2110 | global iter:    828/  2110 | loss: 0.1765 | ds_loss: 0.0000 | lr: 3.3323e-05 | scale:     1.0000 | micro time: 3.798 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    829/  2110 | global iter:    829/  2110 | loss: 0.1972 | ds_loss: 0.0000 | lr: 3.3288e-05 | scale:     1.0000 | micro time: 3.808 | step time: 0.000
train | epoch   3 | Iter:    830/  2110 | global iter:    830/  2110 | loss: 0.1427 | ds_loss: 0.0000 | lr: 3.3253e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   3 | Iter:    831/  2110 | global iter:    831/  2110 | loss: 0.0932 | ds_loss: 0.0000 | lr: 3.3218e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   3 | Iter:    832/  2110 | global iter:    832/  2110 | loss: 0.1390 | ds_loss: 0.0000 | lr: 3.3183e-05 | scale:     1.0000 | micro time: 3.765 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    832/  2110 | global iter:    832/  2110 | loss: 0.1430 | ds_loss: 0.0000 | lr: 3.3183e-05 | scale:     1.0000 | micro time: 3.765 | step time: 3.792
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    833/  2110 | global iter:    833/  2110 | loss: 0.1090 | ds_loss: 0.0000 | lr: 3.3148e-05 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   3 | Iter:    834/  2110 | global iter:    834/  2110 | loss: 0.0987 | ds_loss: 0.0000 | lr: 3.3112e-05 | scale:     1.0000 | micro time: 3.773 | step time: 0.000
train | epoch   3 | Iter:    835/  2110 | global iter:    835/  2110 | loss: 0.1001 | ds_loss: 0.0000 | lr: 3.3077e-05 | scale:     1.0000 | micro time: 3.780 | step time: 0.000
train | epoch   3 | Iter:    836/  2110 | global iter:    836/  2110 | loss: 0.0752 | ds_loss: 0.0000 | lr: 3.3042e-05 | scale:     1.0000 | micro time: 3.809 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    836/  2110 | global iter:    836/  2110 | loss: 0.0958 | ds_loss: 0.0000 | lr: 3.3042e-05 | scale:     1.0000 | micro time: 3.809 | step time: 3.790
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    837/  2110 | global iter:    837/  2110 | loss: 0.2705 | ds_loss: 0.0000 | lr: 3.3007e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   3 | Iter:    838/  2110 | global iter:    838/  2110 | loss: 0.1812 | ds_loss: 0.0000 | lr: 3.2972e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   3 | Iter:    839/  2110 | global iter:    839/  2110 | loss: 0.1199 | ds_loss: 0.0000 | lr: 3.2936e-05 | scale:     1.0000 | micro time: 3.814 | step time: 0.000
train | epoch   3 | Iter:    840/  2110 | global iter:    840/  2110 | loss: 0.1514 | ds_loss: 0.0000 | lr: 3.2901e-05 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    840/  2110 | global iter:    840/  2110 | loss: 0.1808 | ds_loss: 0.0000 | lr: 3.2901e-05 | scale:     1.0000 | micro time: 3.778 | step time: 3.789
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    841/  2110 | global iter:    841/  2110 | loss: 0.0888 | ds_loss: 0.0000 | lr: 3.2866e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   3 | Iter:    842/  2110 | global iter:    842/  2110 | loss: 0.1928 | ds_loss: 0.0000 | lr: 3.2831e-05 | scale:     1.0000 | micro time: 3.822 | step time: 0.000
train | epoch   3 | Iter:    843/  2110 | global iter:    843/  2110 | loss: 0.1671 | ds_loss: 0.0000 | lr: 3.2795e-05 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   3 | Iter:    844/  2110 | global iter:    844/  2110 | loss: 0.1017 | ds_loss: 0.0000 | lr: 3.2760e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    844/  2110 | global iter:    844/  2110 | loss: 0.1376 | ds_loss: 0.0000 | lr: 3.2760e-05 | scale:     1.0000 | micro time: 3.790 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:    845/  2110 | global iter:    845/  2110 | loss: 0.1940 | ds_loss: 0.0000 | lr: 3.2725e-05 | scale:     1.0000 | micro time: 3.812 | step time: 0.000
train | epoch   3 | Iter:    846/  2110 | global iter:    846/  2110 | loss: 0.1674 | ds_loss: 0.0000 | lr: 3.2689e-05 | scale:     1.0000 | micro time: 3.763 | step time: 0.000
train | epoch   3 | Iter:    847/  2110 | global iter:    847/  2110 | loss: 0.1124 | ds_loss: 0.0000 | lr: 3.2654e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   3 | Iter:    848/  2110 | global iter:    848/  2110 | loss: 0.1343 | ds_loss: 0.0000 | lr: 3.2619e-05 | scale:     1.0000 | micro time: 3.350 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:    848/  2110 | global iter:    848/  2110 | loss: 0.1520 | ds_loss: 0.0000 | lr: 3.2619e-05 | scale:     1.0000 | micro time: 3.350 | step time: 3.679
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
Tue Apr  8 07:59:10 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   63C    P0             56W /  250W |   29814MiB /  32768MiB |     43%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |     69%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   50C    P0             48W /  250W |   29848MiB /  32768MiB |     18%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   54C    P0             49W /  250W |   28312MiB /  32768MiB |     81%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 07:59:10 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   63C    P0             56W /  250W |   29814MiB /  32768MiB |     43%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |     69%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   50C    P0             48W /  250W |   29848MiB /  32768MiB |     18%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   54C    P0             49W /  250W |   28312MiB /  32768MiB |     81%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 07:59:10 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   63C    P0             56W /  250W |   29814MiB /  32768MiB |     43%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |     69%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   50C    P0             48W /  250W |   29848MiB /  32768MiB |     18%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   54C    P0             49W /  250W |   28312MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 07:59:10 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   63C    P0             56W /  250W |   29814MiB /  32768MiB |     43%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |     69%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   50C    P0             48W /  250W |   29848MiB /  32768MiB |     18%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   54C    P0             49W /  250W |   28312MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

train | epoch   4 | Iter:    849/  2110 | global iter:    849/  2110 | loss: 0.0782 | ds_loss: 0.0000 | lr: 3.2583e-05 | scale:     1.0000 | micro time: 3.770 | step time: 0.000
train | epoch   4 | Iter:    850/  2110 | global iter:    850/  2110 | loss: 0.0719 | ds_loss: 0.0000 | lr: 3.2548e-05 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
train | epoch   4 | Iter:    851/  2110 | global iter:    851/  2110 | loss: 0.1677 | ds_loss: 0.0000 | lr: 3.2512e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   4 | Iter:    852/  2110 | global iter:    852/  2110 | loss: 0.0556 | ds_loss: 0.0000 | lr: 3.2477e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    852/  2110 | global iter:    852/  2110 | loss: 0.0933 | ds_loss: 0.0000 | lr: 3.2477e-05 | scale:     1.0000 | micro time: 3.798 | step time: 3.788
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    853/  2110 | global iter:    853/  2110 | loss: 0.1104 | ds_loss: 0.0000 | lr: 3.2441e-05 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
train | epoch   4 | Iter:    854/  2110 | global iter:    854/  2110 | loss: 0.0933 | ds_loss: 0.0000 | lr: 3.2406e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   4 | Iter:    855/  2110 | global iter:    855/  2110 | loss: 0.0479 | ds_loss: 0.0000 | lr: 3.2370e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   4 | Iter:    856/  2110 | global iter:    856/  2110 | loss: 0.1149 | ds_loss: 0.0000 | lr: 3.2335e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    856/  2110 | global iter:    856/  2110 | loss: 0.0916 | ds_loss: 0.0000 | lr: 3.2335e-05 | scale:     1.0000 | micro time: 3.782 | step time: 3.793
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    857/  2110 | global iter:    857/  2110 | loss: 0.0585 | ds_loss: 0.0000 | lr: 3.2299e-05 | scale:     1.0000 | micro time: 3.773 | step time: 0.000
train | epoch   4 | Iter:    858/  2110 | global iter:    858/  2110 | loss: 0.1134 | ds_loss: 0.0000 | lr: 3.2264e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   4 | Iter:    859/  2110 | global iter:    859/  2110 | loss: 0.0625 | ds_loss: 0.0000 | lr: 3.2228e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   4 | Iter:    860/  2110 | global iter:    860/  2110 | loss: 0.0302 | ds_loss: 0.0000 | lr: 3.2193e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    860/  2110 | global iter:    860/  2110 | loss: 0.0662 | ds_loss: 0.0000 | lr: 3.2193e-05 | scale:     1.0000 | micro time: 3.818 | step time: 3.799
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    861/  2110 | global iter:    861/  2110 | loss: 0.0233 | ds_loss: 0.0000 | lr: 3.2157e-05 | scale:     1.0000 | micro time: 3.776 | step time: 0.000
train | epoch   4 | Iter:    862/  2110 | global iter:    862/  2110 | loss: 0.0942 | ds_loss: 0.0000 | lr: 3.2121e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   4 | Iter:    863/  2110 | global iter:    863/  2110 | loss: 0.1203 | ds_loss: 0.0000 | lr: 3.2086e-05 | scale:     1.0000 | micro time: 3.841 | step time: 0.000
train | epoch   4 | Iter:    864/  2110 | global iter:    864/  2110 | loss: 0.0526 | ds_loss: 0.0000 | lr: 3.2050e-05 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    864/  2110 | global iter:    864/  2110 | loss: 0.0726 | ds_loss: 0.0000 | lr: 3.2050e-05 | scale:     1.0000 | micro time: 3.803 | step time: 3.802
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    865/  2110 | global iter:    865/  2110 | loss: 0.1188 | ds_loss: 0.0000 | lr: 3.2014e-05 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   4 | Iter:    866/  2110 | global iter:    866/  2110 | loss: 0.0481 | ds_loss: 0.0000 | lr: 3.1979e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   4 | Iter:    867/  2110 | global iter:    867/  2110 | loss: 0.0396 | ds_loss: 0.0000 | lr: 3.1943e-05 | scale:     1.0000 | micro time: 3.830 | step time: 0.000
train | epoch   4 | Iter:    868/  2110 | global iter:    868/  2110 | loss: 0.0946 | ds_loss: 0.0000 | lr: 3.1907e-05 | scale:     1.0000 | micro time: 3.770 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    868/  2110 | global iter:    868/  2110 | loss: 0.0753 | ds_loss: 0.0000 | lr: 3.1907e-05 | scale:     1.0000 | micro time: 3.770 | step time: 3.793
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    869/  2110 | global iter:    869/  2110 | loss: 0.0818 | ds_loss: 0.0000 | lr: 3.1872e-05 | scale:     1.0000 | micro time: 3.816 | step time: 0.000
train | epoch   4 | Iter:    870/  2110 | global iter:    870/  2110 | loss: 0.1055 | ds_loss: 0.0000 | lr: 3.1836e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   4 | Iter:    871/  2110 | global iter:    871/  2110 | loss: 0.1176 | ds_loss: 0.0000 | lr: 3.1800e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   4 | Iter:    872/  2110 | global iter:    872/  2110 | loss: 0.0511 | ds_loss: 0.0000 | lr: 3.1764e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    872/  2110 | global iter:    872/  2110 | loss: 0.0890 | ds_loss: 0.0000 | lr: 3.1764e-05 | scale:     1.0000 | micro time: 3.791 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    873/  2110 | global iter:    873/  2110 | loss: 0.0631 | ds_loss: 0.0000 | lr: 3.1729e-05 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   4 | Iter:    874/  2110 | global iter:    874/  2110 | loss: 0.0486 | ds_loss: 0.0000 | lr: 3.1693e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   4 | Iter:    875/  2110 | global iter:    875/  2110 | loss: 0.0778 | ds_loss: 0.0000 | lr: 3.1657e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   4 | Iter:    876/  2110 | global iter:    876/  2110 | loss: 0.0705 | ds_loss: 0.0000 | lr: 3.1621e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    876/  2110 | global iter:    876/  2110 | loss: 0.0650 | ds_loss: 0.0000 | lr: 3.1621e-05 | scale:     1.0000 | micro time: 3.794 | step time: 3.788
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    877/  2110 | global iter:    877/  2110 | loss: 0.0488 | ds_loss: 0.0000 | lr: 3.1585e-05 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   4 | Iter:    878/  2110 | global iter:    878/  2110 | loss: 0.1007 | ds_loss: 0.0000 | lr: 3.1549e-05 | scale:     1.0000 | micro time: 3.808 | step time: 0.000
train | epoch   4 | Iter:    879/  2110 | global iter:    879/  2110 | loss: 0.1281 | ds_loss: 0.0000 | lr: 3.1514e-05 | scale:     1.0000 | micro time: 3.767 | step time: 0.000
train | epoch   4 | Iter:    880/  2110 | global iter:    880/  2110 | loss: 0.1527 | ds_loss: 0.0000 | lr: 3.1478e-05 | scale:     1.0000 | micro time: 3.814 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    880/  2110 | global iter:    880/  2110 | loss: 0.1076 | ds_loss: 0.0000 | lr: 3.1478e-05 | scale:     1.0000 | micro time: 3.814 | step time: 3.793
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    881/  2110 | global iter:    881/  2110 | loss: 0.0606 | ds_loss: 0.0000 | lr: 3.1442e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   4 | Iter:    882/  2110 | global iter:    882/  2110 | loss: 0.0565 | ds_loss: 0.0000 | lr: 3.1406e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   4 | Iter:    883/  2110 | global iter:    883/  2110 | loss: 0.0640 | ds_loss: 0.0000 | lr: 3.1370e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   4 | Iter:    884/  2110 | global iter:    884/  2110 | loss: 0.0436 | ds_loss: 0.0000 | lr: 3.1334e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    884/  2110 | global iter:    884/  2110 | loss: 0.0562 | ds_loss: 0.0000 | lr: 3.1334e-05 | scale:     1.0000 | micro time: 3.787 | step time: 3.789
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    885/  2110 | global iter:    885/  2110 | loss: 0.0720 | ds_loss: 0.0000 | lr: 3.1298e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   4 | Iter:    886/  2110 | global iter:    886/  2110 | loss: 0.0224 | ds_loss: 0.0000 | lr: 3.1262e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   4 | Iter:    887/  2110 | global iter:    887/  2110 | loss: 0.0708 | ds_loss: 0.0000 | lr: 3.1226e-05 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   4 | Iter:    888/  2110 | global iter:    888/  2110 | loss: 0.0731 | ds_loss: 0.0000 | lr: 3.1190e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    888/  2110 | global iter:    888/  2110 | loss: 0.0596 | ds_loss: 0.0000 | lr: 3.1190e-05 | scale:     1.0000 | micro time: 3.787 | step time: 3.790
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    889/  2110 | global iter:    889/  2110 | loss: 0.0710 | ds_loss: 0.0000 | lr: 3.1154e-05 | scale:     1.0000 | micro time: 3.816 | step time: 0.000
train | epoch   4 | Iter:    890/  2110 | global iter:    890/  2110 | loss: 0.0540 | ds_loss: 0.0000 | lr: 3.1118e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   4 | Iter:    891/  2110 | global iter:    891/  2110 | loss: 0.0737 | ds_loss: 0.0000 | lr: 3.1082e-05 | scale:     1.0000 | micro time: 3.761 | step time: 0.000
train | epoch   4 | Iter:    892/  2110 | global iter:    892/  2110 | loss: 0.0729 | ds_loss: 0.0000 | lr: 3.1046e-05 | scale:     1.0000 | micro time: 3.763 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    892/  2110 | global iter:    892/  2110 | loss: 0.0679 | ds_loss: 0.0000 | lr: 3.1046e-05 | scale:     1.0000 | micro time: 3.763 | step time: 3.781
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    893/  2110 | global iter:    893/  2110 | loss: 0.0582 | ds_loss: 0.0000 | lr: 3.1010e-05 | scale:     1.0000 | micro time: 3.805 | step time: 0.000
train | epoch   4 | Iter:    894/  2110 | global iter:    894/  2110 | loss: 0.1299 | ds_loss: 0.0000 | lr: 3.0974e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   4 | Iter:    895/  2110 | global iter:    895/  2110 | loss: 0.1015 | ds_loss: 0.0000 | lr: 3.0938e-05 | scale:     1.0000 | micro time: 3.773 | step time: 0.000
train | epoch   4 | Iter:    896/  2110 | global iter:    896/  2110 | loss: 0.0928 | ds_loss: 0.0000 | lr: 3.0902e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    896/  2110 | global iter:    896/  2110 | loss: 0.0956 | ds_loss: 0.0000 | lr: 3.0902e-05 | scale:     1.0000 | micro time: 3.786 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    897/  2110 | global iter:    897/  2110 | loss: 0.0566 | ds_loss: 0.0000 | lr: 3.0865e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   4 | Iter:    898/  2110 | global iter:    898/  2110 | loss: 0.0907 | ds_loss: 0.0000 | lr: 3.0829e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   4 | Iter:    899/  2110 | global iter:    899/  2110 | loss: 0.0905 | ds_loss: 0.0000 | lr: 3.0793e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   4 | Iter:    900/  2110 | global iter:    900/  2110 | loss: 0.0643 | ds_loss: 0.0000 | lr: 3.0757e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    900/  2110 | global iter:    900/  2110 | loss: 0.0755 | ds_loss: 0.0000 | lr: 3.0757e-05 | scale:     1.0000 | micro time: 3.786 | step time: 3.793
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    901/  2110 | global iter:    901/  2110 | loss: 0.0432 | ds_loss: 0.0000 | lr: 3.0721e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   4 | Iter:    902/  2110 | global iter:    902/  2110 | loss: 0.0351 | ds_loss: 0.0000 | lr: 3.0685e-05 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   4 | Iter:    903/  2110 | global iter:    903/  2110 | loss: 0.1051 | ds_loss: 0.0000 | lr: 3.0648e-05 | scale:     1.0000 | micro time: 3.819 | step time: 0.000
train | epoch   4 | Iter:    904/  2110 | global iter:    904/  2110 | loss: 0.0412 | ds_loss: 0.0000 | lr: 3.0612e-05 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    904/  2110 | global iter:    904/  2110 | loss: 0.0561 | ds_loss: 0.0000 | lr: 3.0612e-05 | scale:     1.0000 | micro time: 3.789 | step time: 3.796
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    905/  2110 | global iter:    905/  2110 | loss: 0.0806 | ds_loss: 0.0000 | lr: 3.0576e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   4 | Iter:    906/  2110 | global iter:    906/  2110 | loss: 0.0671 | ds_loss: 0.0000 | lr: 3.0540e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   4 | Iter:    907/  2110 | global iter:    907/  2110 | loss: 0.1305 | ds_loss: 0.0000 | lr: 3.0504e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   4 | Iter:    908/  2110 | global iter:    908/  2110 | loss: 0.0924 | ds_loss: 0.0000 | lr: 3.0467e-05 | scale:     1.0000 | micro time: 3.792 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    908/  2110 | global iter:    908/  2110 | loss: 0.0926 | ds_loss: 0.0000 | lr: 3.0467e-05 | scale:     1.0000 | micro time: 3.792 | step time: 3.795
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    909/  2110 | global iter:    909/  2110 | loss: 0.0951 | ds_loss: 0.0000 | lr: 3.0431e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   4 | Iter:    910/  2110 | global iter:    910/  2110 | loss: 0.0819 | ds_loss: 0.0000 | lr: 3.0395e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   4 | Iter:    911/  2110 | global iter:    911/  2110 | loss: 0.1139 | ds_loss: 0.0000 | lr: 3.0358e-05 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
train | epoch   4 | Iter:    912/  2110 | global iter:    912/  2110 | loss: 0.0493 | ds_loss: 0.0000 | lr: 3.0322e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    912/  2110 | global iter:    912/  2110 | loss: 0.0851 | ds_loss: 0.0000 | lr: 3.0322e-05 | scale:     1.0000 | micro time: 3.802 | step time: 3.795
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    913/  2110 | global iter:    913/  2110 | loss: 0.1044 | ds_loss: 0.0000 | lr: 3.0286e-05 | scale:     1.0000 | micro time: 3.800 | step time: 0.000
train | epoch   4 | Iter:    914/  2110 | global iter:    914/  2110 | loss: 0.0505 | ds_loss: 0.0000 | lr: 3.0250e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   4 | Iter:    915/  2110 | global iter:    915/  2110 | loss: 0.1123 | ds_loss: 0.0000 | lr: 3.0213e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   4 | Iter:    916/  2110 | global iter:    916/  2110 | loss: 0.1315 | ds_loss: 0.0000 | lr: 3.0177e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    916/  2110 | global iter:    916/  2110 | loss: 0.0997 | ds_loss: 0.0000 | lr: 3.0177e-05 | scale:     1.0000 | micro time: 3.802 | step time: 3.800
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    917/  2110 | global iter:    917/  2110 | loss: 0.0836 | ds_loss: 0.0000 | lr: 3.0140e-05 | scale:     1.0000 | micro time: 3.822 | step time: 0.000
train | epoch   4 | Iter:    918/  2110 | global iter:    918/  2110 | loss: 0.0327 | ds_loss: 0.0000 | lr: 3.0104e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   4 | Iter:    919/  2110 | global iter:    919/  2110 | loss: 0.1204 | ds_loss: 0.0000 | lr: 3.0068e-05 | scale:     1.0000 | micro time: 3.811 | step time: 0.000
train | epoch   4 | Iter:    920/  2110 | global iter:    920/  2110 | loss: 0.0870 | ds_loss: 0.0000 | lr: 3.0031e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    920/  2110 | global iter:    920/  2110 | loss: 0.0809 | ds_loss: 0.0000 | lr: 3.0031e-05 | scale:     1.0000 | micro time: 3.798 | step time: 3.805
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    921/  2110 | global iter:    921/  2110 | loss: 0.0267 | ds_loss: 0.0000 | lr: 2.9995e-05 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   4 | Iter:    922/  2110 | global iter:    922/  2110 | loss: 0.1215 | ds_loss: 0.0000 | lr: 2.9958e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   4 | Iter:    923/  2110 | global iter:    923/  2110 | loss: 0.0849 | ds_loss: 0.0000 | lr: 2.9922e-05 | scale:     1.0000 | micro time: 3.814 | step time: 0.000
train | epoch   4 | Iter:    924/  2110 | global iter:    924/  2110 | loss: 0.1312 | ds_loss: 0.0000 | lr: 2.9886e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    924/  2110 | global iter:    924/  2110 | loss: 0.0911 | ds_loss: 0.0000 | lr: 2.9886e-05 | scale:     1.0000 | micro time: 3.818 | step time: 3.801
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    925/  2110 | global iter:    925/  2110 | loss: 0.0392 | ds_loss: 0.0000 | lr: 2.9849e-05 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
train | epoch   4 | Iter:    926/  2110 | global iter:    926/  2110 | loss: 0.0428 | ds_loss: 0.0000 | lr: 2.9813e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   4 | Iter:    927/  2110 | global iter:    927/  2110 | loss: 0.1267 | ds_loss: 0.0000 | lr: 2.9776e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   4 | Iter:    928/  2110 | global iter:    928/  2110 | loss: 0.0900 | ds_loss: 0.0000 | lr: 2.9740e-05 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    928/  2110 | global iter:    928/  2110 | loss: 0.0747 | ds_loss: 0.0000 | lr: 2.9740e-05 | scale:     1.0000 | micro time: 3.788 | step time: 3.798
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    929/  2110 | global iter:    929/  2110 | loss: 0.1353 | ds_loss: 0.0000 | lr: 2.9703e-05 | scale:     1.0000 | micro time: 3.773 | step time: 0.000
train | epoch   4 | Iter:    930/  2110 | global iter:    930/  2110 | loss: 0.0357 | ds_loss: 0.0000 | lr: 2.9667e-05 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
train | epoch   4 | Iter:    931/  2110 | global iter:    931/  2110 | loss: 0.0590 | ds_loss: 0.0000 | lr: 2.9630e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   4 | Iter:    932/  2110 | global iter:    932/  2110 | loss: 0.0493 | ds_loss: 0.0000 | lr: 2.9594e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    932/  2110 | global iter:    932/  2110 | loss: 0.0698 | ds_loss: 0.0000 | lr: 2.9594e-05 | scale:     1.0000 | micro time: 3.806 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    933/  2110 | global iter:    933/  2110 | loss: 0.0712 | ds_loss: 0.0000 | lr: 2.9557e-05 | scale:     1.0000 | micro time: 3.773 | step time: 0.000
train | epoch   4 | Iter:    934/  2110 | global iter:    934/  2110 | loss: 0.0328 | ds_loss: 0.0000 | lr: 2.9521e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   4 | Iter:    935/  2110 | global iter:    935/  2110 | loss: 0.0370 | ds_loss: 0.0000 | lr: 2.9484e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   4 | Iter:    936/  2110 | global iter:    936/  2110 | loss: 0.0457 | ds_loss: 0.0000 | lr: 2.9448e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    936/  2110 | global iter:    936/  2110 | loss: 0.0467 | ds_loss: 0.0000 | lr: 2.9448e-05 | scale:     1.0000 | micro time: 3.782 | step time: 3.787
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    937/  2110 | global iter:    937/  2110 | loss: 0.0419 | ds_loss: 0.0000 | lr: 2.9411e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   4 | Iter:    938/  2110 | global iter:    938/  2110 | loss: 0.0894 | ds_loss: 0.0000 | lr: 2.9374e-05 | scale:     1.0000 | micro time: 3.823 | step time: 0.000
train | epoch   4 | Iter:    939/  2110 | global iter:    939/  2110 | loss: 0.0960 | ds_loss: 0.0000 | lr: 2.9338e-05 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   4 | Iter:    940/  2110 | global iter:    940/  2110 | loss: 0.0714 | ds_loss: 0.0000 | lr: 2.9301e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    940/  2110 | global iter:    940/  2110 | loss: 0.0747 | ds_loss: 0.0000 | lr: 2.9301e-05 | scale:     1.0000 | micro time: 3.810 | step time: 3.799
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    941/  2110 | global iter:    941/  2110 | loss: 0.0528 | ds_loss: 0.0000 | lr: 2.9265e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   4 | Iter:    942/  2110 | global iter:    942/  2110 | loss: 0.1231 | ds_loss: 0.0000 | lr: 2.9228e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   4 | Iter:    943/  2110 | global iter:    943/  2110 | loss: 0.1460 | ds_loss: 0.0000 | lr: 2.9191e-05 | scale:     1.0000 | micro time: 3.817 | step time: 0.000
train | epoch   4 | Iter:    944/  2110 | global iter:    944/  2110 | loss: 0.0888 | ds_loss: 0.0000 | lr: 2.9155e-05 | scale:     1.0000 | micro time: 3.772 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    944/  2110 | global iter:    944/  2110 | loss: 0.1027 | ds_loss: 0.0000 | lr: 2.9155e-05 | scale:     1.0000 | micro time: 3.772 | step time: 3.790
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    945/  2110 | global iter:    945/  2110 | loss: 0.0420 | ds_loss: 0.0000 | lr: 2.9118e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   4 | Iter:    946/  2110 | global iter:    946/  2110 | loss: 0.0884 | ds_loss: 0.0000 | lr: 2.9081e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   4 | Iter:    947/  2110 | global iter:    947/  2110 | loss: 0.1161 | ds_loss: 0.0000 | lr: 2.9045e-05 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   4 | Iter:    948/  2110 | global iter:    948/  2110 | loss: 0.0836 | ds_loss: 0.0000 | lr: 2.9008e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    948/  2110 | global iter:    948/  2110 | loss: 0.0825 | ds_loss: 0.0000 | lr: 2.9008e-05 | scale:     1.0000 | micro time: 3.786 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    949/  2110 | global iter:    949/  2110 | loss: 0.1017 | ds_loss: 0.0000 | lr: 2.8971e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   4 | Iter:    950/  2110 | global iter:    950/  2110 | loss: 0.0592 | ds_loss: 0.0000 | lr: 2.8935e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   4 | Iter:    951/  2110 | global iter:    951/  2110 | loss: 0.1139 | ds_loss: 0.0000 | lr: 2.8898e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   4 | Iter:    952/  2110 | global iter:    952/  2110 | loss: 0.0529 | ds_loss: 0.0000 | lr: 2.8861e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    952/  2110 | global iter:    952/  2110 | loss: 0.0819 | ds_loss: 0.0000 | lr: 2.8861e-05 | scale:     1.0000 | micro time: 3.786 | step time: 3.793
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    953/  2110 | global iter:    953/  2110 | loss: 0.0753 | ds_loss: 0.0000 | lr: 2.8825e-05 | scale:     1.0000 | micro time: 3.797 | step time: 0.000
train | epoch   4 | Iter:    954/  2110 | global iter:    954/  2110 | loss: 0.0958 | ds_loss: 0.0000 | lr: 2.8788e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   4 | Iter:    955/  2110 | global iter:    955/  2110 | loss: 0.0562 | ds_loss: 0.0000 | lr: 2.8751e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   4 | Iter:    956/  2110 | global iter:    956/  2110 | loss: 0.0534 | ds_loss: 0.0000 | lr: 2.8714e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    956/  2110 | global iter:    956/  2110 | loss: 0.0702 | ds_loss: 0.0000 | lr: 2.8714e-05 | scale:     1.0000 | micro time: 3.798 | step time: 3.787
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    957/  2110 | global iter:    957/  2110 | loss: 0.1364 | ds_loss: 0.0000 | lr: 2.8678e-05 | scale:     1.0000 | micro time: 3.774 | step time: 0.000
train | epoch   4 | Iter:    958/  2110 | global iter:    958/  2110 | loss: 0.0475 | ds_loss: 0.0000 | lr: 2.8641e-05 | scale:     1.0000 | micro time: 3.796 | step time: 0.000
train | epoch   4 | Iter:    959/  2110 | global iter:    959/  2110 | loss: 0.0571 | ds_loss: 0.0000 | lr: 2.8604e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   4 | Iter:    960/  2110 | global iter:    960/  2110 | loss: 0.1253 | ds_loss: 0.0000 | lr: 2.8567e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    960/  2110 | global iter:    960/  2110 | loss: 0.0916 | ds_loss: 0.0000 | lr: 2.8567e-05 | scale:     1.0000 | micro time: 3.798 | step time: 3.789
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    961/  2110 | global iter:    961/  2110 | loss: 0.1148 | ds_loss: 0.0000 | lr: 2.8531e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   4 | Iter:    962/  2110 | global iter:    962/  2110 | loss: 0.0800 | ds_loss: 0.0000 | lr: 2.8494e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   4 | Iter:    963/  2110 | global iter:    963/  2110 | loss: 0.0477 | ds_loss: 0.0000 | lr: 2.8457e-05 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   4 | Iter:    964/  2110 | global iter:    964/  2110 | loss: 0.1305 | ds_loss: 0.0000 | lr: 2.8420e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    964/  2110 | global iter:    964/  2110 | loss: 0.0933 | ds_loss: 0.0000 | lr: 2.8420e-05 | scale:     1.0000 | micro time: 3.818 | step time: 3.793
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    965/  2110 | global iter:    965/  2110 | loss: 0.0481 | ds_loss: 0.0000 | lr: 2.8383e-05 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
train | epoch   4 | Iter:    966/  2110 | global iter:    966/  2110 | loss: 0.0688 | ds_loss: 0.0000 | lr: 2.8347e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   4 | Iter:    967/  2110 | global iter:    967/  2110 | loss: 0.0696 | ds_loss: 0.0000 | lr: 2.8310e-05 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   4 | Iter:    968/  2110 | global iter:    968/  2110 | loss: 0.0902 | ds_loss: 0.0000 | lr: 2.8273e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    968/  2110 | global iter:    968/  2110 | loss: 0.0692 | ds_loss: 0.0000 | lr: 2.8273e-05 | scale:     1.0000 | micro time: 3.790 | step time: 3.786
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    969/  2110 | global iter:    969/  2110 | loss: 0.0963 | ds_loss: 0.0000 | lr: 2.8236e-05 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   4 | Iter:    970/  2110 | global iter:    970/  2110 | loss: 0.1408 | ds_loss: 0.0000 | lr: 2.8199e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   4 | Iter:    971/  2110 | global iter:    971/  2110 | loss: 0.0502 | ds_loss: 0.0000 | lr: 2.8162e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   4 | Iter:    972/  2110 | global iter:    972/  2110 | loss: 0.0594 | ds_loss: 0.0000 | lr: 2.8125e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    972/  2110 | global iter:    972/  2110 | loss: 0.0867 | ds_loss: 0.0000 | lr: 2.8125e-05 | scale:     1.0000 | micro time: 3.806 | step time: 3.795
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    973/  2110 | global iter:    973/  2110 | loss: 0.0408 | ds_loss: 0.0000 | lr: 2.8089e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   4 | Iter:    974/  2110 | global iter:    974/  2110 | loss: 0.0788 | ds_loss: 0.0000 | lr: 2.8052e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   4 | Iter:    975/  2110 | global iter:    975/  2110 | loss: 0.0672 | ds_loss: 0.0000 | lr: 2.8015e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   4 | Iter:    976/  2110 | global iter:    976/  2110 | loss: 0.0775 | ds_loss: 0.0000 | lr: 2.7978e-05 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    976/  2110 | global iter:    976/  2110 | loss: 0.0661 | ds_loss: 0.0000 | lr: 2.7978e-05 | scale:     1.0000 | micro time: 3.799 | step time: 3.795
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    977/  2110 | global iter:    977/  2110 | loss: 0.0719 | ds_loss: 0.0000 | lr: 2.7941e-05 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   4 | Iter:    978/  2110 | global iter:    978/  2110 | loss: 0.0678 | ds_loss: 0.0000 | lr: 2.7904e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   4 | Iter:    979/  2110 | global iter:    979/  2110 | loss: 0.0718 | ds_loss: 0.0000 | lr: 2.7867e-05 | scale:     1.0000 | micro time: 3.777 | step time: 0.000
train | epoch   4 | Iter:    980/  2110 | global iter:    980/  2110 | loss: 0.0450 | ds_loss: 0.0000 | lr: 2.7830e-05 | scale:     1.0000 | micro time: 3.819 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    980/  2110 | global iter:    980/  2110 | loss: 0.0641 | ds_loss: 0.0000 | lr: 2.7830e-05 | scale:     1.0000 | micro time: 3.819 | step time: 3.802
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    981/  2110 | global iter:    981/  2110 | loss: 0.0745 | ds_loss: 0.0000 | lr: 2.7793e-05 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   4 | Iter:    982/  2110 | global iter:    982/  2110 | loss: 0.0669 | ds_loss: 0.0000 | lr: 2.7756e-05 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
train | epoch   4 | Iter:    983/  2110 | global iter:    983/  2110 | loss: 0.0814 | ds_loss: 0.0000 | lr: 2.7720e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   4 | Iter:    984/  2110 | global iter:    984/  2110 | loss: 0.0700 | ds_loss: 0.0000 | lr: 2.7683e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    984/  2110 | global iter:    984/  2110 | loss: 0.0732 | ds_loss: 0.0000 | lr: 2.7683e-05 | scale:     1.0000 | micro time: 3.791 | step time: 3.793
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    985/  2110 | global iter:    985/  2110 | loss: 0.0711 | ds_loss: 0.0000 | lr: 2.7646e-05 | scale:     1.0000 | micro time: 3.792 | step time: 0.000
train | epoch   4 | Iter:    986/  2110 | global iter:    986/  2110 | loss: 0.0511 | ds_loss: 0.0000 | lr: 2.7609e-05 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   4 | Iter:    987/  2110 | global iter:    987/  2110 | loss: 0.0760 | ds_loss: 0.0000 | lr: 2.7572e-05 | scale:     1.0000 | micro time: 3.807 | step time: 0.000
train | epoch   4 | Iter:    988/  2110 | global iter:    988/  2110 | loss: 0.1197 | ds_loss: 0.0000 | lr: 2.7535e-05 | scale:     1.0000 | micro time: 3.822 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    988/  2110 | global iter:    988/  2110 | loss: 0.0795 | ds_loss: 0.0000 | lr: 2.7535e-05 | scale:     1.0000 | micro time: 3.822 | step time: 3.799
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    989/  2110 | global iter:    989/  2110 | loss: 0.0534 | ds_loss: 0.0000 | lr: 2.7498e-05 | scale:     1.0000 | micro time: 3.813 | step time: 0.000
train | epoch   4 | Iter:    990/  2110 | global iter:    990/  2110 | loss: 0.0575 | ds_loss: 0.0000 | lr: 2.7461e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   4 | Iter:    991/  2110 | global iter:    991/  2110 | loss: 0.1065 | ds_loss: 0.0000 | lr: 2.7424e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   4 | Iter:    992/  2110 | global iter:    992/  2110 | loss: 0.0431 | ds_loss: 0.0000 | lr: 2.7387e-05 | scale:     1.0000 | micro time: 3.815 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    992/  2110 | global iter:    992/  2110 | loss: 0.0651 | ds_loss: 0.0000 | lr: 2.7387e-05 | scale:     1.0000 | micro time: 3.815 | step time: 3.804
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    993/  2110 | global iter:    993/  2110 | loss: 0.0426 | ds_loss: 0.0000 | lr: 2.7350e-05 | scale:     1.0000 | micro time: 3.800 | step time: 0.000
train | epoch   4 | Iter:    994/  2110 | global iter:    994/  2110 | loss: 0.0477 | ds_loss: 0.0000 | lr: 2.7313e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   4 | Iter:    995/  2110 | global iter:    995/  2110 | loss: 0.0857 | ds_loss: 0.0000 | lr: 2.7276e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   4 | Iter:    996/  2110 | global iter:    996/  2110 | loss: 0.0654 | ds_loss: 0.0000 | lr: 2.7239e-05 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:    996/  2110 | global iter:    996/  2110 | loss: 0.0603 | ds_loss: 0.0000 | lr: 2.7239e-05 | scale:     1.0000 | micro time: 3.799 | step time: 3.792
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:    997/  2110 | global iter:    997/  2110 | loss: 0.0792 | ds_loss: 0.0000 | lr: 2.7202e-05 | scale:     1.0000 | micro time: 3.805 | step time: 0.000
train | epoch   4 | Iter:    998/  2110 | global iter:    998/  2110 | loss: 0.0822 | ds_loss: 0.0000 | lr: 2.7165e-05 | scale:     1.0000 | micro time: 3.838 | step time: 0.000
train | epoch   4 | Iter:    999/  2110 | global iter:    999/  2110 | loss: 0.0488 | ds_loss: 0.0000 | lr: 2.7128e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   4 | Iter:   1000/  2110 | global iter:   1000/  2110 | loss: 0.1077 | ds_loss: 0.0000 | lr: 2.7091e-05 | scale:     1.0000 | micro time: 3.763 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   1000/  2110 | global iter:   1000/  2110 | loss: 0.0795 | ds_loss: 0.0000 | lr: 2.7091e-05 | scale:     1.0000 | micro time: 3.763 | step time: 3.802
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   1001/  2110 | global iter:   1001/  2110 | loss: 0.1024 | ds_loss: 0.0000 | lr: 2.7054e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   4 | Iter:   1002/  2110 | global iter:   1002/  2110 | loss: 0.0528 | ds_loss: 0.0000 | lr: 2.7017e-05 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
train | epoch   4 | Iter:   1003/  2110 | global iter:   1003/  2110 | loss: 0.1126 | ds_loss: 0.0000 | lr: 2.6980e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   4 | Iter:   1004/  2110 | global iter:   1004/  2110 | loss: 0.0231 | ds_loss: 0.0000 | lr: 2.6943e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   1004/  2110 | global iter:   1004/  2110 | loss: 0.0727 | ds_loss: 0.0000 | lr: 2.6943e-05 | scale:     1.0000 | micro time: 3.790 | step time: 3.798
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   1005/  2110 | global iter:   1005/  2110 | loss: 0.0606 | ds_loss: 0.0000 | lr: 2.6906e-05 | scale:     1.0000 | micro time: 3.768 | step time: 0.000
train | epoch   4 | Iter:   1006/  2110 | global iter:   1006/  2110 | loss: 0.0341 | ds_loss: 0.0000 | lr: 2.6869e-05 | scale:     1.0000 | micro time: 3.813 | step time: 0.000
train | epoch   4 | Iter:   1007/  2110 | global iter:   1007/  2110 | loss: 0.0604 | ds_loss: 0.0000 | lr: 2.6832e-05 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   4 | Iter:   1008/  2110 | global iter:   1008/  2110 | loss: 0.0870 | ds_loss: 0.0000 | lr: 2.6795e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   1008/  2110 | global iter:   1008/  2110 | loss: 0.0605 | ds_loss: 0.0000 | lr: 2.6795e-05 | scale:     1.0000 | micro time: 3.810 | step time: 3.793
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   1009/  2110 | global iter:   1009/  2110 | loss: 0.0866 | ds_loss: 0.0000 | lr: 2.6757e-05 | scale:     1.0000 | micro time: 3.804 | step time: 0.000
train | epoch   4 | Iter:   1010/  2110 | global iter:   1010/  2110 | loss: 0.0837 | ds_loss: 0.0000 | lr: 2.6720e-05 | scale:     1.0000 | micro time: 3.767 | step time: 0.000
train | epoch   4 | Iter:   1011/  2110 | global iter:   1011/  2110 | loss: 0.0642 | ds_loss: 0.0000 | lr: 2.6683e-05 | scale:     1.0000 | micro time: 3.797 | step time: 0.000
train | epoch   4 | Iter:   1012/  2110 | global iter:   1012/  2110 | loss: 0.0975 | ds_loss: 0.0000 | lr: 2.6646e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   1012/  2110 | global iter:   1012/  2110 | loss: 0.0830 | ds_loss: 0.0000 | lr: 2.6646e-05 | scale:     1.0000 | micro time: 3.786 | step time: 3.788
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   1013/  2110 | global iter:   1013/  2110 | loss: 0.0658 | ds_loss: 0.0000 | lr: 2.6609e-05 | scale:     1.0000 | micro time: 3.797 | step time: 0.000
train | epoch   4 | Iter:   1014/  2110 | global iter:   1014/  2110 | loss: 0.0474 | ds_loss: 0.0000 | lr: 2.6572e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   4 | Iter:   1015/  2110 | global iter:   1015/  2110 | loss: 0.0842 | ds_loss: 0.0000 | lr: 2.6535e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   4 | Iter:   1016/  2110 | global iter:   1016/  2110 | loss: 0.0592 | ds_loss: 0.0000 | lr: 2.6498e-05 | scale:     1.0000 | micro time: 3.784 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   1016/  2110 | global iter:   1016/  2110 | loss: 0.0642 | ds_loss: 0.0000 | lr: 2.6498e-05 | scale:     1.0000 | micro time: 3.784 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   1017/  2110 | global iter:   1017/  2110 | loss: 0.1087 | ds_loss: 0.0000 | lr: 2.6461e-05 | scale:     1.0000 | micro time: 3.797 | step time: 0.000
train | epoch   4 | Iter:   1018/  2110 | global iter:   1018/  2110 | loss: 0.0681 | ds_loss: 0.0000 | lr: 2.6424e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   4 | Iter:   1019/  2110 | global iter:   1019/  2110 | loss: 0.0584 | ds_loss: 0.0000 | lr: 2.6387e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   4 | Iter:   1020/  2110 | global iter:   1020/  2110 | loss: 0.0780 | ds_loss: 0.0000 | lr: 2.6350e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   1020/  2110 | global iter:   1020/  2110 | loss: 0.0783 | ds_loss: 0.0000 | lr: 2.6350e-05 | scale:     1.0000 | micro time: 3.787 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   1021/  2110 | global iter:   1021/  2110 | loss: 0.0673 | ds_loss: 0.0000 | lr: 2.6312e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   4 | Iter:   1022/  2110 | global iter:   1022/  2110 | loss: 0.0793 | ds_loss: 0.0000 | lr: 2.6275e-05 | scale:     1.0000 | micro time: 3.774 | step time: 0.000
train | epoch   4 | Iter:   1023/  2110 | global iter:   1023/  2110 | loss: 0.0960 | ds_loss: 0.0000 | lr: 2.6238e-05 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   4 | Iter:   1024/  2110 | global iter:   1024/  2110 | loss: 0.0948 | ds_loss: 0.0000 | lr: 2.6201e-05 | scale:     1.0000 | micro time: 3.792 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   1024/  2110 | global iter:   1024/  2110 | loss: 0.0844 | ds_loss: 0.0000 | lr: 2.6201e-05 | scale:     1.0000 | micro time: 3.792 | step time: 3.790
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   1025/  2110 | global iter:   1025/  2110 | loss: 0.0784 | ds_loss: 0.0000 | lr: 2.6164e-05 | scale:     1.0000 | micro time: 3.800 | step time: 0.000
train | epoch   4 | Iter:   1026/  2110 | global iter:   1026/  2110 | loss: 0.0538 | ds_loss: 0.0000 | lr: 2.6127e-05 | scale:     1.0000 | micro time: 3.805 | step time: 0.000
train | epoch   4 | Iter:   1027/  2110 | global iter:   1027/  2110 | loss: 0.0718 | ds_loss: 0.0000 | lr: 2.6090e-05 | scale:     1.0000 | micro time: 3.773 | step time: 0.000
train | epoch   4 | Iter:   1028/  2110 | global iter:   1028/  2110 | loss: 0.0661 | ds_loss: 0.0000 | lr: 2.6053e-05 | scale:     1.0000 | micro time: 3.804 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   1028/  2110 | global iter:   1028/  2110 | loss: 0.0675 | ds_loss: 0.0000 | lr: 2.6053e-05 | scale:     1.0000 | micro time: 3.804 | step time: 3.796
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   1029/  2110 | global iter:   1029/  2110 | loss: 0.0658 | ds_loss: 0.0000 | lr: 2.6016e-05 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   4 | Iter:   1030/  2110 | global iter:   1030/  2110 | loss: 0.0631 | ds_loss: 0.0000 | lr: 2.5978e-05 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   4 | Iter:   1031/  2110 | global iter:   1031/  2110 | loss: 0.0488 | ds_loss: 0.0000 | lr: 2.5941e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   4 | Iter:   1032/  2110 | global iter:   1032/  2110 | loss: 0.0466 | ds_loss: 0.0000 | lr: 2.5904e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   1032/  2110 | global iter:   1032/  2110 | loss: 0.0561 | ds_loss: 0.0000 | lr: 2.5904e-05 | scale:     1.0000 | micro time: 3.785 | step time: 3.780
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   1033/  2110 | global iter:   1033/  2110 | loss: 0.0621 | ds_loss: 0.0000 | lr: 2.5867e-05 | scale:     1.0000 | micro time: 3.792 | step time: 0.000
train | epoch   4 | Iter:   1034/  2110 | global iter:   1034/  2110 | loss: 0.1613 | ds_loss: 0.0000 | lr: 2.5830e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   4 | Iter:   1035/  2110 | global iter:   1035/  2110 | loss: 0.0659 | ds_loss: 0.0000 | lr: 2.5793e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   4 | Iter:   1036/  2110 | global iter:   1036/  2110 | loss: 0.0858 | ds_loss: 0.0000 | lr: 2.5756e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   1036/  2110 | global iter:   1036/  2110 | loss: 0.0938 | ds_loss: 0.0000 | lr: 2.5756e-05 | scale:     1.0000 | micro time: 3.798 | step time: 3.797
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   1037/  2110 | global iter:   1037/  2110 | loss: 0.0943 | ds_loss: 0.0000 | lr: 2.5719e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   4 | Iter:   1038/  2110 | global iter:   1038/  2110 | loss: 0.0214 | ds_loss: 0.0000 | lr: 2.5681e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   4 | Iter:   1039/  2110 | global iter:   1039/  2110 | loss: 0.0911 | ds_loss: 0.0000 | lr: 2.5644e-05 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   4 | Iter:   1040/  2110 | global iter:   1040/  2110 | loss: 0.1426 | ds_loss: 0.0000 | lr: 2.5607e-05 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   1040/  2110 | global iter:   1040/  2110 | loss: 0.0874 | ds_loss: 0.0000 | lr: 2.5607e-05 | scale:     1.0000 | micro time: 3.799 | step time: 3.796
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   1041/  2110 | global iter:   1041/  2110 | loss: 0.0506 | ds_loss: 0.0000 | lr: 2.5570e-05 | scale:     1.0000 | micro time: 3.819 | step time: 0.000
train | epoch   4 | Iter:   1042/  2110 | global iter:   1042/  2110 | loss: 0.0533 | ds_loss: 0.0000 | lr: 2.5533e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   4 | Iter:   1043/  2110 | global iter:   1043/  2110 | loss: 0.1231 | ds_loss: 0.0000 | lr: 2.5496e-05 | scale:     1.0000 | micro time: 3.811 | step time: 0.000
train | epoch   4 | Iter:   1044/  2110 | global iter:   1044/  2110 | loss: 0.0631 | ds_loss: 0.0000 | lr: 2.5459e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   1044/  2110 | global iter:   1044/  2110 | loss: 0.0725 | ds_loss: 0.0000 | lr: 2.5459e-05 | scale:     1.0000 | micro time: 3.787 | step time: 3.797
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   1045/  2110 | global iter:   1045/  2110 | loss: 0.0523 | ds_loss: 0.0000 | lr: 2.5421e-05 | scale:     1.0000 | micro time: 3.805 | step time: 0.000
train | epoch   4 | Iter:   1046/  2110 | global iter:   1046/  2110 | loss: 0.0647 | ds_loss: 0.0000 | lr: 2.5384e-05 | scale:     1.0000 | micro time: 3.814 | step time: 0.000
train | epoch   4 | Iter:   1047/  2110 | global iter:   1047/  2110 | loss: 0.0597 | ds_loss: 0.0000 | lr: 2.5347e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   4 | Iter:   1048/  2110 | global iter:   1048/  2110 | loss: 0.0759 | ds_loss: 0.0000 | lr: 2.5310e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   1048/  2110 | global iter:   1048/  2110 | loss: 0.0632 | ds_loss: 0.0000 | lr: 2.5310e-05 | scale:     1.0000 | micro time: 3.802 | step time: 3.803
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   1049/  2110 | global iter:   1049/  2110 | loss: 0.1226 | ds_loss: 0.0000 | lr: 2.5273e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   4 | Iter:   1050/  2110 | global iter:   1050/  2110 | loss: 0.1025 | ds_loss: 0.0000 | lr: 2.5236e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
train | epoch   4 | Iter:   1051/  2110 | global iter:   1051/  2110 | loss: 0.0867 | ds_loss: 0.0000 | lr: 2.5199e-05 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
train | epoch   4 | Iter:   1052/  2110 | global iter:   1052/  2110 | loss: 0.0648 | ds_loss: 0.0000 | lr: 2.5161e-05 | scale:     1.0000 | micro time: 3.824 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   1052/  2110 | global iter:   1052/  2110 | loss: 0.0941 | ds_loss: 0.0000 | lr: 2.5161e-05 | scale:     1.0000 | micro time: 3.824 | step time: 3.804
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   1053/  2110 | global iter:   1053/  2110 | loss: 0.1243 | ds_loss: 0.0000 | lr: 2.5124e-05 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   4 | Iter:   1054/  2110 | global iter:   1054/  2110 | loss: 0.0620 | ds_loss: 0.0000 | lr: 2.5087e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   4 | Iter:   1055/  2110 | global iter:   1055/  2110 | loss: 0.0327 | ds_loss: 0.0000 | lr: 2.5050e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   4 | Iter:   1056/  2110 | global iter:   1056/  2110 | loss: 0.0514 | ds_loss: 0.0000 | lr: 2.5013e-05 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   1056/  2110 | global iter:   1056/  2110 | loss: 0.0676 | ds_loss: 0.0000 | lr: 2.5013e-05 | scale:     1.0000 | micro time: 3.803 | step time: 3.786
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   1057/  2110 | global iter:   1057/  2110 | loss: 0.0764 | ds_loss: 0.0000 | lr: 2.4976e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   4 | Iter:   1058/  2110 | global iter:   1058/  2110 | loss: 0.0470 | ds_loss: 0.0000 | lr: 2.4939e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   4 | Iter:   1059/  2110 | global iter:   1059/  2110 | loss: 0.0255 | ds_loss: 0.0000 | lr: 2.4901e-05 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   4 | Iter:   1060/  2110 | global iter:   1060/  2110 | loss: 0.0744 | ds_loss: 0.0000 | lr: 2.4864e-05 | scale:     1.0000 | micro time: 3.330 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   1060/  2110 | global iter:   1060/  2110 | loss: 0.0559 | ds_loss: 0.0000 | lr: 2.4864e-05 | scale:     1.0000 | micro time: 3.330 | step time: 3.678
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
Tue Apr  8 08:12:36 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   63C    P0             56W /  250W |   29814MiB /  32768MiB |     68%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |     21%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   49C    P0             48W /  250W |   29848MiB /  32768MiB |     27%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   54C    P0             49W /  250W |   28312MiB /  32768MiB |     19%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 08:12:36 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   63C    P0             56W /  250W |   29814MiB /  32768MiB |     68%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   49C    P0             48W /  250W |   29848MiB /  32768MiB |     27%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   54C    P0             49W /  250W |   28312MiB /  32768MiB |     19%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 08:12:36 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   63C    P0             56W /  250W |   29814MiB /  32768MiB |     68%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |     21%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   49C    P0             48W /  250W |   29848MiB /  32768MiB |     27%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   54C    P0             49W /  250W |   28312MiB /  32768MiB |     19%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 08:12:35 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   63C    P0             55W /  250W |   29814MiB /  32768MiB |     68%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |     21%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   49C    P0             48W /  250W |   29848MiB /  32768MiB |     27%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   54C    P0             49W /  250W |   28312MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

train | epoch   5 | Iter:   1061/  2110 | global iter:   1061/  2110 | loss: 0.0481 | ds_loss: 0.0000 | lr: 2.4827e-05 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   5 | Iter:   1062/  2110 | global iter:   1062/  2110 | loss: 0.0484 | ds_loss: 0.0000 | lr: 2.4790e-05 | scale:     1.0000 | micro time: 3.777 | step time: 0.000
train | epoch   5 | Iter:   1063/  2110 | global iter:   1063/  2110 | loss: 0.0386 | ds_loss: 0.0000 | lr: 2.4753e-05 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
train | epoch   5 | Iter:   1064/  2110 | global iter:   1064/  2110 | loss: 0.0452 | ds_loss: 0.0000 | lr: 2.4716e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1064/  2110 | global iter:   1064/  2110 | loss: 0.0451 | ds_loss: 0.0000 | lr: 2.4716e-05 | scale:     1.0000 | micro time: 3.786 | step time: 3.783
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1065/  2110 | global iter:   1065/  2110 | loss: 0.0417 | ds_loss: 0.0000 | lr: 2.4679e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   5 | Iter:   1066/  2110 | global iter:   1066/  2110 | loss: 0.0264 | ds_loss: 0.0000 | lr: 2.4641e-05 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
train | epoch   5 | Iter:   1067/  2110 | global iter:   1067/  2110 | loss: 0.0145 | ds_loss: 0.0000 | lr: 2.4604e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   5 | Iter:   1068/  2110 | global iter:   1068/  2110 | loss: 0.0245 | ds_loss: 0.0000 | lr: 2.4567e-05 | scale:     1.0000 | micro time: 3.776 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1068/  2110 | global iter:   1068/  2110 | loss: 0.0268 | ds_loss: 0.0000 | lr: 2.4567e-05 | scale:     1.0000 | micro time: 3.776 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1069/  2110 | global iter:   1069/  2110 | loss: 0.0370 | ds_loss: 0.0000 | lr: 2.4530e-05 | scale:     1.0000 | micro time: 3.796 | step time: 0.000
train | epoch   5 | Iter:   1070/  2110 | global iter:   1070/  2110 | loss: 0.0488 | ds_loss: 0.0000 | lr: 2.4493e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
train | epoch   5 | Iter:   1071/  2110 | global iter:   1071/  2110 | loss: 0.0566 | ds_loss: 0.0000 | lr: 2.4456e-05 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   5 | Iter:   1072/  2110 | global iter:   1072/  2110 | loss: 0.0573 | ds_loss: 0.0000 | lr: 2.4419e-05 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1072/  2110 | global iter:   1072/  2110 | loss: 0.0499 | ds_loss: 0.0000 | lr: 2.4419e-05 | scale:     1.0000 | micro time: 3.793 | step time: 3.798
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1073/  2110 | global iter:   1073/  2110 | loss: 0.0335 | ds_loss: 0.0000 | lr: 2.4381e-05 | scale:     1.0000 | micro time: 3.776 | step time: 0.000
train | epoch   5 | Iter:   1074/  2110 | global iter:   1074/  2110 | loss: 0.0700 | ds_loss: 0.0000 | lr: 2.4344e-05 | scale:     1.0000 | micro time: 3.763 | step time: 0.000
train | epoch   5 | Iter:   1075/  2110 | global iter:   1075/  2110 | loss: 0.0273 | ds_loss: 0.0000 | lr: 2.4307e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   5 | Iter:   1076/  2110 | global iter:   1076/  2110 | loss: 0.0421 | ds_loss: 0.0000 | lr: 2.4270e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1076/  2110 | global iter:   1076/  2110 | loss: 0.0432 | ds_loss: 0.0000 | lr: 2.4270e-05 | scale:     1.0000 | micro time: 3.798 | step time: 3.781
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1077/  2110 | global iter:   1077/  2110 | loss: 0.0404 | ds_loss: 0.0000 | lr: 2.4233e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   5 | Iter:   1078/  2110 | global iter:   1078/  2110 | loss: 0.0323 | ds_loss: 0.0000 | lr: 2.4196e-05 | scale:     1.0000 | micro time: 3.814 | step time: 0.000
train | epoch   5 | Iter:   1079/  2110 | global iter:   1079/  2110 | loss: 0.0509 | ds_loss: 0.0000 | lr: 2.4159e-05 | scale:     1.0000 | micro time: 3.770 | step time: 0.000
train | epoch   5 | Iter:   1080/  2110 | global iter:   1080/  2110 | loss: 0.0513 | ds_loss: 0.0000 | lr: 2.4122e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1080/  2110 | global iter:   1080/  2110 | loss: 0.0437 | ds_loss: 0.0000 | lr: 2.4122e-05 | scale:     1.0000 | micro time: 3.790 | step time: 3.790
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1081/  2110 | global iter:   1081/  2110 | loss: 0.0393 | ds_loss: 0.0000 | lr: 2.4084e-05 | scale:     1.0000 | micro time: 3.797 | step time: 0.000
train | epoch   5 | Iter:   1082/  2110 | global iter:   1082/  2110 | loss: 0.0428 | ds_loss: 0.0000 | lr: 2.4047e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   5 | Iter:   1083/  2110 | global iter:   1083/  2110 | loss: 0.0351 | ds_loss: 0.0000 | lr: 2.4010e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   5 | Iter:   1084/  2110 | global iter:   1084/  2110 | loss: 0.0457 | ds_loss: 0.0000 | lr: 2.3973e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1084/  2110 | global iter:   1084/  2110 | loss: 0.0407 | ds_loss: 0.0000 | lr: 2.3973e-05 | scale:     1.0000 | micro time: 3.794 | step time: 3.787
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1085/  2110 | global iter:   1085/  2110 | loss: 0.0348 | ds_loss: 0.0000 | lr: 2.3936e-05 | scale:     1.0000 | micro time: 3.777 | step time: 0.000
train | epoch   5 | Iter:   1086/  2110 | global iter:   1086/  2110 | loss: 0.0224 | ds_loss: 0.0000 | lr: 2.3899e-05 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   5 | Iter:   1087/  2110 | global iter:   1087/  2110 | loss: 0.0605 | ds_loss: 0.0000 | lr: 2.3862e-05 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   5 | Iter:   1088/  2110 | global iter:   1088/  2110 | loss: 0.0121 | ds_loss: 0.0000 | lr: 2.3825e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1088/  2110 | global iter:   1088/  2110 | loss: 0.0324 | ds_loss: 0.0000 | lr: 2.3825e-05 | scale:     1.0000 | micro time: 3.791 | step time: 3.786
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1089/  2110 | global iter:   1089/  2110 | loss: 0.0398 | ds_loss: 0.0000 | lr: 2.3788e-05 | scale:     1.0000 | micro time: 3.776 | step time: 0.000
train | epoch   5 | Iter:   1090/  2110 | global iter:   1090/  2110 | loss: 0.0249 | ds_loss: 0.0000 | lr: 2.3750e-05 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
train | epoch   5 | Iter:   1091/  2110 | global iter:   1091/  2110 | loss: 0.0411 | ds_loss: 0.0000 | lr: 2.3713e-05 | scale:     1.0000 | micro time: 3.808 | step time: 0.000
train | epoch   5 | Iter:   1092/  2110 | global iter:   1092/  2110 | loss: 0.0264 | ds_loss: 0.0000 | lr: 2.3676e-05 | scale:     1.0000 | micro time: 3.776 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1092/  2110 | global iter:   1092/  2110 | loss: 0.0330 | ds_loss: 0.0000 | lr: 2.3676e-05 | scale:     1.0000 | micro time: 3.776 | step time: 3.787
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1093/  2110 | global iter:   1093/  2110 | loss: 0.0509 | ds_loss: 0.0000 | lr: 2.3639e-05 | scale:     1.0000 | micro time: 3.768 | step time: 0.000
train | epoch   5 | Iter:   1094/  2110 | global iter:   1094/  2110 | loss: 0.0286 | ds_loss: 0.0000 | lr: 2.3602e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   5 | Iter:   1095/  2110 | global iter:   1095/  2110 | loss: 0.0531 | ds_loss: 0.0000 | lr: 2.3565e-05 | scale:     1.0000 | micro time: 3.792 | step time: 0.000
train | epoch   5 | Iter:   1096/  2110 | global iter:   1096/  2110 | loss: 0.0323 | ds_loss: 0.0000 | lr: 2.3528e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1096/  2110 | global iter:   1096/  2110 | loss: 0.0412 | ds_loss: 0.0000 | lr: 2.3528e-05 | scale:     1.0000 | micro time: 3.798 | step time: 3.782
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1097/  2110 | global iter:   1097/  2110 | loss: 0.0635 | ds_loss: 0.0000 | lr: 2.3491e-05 | scale:     1.0000 | micro time: 3.800 | step time: 0.000
train | epoch   5 | Iter:   1098/  2110 | global iter:   1098/  2110 | loss: 0.0283 | ds_loss: 0.0000 | lr: 2.3454e-05 | scale:     1.0000 | micro time: 3.767 | step time: 0.000
train | epoch   5 | Iter:   1099/  2110 | global iter:   1099/  2110 | loss: 0.0373 | ds_loss: 0.0000 | lr: 2.3417e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   5 | Iter:   1100/  2110 | global iter:   1100/  2110 | loss: 0.0380 | ds_loss: 0.0000 | lr: 2.3380e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1100/  2110 | global iter:   1100/  2110 | loss: 0.0418 | ds_loss: 0.0000 | lr: 2.3380e-05 | scale:     1.0000 | micro time: 3.806 | step time: 3.790
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1101/  2110 | global iter:   1101/  2110 | loss: 0.0484 | ds_loss: 0.0000 | lr: 2.3343e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   5 | Iter:   1102/  2110 | global iter:   1102/  2110 | loss: 0.0772 | ds_loss: 0.0000 | lr: 2.3305e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   5 | Iter:   1103/  2110 | global iter:   1103/  2110 | loss: 0.0262 | ds_loss: 0.0000 | lr: 2.3268e-05 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   5 | Iter:   1104/  2110 | global iter:   1104/  2110 | loss: 0.0338 | ds_loss: 0.0000 | lr: 2.3231e-05 | scale:     1.0000 | micro time: 3.809 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1104/  2110 | global iter:   1104/  2110 | loss: 0.0464 | ds_loss: 0.0000 | lr: 2.3231e-05 | scale:     1.0000 | micro time: 3.809 | step time: 3.796
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1105/  2110 | global iter:   1105/  2110 | loss: 0.0163 | ds_loss: 0.0000 | lr: 2.3194e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   5 | Iter:   1106/  2110 | global iter:   1106/  2110 | loss: 0.0463 | ds_loss: 0.0000 | lr: 2.3157e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   5 | Iter:   1107/  2110 | global iter:   1107/  2110 | loss: 0.0566 | ds_loss: 0.0000 | lr: 2.3120e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   5 | Iter:   1108/  2110 | global iter:   1108/  2110 | loss: 0.0492 | ds_loss: 0.0000 | lr: 2.3083e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1108/  2110 | global iter:   1108/  2110 | loss: 0.0421 | ds_loss: 0.0000 | lr: 2.3083e-05 | scale:     1.0000 | micro time: 3.790 | step time: 3.790
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1109/  2110 | global iter:   1109/  2110 | loss: 0.0479 | ds_loss: 0.0000 | lr: 2.3046e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   5 | Iter:   1110/  2110 | global iter:   1110/  2110 | loss: 0.0339 | ds_loss: 0.0000 | lr: 2.3009e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   5 | Iter:   1111/  2110 | global iter:   1111/  2110 | loss: 0.0349 | ds_loss: 0.0000 | lr: 2.2972e-05 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   5 | Iter:   1112/  2110 | global iter:   1112/  2110 | loss: 0.0981 | ds_loss: 0.0000 | lr: 2.2935e-05 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1112/  2110 | global iter:   1112/  2110 | loss: 0.0537 | ds_loss: 0.0000 | lr: 2.2935e-05 | scale:     1.0000 | micro time: 3.799 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1113/  2110 | global iter:   1113/  2110 | loss: 0.0580 | ds_loss: 0.0000 | lr: 2.2898e-05 | scale:     1.0000 | micro time: 3.820 | step time: 0.000
train | epoch   5 | Iter:   1114/  2110 | global iter:   1114/  2110 | loss: 0.0613 | ds_loss: 0.0000 | lr: 2.2861e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   5 | Iter:   1115/  2110 | global iter:   1115/  2110 | loss: 0.0407 | ds_loss: 0.0000 | lr: 2.2824e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   5 | Iter:   1116/  2110 | global iter:   1116/  2110 | loss: 0.0553 | ds_loss: 0.0000 | lr: 2.2787e-05 | scale:     1.0000 | micro time: 3.773 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1116/  2110 | global iter:   1116/  2110 | loss: 0.0538 | ds_loss: 0.0000 | lr: 2.2787e-05 | scale:     1.0000 | micro time: 3.773 | step time: 3.792
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1117/  2110 | global iter:   1117/  2110 | loss: 0.0339 | ds_loss: 0.0000 | lr: 2.2750e-05 | scale:     1.0000 | micro time: 3.766 | step time: 0.000
train | epoch   5 | Iter:   1118/  2110 | global iter:   1118/  2110 | loss: 0.0528 | ds_loss: 0.0000 | lr: 2.2713e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   5 | Iter:   1119/  2110 | global iter:   1119/  2110 | loss: 0.0778 | ds_loss: 0.0000 | lr: 2.2676e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   5 | Iter:   1120/  2110 | global iter:   1120/  2110 | loss: 0.0424 | ds_loss: 0.0000 | lr: 2.2639e-05 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1120/  2110 | global iter:   1120/  2110 | loss: 0.0517 | ds_loss: 0.0000 | lr: 2.2639e-05 | scale:     1.0000 | micro time: 3.779 | step time: 3.785
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1121/  2110 | global iter:   1121/  2110 | loss: 0.0452 | ds_loss: 0.0000 | lr: 2.2602e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   5 | Iter:   1122/  2110 | global iter:   1122/  2110 | loss: 0.0346 | ds_loss: 0.0000 | lr: 2.2565e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   5 | Iter:   1123/  2110 | global iter:   1123/  2110 | loss: 0.0419 | ds_loss: 0.0000 | lr: 2.2528e-05 | scale:     1.0000 | micro time: 3.826 | step time: 0.000
train | epoch   5 | Iter:   1124/  2110 | global iter:   1124/  2110 | loss: 0.0188 | ds_loss: 0.0000 | lr: 2.2491e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1124/  2110 | global iter:   1124/  2110 | loss: 0.0351 | ds_loss: 0.0000 | lr: 2.2491e-05 | scale:     1.0000 | micro time: 3.782 | step time: 3.798
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1125/  2110 | global iter:   1125/  2110 | loss: 0.0223 | ds_loss: 0.0000 | lr: 2.2454e-05 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   5 | Iter:   1126/  2110 | global iter:   1126/  2110 | loss: 0.0621 | ds_loss: 0.0000 | lr: 2.2417e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   5 | Iter:   1127/  2110 | global iter:   1127/  2110 | loss: 0.0513 | ds_loss: 0.0000 | lr: 2.2380e-05 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   5 | Iter:   1128/  2110 | global iter:   1128/  2110 | loss: 0.0568 | ds_loss: 0.0000 | lr: 2.2344e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1128/  2110 | global iter:   1128/  2110 | loss: 0.0481 | ds_loss: 0.0000 | lr: 2.2344e-05 | scale:     1.0000 | micro time: 3.802 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1129/  2110 | global iter:   1129/  2110 | loss: 0.0722 | ds_loss: 0.0000 | lr: 2.2307e-05 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   5 | Iter:   1130/  2110 | global iter:   1130/  2110 | loss: 0.0458 | ds_loss: 0.0000 | lr: 2.2270e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   5 | Iter:   1131/  2110 | global iter:   1131/  2110 | loss: 0.0243 | ds_loss: 0.0000 | lr: 2.2233e-05 | scale:     1.0000 | micro time: 3.804 | step time: 0.000
train | epoch   5 | Iter:   1132/  2110 | global iter:   1132/  2110 | loss: 0.0459 | ds_loss: 0.0000 | lr: 2.2196e-05 | scale:     1.0000 | micro time: 3.768 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1132/  2110 | global iter:   1132/  2110 | loss: 0.0470 | ds_loss: 0.0000 | lr: 2.2196e-05 | scale:     1.0000 | micro time: 3.768 | step time: 3.784
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1133/  2110 | global iter:   1133/  2110 | loss: 0.0179 | ds_loss: 0.0000 | lr: 2.2159e-05 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
train | epoch   5 | Iter:   1134/  2110 | global iter:   1134/  2110 | loss: 0.1226 | ds_loss: 0.0000 | lr: 2.2122e-05 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
train | epoch   5 | Iter:   1135/  2110 | global iter:   1135/  2110 | loss: 0.0544 | ds_loss: 0.0000 | lr: 2.2085e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   5 | Iter:   1136/  2110 | global iter:   1136/  2110 | loss: 0.0530 | ds_loss: 0.0000 | lr: 2.2048e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1136/  2110 | global iter:   1136/  2110 | loss: 0.0620 | ds_loss: 0.0000 | lr: 2.2048e-05 | scale:     1.0000 | micro time: 3.782 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1137/  2110 | global iter:   1137/  2110 | loss: 0.0364 | ds_loss: 0.0000 | lr: 2.2011e-05 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   5 | Iter:   1138/  2110 | global iter:   1138/  2110 | loss: 0.0120 | ds_loss: 0.0000 | lr: 2.1975e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   5 | Iter:   1139/  2110 | global iter:   1139/  2110 | loss: 0.0315 | ds_loss: 0.0000 | lr: 2.1938e-05 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   5 | Iter:   1140/  2110 | global iter:   1140/  2110 | loss: 0.0465 | ds_loss: 0.0000 | lr: 2.1901e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1140/  2110 | global iter:   1140/  2110 | loss: 0.0316 | ds_loss: 0.0000 | lr: 2.1901e-05 | scale:     1.0000 | micro time: 3.806 | step time: 3.789
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1141/  2110 | global iter:   1141/  2110 | loss: 0.0328 | ds_loss: 0.0000 | lr: 2.1864e-05 | scale:     1.0000 | micro time: 3.792 | step time: 0.000
train | epoch   5 | Iter:   1142/  2110 | global iter:   1142/  2110 | loss: 0.0279 | ds_loss: 0.0000 | lr: 2.1827e-05 | scale:     1.0000 | micro time: 3.774 | step time: 0.000
train | epoch   5 | Iter:   1143/  2110 | global iter:   1143/  2110 | loss: 0.0296 | ds_loss: 0.0000 | lr: 2.1790e-05 | scale:     1.0000 | micro time: 3.763 | step time: 0.000
train | epoch   5 | Iter:   1144/  2110 | global iter:   1144/  2110 | loss: 0.0322 | ds_loss: 0.0000 | lr: 2.1753e-05 | scale:     1.0000 | micro time: 3.811 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1144/  2110 | global iter:   1144/  2110 | loss: 0.0306 | ds_loss: 0.0000 | lr: 2.1753e-05 | scale:     1.0000 | micro time: 3.811 | step time: 3.785
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1145/  2110 | global iter:   1145/  2110 | loss: 0.0595 | ds_loss: 0.0000 | lr: 2.1717e-05 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   5 | Iter:   1146/  2110 | global iter:   1146/  2110 | loss: 0.0675 | ds_loss: 0.0000 | lr: 2.1680e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   5 | Iter:   1147/  2110 | global iter:   1147/  2110 | loss: 0.0397 | ds_loss: 0.0000 | lr: 2.1643e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   5 | Iter:   1148/  2110 | global iter:   1148/  2110 | loss: 0.0709 | ds_loss: 0.0000 | lr: 2.1606e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1148/  2110 | global iter:   1148/  2110 | loss: 0.0594 | ds_loss: 0.0000 | lr: 2.1606e-05 | scale:     1.0000 | micro time: 3.787 | step time: 3.793
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1149/  2110 | global iter:   1149/  2110 | loss: 0.0149 | ds_loss: 0.0000 | lr: 2.1569e-05 | scale:     1.0000 | micro time: 3.756 | step time: 0.000
train | epoch   5 | Iter:   1150/  2110 | global iter:   1150/  2110 | loss: 0.0391 | ds_loss: 0.0000 | lr: 2.1533e-05 | scale:     1.0000 | micro time: 3.796 | step time: 0.000
train | epoch   5 | Iter:   1151/  2110 | global iter:   1151/  2110 | loss: 0.0408 | ds_loss: 0.0000 | lr: 2.1496e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   5 | Iter:   1152/  2110 | global iter:   1152/  2110 | loss: 0.0759 | ds_loss: 0.0000 | lr: 2.1459e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1152/  2110 | global iter:   1152/  2110 | loss: 0.0427 | ds_loss: 0.0000 | lr: 2.1459e-05 | scale:     1.0000 | micro time: 3.786 | step time: 3.785
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1153/  2110 | global iter:   1153/  2110 | loss: 0.0366 | ds_loss: 0.0000 | lr: 2.1422e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   5 | Iter:   1154/  2110 | global iter:   1154/  2110 | loss: 0.0389 | ds_loss: 0.0000 | lr: 2.1386e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   5 | Iter:   1155/  2110 | global iter:   1155/  2110 | loss: 0.0801 | ds_loss: 0.0000 | lr: 2.1349e-05 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
train | epoch   5 | Iter:   1156/  2110 | global iter:   1156/  2110 | loss: 0.0560 | ds_loss: 0.0000 | lr: 2.1312e-05 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1156/  2110 | global iter:   1156/  2110 | loss: 0.0529 | ds_loss: 0.0000 | lr: 2.1312e-05 | scale:     1.0000 | micro time: 3.788 | step time: 3.793
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1157/  2110 | global iter:   1157/  2110 | loss: 0.0588 | ds_loss: 0.0000 | lr: 2.1275e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
train | epoch   5 | Iter:   1158/  2110 | global iter:   1158/  2110 | loss: 0.0712 | ds_loss: 0.0000 | lr: 2.1239e-05 | scale:     1.0000 | micro time: 3.767 | step time: 0.000
train | epoch   5 | Iter:   1159/  2110 | global iter:   1159/  2110 | loss: 0.0463 | ds_loss: 0.0000 | lr: 2.1202e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   5 | Iter:   1160/  2110 | global iter:   1160/  2110 | loss: 0.0257 | ds_loss: 0.0000 | lr: 2.1165e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1160/  2110 | global iter:   1160/  2110 | loss: 0.0505 | ds_loss: 0.0000 | lr: 2.1165e-05 | scale:     1.0000 | micro time: 3.786 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1161/  2110 | global iter:   1161/  2110 | loss: 0.0633 | ds_loss: 0.0000 | lr: 2.1129e-05 | scale:     1.0000 | micro time: 3.766 | step time: 0.000
train | epoch   5 | Iter:   1162/  2110 | global iter:   1162/  2110 | loss: 0.0350 | ds_loss: 0.0000 | lr: 2.1092e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   5 | Iter:   1163/  2110 | global iter:   1163/  2110 | loss: 0.0482 | ds_loss: 0.0000 | lr: 2.1055e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   5 | Iter:   1164/  2110 | global iter:   1164/  2110 | loss: 0.0617 | ds_loss: 0.0000 | lr: 2.1019e-05 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1164/  2110 | global iter:   1164/  2110 | loss: 0.0521 | ds_loss: 0.0000 | lr: 2.1019e-05 | scale:     1.0000 | micro time: 3.788 | step time: 3.783
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1165/  2110 | global iter:   1165/  2110 | loss: 0.0417 | ds_loss: 0.0000 | lr: 2.0982e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   5 | Iter:   1166/  2110 | global iter:   1166/  2110 | loss: 0.0345 | ds_loss: 0.0000 | lr: 2.0945e-05 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   5 | Iter:   1167/  2110 | global iter:   1167/  2110 | loss: 0.0434 | ds_loss: 0.0000 | lr: 2.0909e-05 | scale:     1.0000 | micro time: 3.792 | step time: 0.000
train | epoch   5 | Iter:   1168/  2110 | global iter:   1168/  2110 | loss: 0.0283 | ds_loss: 0.0000 | lr: 2.0872e-05 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1168/  2110 | global iter:   1168/  2110 | loss: 0.0370 | ds_loss: 0.0000 | lr: 2.0872e-05 | scale:     1.0000 | micro time: 3.783 | step time: 3.784
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1169/  2110 | global iter:   1169/  2110 | loss: 0.0800 | ds_loss: 0.0000 | lr: 2.0835e-05 | scale:     1.0000 | micro time: 3.764 | step time: 0.000
train | epoch   5 | Iter:   1170/  2110 | global iter:   1170/  2110 | loss: 0.0159 | ds_loss: 0.0000 | lr: 2.0799e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   5 | Iter:   1171/  2110 | global iter:   1171/  2110 | loss: 0.0575 | ds_loss: 0.0000 | lr: 2.0762e-05 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
train | epoch   5 | Iter:   1172/  2110 | global iter:   1172/  2110 | loss: 0.0443 | ds_loss: 0.0000 | lr: 2.0726e-05 | scale:     1.0000 | micro time: 3.816 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1172/  2110 | global iter:   1172/  2110 | loss: 0.0495 | ds_loss: 0.0000 | lr: 2.0726e-05 | scale:     1.0000 | micro time: 3.816 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1173/  2110 | global iter:   1173/  2110 | loss: 0.0570 | ds_loss: 0.0000 | lr: 2.0689e-05 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
train | epoch   5 | Iter:   1174/  2110 | global iter:   1174/  2110 | loss: 0.0746 | ds_loss: 0.0000 | lr: 2.0652e-05 | scale:     1.0000 | micro time: 3.774 | step time: 0.000
train | epoch   5 | Iter:   1175/  2110 | global iter:   1175/  2110 | loss: 0.0263 | ds_loss: 0.0000 | lr: 2.0616e-05 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   5 | Iter:   1176/  2110 | global iter:   1176/  2110 | loss: 0.0505 | ds_loss: 0.0000 | lr: 2.0579e-05 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1176/  2110 | global iter:   1176/  2110 | loss: 0.0521 | ds_loss: 0.0000 | lr: 2.0579e-05 | scale:     1.0000 | micro time: 3.783 | step time: 3.782
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1177/  2110 | global iter:   1177/  2110 | loss: 0.0168 | ds_loss: 0.0000 | lr: 2.0543e-05 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   5 | Iter:   1178/  2110 | global iter:   1178/  2110 | loss: 0.0526 | ds_loss: 0.0000 | lr: 2.0506e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   5 | Iter:   1179/  2110 | global iter:   1179/  2110 | loss: 0.0751 | ds_loss: 0.0000 | lr: 2.0470e-05 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
train | epoch   5 | Iter:   1180/  2110 | global iter:   1180/  2110 | loss: 0.0584 | ds_loss: 0.0000 | lr: 2.0433e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1180/  2110 | global iter:   1180/  2110 | loss: 0.0507 | ds_loss: 0.0000 | lr: 2.0433e-05 | scale:     1.0000 | micro time: 3.801 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1181/  2110 | global iter:   1181/  2110 | loss: 0.0593 | ds_loss: 0.0000 | lr: 2.0397e-05 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   5 | Iter:   1182/  2110 | global iter:   1182/  2110 | loss: 0.0290 | ds_loss: 0.0000 | lr: 2.0360e-05 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
train | epoch   5 | Iter:   1183/  2110 | global iter:   1183/  2110 | loss: 0.0347 | ds_loss: 0.0000 | lr: 2.0324e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   5 | Iter:   1184/  2110 | global iter:   1184/  2110 | loss: 0.1082 | ds_loss: 0.0000 | lr: 2.0287e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1184/  2110 | global iter:   1184/  2110 | loss: 0.0578 | ds_loss: 0.0000 | lr: 2.0287e-05 | scale:     1.0000 | micro time: 3.818 | step time: 3.802
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1185/  2110 | global iter:   1185/  2110 | loss: 0.0556 | ds_loss: 0.0000 | lr: 2.0251e-05 | scale:     1.0000 | micro time: 3.784 | step time: 0.000
train | epoch   5 | Iter:   1186/  2110 | global iter:   1186/  2110 | loss: 0.0513 | ds_loss: 0.0000 | lr: 2.0214e-05 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
train | epoch   5 | Iter:   1187/  2110 | global iter:   1187/  2110 | loss: 0.0497 | ds_loss: 0.0000 | lr: 2.0178e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   5 | Iter:   1188/  2110 | global iter:   1188/  2110 | loss: 0.0434 | ds_loss: 0.0000 | lr: 2.0142e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1188/  2110 | global iter:   1188/  2110 | loss: 0.0500 | ds_loss: 0.0000 | lr: 2.0142e-05 | scale:     1.0000 | micro time: 3.801 | step time: 3.793
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1189/  2110 | global iter:   1189/  2110 | loss: 0.0078 | ds_loss: 0.0000 | lr: 2.0105e-05 | scale:     1.0000 | micro time: 3.784 | step time: 0.000
train | epoch   5 | Iter:   1190/  2110 | global iter:   1190/  2110 | loss: 0.0976 | ds_loss: 0.0000 | lr: 2.0069e-05 | scale:     1.0000 | micro time: 3.822 | step time: 0.000
train | epoch   5 | Iter:   1191/  2110 | global iter:   1191/  2110 | loss: 0.0401 | ds_loss: 0.0000 | lr: 2.0032e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   5 | Iter:   1192/  2110 | global iter:   1192/  2110 | loss: 0.0211 | ds_loss: 0.0000 | lr: 1.9996e-05 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1192/  2110 | global iter:   1192/  2110 | loss: 0.0416 | ds_loss: 0.0000 | lr: 1.9996e-05 | scale:     1.0000 | micro time: 3.803 | step time: 3.799
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1193/  2110 | global iter:   1193/  2110 | loss: 0.0555 | ds_loss: 0.0000 | lr: 1.9960e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   5 | Iter:   1194/  2110 | global iter:   1194/  2110 | loss: 0.0425 | ds_loss: 0.0000 | lr: 1.9923e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   5 | Iter:   1195/  2110 | global iter:   1195/  2110 | loss: 0.0268 | ds_loss: 0.0000 | lr: 1.9887e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   5 | Iter:   1196/  2110 | global iter:   1196/  2110 | loss: 0.0596 | ds_loss: 0.0000 | lr: 1.9850e-05 | scale:     1.0000 | micro time: 3.774 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1196/  2110 | global iter:   1196/  2110 | loss: 0.0461 | ds_loss: 0.0000 | lr: 1.9850e-05 | scale:     1.0000 | micro time: 3.774 | step time: 3.792
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1197/  2110 | global iter:   1197/  2110 | loss: 0.0444 | ds_loss: 0.0000 | lr: 1.9814e-05 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
train | epoch   5 | Iter:   1198/  2110 | global iter:   1198/  2110 | loss: 0.0541 | ds_loss: 0.0000 | lr: 1.9778e-05 | scale:     1.0000 | micro time: 3.770 | step time: 0.000
train | epoch   5 | Iter:   1199/  2110 | global iter:   1199/  2110 | loss: 0.0292 | ds_loss: 0.0000 | lr: 1.9742e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   5 | Iter:   1200/  2110 | global iter:   1200/  2110 | loss: 0.0451 | ds_loss: 0.0000 | lr: 1.9705e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1200/  2110 | global iter:   1200/  2110 | loss: 0.0432 | ds_loss: 0.0000 | lr: 1.9705e-05 | scale:     1.0000 | micro time: 3.791 | step time: 3.786
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1201/  2110 | global iter:   1201/  2110 | loss: 0.0666 | ds_loss: 0.0000 | lr: 1.9669e-05 | scale:     1.0000 | micro time: 3.767 | step time: 0.000
train | epoch   5 | Iter:   1202/  2110 | global iter:   1202/  2110 | loss: 0.0313 | ds_loss: 0.0000 | lr: 1.9633e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   5 | Iter:   1203/  2110 | global iter:   1203/  2110 | loss: 0.0471 | ds_loss: 0.0000 | lr: 1.9596e-05 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   5 | Iter:   1204/  2110 | global iter:   1204/  2110 | loss: 0.0267 | ds_loss: 0.0000 | lr: 1.9560e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1204/  2110 | global iter:   1204/  2110 | loss: 0.0429 | ds_loss: 0.0000 | lr: 1.9560e-05 | scale:     1.0000 | micro time: 3.806 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1205/  2110 | global iter:   1205/  2110 | loss: 0.0496 | ds_loss: 0.0000 | lr: 1.9524e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   5 | Iter:   1206/  2110 | global iter:   1206/  2110 | loss: 0.0466 | ds_loss: 0.0000 | lr: 1.9488e-05 | scale:     1.0000 | micro time: 3.762 | step time: 0.000
train | epoch   5 | Iter:   1207/  2110 | global iter:   1207/  2110 | loss: 0.0506 | ds_loss: 0.0000 | lr: 1.9452e-05 | scale:     1.0000 | micro time: 3.808 | step time: 0.000
train | epoch   5 | Iter:   1208/  2110 | global iter:   1208/  2110 | loss: 0.0460 | ds_loss: 0.0000 | lr: 1.9415e-05 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1208/  2110 | global iter:   1208/  2110 | loss: 0.0482 | ds_loss: 0.0000 | lr: 1.9415e-05 | scale:     1.0000 | micro time: 3.781 | step time: 3.784
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1209/  2110 | global iter:   1209/  2110 | loss: 0.0446 | ds_loss: 0.0000 | lr: 1.9379e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   5 | Iter:   1210/  2110 | global iter:   1210/  2110 | loss: 0.0635 | ds_loss: 0.0000 | lr: 1.9343e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   5 | Iter:   1211/  2110 | global iter:   1211/  2110 | loss: 0.0836 | ds_loss: 0.0000 | lr: 1.9307e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   5 | Iter:   1212/  2110 | global iter:   1212/  2110 | loss: 0.0290 | ds_loss: 0.0000 | lr: 1.9271e-05 | scale:     1.0000 | micro time: 3.760 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1212/  2110 | global iter:   1212/  2110 | loss: 0.0552 | ds_loss: 0.0000 | lr: 1.9271e-05 | scale:     1.0000 | micro time: 3.760 | step time: 3.785
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1213/  2110 | global iter:   1213/  2110 | loss: 0.0437 | ds_loss: 0.0000 | lr: 1.9235e-05 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   5 | Iter:   1214/  2110 | global iter:   1214/  2110 | loss: 0.0346 | ds_loss: 0.0000 | lr: 1.9198e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   5 | Iter:   1215/  2110 | global iter:   1215/  2110 | loss: 0.0268 | ds_loss: 0.0000 | lr: 1.9162e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   5 | Iter:   1216/  2110 | global iter:   1216/  2110 | loss: 0.0076 | ds_loss: 0.0000 | lr: 1.9126e-05 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1216/  2110 | global iter:   1216/  2110 | loss: 0.0282 | ds_loss: 0.0000 | lr: 1.9126e-05 | scale:     1.0000 | micro time: 3.778 | step time: 3.788
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1217/  2110 | global iter:   1217/  2110 | loss: 0.0324 | ds_loss: 0.0000 | lr: 1.9090e-05 | scale:     1.0000 | micro time: 3.760 | step time: 0.000
train | epoch   5 | Iter:   1218/  2110 | global iter:   1218/  2110 | loss: 0.0426 | ds_loss: 0.0000 | lr: 1.9054e-05 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   5 | Iter:   1219/  2110 | global iter:   1219/  2110 | loss: 0.0499 | ds_loss: 0.0000 | lr: 1.9018e-05 | scale:     1.0000 | micro time: 3.773 | step time: 0.000
train | epoch   5 | Iter:   1220/  2110 | global iter:   1220/  2110 | loss: 0.0323 | ds_loss: 0.0000 | lr: 1.8982e-05 | scale:     1.0000 | micro time: 3.814 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1220/  2110 | global iter:   1220/  2110 | loss: 0.0393 | ds_loss: 0.0000 | lr: 1.8982e-05 | scale:     1.0000 | micro time: 3.814 | step time: 3.786
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1221/  2110 | global iter:   1221/  2110 | loss: 0.0305 | ds_loss: 0.0000 | lr: 1.8946e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   5 | Iter:   1222/  2110 | global iter:   1222/  2110 | loss: 0.0671 | ds_loss: 0.0000 | lr: 1.8910e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   5 | Iter:   1223/  2110 | global iter:   1223/  2110 | loss: 0.0492 | ds_loss: 0.0000 | lr: 1.8874e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   5 | Iter:   1224/  2110 | global iter:   1224/  2110 | loss: 0.0612 | ds_loss: 0.0000 | lr: 1.8838e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1224/  2110 | global iter:   1224/  2110 | loss: 0.0520 | ds_loss: 0.0000 | lr: 1.8838e-05 | scale:     1.0000 | micro time: 3.771 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1225/  2110 | global iter:   1225/  2110 | loss: 0.0235 | ds_loss: 0.0000 | lr: 1.8802e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   5 | Iter:   1226/  2110 | global iter:   1226/  2110 | loss: 0.0698 | ds_loss: 0.0000 | lr: 1.8766e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   5 | Iter:   1227/  2110 | global iter:   1227/  2110 | loss: 0.0260 | ds_loss: 0.0000 | lr: 1.8730e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   5 | Iter:   1228/  2110 | global iter:   1228/  2110 | loss: 0.0323 | ds_loss: 0.0000 | lr: 1.8694e-05 | scale:     1.0000 | micro time: 3.809 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1228/  2110 | global iter:   1228/  2110 | loss: 0.0379 | ds_loss: 0.0000 | lr: 1.8694e-05 | scale:     1.0000 | micro time: 3.809 | step time: 3.796
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1229/  2110 | global iter:   1229/  2110 | loss: 0.0331 | ds_loss: 0.0000 | lr: 1.8658e-05 | scale:     1.0000 | micro time: 3.797 | step time: 0.000
train | epoch   5 | Iter:   1230/  2110 | global iter:   1230/  2110 | loss: 0.0519 | ds_loss: 0.0000 | lr: 1.8622e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   5 | Iter:   1231/  2110 | global iter:   1231/  2110 | loss: 0.0913 | ds_loss: 0.0000 | lr: 1.8586e-05 | scale:     1.0000 | micro time: 3.770 | step time: 0.000
train | epoch   5 | Iter:   1232/  2110 | global iter:   1232/  2110 | loss: 0.0345 | ds_loss: 0.0000 | lr: 1.8551e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1232/  2110 | global iter:   1232/  2110 | loss: 0.0527 | ds_loss: 0.0000 | lr: 1.8551e-05 | scale:     1.0000 | micro time: 3.806 | step time: 3.792
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1233/  2110 | global iter:   1233/  2110 | loss: 0.0201 | ds_loss: 0.0000 | lr: 1.8515e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   5 | Iter:   1234/  2110 | global iter:   1234/  2110 | loss: 0.0099 | ds_loss: 0.0000 | lr: 1.8479e-05 | scale:     1.0000 | micro time: 3.773 | step time: 0.000
train | epoch   5 | Iter:   1235/  2110 | global iter:   1235/  2110 | loss: 0.0119 | ds_loss: 0.0000 | lr: 1.8443e-05 | scale:     1.0000 | micro time: 3.792 | step time: 0.000
train | epoch   5 | Iter:   1236/  2110 | global iter:   1236/  2110 | loss: 0.0892 | ds_loss: 0.0000 | lr: 1.8407e-05 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1236/  2110 | global iter:   1236/  2110 | loss: 0.0328 | ds_loss: 0.0000 | lr: 1.8407e-05 | scale:     1.0000 | micro time: 3.779 | step time: 3.785
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1237/  2110 | global iter:   1237/  2110 | loss: 0.0632 | ds_loss: 0.0000 | lr: 1.8371e-05 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   5 | Iter:   1238/  2110 | global iter:   1238/  2110 | loss: 0.0458 | ds_loss: 0.0000 | lr: 1.8336e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   5 | Iter:   1239/  2110 | global iter:   1239/  2110 | loss: 0.0364 | ds_loss: 0.0000 | lr: 1.8300e-05 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   5 | Iter:   1240/  2110 | global iter:   1240/  2110 | loss: 0.0351 | ds_loss: 0.0000 | lr: 1.8264e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1240/  2110 | global iter:   1240/  2110 | loss: 0.0451 | ds_loss: 0.0000 | lr: 1.8264e-05 | scale:     1.0000 | micro time: 3.786 | step time: 3.788
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1241/  2110 | global iter:   1241/  2110 | loss: 0.0355 | ds_loss: 0.0000 | lr: 1.8228e-05 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
train | epoch   5 | Iter:   1242/  2110 | global iter:   1242/  2110 | loss: 0.0557 | ds_loss: 0.0000 | lr: 1.8193e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   5 | Iter:   1243/  2110 | global iter:   1243/  2110 | loss: 0.0403 | ds_loss: 0.0000 | lr: 1.8157e-05 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   5 | Iter:   1244/  2110 | global iter:   1244/  2110 | loss: 0.0388 | ds_loss: 0.0000 | lr: 1.8121e-05 | scale:     1.0000 | micro time: 3.822 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1244/  2110 | global iter:   1244/  2110 | loss: 0.0426 | ds_loss: 0.0000 | lr: 1.8121e-05 | scale:     1.0000 | micro time: 3.822 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1245/  2110 | global iter:   1245/  2110 | loss: 0.0425 | ds_loss: 0.0000 | lr: 1.8086e-05 | scale:     1.0000 | micro time: 3.817 | step time: 0.000
train | epoch   5 | Iter:   1246/  2110 | global iter:   1246/  2110 | loss: 0.0502 | ds_loss: 0.0000 | lr: 1.8050e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   5 | Iter:   1247/  2110 | global iter:   1247/  2110 | loss: 0.0409 | ds_loss: 0.0000 | lr: 1.8014e-05 | scale:     1.0000 | micro time: 3.767 | step time: 0.000
train | epoch   5 | Iter:   1248/  2110 | global iter:   1248/  2110 | loss: 0.0448 | ds_loss: 0.0000 | lr: 1.7979e-05 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1248/  2110 | global iter:   1248/  2110 | loss: 0.0446 | ds_loss: 0.0000 | lr: 1.7979e-05 | scale:     1.0000 | micro time: 3.779 | step time: 3.790
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1249/  2110 | global iter:   1249/  2110 | loss: 0.0758 | ds_loss: 0.0000 | lr: 1.7943e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   5 | Iter:   1250/  2110 | global iter:   1250/  2110 | loss: 0.0151 | ds_loss: 0.0000 | lr: 1.7907e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   5 | Iter:   1251/  2110 | global iter:   1251/  2110 | loss: 0.0492 | ds_loss: 0.0000 | lr: 1.7872e-05 | scale:     1.0000 | micro time: 3.800 | step time: 0.000
train | epoch   5 | Iter:   1252/  2110 | global iter:   1252/  2110 | loss: 0.0288 | ds_loss: 0.0000 | lr: 1.7836e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1252/  2110 | global iter:   1252/  2110 | loss: 0.0422 | ds_loss: 0.0000 | lr: 1.7836e-05 | scale:     1.0000 | micro time: 3.794 | step time: 3.795
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1253/  2110 | global iter:   1253/  2110 | loss: 0.0391 | ds_loss: 0.0000 | lr: 1.7801e-05 | scale:     1.0000 | micro time: 3.797 | step time: 0.000
train | epoch   5 | Iter:   1254/  2110 | global iter:   1254/  2110 | loss: 0.0781 | ds_loss: 0.0000 | lr: 1.7765e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   5 | Iter:   1255/  2110 | global iter:   1255/  2110 | loss: 0.0283 | ds_loss: 0.0000 | lr: 1.7730e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   5 | Iter:   1256/  2110 | global iter:   1256/  2110 | loss: 0.0370 | ds_loss: 0.0000 | lr: 1.7694e-05 | scale:     1.0000 | micro time: 3.813 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1256/  2110 | global iter:   1256/  2110 | loss: 0.0457 | ds_loss: 0.0000 | lr: 1.7694e-05 | scale:     1.0000 | micro time: 3.813 | step time: 3.800
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1257/  2110 | global iter:   1257/  2110 | loss: 0.0189 | ds_loss: 0.0000 | lr: 1.7659e-05 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   5 | Iter:   1258/  2110 | global iter:   1258/  2110 | loss: 0.0409 | ds_loss: 0.0000 | lr: 1.7623e-05 | scale:     1.0000 | micro time: 3.792 | step time: 0.000
train | epoch   5 | Iter:   1259/  2110 | global iter:   1259/  2110 | loss: 0.0251 | ds_loss: 0.0000 | lr: 1.7588e-05 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   5 | Iter:   1260/  2110 | global iter:   1260/  2110 | loss: 0.0771 | ds_loss: 0.0000 | lr: 1.7552e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1260/  2110 | global iter:   1260/  2110 | loss: 0.0405 | ds_loss: 0.0000 | lr: 1.7552e-05 | scale:     1.0000 | micro time: 3.786 | step time: 3.783
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1261/  2110 | global iter:   1261/  2110 | loss: 0.1053 | ds_loss: 0.0000 | lr: 1.7517e-05 | scale:     1.0000 | micro time: 3.805 | step time: 0.000
train | epoch   5 | Iter:   1262/  2110 | global iter:   1262/  2110 | loss: 0.0135 | ds_loss: 0.0000 | lr: 1.7481e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   5 | Iter:   1263/  2110 | global iter:   1263/  2110 | loss: 0.0241 | ds_loss: 0.0000 | lr: 1.7446e-05 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
train | epoch   5 | Iter:   1264/  2110 | global iter:   1264/  2110 | loss: 0.0431 | ds_loss: 0.0000 | lr: 1.7411e-05 | scale:     1.0000 | micro time: 3.780 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1264/  2110 | global iter:   1264/  2110 | loss: 0.0465 | ds_loss: 0.0000 | lr: 1.7411e-05 | scale:     1.0000 | micro time: 3.780 | step time: 3.790
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1265/  2110 | global iter:   1265/  2110 | loss: 0.0454 | ds_loss: 0.0000 | lr: 1.7375e-05 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   5 | Iter:   1266/  2110 | global iter:   1266/  2110 | loss: 0.0367 | ds_loss: 0.0000 | lr: 1.7340e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   5 | Iter:   1267/  2110 | global iter:   1267/  2110 | loss: 0.0412 | ds_loss: 0.0000 | lr: 1.7305e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   5 | Iter:   1268/  2110 | global iter:   1268/  2110 | loss: 0.0304 | ds_loss: 0.0000 | lr: 1.7269e-05 | scale:     1.0000 | micro time: 3.770 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1268/  2110 | global iter:   1268/  2110 | loss: 0.0384 | ds_loss: 0.0000 | lr: 1.7269e-05 | scale:     1.0000 | micro time: 3.770 | step time: 3.783
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   1269/  2110 | global iter:   1269/  2110 | loss: 0.0319 | ds_loss: 0.0000 | lr: 1.7234e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   5 | Iter:   1270/  2110 | global iter:   1270/  2110 | loss: 0.0427 | ds_loss: 0.0000 | lr: 1.7199e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   5 | Iter:   1271/  2110 | global iter:   1271/  2110 | loss: 0.0431 | ds_loss: 0.0000 | lr: 1.7164e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   5 | Iter:   1272/  2110 | global iter:   1272/  2110 | loss: 0.0195 | ds_loss: 0.0000 | lr: 1.7128e-05 | scale:     1.0000 | micro time: 3.315 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   1272/  2110 | global iter:   1272/  2110 | loss: 0.0343 | ds_loss: 0.0000 | lr: 1.7128e-05 | scale:     1.0000 | micro time: 3.315 | step time: 3.670
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
Tue Apr  8 08:26:00 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   63C    P0             56W /  250W |   29814MiB /  32768MiB |     83%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   50C    P0             49W /  250W |   29848MiB /  32768MiB |     95%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   54C    P0             49W /  250W |   28312MiB /  32768MiB |     31%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 08:26:00 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   63C    P0             56W /  250W |   29814MiB /  32768MiB |     83%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |     28%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   50C    P0             49W /  250W |   29848MiB /  32768MiB |     95%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   55C    P0            197W /  250W |   28312MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 08:26:00 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   63C    P0             56W /  250W |   29814MiB /  32768MiB |     83%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |     28%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   50C    P0             49W /  250W |   29848MiB /  32768MiB |     95%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   54C    P0             49W /  250W |   28312MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 08:26:00 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   63C    P0             56W /  250W |   29814MiB /  32768MiB |     83%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |     28%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   50C    P0             49W /  250W |   29848MiB /  32768MiB |      1%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   58C    P0            247W /  250W |   28312MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

train | epoch   6 | Iter:   1273/  2110 | global iter:   1273/  2110 | loss: 0.0130 | ds_loss: 0.0000 | lr: 1.7093e-05 | scale:     1.0000 | micro time: 3.902 | step time: 0.000
train | epoch   6 | Iter:   1274/  2110 | global iter:   1274/  2110 | loss: 0.0205 | ds_loss: 0.0000 | lr: 1.7058e-05 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
train | epoch   6 | Iter:   1275/  2110 | global iter:   1275/  2110 | loss: 0.0296 | ds_loss: 0.0000 | lr: 1.7023e-05 | scale:     1.0000 | micro time: 3.772 | step time: 0.000
train | epoch   6 | Iter:   1276/  2110 | global iter:   1276/  2110 | loss: 0.0238 | ds_loss: 0.0000 | lr: 1.6988e-05 | scale:     1.0000 | micro time: 3.756 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1276/  2110 | global iter:   1276/  2110 | loss: 0.0217 | ds_loss: 0.0000 | lr: 1.6988e-05 | scale:     1.0000 | micro time: 3.756 | step time: 3.804
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1277/  2110 | global iter:   1277/  2110 | loss: 0.0196 | ds_loss: 0.0000 | lr: 1.6952e-05 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   6 | Iter:   1278/  2110 | global iter:   1278/  2110 | loss: 0.0372 | ds_loss: 0.0000 | lr: 1.6917e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   6 | Iter:   1279/  2110 | global iter:   1279/  2110 | loss: 0.0217 | ds_loss: 0.0000 | lr: 1.6882e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   6 | Iter:   1280/  2110 | global iter:   1280/  2110 | loss: 0.0199 | ds_loss: 0.0000 | lr: 1.6847e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1280/  2110 | global iter:   1280/  2110 | loss: 0.0246 | ds_loss: 0.0000 | lr: 1.6847e-05 | scale:     1.0000 | micro time: 3.801 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1281/  2110 | global iter:   1281/  2110 | loss: 0.0474 | ds_loss: 0.0000 | lr: 1.6812e-05 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   6 | Iter:   1282/  2110 | global iter:   1282/  2110 | loss: 0.0100 | ds_loss: 0.0000 | lr: 1.6777e-05 | scale:     1.0000 | micro time: 3.772 | step time: 0.000
train | epoch   6 | Iter:   1283/  2110 | global iter:   1283/  2110 | loss: 0.0134 | ds_loss: 0.0000 | lr: 1.6742e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
train | epoch   6 | Iter:   1284/  2110 | global iter:   1284/  2110 | loss: 0.0206 | ds_loss: 0.0000 | lr: 1.6707e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1284/  2110 | global iter:   1284/  2110 | loss: 0.0229 | ds_loss: 0.0000 | lr: 1.6707e-05 | scale:     1.0000 | micro time: 3.794 | step time: 3.792
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1285/  2110 | global iter:   1285/  2110 | loss: 0.0103 | ds_loss: 0.0000 | lr: 1.6672e-05 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
train | epoch   6 | Iter:   1286/  2110 | global iter:   1286/  2110 | loss: 0.0289 | ds_loss: 0.0000 | lr: 1.6637e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   6 | Iter:   1287/  2110 | global iter:   1287/  2110 | loss: 0.0443 | ds_loss: 0.0000 | lr: 1.6602e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   6 | Iter:   1288/  2110 | global iter:   1288/  2110 | loss: 0.0390 | ds_loss: 0.0000 | lr: 1.6567e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1288/  2110 | global iter:   1288/  2110 | loss: 0.0306 | ds_loss: 0.0000 | lr: 1.6567e-05 | scale:     1.0000 | micro time: 3.790 | step time: 3.788
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1289/  2110 | global iter:   1289/  2110 | loss: 0.0366 | ds_loss: 0.0000 | lr: 1.6532e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   6 | Iter:   1290/  2110 | global iter:   1290/  2110 | loss: 0.0251 | ds_loss: 0.0000 | lr: 1.6497e-05 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   6 | Iter:   1291/  2110 | global iter:   1291/  2110 | loss: 0.0095 | ds_loss: 0.0000 | lr: 1.6462e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   6 | Iter:   1292/  2110 | global iter:   1292/  2110 | loss: 0.0297 | ds_loss: 0.0000 | lr: 1.6427e-05 | scale:     1.0000 | micro time: 3.815 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1292/  2110 | global iter:   1292/  2110 | loss: 0.0252 | ds_loss: 0.0000 | lr: 1.6427e-05 | scale:     1.0000 | micro time: 3.815 | step time: 3.798
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1293/  2110 | global iter:   1293/  2110 | loss: 0.0168 | ds_loss: 0.0000 | lr: 1.6393e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   6 | Iter:   1294/  2110 | global iter:   1294/  2110 | loss: 0.0172 | ds_loss: 0.0000 | lr: 1.6358e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   6 | Iter:   1295/  2110 | global iter:   1295/  2110 | loss: 0.0219 | ds_loss: 0.0000 | lr: 1.6323e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   6 | Iter:   1296/  2110 | global iter:   1296/  2110 | loss: 0.0211 | ds_loss: 0.0000 | lr: 1.6288e-05 | scale:     1.0000 | micro time: 3.813 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1296/  2110 | global iter:   1296/  2110 | loss: 0.0192 | ds_loss: 0.0000 | lr: 1.6288e-05 | scale:     1.0000 | micro time: 3.813 | step time: 3.793
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1297/  2110 | global iter:   1297/  2110 | loss: 0.0270 | ds_loss: 0.0000 | lr: 1.6253e-05 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
train | epoch   6 | Iter:   1298/  2110 | global iter:   1298/  2110 | loss: 0.0066 | ds_loss: 0.0000 | lr: 1.6219e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   6 | Iter:   1299/  2110 | global iter:   1299/  2110 | loss: 0.0372 | ds_loss: 0.0000 | lr: 1.6184e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   6 | Iter:   1300/  2110 | global iter:   1300/  2110 | loss: 0.0028 | ds_loss: 0.0000 | lr: 1.6149e-05 | scale:     1.0000 | micro time: 3.826 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1300/  2110 | global iter:   1300/  2110 | loss: 0.0184 | ds_loss: 0.0000 | lr: 1.6149e-05 | scale:     1.0000 | micro time: 3.826 | step time: 3.802
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1301/  2110 | global iter:   1301/  2110 | loss: 0.0154 | ds_loss: 0.0000 | lr: 1.6115e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   6 | Iter:   1302/  2110 | global iter:   1302/  2110 | loss: 0.0251 | ds_loss: 0.0000 | lr: 1.6080e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   6 | Iter:   1303/  2110 | global iter:   1303/  2110 | loss: 0.0436 | ds_loss: 0.0000 | lr: 1.6045e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   6 | Iter:   1304/  2110 | global iter:   1304/  2110 | loss: 0.0185 | ds_loss: 0.0000 | lr: 1.6011e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1304/  2110 | global iter:   1304/  2110 | loss: 0.0256 | ds_loss: 0.0000 | lr: 1.6011e-05 | scale:     1.0000 | micro time: 3.771 | step time: 3.786
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1305/  2110 | global iter:   1305/  2110 | loss: 0.0225 | ds_loss: 0.0000 | lr: 1.5976e-05 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   6 | Iter:   1306/  2110 | global iter:   1306/  2110 | loss: 0.0068 | ds_loss: 0.0000 | lr: 1.5941e-05 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
train | epoch   6 | Iter:   1307/  2110 | global iter:   1307/  2110 | loss: 0.0346 | ds_loss: 0.0000 | lr: 1.5907e-05 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
train | epoch   6 | Iter:   1308/  2110 | global iter:   1308/  2110 | loss: 0.0228 | ds_loss: 0.0000 | lr: 1.5872e-05 | scale:     1.0000 | micro time: 3.797 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1308/  2110 | global iter:   1308/  2110 | loss: 0.0217 | ds_loss: 0.0000 | lr: 1.5872e-05 | scale:     1.0000 | micro time: 3.797 | step time: 3.796
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1309/  2110 | global iter:   1309/  2110 | loss: 0.0390 | ds_loss: 0.0000 | lr: 1.5838e-05 | scale:     1.0000 | micro time: 3.805 | step time: 0.000
train | epoch   6 | Iter:   1310/  2110 | global iter:   1310/  2110 | loss: 0.0193 | ds_loss: 0.0000 | lr: 1.5803e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   6 | Iter:   1311/  2110 | global iter:   1311/  2110 | loss: 0.0151 | ds_loss: 0.0000 | lr: 1.5769e-05 | scale:     1.0000 | micro time: 3.814 | step time: 0.000
train | epoch   6 | Iter:   1312/  2110 | global iter:   1312/  2110 | loss: 0.0137 | ds_loss: 0.0000 | lr: 1.5734e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1312/  2110 | global iter:   1312/  2110 | loss: 0.0218 | ds_loss: 0.0000 | lr: 1.5734e-05 | scale:     1.0000 | micro time: 3.818 | step time: 3.808
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1313/  2110 | global iter:   1313/  2110 | loss: 0.0613 | ds_loss: 0.0000 | lr: 1.5700e-05 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   6 | Iter:   1314/  2110 | global iter:   1314/  2110 | loss: 0.0080 | ds_loss: 0.0000 | lr: 1.5665e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   6 | Iter:   1315/  2110 | global iter:   1315/  2110 | loss: 0.0314 | ds_loss: 0.0000 | lr: 1.5631e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   6 | Iter:   1316/  2110 | global iter:   1316/  2110 | loss: 0.0290 | ds_loss: 0.0000 | lr: 1.5597e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1316/  2110 | global iter:   1316/  2110 | loss: 0.0324 | ds_loss: 0.0000 | lr: 1.5597e-05 | scale:     1.0000 | micro time: 3.810 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1317/  2110 | global iter:   1317/  2110 | loss: 0.0648 | ds_loss: 0.0000 | lr: 1.5562e-05 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   6 | Iter:   1318/  2110 | global iter:   1318/  2110 | loss: 0.0371 | ds_loss: 0.0000 | lr: 1.5528e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   6 | Iter:   1319/  2110 | global iter:   1319/  2110 | loss: 0.0331 | ds_loss: 0.0000 | lr: 1.5493e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
train | epoch   6 | Iter:   1320/  2110 | global iter:   1320/  2110 | loss: 0.0151 | ds_loss: 0.0000 | lr: 1.5459e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1320/  2110 | global iter:   1320/  2110 | loss: 0.0375 | ds_loss: 0.0000 | lr: 1.5459e-05 | scale:     1.0000 | micro time: 3.790 | step time: 3.798
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1321/  2110 | global iter:   1321/  2110 | loss: 0.0503 | ds_loss: 0.0000 | lr: 1.5425e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   6 | Iter:   1322/  2110 | global iter:   1322/  2110 | loss: 0.0162 | ds_loss: 0.0000 | lr: 1.5391e-05 | scale:     1.0000 | micro time: 3.805 | step time: 0.000
train | epoch   6 | Iter:   1323/  2110 | global iter:   1323/  2110 | loss: 0.0128 | ds_loss: 0.0000 | lr: 1.5356e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   6 | Iter:   1324/  2110 | global iter:   1324/  2110 | loss: 0.0313 | ds_loss: 0.0000 | lr: 1.5322e-05 | scale:     1.0000 | micro time: 3.762 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1324/  2110 | global iter:   1324/  2110 | loss: 0.0277 | ds_loss: 0.0000 | lr: 1.5322e-05 | scale:     1.0000 | micro time: 3.762 | step time: 3.784
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1325/  2110 | global iter:   1325/  2110 | loss: 0.0066 | ds_loss: 0.0000 | lr: 1.5288e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   6 | Iter:   1326/  2110 | global iter:   1326/  2110 | loss: 0.0312 | ds_loss: 0.0000 | lr: 1.5254e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   6 | Iter:   1327/  2110 | global iter:   1327/  2110 | loss: 0.0396 | ds_loss: 0.0000 | lr: 1.5220e-05 | scale:     1.0000 | micro time: 3.784 | step time: 0.000
train | epoch   6 | Iter:   1328/  2110 | global iter:   1328/  2110 | loss: 0.0026 | ds_loss: 0.0000 | lr: 1.5185e-05 | scale:     1.0000 | micro time: 3.817 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1328/  2110 | global iter:   1328/  2110 | loss: 0.0200 | ds_loss: 0.0000 | lr: 1.5185e-05 | scale:     1.0000 | micro time: 3.817 | step time: 3.795
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1329/  2110 | global iter:   1329/  2110 | loss: 0.0109 | ds_loss: 0.0000 | lr: 1.5151e-05 | scale:     1.0000 | micro time: 3.776 | step time: 0.000
train | epoch   6 | Iter:   1330/  2110 | global iter:   1330/  2110 | loss: 0.0226 | ds_loss: 0.0000 | lr: 1.5117e-05 | scale:     1.0000 | micro time: 3.811 | step time: 0.000
train | epoch   6 | Iter:   1331/  2110 | global iter:   1331/  2110 | loss: 0.0189 | ds_loss: 0.0000 | lr: 1.5083e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   6 | Iter:   1332/  2110 | global iter:   1332/  2110 | loss: 0.0212 | ds_loss: 0.0000 | lr: 1.5049e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1332/  2110 | global iter:   1332/  2110 | loss: 0.0184 | ds_loss: 0.0000 | lr: 1.5049e-05 | scale:     1.0000 | micro time: 3.794 | step time: 3.792
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1333/  2110 | global iter:   1333/  2110 | loss: 0.0194 | ds_loss: 0.0000 | lr: 1.5015e-05 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   6 | Iter:   1334/  2110 | global iter:   1334/  2110 | loss: 0.0063 | ds_loss: 0.0000 | lr: 1.4981e-05 | scale:     1.0000 | micro time: 3.780 | step time: 0.000
train | epoch   6 | Iter:   1335/  2110 | global iter:   1335/  2110 | loss: 0.0091 | ds_loss: 0.0000 | lr: 1.4947e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   6 | Iter:   1336/  2110 | global iter:   1336/  2110 | loss: 0.0177 | ds_loss: 0.0000 | lr: 1.4913e-05 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1336/  2110 | global iter:   1336/  2110 | loss: 0.0131 | ds_loss: 0.0000 | lr: 1.4913e-05 | scale:     1.0000 | micro time: 3.783 | step time: 3.786
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1337/  2110 | global iter:   1337/  2110 | loss: 0.0391 | ds_loss: 0.0000 | lr: 1.4879e-05 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   6 | Iter:   1338/  2110 | global iter:   1338/  2110 | loss: 0.0393 | ds_loss: 0.0000 | lr: 1.4845e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   6 | Iter:   1339/  2110 | global iter:   1339/  2110 | loss: 0.0381 | ds_loss: 0.0000 | lr: 1.4812e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   6 | Iter:   1340/  2110 | global iter:   1340/  2110 | loss: 0.0242 | ds_loss: 0.0000 | lr: 1.4778e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1340/  2110 | global iter:   1340/  2110 | loss: 0.0352 | ds_loss: 0.0000 | lr: 1.4778e-05 | scale:     1.0000 | micro time: 3.790 | step time: 3.785
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1341/  2110 | global iter:   1341/  2110 | loss: 0.0154 | ds_loss: 0.0000 | lr: 1.4744e-05 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
train | epoch   6 | Iter:   1342/  2110 | global iter:   1342/  2110 | loss: 0.0346 | ds_loss: 0.0000 | lr: 1.4710e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   6 | Iter:   1343/  2110 | global iter:   1343/  2110 | loss: 0.0366 | ds_loss: 0.0000 | lr: 1.4676e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   6 | Iter:   1344/  2110 | global iter:   1344/  2110 | loss: 0.0318 | ds_loss: 0.0000 | lr: 1.4642e-05 | scale:     1.0000 | micro time: 3.760 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1344/  2110 | global iter:   1344/  2110 | loss: 0.0296 | ds_loss: 0.0000 | lr: 1.4642e-05 | scale:     1.0000 | micro time: 3.760 | step time: 3.785
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1345/  2110 | global iter:   1345/  2110 | loss: 0.0066 | ds_loss: 0.0000 | lr: 1.4609e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   6 | Iter:   1346/  2110 | global iter:   1346/  2110 | loss: 0.0220 | ds_loss: 0.0000 | lr: 1.4575e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   6 | Iter:   1347/  2110 | global iter:   1347/  2110 | loss: 0.0473 | ds_loss: 0.0000 | lr: 1.4541e-05 | scale:     1.0000 | micro time: 3.751 | step time: 0.000
train | epoch   6 | Iter:   1348/  2110 | global iter:   1348/  2110 | loss: 0.0268 | ds_loss: 0.0000 | lr: 1.4508e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1348/  2110 | global iter:   1348/  2110 | loss: 0.0257 | ds_loss: 0.0000 | lr: 1.4508e-05 | scale:     1.0000 | micro time: 3.794 | step time: 3.783
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1349/  2110 | global iter:   1349/  2110 | loss: 0.0175 | ds_loss: 0.0000 | lr: 1.4474e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   6 | Iter:   1350/  2110 | global iter:   1350/  2110 | loss: 0.0105 | ds_loss: 0.0000 | lr: 1.4440e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   6 | Iter:   1351/  2110 | global iter:   1351/  2110 | loss: 0.0120 | ds_loss: 0.0000 | lr: 1.4407e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   6 | Iter:   1352/  2110 | global iter:   1352/  2110 | loss: 0.0393 | ds_loss: 0.0000 | lr: 1.4373e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1352/  2110 | global iter:   1352/  2110 | loss: 0.0198 | ds_loss: 0.0000 | lr: 1.4373e-05 | scale:     1.0000 | micro time: 3.785 | step time: 3.790
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1353/  2110 | global iter:   1353/  2110 | loss: 0.0134 | ds_loss: 0.0000 | lr: 1.4339e-05 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
train | epoch   6 | Iter:   1354/  2110 | global iter:   1354/  2110 | loss: 0.0299 | ds_loss: 0.0000 | lr: 1.4306e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   6 | Iter:   1355/  2110 | global iter:   1355/  2110 | loss: 0.0192 | ds_loss: 0.0000 | lr: 1.4272e-05 | scale:     1.0000 | micro time: 3.774 | step time: 0.000
train | epoch   6 | Iter:   1356/  2110 | global iter:   1356/  2110 | loss: 0.0199 | ds_loss: 0.0000 | lr: 1.4239e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1356/  2110 | global iter:   1356/  2110 | loss: 0.0206 | ds_loss: 0.0000 | lr: 1.4239e-05 | scale:     1.0000 | micro time: 3.790 | step time: 3.788
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1357/  2110 | global iter:   1357/  2110 | loss: 0.0220 | ds_loss: 0.0000 | lr: 1.4205e-05 | scale:     1.0000 | micro time: 3.777 | step time: 0.000
train | epoch   6 | Iter:   1358/  2110 | global iter:   1358/  2110 | loss: 0.0113 | ds_loss: 0.0000 | lr: 1.4172e-05 | scale:     1.0000 | micro time: 3.792 | step time: 0.000
train | epoch   6 | Iter:   1359/  2110 | global iter:   1359/  2110 | loss: 0.0336 | ds_loss: 0.0000 | lr: 1.4139e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   6 | Iter:   1360/  2110 | global iter:   1360/  2110 | loss: 0.0195 | ds_loss: 0.0000 | lr: 1.4105e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1360/  2110 | global iter:   1360/  2110 | loss: 0.0216 | ds_loss: 0.0000 | lr: 1.4105e-05 | scale:     1.0000 | micro time: 3.798 | step time: 3.792
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1361/  2110 | global iter:   1361/  2110 | loss: 0.0405 | ds_loss: 0.0000 | lr: 1.4072e-05 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   6 | Iter:   1362/  2110 | global iter:   1362/  2110 | loss: 0.0100 | ds_loss: 0.0000 | lr: 1.4039e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   6 | Iter:   1363/  2110 | global iter:   1363/  2110 | loss: 0.0269 | ds_loss: 0.0000 | lr: 1.4005e-05 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
train | epoch   6 | Iter:   1364/  2110 | global iter:   1364/  2110 | loss: 0.0300 | ds_loss: 0.0000 | lr: 1.3972e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1364/  2110 | global iter:   1364/  2110 | loss: 0.0269 | ds_loss: 0.0000 | lr: 1.3972e-05 | scale:     1.0000 | micro time: 3.771 | step time: 3.787
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1365/  2110 | global iter:   1365/  2110 | loss: 0.0189 | ds_loss: 0.0000 | lr: 1.3939e-05 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
train | epoch   6 | Iter:   1366/  2110 | global iter:   1366/  2110 | loss: 0.0133 | ds_loss: 0.0000 | lr: 1.3905e-05 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
train | epoch   6 | Iter:   1367/  2110 | global iter:   1367/  2110 | loss: 0.0213 | ds_loss: 0.0000 | lr: 1.3872e-05 | scale:     1.0000 | micro time: 3.770 | step time: 0.000
train | epoch   6 | Iter:   1368/  2110 | global iter:   1368/  2110 | loss: 0.0179 | ds_loss: 0.0000 | lr: 1.3839e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1368/  2110 | global iter:   1368/  2110 | loss: 0.0179 | ds_loss: 0.0000 | lr: 1.3839e-05 | scale:     1.0000 | micro time: 3.818 | step time: 3.795
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1369/  2110 | global iter:   1369/  2110 | loss: 0.0548 | ds_loss: 0.0000 | lr: 1.3806e-05 | scale:     1.0000 | micro time: 3.770 | step time: 0.000
train | epoch   6 | Iter:   1370/  2110 | global iter:   1370/  2110 | loss: 0.0265 | ds_loss: 0.0000 | lr: 1.3773e-05 | scale:     1.0000 | micro time: 3.792 | step time: 0.000
train | epoch   6 | Iter:   1371/  2110 | global iter:   1371/  2110 | loss: 0.0104 | ds_loss: 0.0000 | lr: 1.3739e-05 | scale:     1.0000 | micro time: 3.808 | step time: 0.000
train | epoch   6 | Iter:   1372/  2110 | global iter:   1372/  2110 | loss: 0.0115 | ds_loss: 0.0000 | lr: 1.3706e-05 | scale:     1.0000 | micro time: 3.792 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1372/  2110 | global iter:   1372/  2110 | loss: 0.0258 | ds_loss: 0.0000 | lr: 1.3706e-05 | scale:     1.0000 | micro time: 3.792 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1373/  2110 | global iter:   1373/  2110 | loss: 0.0292 | ds_loss: 0.0000 | lr: 1.3673e-05 | scale:     1.0000 | micro time: 3.774 | step time: 0.000
train | epoch   6 | Iter:   1374/  2110 | global iter:   1374/  2110 | loss: 0.0280 | ds_loss: 0.0000 | lr: 1.3640e-05 | scale:     1.0000 | micro time: 3.812 | step time: 0.000
train | epoch   6 | Iter:   1375/  2110 | global iter:   1375/  2110 | loss: 0.0241 | ds_loss: 0.0000 | lr: 1.3607e-05 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
train | epoch   6 | Iter:   1376/  2110 | global iter:   1376/  2110 | loss: 0.0239 | ds_loss: 0.0000 | lr: 1.3574e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1376/  2110 | global iter:   1376/  2110 | loss: 0.0263 | ds_loss: 0.0000 | lr: 1.3574e-05 | scale:     1.0000 | micro time: 3.786 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1377/  2110 | global iter:   1377/  2110 | loss: 0.0257 | ds_loss: 0.0000 | lr: 1.3541e-05 | scale:     1.0000 | micro time: 3.807 | step time: 0.000
train | epoch   6 | Iter:   1378/  2110 | global iter:   1378/  2110 | loss: 0.0046 | ds_loss: 0.0000 | lr: 1.3508e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   6 | Iter:   1379/  2110 | global iter:   1379/  2110 | loss: 0.0119 | ds_loss: 0.0000 | lr: 1.3475e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   6 | Iter:   1380/  2110 | global iter:   1380/  2110 | loss: 0.0419 | ds_loss: 0.0000 | lr: 1.3443e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1380/  2110 | global iter:   1380/  2110 | loss: 0.0210 | ds_loss: 0.0000 | lr: 1.3443e-05 | scale:     1.0000 | micro time: 3.802 | step time: 3.803
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1381/  2110 | global iter:   1381/  2110 | loss: 0.0024 | ds_loss: 0.0000 | lr: 1.3410e-05 | scale:     1.0000 | micro time: 3.765 | step time: 0.000
train | epoch   6 | Iter:   1382/  2110 | global iter:   1382/  2110 | loss: 0.0203 | ds_loss: 0.0000 | lr: 1.3377e-05 | scale:     1.0000 | micro time: 3.805 | step time: 0.000
train | epoch   6 | Iter:   1383/  2110 | global iter:   1383/  2110 | loss: 0.0168 | ds_loss: 0.0000 | lr: 1.3344e-05 | scale:     1.0000 | micro time: 3.770 | step time: 0.000
train | epoch   6 | Iter:   1384/  2110 | global iter:   1384/  2110 | loss: 0.0244 | ds_loss: 0.0000 | lr: 1.3311e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1384/  2110 | global iter:   1384/  2110 | loss: 0.0159 | ds_loss: 0.0000 | lr: 1.3311e-05 | scale:     1.0000 | micro time: 3.818 | step time: 3.790
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1385/  2110 | global iter:   1385/  2110 | loss: 0.0143 | ds_loss: 0.0000 | lr: 1.3278e-05 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   6 | Iter:   1386/  2110 | global iter:   1386/  2110 | loss: 0.0126 | ds_loss: 0.0000 | lr: 1.3246e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   6 | Iter:   1387/  2110 | global iter:   1387/  2110 | loss: 0.0195 | ds_loss: 0.0000 | lr: 1.3213e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   6 | Iter:   1388/  2110 | global iter:   1388/  2110 | loss: 0.0232 | ds_loss: 0.0000 | lr: 1.3180e-05 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1388/  2110 | global iter:   1388/  2110 | loss: 0.0174 | ds_loss: 0.0000 | lr: 1.3180e-05 | scale:     1.0000 | micro time: 3.779 | step time: 3.787
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1389/  2110 | global iter:   1389/  2110 | loss: 0.0290 | ds_loss: 0.0000 | lr: 1.3148e-05 | scale:     1.0000 | micro time: 3.804 | step time: 0.000
train | epoch   6 | Iter:   1390/  2110 | global iter:   1390/  2110 | loss: 0.0309 | ds_loss: 0.0000 | lr: 1.3115e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   6 | Iter:   1391/  2110 | global iter:   1391/  2110 | loss: 0.0171 | ds_loss: 0.0000 | lr: 1.3082e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   6 | Iter:   1392/  2110 | global iter:   1392/  2110 | loss: 0.0258 | ds_loss: 0.0000 | lr: 1.3050e-05 | scale:     1.0000 | micro time: 3.770 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1392/  2110 | global iter:   1392/  2110 | loss: 0.0257 | ds_loss: 0.0000 | lr: 1.3050e-05 | scale:     1.0000 | micro time: 3.770 | step time: 3.793
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1393/  2110 | global iter:   1393/  2110 | loss: 0.0445 | ds_loss: 0.0000 | lr: 1.3017e-05 | scale:     1.0000 | micro time: 3.784 | step time: 0.000
train | epoch   6 | Iter:   1394/  2110 | global iter:   1394/  2110 | loss: 0.0157 | ds_loss: 0.0000 | lr: 1.2985e-05 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
train | epoch   6 | Iter:   1395/  2110 | global iter:   1395/  2110 | loss: 0.0315 | ds_loss: 0.0000 | lr: 1.2952e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   6 | Iter:   1396/  2110 | global iter:   1396/  2110 | loss: 0.0239 | ds_loss: 0.0000 | lr: 1.2920e-05 | scale:     1.0000 | micro time: 3.762 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1396/  2110 | global iter:   1396/  2110 | loss: 0.0289 | ds_loss: 0.0000 | lr: 1.2920e-05 | scale:     1.0000 | micro time: 3.762 | step time: 3.787
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1397/  2110 | global iter:   1397/  2110 | loss: 0.0222 | ds_loss: 0.0000 | lr: 1.2887e-05 | scale:     1.0000 | micro time: 3.809 | step time: 0.000
train | epoch   6 | Iter:   1398/  2110 | global iter:   1398/  2110 | loss: 0.0374 | ds_loss: 0.0000 | lr: 1.2855e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   6 | Iter:   1399/  2110 | global iter:   1399/  2110 | loss: 0.0309 | ds_loss: 0.0000 | lr: 1.2822e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   6 | Iter:   1400/  2110 | global iter:   1400/  2110 | loss: 0.0133 | ds_loss: 0.0000 | lr: 1.2790e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1400/  2110 | global iter:   1400/  2110 | loss: 0.0259 | ds_loss: 0.0000 | lr: 1.2790e-05 | scale:     1.0000 | micro time: 3.798 | step time: 3.797
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1401/  2110 | global iter:   1401/  2110 | loss: 0.0163 | ds_loss: 0.0000 | lr: 1.2758e-05 | scale:     1.0000 | micro time: 3.776 | step time: 0.000
train | epoch   6 | Iter:   1402/  2110 | global iter:   1402/  2110 | loss: 0.0515 | ds_loss: 0.0000 | lr: 1.2725e-05 | scale:     1.0000 | micro time: 3.776 | step time: 0.000
train | epoch   6 | Iter:   1403/  2110 | global iter:   1403/  2110 | loss: 0.0231 | ds_loss: 0.0000 | lr: 1.2693e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   6 | Iter:   1404/  2110 | global iter:   1404/  2110 | loss: 0.0309 | ds_loss: 0.0000 | lr: 1.2661e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1404/  2110 | global iter:   1404/  2110 | loss: 0.0305 | ds_loss: 0.0000 | lr: 1.2661e-05 | scale:     1.0000 | micro time: 3.802 | step time: 3.786
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1405/  2110 | global iter:   1405/  2110 | loss: 0.0225 | ds_loss: 0.0000 | lr: 1.2629e-05 | scale:     1.0000 | micro time: 3.764 | step time: 0.000
train | epoch   6 | Iter:   1406/  2110 | global iter:   1406/  2110 | loss: 0.0321 | ds_loss: 0.0000 | lr: 1.2596e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   6 | Iter:   1407/  2110 | global iter:   1407/  2110 | loss: 0.0395 | ds_loss: 0.0000 | lr: 1.2564e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   6 | Iter:   1408/  2110 | global iter:   1408/  2110 | loss: 0.0165 | ds_loss: 0.0000 | lr: 1.2532e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1408/  2110 | global iter:   1408/  2110 | loss: 0.0277 | ds_loss: 0.0000 | lr: 1.2532e-05 | scale:     1.0000 | micro time: 3.802 | step time: 3.785
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1409/  2110 | global iter:   1409/  2110 | loss: 0.0232 | ds_loss: 0.0000 | lr: 1.2500e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   6 | Iter:   1410/  2110 | global iter:   1410/  2110 | loss: 0.0121 | ds_loss: 0.0000 | lr: 1.2468e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
train | epoch   6 | Iter:   1411/  2110 | global iter:   1411/  2110 | loss: 0.0442 | ds_loss: 0.0000 | lr: 1.2436e-05 | scale:     1.0000 | micro time: 3.780 | step time: 0.000
train | epoch   6 | Iter:   1412/  2110 | global iter:   1412/  2110 | loss: 0.0204 | ds_loss: 0.0000 | lr: 1.2404e-05 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1412/  2110 | global iter:   1412/  2110 | loss: 0.0250 | ds_loss: 0.0000 | lr: 1.2404e-05 | scale:     1.0000 | micro time: 3.793 | step time: 3.795
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1413/  2110 | global iter:   1413/  2110 | loss: 0.0340 | ds_loss: 0.0000 | lr: 1.2372e-05 | scale:     1.0000 | micro time: 3.796 | step time: 0.000
train | epoch   6 | Iter:   1414/  2110 | global iter:   1414/  2110 | loss: 0.0160 | ds_loss: 0.0000 | lr: 1.2340e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   6 | Iter:   1415/  2110 | global iter:   1415/  2110 | loss: 0.0227 | ds_loss: 0.0000 | lr: 1.2308e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   6 | Iter:   1416/  2110 | global iter:   1416/  2110 | loss: 0.0298 | ds_loss: 0.0000 | lr: 1.2276e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1416/  2110 | global iter:   1416/  2110 | loss: 0.0256 | ds_loss: 0.0000 | lr: 1.2276e-05 | scale:     1.0000 | micro time: 3.786 | step time: 3.792
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1417/  2110 | global iter:   1417/  2110 | loss: 0.0447 | ds_loss: 0.0000 | lr: 1.2244e-05 | scale:     1.0000 | micro time: 3.813 | step time: 0.000
train | epoch   6 | Iter:   1418/  2110 | global iter:   1418/  2110 | loss: 0.0161 | ds_loss: 0.0000 | lr: 1.2212e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   6 | Iter:   1419/  2110 | global iter:   1419/  2110 | loss: 0.0280 | ds_loss: 0.0000 | lr: 1.2180e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   6 | Iter:   1420/  2110 | global iter:   1420/  2110 | loss: 0.0175 | ds_loss: 0.0000 | lr: 1.2149e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1420/  2110 | global iter:   1420/  2110 | loss: 0.0266 | ds_loss: 0.0000 | lr: 1.2149e-05 | scale:     1.0000 | micro time: 3.786 | step time: 3.800
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1421/  2110 | global iter:   1421/  2110 | loss: 0.0167 | ds_loss: 0.0000 | lr: 1.2117e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   6 | Iter:   1422/  2110 | global iter:   1422/  2110 | loss: 0.0441 | ds_loss: 0.0000 | lr: 1.2085e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   6 | Iter:   1423/  2110 | global iter:   1423/  2110 | loss: 0.0212 | ds_loss: 0.0000 | lr: 1.2053e-05 | scale:     1.0000 | micro time: 3.814 | step time: 0.000
train | epoch   6 | Iter:   1424/  2110 | global iter:   1424/  2110 | loss: 0.0440 | ds_loss: 0.0000 | lr: 1.2022e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1424/  2110 | global iter:   1424/  2110 | loss: 0.0315 | ds_loss: 0.0000 | lr: 1.2022e-05 | scale:     1.0000 | micro time: 3.791 | step time: 3.800
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1425/  2110 | global iter:   1425/  2110 | loss: 0.0254 | ds_loss: 0.0000 | lr: 1.1990e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   6 | Iter:   1426/  2110 | global iter:   1426/  2110 | loss: 0.0231 | ds_loss: 0.0000 | lr: 1.1958e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   6 | Iter:   1427/  2110 | global iter:   1427/  2110 | loss: 0.0181 | ds_loss: 0.0000 | lr: 1.1927e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   6 | Iter:   1428/  2110 | global iter:   1428/  2110 | loss: 0.0224 | ds_loss: 0.0000 | lr: 1.1895e-05 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1428/  2110 | global iter:   1428/  2110 | loss: 0.0222 | ds_loss: 0.0000 | lr: 1.1895e-05 | scale:     1.0000 | micro time: 3.818 | step time: 3.796
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1429/  2110 | global iter:   1429/  2110 | loss: 0.0224 | ds_loss: 0.0000 | lr: 1.1864e-05 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   6 | Iter:   1430/  2110 | global iter:   1430/  2110 | loss: 0.0160 | ds_loss: 0.0000 | lr: 1.1832e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   6 | Iter:   1431/  2110 | global iter:   1431/  2110 | loss: 0.0109 | ds_loss: 0.0000 | lr: 1.1801e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   6 | Iter:   1432/  2110 | global iter:   1432/  2110 | loss: 0.0140 | ds_loss: 0.0000 | lr: 1.1769e-05 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1432/  2110 | global iter:   1432/  2110 | loss: 0.0158 | ds_loss: 0.0000 | lr: 1.1769e-05 | scale:     1.0000 | micro time: 3.779 | step time: 3.795
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1433/  2110 | global iter:   1433/  2110 | loss: 0.0066 | ds_loss: 0.0000 | lr: 1.1738e-05 | scale:     1.0000 | micro time: 3.817 | step time: 0.000
train | epoch   6 | Iter:   1434/  2110 | global iter:   1434/  2110 | loss: 0.0178 | ds_loss: 0.0000 | lr: 1.1706e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   6 | Iter:   1435/  2110 | global iter:   1435/  2110 | loss: 0.0162 | ds_loss: 0.0000 | lr: 1.1675e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   6 | Iter:   1436/  2110 | global iter:   1436/  2110 | loss: 0.0249 | ds_loss: 0.0000 | lr: 1.1644e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1436/  2110 | global iter:   1436/  2110 | loss: 0.0164 | ds_loss: 0.0000 | lr: 1.1644e-05 | scale:     1.0000 | micro time: 3.806 | step time: 3.805
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1437/  2110 | global iter:   1437/  2110 | loss: 0.0125 | ds_loss: 0.0000 | lr: 1.1612e-05 | scale:     1.0000 | micro time: 3.804 | step time: 0.000
train | epoch   6 | Iter:   1438/  2110 | global iter:   1438/  2110 | loss: 0.0310 | ds_loss: 0.0000 | lr: 1.1581e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   6 | Iter:   1439/  2110 | global iter:   1439/  2110 | loss: 0.0298 | ds_loss: 0.0000 | lr: 1.1550e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   6 | Iter:   1440/  2110 | global iter:   1440/  2110 | loss: 0.0491 | ds_loss: 0.0000 | lr: 1.1518e-05 | scale:     1.0000 | micro time: 3.784 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1440/  2110 | global iter:   1440/  2110 | loss: 0.0306 | ds_loss: 0.0000 | lr: 1.1518e-05 | scale:     1.0000 | micro time: 3.784 | step time: 3.797
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1441/  2110 | global iter:   1441/  2110 | loss: 0.0180 | ds_loss: 0.0000 | lr: 1.1487e-05 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   6 | Iter:   1442/  2110 | global iter:   1442/  2110 | loss: 0.0254 | ds_loss: 0.0000 | lr: 1.1456e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   6 | Iter:   1443/  2110 | global iter:   1443/  2110 | loss: 0.0144 | ds_loss: 0.0000 | lr: 1.1425e-05 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   6 | Iter:   1444/  2110 | global iter:   1444/  2110 | loss: 0.0288 | ds_loss: 0.0000 | lr: 1.1394e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1444/  2110 | global iter:   1444/  2110 | loss: 0.0217 | ds_loss: 0.0000 | lr: 1.1394e-05 | scale:     1.0000 | micro time: 3.790 | step time: 3.798
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1445/  2110 | global iter:   1445/  2110 | loss: 0.0148 | ds_loss: 0.0000 | lr: 1.1363e-05 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   6 | Iter:   1446/  2110 | global iter:   1446/  2110 | loss: 0.0145 | ds_loss: 0.0000 | lr: 1.1332e-05 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   6 | Iter:   1447/  2110 | global iter:   1447/  2110 | loss: 0.0331 | ds_loss: 0.0000 | lr: 1.1301e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   6 | Iter:   1448/  2110 | global iter:   1448/  2110 | loss: 0.0309 | ds_loss: 0.0000 | lr: 1.1270e-05 | scale:     1.0000 | micro time: 3.814 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1448/  2110 | global iter:   1448/  2110 | loss: 0.0233 | ds_loss: 0.0000 | lr: 1.1270e-05 | scale:     1.0000 | micro time: 3.814 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1449/  2110 | global iter:   1449/  2110 | loss: 0.0106 | ds_loss: 0.0000 | lr: 1.1239e-05 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   6 | Iter:   1450/  2110 | global iter:   1450/  2110 | loss: 0.0309 | ds_loss: 0.0000 | lr: 1.1208e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   6 | Iter:   1451/  2110 | global iter:   1451/  2110 | loss: 0.0144 | ds_loss: 0.0000 | lr: 1.1177e-05 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   6 | Iter:   1452/  2110 | global iter:   1452/  2110 | loss: 0.0158 | ds_loss: 0.0000 | lr: 1.1146e-05 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1452/  2110 | global iter:   1452/  2110 | loss: 0.0179 | ds_loss: 0.0000 | lr: 1.1146e-05 | scale:     1.0000 | micro time: 3.788 | step time: 3.783
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1453/  2110 | global iter:   1453/  2110 | loss: 0.0246 | ds_loss: 0.0000 | lr: 1.1115e-05 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   6 | Iter:   1454/  2110 | global iter:   1454/  2110 | loss: 0.0056 | ds_loss: 0.0000 | lr: 1.1084e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   6 | Iter:   1455/  2110 | global iter:   1455/  2110 | loss: 0.0259 | ds_loss: 0.0000 | lr: 1.1054e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   6 | Iter:   1456/  2110 | global iter:   1456/  2110 | loss: 0.0237 | ds_loss: 0.0000 | lr: 1.1023e-05 | scale:     1.0000 | micro time: 3.762 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1456/  2110 | global iter:   1456/  2110 | loss: 0.0200 | ds_loss: 0.0000 | lr: 1.1023e-05 | scale:     1.0000 | micro time: 3.762 | step time: 3.789
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1457/  2110 | global iter:   1457/  2110 | loss: 0.0406 | ds_loss: 0.0000 | lr: 1.0992e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   6 | Iter:   1458/  2110 | global iter:   1458/  2110 | loss: 0.0384 | ds_loss: 0.0000 | lr: 1.0962e-05 | scale:     1.0000 | micro time: 3.805 | step time: 0.000
train | epoch   6 | Iter:   1459/  2110 | global iter:   1459/  2110 | loss: 0.0178 | ds_loss: 0.0000 | lr: 1.0931e-05 | scale:     1.0000 | micro time: 3.776 | step time: 0.000
train | epoch   6 | Iter:   1460/  2110 | global iter:   1460/  2110 | loss: 0.0188 | ds_loss: 0.0000 | lr: 1.0900e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1460/  2110 | global iter:   1460/  2110 | loss: 0.0289 | ds_loss: 0.0000 | lr: 1.0900e-05 | scale:     1.0000 | micro time: 3.802 | step time: 3.792
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1461/  2110 | global iter:   1461/  2110 | loss: 0.0325 | ds_loss: 0.0000 | lr: 1.0870e-05 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   6 | Iter:   1462/  2110 | global iter:   1462/  2110 | loss: 0.0268 | ds_loss: 0.0000 | lr: 1.0839e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   6 | Iter:   1463/  2110 | global iter:   1463/  2110 | loss: 0.0262 | ds_loss: 0.0000 | lr: 1.0809e-05 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   6 | Iter:   1464/  2110 | global iter:   1464/  2110 | loss: 0.0092 | ds_loss: 0.0000 | lr: 1.0778e-05 | scale:     1.0000 | micro time: 3.777 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1464/  2110 | global iter:   1464/  2110 | loss: 0.0237 | ds_loss: 0.0000 | lr: 1.0778e-05 | scale:     1.0000 | micro time: 3.777 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1465/  2110 | global iter:   1465/  2110 | loss: 0.0323 | ds_loss: 0.0000 | lr: 1.0748e-05 | scale:     1.0000 | micro time: 3.774 | step time: 0.000
train | epoch   6 | Iter:   1466/  2110 | global iter:   1466/  2110 | loss: 0.0207 | ds_loss: 0.0000 | lr: 1.0717e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   6 | Iter:   1467/  2110 | global iter:   1467/  2110 | loss: 0.0405 | ds_loss: 0.0000 | lr: 1.0687e-05 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
train | epoch   6 | Iter:   1468/  2110 | global iter:   1468/  2110 | loss: 0.0218 | ds_loss: 0.0000 | lr: 1.0657e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1468/  2110 | global iter:   1468/  2110 | loss: 0.0288 | ds_loss: 0.0000 | lr: 1.0657e-05 | scale:     1.0000 | micro time: 3.787 | step time: 3.785
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1469/  2110 | global iter:   1469/  2110 | loss: 0.0465 | ds_loss: 0.0000 | lr: 1.0626e-05 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
train | epoch   6 | Iter:   1470/  2110 | global iter:   1470/  2110 | loss: 0.0109 | ds_loss: 0.0000 | lr: 1.0596e-05 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   6 | Iter:   1471/  2110 | global iter:   1471/  2110 | loss: 0.0196 | ds_loss: 0.0000 | lr: 1.0566e-05 | scale:     1.0000 | micro time: 3.763 | step time: 0.000
train | epoch   6 | Iter:   1472/  2110 | global iter:   1472/  2110 | loss: 0.0241 | ds_loss: 0.0000 | lr: 1.0535e-05 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1472/  2110 | global iter:   1472/  2110 | loss: 0.0253 | ds_loss: 0.0000 | lr: 1.0535e-05 | scale:     1.0000 | micro time: 3.790 | step time: 3.787
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1473/  2110 | global iter:   1473/  2110 | loss: 0.0111 | ds_loss: 0.0000 | lr: 1.0505e-05 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   6 | Iter:   1474/  2110 | global iter:   1474/  2110 | loss: 0.0168 | ds_loss: 0.0000 | lr: 1.0475e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   6 | Iter:   1475/  2110 | global iter:   1475/  2110 | loss: 0.0034 | ds_loss: 0.0000 | lr: 1.0445e-05 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   6 | Iter:   1476/  2110 | global iter:   1476/  2110 | loss: 0.0187 | ds_loss: 0.0000 | lr: 1.0415e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1476/  2110 | global iter:   1476/  2110 | loss: 0.0125 | ds_loss: 0.0000 | lr: 1.0415e-05 | scale:     1.0000 | micro time: 3.802 | step time: 3.793
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1477/  2110 | global iter:   1477/  2110 | loss: 0.0342 | ds_loss: 0.0000 | lr: 1.0385e-05 | scale:     1.0000 | micro time: 3.773 | step time: 0.000
train | epoch   6 | Iter:   1478/  2110 | global iter:   1478/  2110 | loss: 0.0251 | ds_loss: 0.0000 | lr: 1.0355e-05 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   6 | Iter:   1479/  2110 | global iter:   1479/  2110 | loss: 0.0210 | ds_loss: 0.0000 | lr: 1.0325e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   6 | Iter:   1480/  2110 | global iter:   1480/  2110 | loss: 0.0175 | ds_loss: 0.0000 | lr: 1.0295e-05 | scale:     1.0000 | micro time: 3.770 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1480/  2110 | global iter:   1480/  2110 | loss: 0.0245 | ds_loss: 0.0000 | lr: 1.0295e-05 | scale:     1.0000 | micro time: 3.770 | step time: 3.783
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   1481/  2110 | global iter:   1481/  2110 | loss: 0.0309 | ds_loss: 0.0000 | lr: 1.0265e-05 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   6 | Iter:   1482/  2110 | global iter:   1482/  2110 | loss: 0.0243 | ds_loss: 0.0000 | lr: 1.0235e-05 | scale:     1.0000 | micro time: 3.796 | step time: 0.000
train | epoch   6 | Iter:   1483/  2110 | global iter:   1483/  2110 | loss: 0.0604 | ds_loss: 0.0000 | lr: 1.0205e-05 | scale:     1.0000 | micro time: 3.813 | step time: 0.000
train | epoch   6 | Iter:   1484/  2110 | global iter:   1484/  2110 | loss: 0.0493 | ds_loss: 0.0000 | lr: 1.0175e-05 | scale:     1.0000 | micro time: 3.339 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   1484/  2110 | global iter:   1484/  2110 | loss: 0.0413 | ds_loss: 0.0000 | lr: 1.0175e-05 | scale:     1.0000 | micro time: 3.339 | step time: 3.681
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
Tue Apr  8 08:39:25 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   63C    P0             55W /  250W |   29814MiB /  32768MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |     62%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   50C    P0             49W /  250W |   29848MiB /  32768MiB |     98%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   54C    P0             49W /  250W |   28312MiB /  32768MiB |     16%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 08:39:25 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   63C    P0             55W /  250W |   29814MiB /  32768MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |     62%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   50C    P0             49W /  250W |   29848MiB /  32768MiB |     98%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   54C    P0             49W /  250W |   28312MiB /  32768MiB |     16%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 08:39:25 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   63C    P0             55W /  250W |   29814MiB /  32768MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |     62%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   50C    P0             49W /  250W |   29848MiB /  32768MiB |     98%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   55C    P0            237W /  250W |   28312MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 08:39:25 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   63C    P0             56W /  250W |   29814MiB /  32768MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   50C    P0             49W /  250W |   29848MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   60C    P0            237W /  250W |   28312MiB /  32768MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

train | epoch   7 | Iter:   1485/  2110 | global iter:   1485/  2110 | loss: 0.0138 | ds_loss: 0.0000 | lr: 1.0145e-05 | scale:     1.0000 | micro time: 4.032 | step time: 0.000
train | epoch   7 | Iter:   1486/  2110 | global iter:   1486/  2110 | loss: 0.0232 | ds_loss: 0.0000 | lr: 1.0116e-05 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   7 | Iter:   1487/  2110 | global iter:   1487/  2110 | loss: 0.0086 | ds_loss: 0.0000 | lr: 1.0086e-05 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
train | epoch   7 | Iter:   1488/  2110 | global iter:   1488/  2110 | loss: 0.0145 | ds_loss: 0.0000 | lr: 1.0056e-05 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1488/  2110 | global iter:   1488/  2110 | loss: 0.0150 | ds_loss: 0.0000 | lr: 1.0056e-05 | scale:     1.0000 | micro time: 3.788 | step time: 3.851
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1489/  2110 | global iter:   1489/  2110 | loss: 0.0433 | ds_loss: 0.0000 | lr: 1.0026e-05 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   7 | Iter:   1490/  2110 | global iter:   1490/  2110 | loss: 0.0121 | ds_loss: 0.0000 | lr: 9.9968e-06 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
train | epoch   7 | Iter:   1491/  2110 | global iter:   1491/  2110 | loss: 0.0244 | ds_loss: 0.0000 | lr: 9.9672e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   7 | Iter:   1492/  2110 | global iter:   1492/  2110 | loss: 0.0156 | ds_loss: 0.0000 | lr: 9.9376e-06 | scale:     1.0000 | micro time: 3.777 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1492/  2110 | global iter:   1492/  2110 | loss: 0.0238 | ds_loss: 0.0000 | lr: 9.9376e-06 | scale:     1.0000 | micro time: 3.777 | step time: 3.789
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1493/  2110 | global iter:   1493/  2110 | loss: 0.0127 | ds_loss: 0.0000 | lr: 9.9081e-06 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   7 | Iter:   1494/  2110 | global iter:   1494/  2110 | loss: 0.0123 | ds_loss: 0.0000 | lr: 9.8786e-06 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
train | epoch   7 | Iter:   1495/  2110 | global iter:   1495/  2110 | loss: 0.0178 | ds_loss: 0.0000 | lr: 9.8491e-06 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   7 | Iter:   1496/  2110 | global iter:   1496/  2110 | loss: 0.0284 | ds_loss: 0.0000 | lr: 9.8197e-06 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1496/  2110 | global iter:   1496/  2110 | loss: 0.0178 | ds_loss: 0.0000 | lr: 9.8197e-06 | scale:     1.0000 | micro time: 3.788 | step time: 3.787
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1497/  2110 | global iter:   1497/  2110 | loss: 0.0036 | ds_loss: 0.0000 | lr: 9.7903e-06 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   7 | Iter:   1498/  2110 | global iter:   1498/  2110 | loss: 0.0141 | ds_loss: 0.0000 | lr: 9.7609e-06 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   7 | Iter:   1499/  2110 | global iter:   1499/  2110 | loss: 0.0203 | ds_loss: 0.0000 | lr: 9.7316e-06 | scale:     1.0000 | micro time: 3.834 | step time: 0.000
train | epoch   7 | Iter:   1500/  2110 | global iter:   1500/  2110 | loss: 0.0153 | ds_loss: 0.0000 | lr: 9.7023e-06 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1500/  2110 | global iter:   1500/  2110 | loss: 0.0134 | ds_loss: 0.0000 | lr: 9.7023e-06 | scale:     1.0000 | micro time: 3.787 | step time: 3.799
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1501/  2110 | global iter:   1501/  2110 | loss: 0.0074 | ds_loss: 0.0000 | lr: 9.6730e-06 | scale:     1.0000 | micro time: 3.817 | step time: 0.000
train | epoch   7 | Iter:   1502/  2110 | global iter:   1502/  2110 | loss: 0.0120 | ds_loss: 0.0000 | lr: 9.6438e-06 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   7 | Iter:   1503/  2110 | global iter:   1503/  2110 | loss: 0.0250 | ds_loss: 0.0000 | lr: 9.6145e-06 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   7 | Iter:   1504/  2110 | global iter:   1504/  2110 | loss: 0.0162 | ds_loss: 0.0000 | lr: 9.5854e-06 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1504/  2110 | global iter:   1504/  2110 | loss: 0.0151 | ds_loss: 0.0000 | lr: 9.5854e-06 | scale:     1.0000 | micro time: 3.787 | step time: 3.797
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1505/  2110 | global iter:   1505/  2110 | loss: 0.0066 | ds_loss: 0.0000 | lr: 9.5562e-06 | scale:     1.0000 | micro time: 3.811 | step time: 0.000
train | epoch   7 | Iter:   1506/  2110 | global iter:   1506/  2110 | loss: 0.0307 | ds_loss: 0.0000 | lr: 9.5271e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   7 | Iter:   1507/  2110 | global iter:   1507/  2110 | loss: 0.0151 | ds_loss: 0.0000 | lr: 9.4981e-06 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   7 | Iter:   1508/  2110 | global iter:   1508/  2110 | loss: 0.0095 | ds_loss: 0.0000 | lr: 9.4690e-06 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1508/  2110 | global iter:   1508/  2110 | loss: 0.0155 | ds_loss: 0.0000 | lr: 9.4690e-06 | scale:     1.0000 | micro time: 3.798 | step time: 3.801
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1509/  2110 | global iter:   1509/  2110 | loss: 0.0292 | ds_loss: 0.0000 | lr: 9.4401e-06 | scale:     1.0000 | micro time: 3.777 | step time: 0.000
train | epoch   7 | Iter:   1510/  2110 | global iter:   1510/  2110 | loss: 0.0214 | ds_loss: 0.0000 | lr: 9.4111e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   7 | Iter:   1511/  2110 | global iter:   1511/  2110 | loss: 0.0043 | ds_loss: 0.0000 | lr: 9.3822e-06 | scale:     1.0000 | micro time: 3.784 | step time: 0.000
train | epoch   7 | Iter:   1512/  2110 | global iter:   1512/  2110 | loss: 0.0068 | ds_loss: 0.0000 | lr: 9.3533e-06 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1512/  2110 | global iter:   1512/  2110 | loss: 0.0154 | ds_loss: 0.0000 | lr: 9.3533e-06 | scale:     1.0000 | micro time: 3.798 | step time: 3.790
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1513/  2110 | global iter:   1513/  2110 | loss: 0.0052 | ds_loss: 0.0000 | lr: 9.3244e-06 | scale:     1.0000 | micro time: 3.765 | step time: 0.000
train | epoch   7 | Iter:   1514/  2110 | global iter:   1514/  2110 | loss: 0.0066 | ds_loss: 0.0000 | lr: 9.2956e-06 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   7 | Iter:   1515/  2110 | global iter:   1515/  2110 | loss: 0.0122 | ds_loss: 0.0000 | lr: 9.2668e-06 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   7 | Iter:   1516/  2110 | global iter:   1516/  2110 | loss: 0.0119 | ds_loss: 0.0000 | lr: 9.2380e-06 | scale:     1.0000 | micro time: 3.767 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1516/  2110 | global iter:   1516/  2110 | loss: 0.0090 | ds_loss: 0.0000 | lr: 9.2380e-06 | scale:     1.0000 | micro time: 3.767 | step time: 3.781
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1517/  2110 | global iter:   1517/  2110 | loss: 0.0122 | ds_loss: 0.0000 | lr: 9.2093e-06 | scale:     1.0000 | micro time: 3.800 | step time: 0.000
train | epoch   7 | Iter:   1518/  2110 | global iter:   1518/  2110 | loss: 0.0126 | ds_loss: 0.0000 | lr: 9.1806e-06 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   7 | Iter:   1519/  2110 | global iter:   1519/  2110 | loss: 0.0158 | ds_loss: 0.0000 | lr: 9.1520e-06 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   7 | Iter:   1520/  2110 | global iter:   1520/  2110 | loss: 0.0235 | ds_loss: 0.0000 | lr: 9.1234e-06 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1520/  2110 | global iter:   1520/  2110 | loss: 0.0160 | ds_loss: 0.0000 | lr: 9.1234e-06 | scale:     1.0000 | micro time: 3.795 | step time: 3.796
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1521/  2110 | global iter:   1521/  2110 | loss: 0.0096 | ds_loss: 0.0000 | lr: 9.0948e-06 | scale:     1.0000 | micro time: 3.776 | step time: 0.000
train | epoch   7 | Iter:   1522/  2110 | global iter:   1522/  2110 | loss: 0.0055 | ds_loss: 0.0000 | lr: 9.0663e-06 | scale:     1.0000 | micro time: 3.822 | step time: 0.000
train | epoch   7 | Iter:   1523/  2110 | global iter:   1523/  2110 | loss: 0.0079 | ds_loss: 0.0000 | lr: 9.0378e-06 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   7 | Iter:   1524/  2110 | global iter:   1524/  2110 | loss: 0.0038 | ds_loss: 0.0000 | lr: 9.0093e-06 | scale:     1.0000 | micro time: 3.813 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1524/  2110 | global iter:   1524/  2110 | loss: 0.0067 | ds_loss: 0.0000 | lr: 9.0093e-06 | scale:     1.0000 | micro time: 3.813 | step time: 3.798
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1525/  2110 | global iter:   1525/  2110 | loss: 0.0225 | ds_loss: 0.0000 | lr: 8.9809e-06 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   7 | Iter:   1526/  2110 | global iter:   1526/  2110 | loss: 0.0183 | ds_loss: 0.0000 | lr: 8.9525e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   7 | Iter:   1527/  2110 | global iter:   1527/  2110 | loss: 0.0152 | ds_loss: 0.0000 | lr: 8.9241e-06 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   7 | Iter:   1528/  2110 | global iter:   1528/  2110 | loss: 0.0115 | ds_loss: 0.0000 | lr: 8.8958e-06 | scale:     1.0000 | micro time: 3.792 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1528/  2110 | global iter:   1528/  2110 | loss: 0.0169 | ds_loss: 0.0000 | lr: 8.8958e-06 | scale:     1.0000 | micro time: 3.792 | step time: 3.795
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1529/  2110 | global iter:   1529/  2110 | loss: 0.0212 | ds_loss: 0.0000 | lr: 8.8675e-06 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
train | epoch   7 | Iter:   1530/  2110 | global iter:   1530/  2110 | loss: 0.0120 | ds_loss: 0.0000 | lr: 8.8392e-06 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   7 | Iter:   1531/  2110 | global iter:   1531/  2110 | loss: 0.0271 | ds_loss: 0.0000 | lr: 8.8110e-06 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   7 | Iter:   1532/  2110 | global iter:   1532/  2110 | loss: 0.0070 | ds_loss: 0.0000 | lr: 8.7828e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1532/  2110 | global iter:   1532/  2110 | loss: 0.0168 | ds_loss: 0.0000 | lr: 8.7828e-06 | scale:     1.0000 | micro time: 3.802 | step time: 3.792
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1533/  2110 | global iter:   1533/  2110 | loss: 0.0059 | ds_loss: 0.0000 | lr: 8.7547e-06 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   7 | Iter:   1534/  2110 | global iter:   1534/  2110 | loss: 0.0177 | ds_loss: 0.0000 | lr: 8.7265e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   7 | Iter:   1535/  2110 | global iter:   1535/  2110 | loss: 0.0185 | ds_loss: 0.0000 | lr: 8.6985e-06 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   7 | Iter:   1536/  2110 | global iter:   1536/  2110 | loss: 0.0048 | ds_loss: 0.0000 | lr: 8.6704e-06 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1536/  2110 | global iter:   1536/  2110 | loss: 0.0117 | ds_loss: 0.0000 | lr: 8.6704e-06 | scale:     1.0000 | micro time: 3.798 | step time: 3.792
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1537/  2110 | global iter:   1537/  2110 | loss: 0.0212 | ds_loss: 0.0000 | lr: 8.6424e-06 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   7 | Iter:   1538/  2110 | global iter:   1538/  2110 | loss: 0.0126 | ds_loss: 0.0000 | lr: 8.6145e-06 | scale:     1.0000 | micro time: 3.820 | step time: 0.000
train | epoch   7 | Iter:   1539/  2110 | global iter:   1539/  2110 | loss: 0.0420 | ds_loss: 0.0000 | lr: 8.5865e-06 | scale:     1.0000 | micro time: 3.767 | step time: 0.000
train | epoch   7 | Iter:   1540/  2110 | global iter:   1540/  2110 | loss: 0.0119 | ds_loss: 0.0000 | lr: 8.5586e-06 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1540/  2110 | global iter:   1540/  2110 | loss: 0.0219 | ds_loss: 0.0000 | lr: 8.5586e-06 | scale:     1.0000 | micro time: 3.791 | step time: 3.788
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1541/  2110 | global iter:   1541/  2110 | loss: 0.0145 | ds_loss: 0.0000 | lr: 8.5308e-06 | scale:     1.0000 | micro time: 3.770 | step time: 0.000
train | epoch   7 | Iter:   1542/  2110 | global iter:   1542/  2110 | loss: 0.0210 | ds_loss: 0.0000 | lr: 8.5030e-06 | scale:     1.0000 | micro time: 3.822 | step time: 0.000
train | epoch   7 | Iter:   1543/  2110 | global iter:   1543/  2110 | loss: 0.0114 | ds_loss: 0.0000 | lr: 8.4752e-06 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
train | epoch   7 | Iter:   1544/  2110 | global iter:   1544/  2110 | loss: 0.0288 | ds_loss: 0.0000 | lr: 8.4474e-06 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1544/  2110 | global iter:   1544/  2110 | loss: 0.0189 | ds_loss: 0.0000 | lr: 8.4474e-06 | scale:     1.0000 | micro time: 3.790 | step time: 3.796
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1545/  2110 | global iter:   1545/  2110 | loss: 0.0099 | ds_loss: 0.0000 | lr: 8.4197e-06 | scale:     1.0000 | micro time: 3.804 | step time: 0.000
train | epoch   7 | Iter:   1546/  2110 | global iter:   1546/  2110 | loss: 0.0119 | ds_loss: 0.0000 | lr: 8.3920e-06 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   7 | Iter:   1547/  2110 | global iter:   1547/  2110 | loss: 0.0085 | ds_loss: 0.0000 | lr: 8.3644e-06 | scale:     1.0000 | micro time: 3.809 | step time: 0.000
train | epoch   7 | Iter:   1548/  2110 | global iter:   1548/  2110 | loss: 0.0018 | ds_loss: 0.0000 | lr: 8.3368e-06 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1548/  2110 | global iter:   1548/  2110 | loss: 0.0080 | ds_loss: 0.0000 | lr: 8.3368e-06 | scale:     1.0000 | micro time: 3.788 | step time: 3.802
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1549/  2110 | global iter:   1549/  2110 | loss: 0.0387 | ds_loss: 0.0000 | lr: 8.3092e-06 | scale:     1.0000 | micro time: 3.777 | step time: 0.000
train | epoch   7 | Iter:   1550/  2110 | global iter:   1550/  2110 | loss: 0.0067 | ds_loss: 0.0000 | lr: 8.2817e-06 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   7 | Iter:   1551/  2110 | global iter:   1551/  2110 | loss: 0.0109 | ds_loss: 0.0000 | lr: 8.2542e-06 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   7 | Iter:   1552/  2110 | global iter:   1552/  2110 | loss: 0.0108 | ds_loss: 0.0000 | lr: 8.2268e-06 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1552/  2110 | global iter:   1552/  2110 | loss: 0.0168 | ds_loss: 0.0000 | lr: 8.2268e-06 | scale:     1.0000 | micro time: 3.790 | step time: 3.787
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1553/  2110 | global iter:   1553/  2110 | loss: 0.0101 | ds_loss: 0.0000 | lr: 8.1994e-06 | scale:     1.0000 | micro time: 3.831 | step time: 0.000
train | epoch   7 | Iter:   1554/  2110 | global iter:   1554/  2110 | loss: 0.0138 | ds_loss: 0.0000 | lr: 8.1720e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   7 | Iter:   1555/  2110 | global iter:   1555/  2110 | loss: 0.0126 | ds_loss: 0.0000 | lr: 8.1446e-06 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   7 | Iter:   1556/  2110 | global iter:   1556/  2110 | loss: 0.0152 | ds_loss: 0.0000 | lr: 8.1173e-06 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1556/  2110 | global iter:   1556/  2110 | loss: 0.0129 | ds_loss: 0.0000 | lr: 8.1173e-06 | scale:     1.0000 | micro time: 3.786 | step time: 3.799
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1557/  2110 | global iter:   1557/  2110 | loss: 0.0087 | ds_loss: 0.0000 | lr: 8.0901e-06 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
train | epoch   7 | Iter:   1558/  2110 | global iter:   1558/  2110 | loss: 0.0111 | ds_loss: 0.0000 | lr: 8.0628e-06 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   7 | Iter:   1559/  2110 | global iter:   1559/  2110 | loss: 0.0093 | ds_loss: 0.0000 | lr: 8.0357e-06 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   7 | Iter:   1560/  2110 | global iter:   1560/  2110 | loss: 0.0268 | ds_loss: 0.0000 | lr: 8.0085e-06 | scale:     1.0000 | micro time: 3.770 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1560/  2110 | global iter:   1560/  2110 | loss: 0.0140 | ds_loss: 0.0000 | lr: 8.0085e-06 | scale:     1.0000 | micro time: 3.770 | step time: 3.784
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1561/  2110 | global iter:   1561/  2110 | loss: 0.0123 | ds_loss: 0.0000 | lr: 7.9814e-06 | scale:     1.0000 | micro time: 3.817 | step time: 0.000
train | epoch   7 | Iter:   1562/  2110 | global iter:   1562/  2110 | loss: 0.0259 | ds_loss: 0.0000 | lr: 7.9543e-06 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   7 | Iter:   1563/  2110 | global iter:   1563/  2110 | loss: 0.0123 | ds_loss: 0.0000 | lr: 7.9273e-06 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   7 | Iter:   1564/  2110 | global iter:   1564/  2110 | loss: 0.0121 | ds_loss: 0.0000 | lr: 7.9003e-06 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1564/  2110 | global iter:   1564/  2110 | loss: 0.0157 | ds_loss: 0.0000 | lr: 7.9003e-06 | scale:     1.0000 | micro time: 3.795 | step time: 3.799
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1565/  2110 | global iter:   1565/  2110 | loss: 0.0023 | ds_loss: 0.0000 | lr: 7.8733e-06 | scale:     1.0000 | micro time: 3.797 | step time: 0.000
train | epoch   7 | Iter:   1566/  2110 | global iter:   1566/  2110 | loss: 0.0192 | ds_loss: 0.0000 | lr: 7.8464e-06 | scale:     1.0000 | micro time: 3.811 | step time: 0.000
train | epoch   7 | Iter:   1567/  2110 | global iter:   1567/  2110 | loss: 0.0052 | ds_loss: 0.0000 | lr: 7.8195e-06 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   7 | Iter:   1568/  2110 | global iter:   1568/  2110 | loss: 0.0187 | ds_loss: 0.0000 | lr: 7.7926e-06 | scale:     1.0000 | micro time: 3.773 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1568/  2110 | global iter:   1568/  2110 | loss: 0.0114 | ds_loss: 0.0000 | lr: 7.7926e-06 | scale:     1.0000 | micro time: 3.773 | step time: 3.796
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1569/  2110 | global iter:   1569/  2110 | loss: 0.0074 | ds_loss: 0.0000 | lr: 7.7658e-06 | scale:     1.0000 | micro time: 3.804 | step time: 0.000
train | epoch   7 | Iter:   1570/  2110 | global iter:   1570/  2110 | loss: 0.0162 | ds_loss: 0.0000 | lr: 7.7391e-06 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   7 | Iter:   1571/  2110 | global iter:   1571/  2110 | loss: 0.0373 | ds_loss: 0.0000 | lr: 7.7123e-06 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   7 | Iter:   1572/  2110 | global iter:   1572/  2110 | loss: 0.0176 | ds_loss: 0.0000 | lr: 7.6856e-06 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1572/  2110 | global iter:   1572/  2110 | loss: 0.0196 | ds_loss: 0.0000 | lr: 7.6856e-06 | scale:     1.0000 | micro time: 3.790 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1573/  2110 | global iter:   1573/  2110 | loss: 0.0169 | ds_loss: 0.0000 | lr: 7.6590e-06 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   7 | Iter:   1574/  2110 | global iter:   1574/  2110 | loss: 0.0202 | ds_loss: 0.0000 | lr: 7.6324e-06 | scale:     1.0000 | micro time: 3.805 | step time: 0.000
train | epoch   7 | Iter:   1575/  2110 | global iter:   1575/  2110 | loss: 0.0260 | ds_loss: 0.0000 | lr: 7.6058e-06 | scale:     1.0000 | micro time: 3.772 | step time: 0.000
train | epoch   7 | Iter:   1576/  2110 | global iter:   1576/  2110 | loss: 0.0196 | ds_loss: 0.0000 | lr: 7.5792e-06 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1576/  2110 | global iter:   1576/  2110 | loss: 0.0207 | ds_loss: 0.0000 | lr: 7.5792e-06 | scale:     1.0000 | micro time: 3.794 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1577/  2110 | global iter:   1577/  2110 | loss: 0.0173 | ds_loss: 0.0000 | lr: 7.5527e-06 | scale:     1.0000 | micro time: 3.800 | step time: 0.000
train | epoch   7 | Iter:   1578/  2110 | global iter:   1578/  2110 | loss: 0.0094 | ds_loss: 0.0000 | lr: 7.5263e-06 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   7 | Iter:   1579/  2110 | global iter:   1579/  2110 | loss: 0.0139 | ds_loss: 0.0000 | lr: 7.4999e-06 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   7 | Iter:   1580/  2110 | global iter:   1580/  2110 | loss: 0.0138 | ds_loss: 0.0000 | lr: 7.4735e-06 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1580/  2110 | global iter:   1580/  2110 | loss: 0.0136 | ds_loss: 0.0000 | lr: 7.4735e-06 | scale:     1.0000 | micro time: 3.778 | step time: 3.786
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1581/  2110 | global iter:   1581/  2110 | loss: 0.0137 | ds_loss: 0.0000 | lr: 7.4471e-06 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
train | epoch   7 | Iter:   1582/  2110 | global iter:   1582/  2110 | loss: 0.0128 | ds_loss: 0.0000 | lr: 7.4208e-06 | scale:     1.0000 | micro time: 3.770 | step time: 0.000
train | epoch   7 | Iter:   1583/  2110 | global iter:   1583/  2110 | loss: 0.0064 | ds_loss: 0.0000 | lr: 7.3946e-06 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   7 | Iter:   1584/  2110 | global iter:   1584/  2110 | loss: 0.0038 | ds_loss: 0.0000 | lr: 7.3683e-06 | scale:     1.0000 | micro time: 3.769 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1584/  2110 | global iter:   1584/  2110 | loss: 0.0092 | ds_loss: 0.0000 | lr: 7.3683e-06 | scale:     1.0000 | micro time: 3.769 | step time: 3.778
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1585/  2110 | global iter:   1585/  2110 | loss: 0.0123 | ds_loss: 0.0000 | lr: 7.3421e-06 | scale:     1.0000 | micro time: 3.777 | step time: 0.000
train | epoch   7 | Iter:   1586/  2110 | global iter:   1586/  2110 | loss: 0.0097 | ds_loss: 0.0000 | lr: 7.3160e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   7 | Iter:   1587/  2110 | global iter:   1587/  2110 | loss: 0.0032 | ds_loss: 0.0000 | lr: 7.2899e-06 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   7 | Iter:   1588/  2110 | global iter:   1588/  2110 | loss: 0.0128 | ds_loss: 0.0000 | lr: 7.2638e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1588/  2110 | global iter:   1588/  2110 | loss: 0.0095 | ds_loss: 0.0000 | lr: 7.2638e-06 | scale:     1.0000 | micro time: 3.802 | step time: 3.789
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1589/  2110 | global iter:   1589/  2110 | loss: 0.0152 | ds_loss: 0.0000 | lr: 7.2378e-06 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   7 | Iter:   1590/  2110 | global iter:   1590/  2110 | loss: 0.0208 | ds_loss: 0.0000 | lr: 7.2118e-06 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   7 | Iter:   1591/  2110 | global iter:   1591/  2110 | loss: 0.0114 | ds_loss: 0.0000 | lr: 7.1858e-06 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   7 | Iter:   1592/  2110 | global iter:   1592/  2110 | loss: 0.0042 | ds_loss: 0.0000 | lr: 7.1599e-06 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1592/  2110 | global iter:   1592/  2110 | loss: 0.0129 | ds_loss: 0.0000 | lr: 7.1599e-06 | scale:     1.0000 | micro time: 3.790 | step time: 3.787
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1593/  2110 | global iter:   1593/  2110 | loss: 0.0086 | ds_loss: 0.0000 | lr: 7.1340e-06 | scale:     1.0000 | micro time: 3.765 | step time: 0.000
train | epoch   7 | Iter:   1594/  2110 | global iter:   1594/  2110 | loss: 0.0147 | ds_loss: 0.0000 | lr: 7.1082e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   7 | Iter:   1595/  2110 | global iter:   1595/  2110 | loss: 0.0059 | ds_loss: 0.0000 | lr: 7.0824e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   7 | Iter:   1596/  2110 | global iter:   1596/  2110 | loss: 0.0106 | ds_loss: 0.0000 | lr: 7.0567e-06 | scale:     1.0000 | micro time: 3.774 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1596/  2110 | global iter:   1596/  2110 | loss: 0.0099 | ds_loss: 0.0000 | lr: 7.0567e-06 | scale:     1.0000 | micro time: 3.774 | step time: 3.786
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1597/  2110 | global iter:   1597/  2110 | loss: 0.0044 | ds_loss: 0.0000 | lr: 7.0309e-06 | scale:     1.0000 | micro time: 3.762 | step time: 0.000
train | epoch   7 | Iter:   1598/  2110 | global iter:   1598/  2110 | loss: 0.0060 | ds_loss: 0.0000 | lr: 7.0053e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   7 | Iter:   1599/  2110 | global iter:   1599/  2110 | loss: 0.0232 | ds_loss: 0.0000 | lr: 6.9796e-06 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   7 | Iter:   1600/  2110 | global iter:   1600/  2110 | loss: 0.0205 | ds_loss: 0.0000 | lr: 6.9540e-06 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1600/  2110 | global iter:   1600/  2110 | loss: 0.0135 | ds_loss: 0.0000 | lr: 6.9540e-06 | scale:     1.0000 | micro time: 3.787 | step time: 3.789
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1601/  2110 | global iter:   1601/  2110 | loss: 0.0113 | ds_loss: 0.0000 | lr: 6.9285e-06 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   7 | Iter:   1602/  2110 | global iter:   1602/  2110 | loss: 0.0231 | ds_loss: 0.0000 | lr: 6.9030e-06 | scale:     1.0000 | micro time: 3.772 | step time: 0.000
train | epoch   7 | Iter:   1603/  2110 | global iter:   1603/  2110 | loss: 0.0091 | ds_loss: 0.0000 | lr: 6.8775e-06 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
train | epoch   7 | Iter:   1604/  2110 | global iter:   1604/  2110 | loss: 0.0065 | ds_loss: 0.0000 | lr: 6.8521e-06 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1604/  2110 | global iter:   1604/  2110 | loss: 0.0125 | ds_loss: 0.0000 | lr: 6.8521e-06 | scale:     1.0000 | micro time: 3.785 | step time: 3.790
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1605/  2110 | global iter:   1605/  2110 | loss: 0.0192 | ds_loss: 0.0000 | lr: 6.8267e-06 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   7 | Iter:   1606/  2110 | global iter:   1606/  2110 | loss: 0.0029 | ds_loss: 0.0000 | lr: 6.8013e-06 | scale:     1.0000 | micro time: 3.774 | step time: 0.000
train | epoch   7 | Iter:   1607/  2110 | global iter:   1607/  2110 | loss: 0.0092 | ds_loss: 0.0000 | lr: 6.7760e-06 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   7 | Iter:   1608/  2110 | global iter:   1608/  2110 | loss: 0.0088 | ds_loss: 0.0000 | lr: 6.7507e-06 | scale:     1.0000 | micro time: 3.770 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1608/  2110 | global iter:   1608/  2110 | loss: 0.0100 | ds_loss: 0.0000 | lr: 6.7507e-06 | scale:     1.0000 | micro time: 3.770 | step time: 3.782
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1609/  2110 | global iter:   1609/  2110 | loss: 0.0122 | ds_loss: 0.0000 | lr: 6.7255e-06 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   7 | Iter:   1610/  2110 | global iter:   1610/  2110 | loss: 0.0170 | ds_loss: 0.0000 | lr: 6.7003e-06 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   7 | Iter:   1611/  2110 | global iter:   1611/  2110 | loss: 0.0167 | ds_loss: 0.0000 | lr: 6.6752e-06 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   7 | Iter:   1612/  2110 | global iter:   1612/  2110 | loss: 0.0041 | ds_loss: 0.0000 | lr: 6.6501e-06 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1612/  2110 | global iter:   1612/  2110 | loss: 0.0125 | ds_loss: 0.0000 | lr: 6.6501e-06 | scale:     1.0000 | micro time: 3.787 | step time: 3.786
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1613/  2110 | global iter:   1613/  2110 | loss: 0.0218 | ds_loss: 0.0000 | lr: 6.6250e-06 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   7 | Iter:   1614/  2110 | global iter:   1614/  2110 | loss: 0.0082 | ds_loss: 0.0000 | lr: 6.6000e-06 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   7 | Iter:   1615/  2110 | global iter:   1615/  2110 | loss: 0.0181 | ds_loss: 0.0000 | lr: 6.5750e-06 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   7 | Iter:   1616/  2110 | global iter:   1616/  2110 | loss: 0.0202 | ds_loss: 0.0000 | lr: 6.5500e-06 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1616/  2110 | global iter:   1616/  2110 | loss: 0.0171 | ds_loss: 0.0000 | lr: 6.5500e-06 | scale:     1.0000 | micro time: 3.779 | step time: 3.786
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1617/  2110 | global iter:   1617/  2110 | loss: 0.0175 | ds_loss: 0.0000 | lr: 6.5251e-06 | scale:     1.0000 | micro time: 3.784 | step time: 0.000
train | epoch   7 | Iter:   1618/  2110 | global iter:   1618/  2110 | loss: 0.0058 | ds_loss: 0.0000 | lr: 6.5003e-06 | scale:     1.0000 | micro time: 3.797 | step time: 0.000
train | epoch   7 | Iter:   1619/  2110 | global iter:   1619/  2110 | loss: 0.0031 | ds_loss: 0.0000 | lr: 6.4754e-06 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   7 | Iter:   1620/  2110 | global iter:   1620/  2110 | loss: 0.0155 | ds_loss: 0.0000 | lr: 6.4506e-06 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1620/  2110 | global iter:   1620/  2110 | loss: 0.0105 | ds_loss: 0.0000 | lr: 6.4506e-06 | scale:     1.0000 | micro time: 3.806 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1621/  2110 | global iter:   1621/  2110 | loss: 0.0198 | ds_loss: 0.0000 | lr: 6.4259e-06 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   7 | Iter:   1622/  2110 | global iter:   1622/  2110 | loss: 0.0159 | ds_loss: 0.0000 | lr: 6.4012e-06 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   7 | Iter:   1623/  2110 | global iter:   1623/  2110 | loss: 0.0138 | ds_loss: 0.0000 | lr: 6.3766e-06 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   7 | Iter:   1624/  2110 | global iter:   1624/  2110 | loss: 0.0077 | ds_loss: 0.0000 | lr: 6.3519e-06 | scale:     1.0000 | micro time: 3.765 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1624/  2110 | global iter:   1624/  2110 | loss: 0.0143 | ds_loss: 0.0000 | lr: 6.3519e-06 | scale:     1.0000 | micro time: 3.765 | step time: 3.781
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1625/  2110 | global iter:   1625/  2110 | loss: 0.0208 | ds_loss: 0.0000 | lr: 6.3274e-06 | scale:     1.0000 | micro time: 3.797 | step time: 0.000
train | epoch   7 | Iter:   1626/  2110 | global iter:   1626/  2110 | loss: 0.0127 | ds_loss: 0.0000 | lr: 6.3028e-06 | scale:     1.0000 | micro time: 3.830 | step time: 0.000
train | epoch   7 | Iter:   1627/  2110 | global iter:   1627/  2110 | loss: 0.0138 | ds_loss: 0.0000 | lr: 6.2783e-06 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   7 | Iter:   1628/  2110 | global iter:   1628/  2110 | loss: 0.0160 | ds_loss: 0.0000 | lr: 6.2539e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1628/  2110 | global iter:   1628/  2110 | loss: 0.0158 | ds_loss: 0.0000 | lr: 6.2539e-06 | scale:     1.0000 | micro time: 3.802 | step time: 3.802
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1629/  2110 | global iter:   1629/  2110 | loss: 0.0226 | ds_loss: 0.0000 | lr: 6.2295e-06 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   7 | Iter:   1630/  2110 | global iter:   1630/  2110 | loss: 0.0195 | ds_loss: 0.0000 | lr: 6.2051e-06 | scale:     1.0000 | micro time: 3.814 | step time: 0.000
train | epoch   7 | Iter:   1631/  2110 | global iter:   1631/  2110 | loss: 0.0090 | ds_loss: 0.0000 | lr: 6.1808e-06 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   7 | Iter:   1632/  2110 | global iter:   1632/  2110 | loss: 0.0136 | ds_loss: 0.0000 | lr: 6.1565e-06 | scale:     1.0000 | micro time: 3.822 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1632/  2110 | global iter:   1632/  2110 | loss: 0.0162 | ds_loss: 0.0000 | lr: 6.1565e-06 | scale:     1.0000 | micro time: 3.822 | step time: 3.803
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1633/  2110 | global iter:   1633/  2110 | loss: 0.0126 | ds_loss: 0.0000 | lr: 6.1323e-06 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   7 | Iter:   1634/  2110 | global iter:   1634/  2110 | loss: 0.0208 | ds_loss: 0.0000 | lr: 6.1081e-06 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   7 | Iter:   1635/  2110 | global iter:   1635/  2110 | loss: 0.0038 | ds_loss: 0.0000 | lr: 6.0839e-06 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   7 | Iter:   1636/  2110 | global iter:   1636/  2110 | loss: 0.0374 | ds_loss: 0.0000 | lr: 6.0598e-06 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1636/  2110 | global iter:   1636/  2110 | loss: 0.0187 | ds_loss: 0.0000 | lr: 6.0598e-06 | scale:     1.0000 | micro time: 3.790 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1637/  2110 | global iter:   1637/  2110 | loss: 0.0169 | ds_loss: 0.0000 | lr: 6.0357e-06 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   7 | Iter:   1638/  2110 | global iter:   1638/  2110 | loss: 0.0072 | ds_loss: 0.0000 | lr: 6.0117e-06 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
train | epoch   7 | Iter:   1639/  2110 | global iter:   1639/  2110 | loss: 0.0065 | ds_loss: 0.0000 | lr: 5.9877e-06 | scale:     1.0000 | micro time: 3.800 | step time: 0.000
train | epoch   7 | Iter:   1640/  2110 | global iter:   1640/  2110 | loss: 0.0230 | ds_loss: 0.0000 | lr: 5.9637e-06 | scale:     1.0000 | micro time: 3.819 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1640/  2110 | global iter:   1640/  2110 | loss: 0.0134 | ds_loss: 0.0000 | lr: 5.9637e-06 | scale:     1.0000 | micro time: 3.819 | step time: 3.801
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1641/  2110 | global iter:   1641/  2110 | loss: 0.0180 | ds_loss: 0.0000 | lr: 5.9398e-06 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   7 | Iter:   1642/  2110 | global iter:   1642/  2110 | loss: 0.0152 | ds_loss: 0.0000 | lr: 5.9160e-06 | scale:     1.0000 | micro time: 3.797 | step time: 0.000
train | epoch   7 | Iter:   1643/  2110 | global iter:   1643/  2110 | loss: 0.0130 | ds_loss: 0.0000 | lr: 5.8922e-06 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   7 | Iter:   1644/  2110 | global iter:   1644/  2110 | loss: 0.0064 | ds_loss: 0.0000 | lr: 5.8684e-06 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1644/  2110 | global iter:   1644/  2110 | loss: 0.0131 | ds_loss: 0.0000 | lr: 5.8684e-06 | scale:     1.0000 | micro time: 3.775 | step time: 3.782
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1645/  2110 | global iter:   1645/  2110 | loss: 0.0040 | ds_loss: 0.0000 | lr: 5.8447e-06 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   7 | Iter:   1646/  2110 | global iter:   1646/  2110 | loss: 0.0228 | ds_loss: 0.0000 | lr: 5.8210e-06 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   7 | Iter:   1647/  2110 | global iter:   1647/  2110 | loss: 0.0088 | ds_loss: 0.0000 | lr: 5.7973e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   7 | Iter:   1648/  2110 | global iter:   1648/  2110 | loss: 0.0222 | ds_loss: 0.0000 | lr: 5.7737e-06 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1648/  2110 | global iter:   1648/  2110 | loss: 0.0145 | ds_loss: 0.0000 | lr: 5.7737e-06 | scale:     1.0000 | micro time: 3.794 | step time: 3.792
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1649/  2110 | global iter:   1649/  2110 | loss: 0.0175 | ds_loss: 0.0000 | lr: 5.7501e-06 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   7 | Iter:   1650/  2110 | global iter:   1650/  2110 | loss: 0.0068 | ds_loss: 0.0000 | lr: 5.7266e-06 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
train | epoch   7 | Iter:   1651/  2110 | global iter:   1651/  2110 | loss: 0.0088 | ds_loss: 0.0000 | lr: 5.7031e-06 | scale:     1.0000 | micro time: 3.777 | step time: 0.000
train | epoch   7 | Iter:   1652/  2110 | global iter:   1652/  2110 | loss: 0.0230 | ds_loss: 0.0000 | lr: 5.6797e-06 | scale:     1.0000 | micro time: 3.765 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1652/  2110 | global iter:   1652/  2110 | loss: 0.0140 | ds_loss: 0.0000 | lr: 5.6797e-06 | scale:     1.0000 | micro time: 3.765 | step time: 3.776
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1653/  2110 | global iter:   1653/  2110 | loss: 0.0045 | ds_loss: 0.0000 | lr: 5.6563e-06 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   7 | Iter:   1654/  2110 | global iter:   1654/  2110 | loss: 0.0054 | ds_loss: 0.0000 | lr: 5.6330e-06 | scale:     1.0000 | micro time: 3.770 | step time: 0.000
train | epoch   7 | Iter:   1655/  2110 | global iter:   1655/  2110 | loss: 0.0145 | ds_loss: 0.0000 | lr: 5.6097e-06 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   7 | Iter:   1656/  2110 | global iter:   1656/  2110 | loss: 0.0080 | ds_loss: 0.0000 | lr: 5.5864e-06 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1656/  2110 | global iter:   1656/  2110 | loss: 0.0081 | ds_loss: 0.0000 | lr: 5.5864e-06 | scale:     1.0000 | micro time: 3.798 | step time: 3.784
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1657/  2110 | global iter:   1657/  2110 | loss: 0.0062 | ds_loss: 0.0000 | lr: 5.5632e-06 | scale:     1.0000 | micro time: 3.780 | step time: 0.000
train | epoch   7 | Iter:   1658/  2110 | global iter:   1658/  2110 | loss: 0.0247 | ds_loss: 0.0000 | lr: 5.5400e-06 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   7 | Iter:   1659/  2110 | global iter:   1659/  2110 | loss: 0.0139 | ds_loss: 0.0000 | lr: 5.5169e-06 | scale:     1.0000 | micro time: 3.767 | step time: 0.000
train | epoch   7 | Iter:   1660/  2110 | global iter:   1660/  2110 | loss: 0.0087 | ds_loss: 0.0000 | lr: 5.4938e-06 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1660/  2110 | global iter:   1660/  2110 | loss: 0.0133 | ds_loss: 0.0000 | lr: 5.4938e-06 | scale:     1.0000 | micro time: 3.786 | step time: 3.779
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1661/  2110 | global iter:   1661/  2110 | loss: 0.0093 | ds_loss: 0.0000 | lr: 5.4707e-06 | scale:     1.0000 | micro time: 3.770 | step time: 0.000
train | epoch   7 | Iter:   1662/  2110 | global iter:   1662/  2110 | loss: 0.0123 | ds_loss: 0.0000 | lr: 5.4477e-06 | scale:     1.0000 | micro time: 3.813 | step time: 0.000
train | epoch   7 | Iter:   1663/  2110 | global iter:   1663/  2110 | loss: 0.0124 | ds_loss: 0.0000 | lr: 5.4248e-06 | scale:     1.0000 | micro time: 3.767 | step time: 0.000
train | epoch   7 | Iter:   1664/  2110 | global iter:   1664/  2110 | loss: 0.0049 | ds_loss: 0.0000 | lr: 5.4018e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1664/  2110 | global iter:   1664/  2110 | loss: 0.0097 | ds_loss: 0.0000 | lr: 5.4018e-06 | scale:     1.0000 | micro time: 3.802 | step time: 3.788
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1665/  2110 | global iter:   1665/  2110 | loss: 0.0069 | ds_loss: 0.0000 | lr: 5.3790e-06 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   7 | Iter:   1666/  2110 | global iter:   1666/  2110 | loss: 0.0110 | ds_loss: 0.0000 | lr: 5.3561e-06 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   7 | Iter:   1667/  2110 | global iter:   1667/  2110 | loss: 0.0157 | ds_loss: 0.0000 | lr: 5.3334e-06 | scale:     1.0000 | micro time: 3.767 | step time: 0.000
train | epoch   7 | Iter:   1668/  2110 | global iter:   1668/  2110 | loss: 0.0114 | ds_loss: 0.0000 | lr: 5.3106e-06 | scale:     1.0000 | micro time: 3.822 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1668/  2110 | global iter:   1668/  2110 | loss: 0.0113 | ds_loss: 0.0000 | lr: 5.3106e-06 | scale:     1.0000 | micro time: 3.822 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1669/  2110 | global iter:   1669/  2110 | loss: 0.0103 | ds_loss: 0.0000 | lr: 5.2879e-06 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   7 | Iter:   1670/  2110 | global iter:   1670/  2110 | loss: 0.0072 | ds_loss: 0.0000 | lr: 5.2653e-06 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   7 | Iter:   1671/  2110 | global iter:   1671/  2110 | loss: 0.0147 | ds_loss: 0.0000 | lr: 5.2426e-06 | scale:     1.0000 | micro time: 3.770 | step time: 0.000
train | epoch   7 | Iter:   1672/  2110 | global iter:   1672/  2110 | loss: 0.0171 | ds_loss: 0.0000 | lr: 5.2201e-06 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1672/  2110 | global iter:   1672/  2110 | loss: 0.0123 | ds_loss: 0.0000 | lr: 5.2201e-06 | scale:     1.0000 | micro time: 3.779 | step time: 3.781
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1673/  2110 | global iter:   1673/  2110 | loss: 0.0096 | ds_loss: 0.0000 | lr: 5.1976e-06 | scale:     1.0000 | micro time: 3.770 | step time: 0.000
train | epoch   7 | Iter:   1674/  2110 | global iter:   1674/  2110 | loss: 0.0136 | ds_loss: 0.0000 | lr: 5.1751e-06 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   7 | Iter:   1675/  2110 | global iter:   1675/  2110 | loss: 0.0066 | ds_loss: 0.0000 | lr: 5.1526e-06 | scale:     1.0000 | micro time: 3.776 | step time: 0.000
train | epoch   7 | Iter:   1676/  2110 | global iter:   1676/  2110 | loss: 0.0123 | ds_loss: 0.0000 | lr: 5.1303e-06 | scale:     1.0000 | micro time: 3.813 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1676/  2110 | global iter:   1676/  2110 | loss: 0.0105 | ds_loss: 0.0000 | lr: 5.1303e-06 | scale:     1.0000 | micro time: 3.813 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1677/  2110 | global iter:   1677/  2110 | loss: 0.0116 | ds_loss: 0.0000 | lr: 5.1079e-06 | scale:     1.0000 | micro time: 3.769 | step time: 0.000
train | epoch   7 | Iter:   1678/  2110 | global iter:   1678/  2110 | loss: 0.0089 | ds_loss: 0.0000 | lr: 5.0856e-06 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   7 | Iter:   1679/  2110 | global iter:   1679/  2110 | loss: 0.0246 | ds_loss: 0.0000 | lr: 5.0633e-06 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   7 | Iter:   1680/  2110 | global iter:   1680/  2110 | loss: 0.0192 | ds_loss: 0.0000 | lr: 5.0411e-06 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1680/  2110 | global iter:   1680/  2110 | loss: 0.0161 | ds_loss: 0.0000 | lr: 5.0411e-06 | scale:     1.0000 | micro time: 3.790 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1681/  2110 | global iter:   1681/  2110 | loss: 0.0231 | ds_loss: 0.0000 | lr: 5.0190e-06 | scale:     1.0000 | micro time: 3.800 | step time: 0.000
train | epoch   7 | Iter:   1682/  2110 | global iter:   1682/  2110 | loss: 0.0087 | ds_loss: 0.0000 | lr: 4.9968e-06 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   7 | Iter:   1683/  2110 | global iter:   1683/  2110 | loss: 0.0223 | ds_loss: 0.0000 | lr: 4.9748e-06 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   7 | Iter:   1684/  2110 | global iter:   1684/  2110 | loss: 0.0061 | ds_loss: 0.0000 | lr: 4.9527e-06 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1684/  2110 | global iter:   1684/  2110 | loss: 0.0151 | ds_loss: 0.0000 | lr: 4.9527e-06 | scale:     1.0000 | micro time: 3.799 | step time: 3.802
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1685/  2110 | global iter:   1685/  2110 | loss: 0.0154 | ds_loss: 0.0000 | lr: 4.9307e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   7 | Iter:   1686/  2110 | global iter:   1686/  2110 | loss: 0.0089 | ds_loss: 0.0000 | lr: 4.9088e-06 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   7 | Iter:   1687/  2110 | global iter:   1687/  2110 | loss: 0.0066 | ds_loss: 0.0000 | lr: 4.8869e-06 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   7 | Iter:   1688/  2110 | global iter:   1688/  2110 | loss: 0.0133 | ds_loss: 0.0000 | lr: 4.8650e-06 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1688/  2110 | global iter:   1688/  2110 | loss: 0.0111 | ds_loss: 0.0000 | lr: 4.8650e-06 | scale:     1.0000 | micro time: 3.786 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1689/  2110 | global iter:   1689/  2110 | loss: 0.0169 | ds_loss: 0.0000 | lr: 4.8432e-06 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   7 | Iter:   1690/  2110 | global iter:   1690/  2110 | loss: 0.0171 | ds_loss: 0.0000 | lr: 4.8214e-06 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   7 | Iter:   1691/  2110 | global iter:   1691/  2110 | loss: 0.0245 | ds_loss: 0.0000 | lr: 4.7997e-06 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   7 | Iter:   1692/  2110 | global iter:   1692/  2110 | loss: 0.0153 | ds_loss: 0.0000 | lr: 4.7780e-06 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1692/  2110 | global iter:   1692/  2110 | loss: 0.0185 | ds_loss: 0.0000 | lr: 4.7780e-06 | scale:     1.0000 | micro time: 3.803 | step time: 3.796
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   7 | Iter:   1693/  2110 | global iter:   1693/  2110 | loss: 0.0151 | ds_loss: 0.0000 | lr: 4.7564e-06 | scale:     1.0000 | micro time: 3.805 | step time: 0.000
train | epoch   7 | Iter:   1694/  2110 | global iter:   1694/  2110 | loss: 0.0314 | ds_loss: 0.0000 | lr: 4.7348e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   7 | Iter:   1695/  2110 | global iter:   1695/  2110 | loss: 0.0047 | ds_loss: 0.0000 | lr: 4.7133e-06 | scale:     1.0000 | micro time: 3.780 | step time: 0.000
train | epoch   7 | Iter:   1696/  2110 | global iter:   1696/  2110 | loss: 0.0056 | ds_loss: 0.0000 | lr: 4.6918e-06 | scale:     1.0000 | micro time: 3.374 | step time: 0.000
****************************************************************************************************
train | epoch   7 | Iter:   1696/  2110 | global iter:   1696/  2110 | loss: 0.0142 | ds_loss: 0.0000 | lr: 4.6918e-06 | scale:     1.0000 | micro time: 3.374 | step time: 3.690
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
Tue Apr  8 08:52:50 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   62C    P0             55W /  250W |   29814MiB /  32768MiB |     63%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |     97%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   49C    P0             48W /  250W |   29848MiB /  32768MiB |     63%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   54C    P0             49W /  250W |   28312MiB /  32768MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 08:52:50 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   62C    P0             55W /  250W |   29814MiB /  32768MiB |     63%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |     97%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   49C    P0             49W /  250W |   29848MiB /  32768MiB |     63%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   54C    P0             49W /  250W |   28312MiB /  32768MiB |      8%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 08:52:50 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   62C    P0             55W /  250W |   29814MiB /  32768MiB |     63%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |     97%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   49C    P0             49W /  250W |   29848MiB /  32768MiB |     63%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   54C    P0             49W /  250W |   28312MiB /  32768MiB |      8%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 08:52:50 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   62C    P0             55W /  250W |   29814MiB /  32768MiB |     63%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |     97%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   49C    P0             49W /  250W |   29848MiB /  32768MiB |     63%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   54C    P0             49W /  250W |   28312MiB /  32768MiB |      1%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

train | epoch   8 | Iter:   1697/  2110 | global iter:   1697/  2110 | loss: 0.0129 | ds_loss: 0.0000 | lr: 4.6703e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   8 | Iter:   1698/  2110 | global iter:   1698/  2110 | loss: 0.0041 | ds_loss: 0.0000 | lr: 4.6489e-06 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   8 | Iter:   1699/  2110 | global iter:   1699/  2110 | loss: 0.0040 | ds_loss: 0.0000 | lr: 4.6276e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   8 | Iter:   1700/  2110 | global iter:   1700/  2110 | loss: 0.0056 | ds_loss: 0.0000 | lr: 4.6062e-06 | scale:     1.0000 | micro time: 3.817 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1700/  2110 | global iter:   1700/  2110 | loss: 0.0067 | ds_loss: 0.0000 | lr: 4.6062e-06 | scale:     1.0000 | micro time: 3.817 | step time: 3.803
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1701/  2110 | global iter:   1701/  2110 | loss: 0.0173 | ds_loss: 0.0000 | lr: 4.5850e-06 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   8 | Iter:   1702/  2110 | global iter:   1702/  2110 | loss: 0.0029 | ds_loss: 0.0000 | lr: 4.5637e-06 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
train | epoch   8 | Iter:   1703/  2110 | global iter:   1703/  2110 | loss: 0.0069 | ds_loss: 0.0000 | lr: 4.5426e-06 | scale:     1.0000 | micro time: 3.813 | step time: 0.000
train | epoch   8 | Iter:   1704/  2110 | global iter:   1704/  2110 | loss: 0.0130 | ds_loss: 0.0000 | lr: 4.5214e-06 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1704/  2110 | global iter:   1704/  2110 | loss: 0.0100 | ds_loss: 0.0000 | lr: 4.5214e-06 | scale:     1.0000 | micro time: 3.801 | step time: 3.801
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1705/  2110 | global iter:   1705/  2110 | loss: 0.0111 | ds_loss: 0.0000 | lr: 4.5003e-06 | scale:     1.0000 | micro time: 3.766 | step time: 0.000
train | epoch   8 | Iter:   1706/  2110 | global iter:   1706/  2110 | loss: 0.0178 | ds_loss: 0.0000 | lr: 4.4793e-06 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
train | epoch   8 | Iter:   1707/  2110 | global iter:   1707/  2110 | loss: 0.0023 | ds_loss: 0.0000 | lr: 4.4583e-06 | scale:     1.0000 | micro time: 3.774 | step time: 0.000
train | epoch   8 | Iter:   1708/  2110 | global iter:   1708/  2110 | loss: 0.0086 | ds_loss: 0.0000 | lr: 4.4373e-06 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1708/  2110 | global iter:   1708/  2110 | loss: 0.0100 | ds_loss: 0.0000 | lr: 4.4373e-06 | scale:     1.0000 | micro time: 3.786 | step time: 3.780
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1709/  2110 | global iter:   1709/  2110 | loss: 0.0094 | ds_loss: 0.0000 | lr: 4.4164e-06 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   8 | Iter:   1710/  2110 | global iter:   1710/  2110 | loss: 0.0067 | ds_loss: 0.0000 | lr: 4.3956e-06 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   8 | Iter:   1711/  2110 | global iter:   1711/  2110 | loss: 0.0045 | ds_loss: 0.0000 | lr: 4.3747e-06 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   8 | Iter:   1712/  2110 | global iter:   1712/  2110 | loss: 0.0112 | ds_loss: 0.0000 | lr: 4.3540e-06 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1712/  2110 | global iter:   1712/  2110 | loss: 0.0079 | ds_loss: 0.0000 | lr: 4.3540e-06 | scale:     1.0000 | micro time: 3.810 | step time: 3.788
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1713/  2110 | global iter:   1713/  2110 | loss: 0.0118 | ds_loss: 0.0000 | lr: 4.3333e-06 | scale:     1.0000 | micro time: 3.784 | step time: 0.000
train | epoch   8 | Iter:   1714/  2110 | global iter:   1714/  2110 | loss: 0.0145 | ds_loss: 0.0000 | lr: 4.3126e-06 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   8 | Iter:   1715/  2110 | global iter:   1715/  2110 | loss: 0.0074 | ds_loss: 0.0000 | lr: 4.2919e-06 | scale:     1.0000 | micro time: 3.784 | step time: 0.000
train | epoch   8 | Iter:   1716/  2110 | global iter:   1716/  2110 | loss: 0.0137 | ds_loss: 0.0000 | lr: 4.2714e-06 | scale:     1.0000 | micro time: 3.770 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1716/  2110 | global iter:   1716/  2110 | loss: 0.0118 | ds_loss: 0.0000 | lr: 4.2714e-06 | scale:     1.0000 | micro time: 3.770 | step time: 3.780
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1717/  2110 | global iter:   1717/  2110 | loss: 0.0150 | ds_loss: 0.0000 | lr: 4.2508e-06 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   8 | Iter:   1718/  2110 | global iter:   1718/  2110 | loss: 0.0058 | ds_loss: 0.0000 | lr: 4.2303e-06 | scale:     1.0000 | micro time: 3.777 | step time: 0.000
train | epoch   8 | Iter:   1719/  2110 | global iter:   1719/  2110 | loss: 0.0047 | ds_loss: 0.0000 | lr: 4.2099e-06 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   8 | Iter:   1720/  2110 | global iter:   1720/  2110 | loss: 0.0074 | ds_loss: 0.0000 | lr: 4.1895e-06 | scale:     1.0000 | micro time: 3.774 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1720/  2110 | global iter:   1720/  2110 | loss: 0.0082 | ds_loss: 0.0000 | lr: 4.1895e-06 | scale:     1.0000 | micro time: 3.774 | step time: 3.783
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1721/  2110 | global iter:   1721/  2110 | loss: 0.0188 | ds_loss: 0.0000 | lr: 4.1691e-06 | scale:     1.0000 | micro time: 3.797 | step time: 0.000
train | epoch   8 | Iter:   1722/  2110 | global iter:   1722/  2110 | loss: 0.0105 | ds_loss: 0.0000 | lr: 4.1488e-06 | scale:     1.0000 | micro time: 3.750 | step time: 0.000
train | epoch   8 | Iter:   1723/  2110 | global iter:   1723/  2110 | loss: 0.0161 | ds_loss: 0.0000 | lr: 4.1285e-06 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   8 | Iter:   1724/  2110 | global iter:   1724/  2110 | loss: 0.0134 | ds_loss: 0.0000 | lr: 4.1083e-06 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1724/  2110 | global iter:   1724/  2110 | loss: 0.0147 | ds_loss: 0.0000 | lr: 4.1083e-06 | scale:     1.0000 | micro time: 3.775 | step time: 3.780
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1725/  2110 | global iter:   1725/  2110 | loss: 0.0024 | ds_loss: 0.0000 | lr: 4.0882e-06 | scale:     1.0000 | micro time: 3.804 | step time: 0.000
train | epoch   8 | Iter:   1726/  2110 | global iter:   1726/  2110 | loss: 0.0112 | ds_loss: 0.0000 | lr: 4.0680e-06 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   8 | Iter:   1727/  2110 | global iter:   1727/  2110 | loss: 0.0077 | ds_loss: 0.0000 | lr: 4.0480e-06 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   8 | Iter:   1728/  2110 | global iter:   1728/  2110 | loss: 0.0088 | ds_loss: 0.0000 | lr: 4.0279e-06 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1728/  2110 | global iter:   1728/  2110 | loss: 0.0075 | ds_loss: 0.0000 | lr: 4.0279e-06 | scale:     1.0000 | micro time: 3.783 | step time: 3.796
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1729/  2110 | global iter:   1729/  2110 | loss: 0.0123 | ds_loss: 0.0000 | lr: 4.0079e-06 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   8 | Iter:   1730/  2110 | global iter:   1730/  2110 | loss: 0.0101 | ds_loss: 0.0000 | lr: 3.9880e-06 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   8 | Iter:   1731/  2110 | global iter:   1731/  2110 | loss: 0.0071 | ds_loss: 0.0000 | lr: 3.9681e-06 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   8 | Iter:   1732/  2110 | global iter:   1732/  2110 | loss: 0.0038 | ds_loss: 0.0000 | lr: 3.9483e-06 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1732/  2110 | global iter:   1732/  2110 | loss: 0.0083 | ds_loss: 0.0000 | lr: 3.9483e-06 | scale:     1.0000 | micro time: 3.806 | step time: 3.792
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1733/  2110 | global iter:   1733/  2110 | loss: 0.0096 | ds_loss: 0.0000 | lr: 3.9285e-06 | scale:     1.0000 | micro time: 3.765 | step time: 0.000
train | epoch   8 | Iter:   1734/  2110 | global iter:   1734/  2110 | loss: 0.0057 | ds_loss: 0.0000 | lr: 3.9087e-06 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   8 | Iter:   1735/  2110 | global iter:   1735/  2110 | loss: 0.0056 | ds_loss: 0.0000 | lr: 3.8890e-06 | scale:     1.0000 | micro time: 3.814 | step time: 0.000
train | epoch   8 | Iter:   1736/  2110 | global iter:   1736/  2110 | loss: 0.0048 | ds_loss: 0.0000 | lr: 3.8694e-06 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1736/  2110 | global iter:   1736/  2110 | loss: 0.0064 | ds_loss: 0.0000 | lr: 3.8694e-06 | scale:     1.0000 | micro time: 3.782 | step time: 3.787
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1737/  2110 | global iter:   1737/  2110 | loss: 0.0072 | ds_loss: 0.0000 | lr: 3.8497e-06 | scale:     1.0000 | micro time: 3.797 | step time: 0.000
train | epoch   8 | Iter:   1738/  2110 | global iter:   1738/  2110 | loss: 0.0041 | ds_loss: 0.0000 | lr: 3.8302e-06 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   8 | Iter:   1739/  2110 | global iter:   1739/  2110 | loss: 0.0019 | ds_loss: 0.0000 | lr: 3.8107e-06 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   8 | Iter:   1740/  2110 | global iter:   1740/  2110 | loss: 0.0098 | ds_loss: 0.0000 | lr: 3.7912e-06 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1740/  2110 | global iter:   1740/  2110 | loss: 0.0057 | ds_loss: 0.0000 | lr: 3.7912e-06 | scale:     1.0000 | micro time: 3.798 | step time: 3.786
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1741/  2110 | global iter:   1741/  2110 | loss: 0.0040 | ds_loss: 0.0000 | lr: 3.7718e-06 | scale:     1.0000 | micro time: 3.766 | step time: 0.000
train | epoch   8 | Iter:   1742/  2110 | global iter:   1742/  2110 | loss: 0.0082 | ds_loss: 0.0000 | lr: 3.7524e-06 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   8 | Iter:   1743/  2110 | global iter:   1743/  2110 | loss: 0.0063 | ds_loss: 0.0000 | lr: 3.7331e-06 | scale:     1.0000 | micro time: 3.822 | step time: 0.000
train | epoch   8 | Iter:   1744/  2110 | global iter:   1744/  2110 | loss: 0.0047 | ds_loss: 0.0000 | lr: 3.7138e-06 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1744/  2110 | global iter:   1744/  2110 | loss: 0.0058 | ds_loss: 0.0000 | lr: 3.7138e-06 | scale:     1.0000 | micro time: 3.790 | step time: 3.796
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1745/  2110 | global iter:   1745/  2110 | loss: 0.0153 | ds_loss: 0.0000 | lr: 3.6946e-06 | scale:     1.0000 | micro time: 3.817 | step time: 0.000
train | epoch   8 | Iter:   1746/  2110 | global iter:   1746/  2110 | loss: 0.0156 | ds_loss: 0.0000 | lr: 3.6754e-06 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   8 | Iter:   1747/  2110 | global iter:   1747/  2110 | loss: 0.0093 | ds_loss: 0.0000 | lr: 3.6562e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   8 | Iter:   1748/  2110 | global iter:   1748/  2110 | loss: 0.0069 | ds_loss: 0.0000 | lr: 3.6371e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1748/  2110 | global iter:   1748/  2110 | loss: 0.0118 | ds_loss: 0.0000 | lr: 3.6371e-06 | scale:     1.0000 | micro time: 3.802 | step time: 3.802
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1749/  2110 | global iter:   1749/  2110 | loss: 0.0084 | ds_loss: 0.0000 | lr: 3.6181e-06 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   8 | Iter:   1750/  2110 | global iter:   1750/  2110 | loss: 0.0050 | ds_loss: 0.0000 | lr: 3.5991e-06 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   8 | Iter:   1751/  2110 | global iter:   1751/  2110 | loss: 0.0187 | ds_loss: 0.0000 | lr: 3.5802e-06 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
train | epoch   8 | Iter:   1752/  2110 | global iter:   1752/  2110 | loss: 0.0060 | ds_loss: 0.0000 | lr: 3.5613e-06 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1752/  2110 | global iter:   1752/  2110 | loss: 0.0095 | ds_loss: 0.0000 | lr: 3.5613e-06 | scale:     1.0000 | micro time: 3.775 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1753/  2110 | global iter:   1753/  2110 | loss: 0.0060 | ds_loss: 0.0000 | lr: 3.5424e-06 | scale:     1.0000 | micro time: 3.773 | step time: 0.000
train | epoch   8 | Iter:   1754/  2110 | global iter:   1754/  2110 | loss: 0.0189 | ds_loss: 0.0000 | lr: 3.5236e-06 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   8 | Iter:   1755/  2110 | global iter:   1755/  2110 | loss: 0.0142 | ds_loss: 0.0000 | lr: 3.5048e-06 | scale:     1.0000 | micro time: 3.800 | step time: 0.000
train | epoch   8 | Iter:   1756/  2110 | global iter:   1756/  2110 | loss: 0.0154 | ds_loss: 0.0000 | lr: 3.4861e-06 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1756/  2110 | global iter:   1756/  2110 | loss: 0.0136 | ds_loss: 0.0000 | lr: 3.4861e-06 | scale:     1.0000 | micro time: 3.787 | step time: 3.789
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1757/  2110 | global iter:   1757/  2110 | loss: 0.0085 | ds_loss: 0.0000 | lr: 3.4675e-06 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
train | epoch   8 | Iter:   1758/  2110 | global iter:   1758/  2110 | loss: 0.0039 | ds_loss: 0.0000 | lr: 3.4489e-06 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
train | epoch   8 | Iter:   1759/  2110 | global iter:   1759/  2110 | loss: 0.0046 | ds_loss: 0.0000 | lr: 3.4303e-06 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   8 | Iter:   1760/  2110 | global iter:   1760/  2110 | loss: 0.0046 | ds_loss: 0.0000 | lr: 3.4118e-06 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1760/  2110 | global iter:   1760/  2110 | loss: 0.0054 | ds_loss: 0.0000 | lr: 3.4118e-06 | scale:     1.0000 | micro time: 3.791 | step time: 3.796
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1761/  2110 | global iter:   1761/  2110 | loss: 0.0104 | ds_loss: 0.0000 | lr: 3.3933e-06 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
train | epoch   8 | Iter:   1762/  2110 | global iter:   1762/  2110 | loss: 0.0063 | ds_loss: 0.0000 | lr: 3.3749e-06 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   8 | Iter:   1763/  2110 | global iter:   1763/  2110 | loss: 0.0038 | ds_loss: 0.0000 | lr: 3.3565e-06 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   8 | Iter:   1764/  2110 | global iter:   1764/  2110 | loss: 0.0039 | ds_loss: 0.0000 | lr: 3.3382e-06 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1764/  2110 | global iter:   1764/  2110 | loss: 0.0061 | ds_loss: 0.0000 | lr: 3.3382e-06 | scale:     1.0000 | micro time: 3.775 | step time: 3.779
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1765/  2110 | global iter:   1765/  2110 | loss: 0.0099 | ds_loss: 0.0000 | lr: 3.3199e-06 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   8 | Iter:   1766/  2110 | global iter:   1766/  2110 | loss: 0.0092 | ds_loss: 0.0000 | lr: 3.3017e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   8 | Iter:   1767/  2110 | global iter:   1767/  2110 | loss: 0.0072 | ds_loss: 0.0000 | lr: 3.2835e-06 | scale:     1.0000 | micro time: 3.800 | step time: 0.000
train | epoch   8 | Iter:   1768/  2110 | global iter:   1768/  2110 | loss: 0.0079 | ds_loss: 0.0000 | lr: 3.2654e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1768/  2110 | global iter:   1768/  2110 | loss: 0.0086 | ds_loss: 0.0000 | lr: 3.2654e-06 | scale:     1.0000 | micro time: 3.802 | step time: 3.797
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1769/  2110 | global iter:   1769/  2110 | loss: 0.0087 | ds_loss: 0.0000 | lr: 3.2473e-06 | scale:     1.0000 | micro time: 3.784 | step time: 0.000
train | epoch   8 | Iter:   1770/  2110 | global iter:   1770/  2110 | loss: 0.0116 | ds_loss: 0.0000 | lr: 3.2292e-06 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   8 | Iter:   1771/  2110 | global iter:   1771/  2110 | loss: 0.0112 | ds_loss: 0.0000 | lr: 3.2113e-06 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   8 | Iter:   1772/  2110 | global iter:   1772/  2110 | loss: 0.0046 | ds_loss: 0.0000 | lr: 3.1933e-06 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1772/  2110 | global iter:   1772/  2110 | loss: 0.0090 | ds_loss: 0.0000 | lr: 3.1933e-06 | scale:     1.0000 | micro time: 3.818 | step time: 3.790
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1773/  2110 | global iter:   1773/  2110 | loss: 0.0131 | ds_loss: 0.0000 | lr: 3.1754e-06 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   8 | Iter:   1774/  2110 | global iter:   1774/  2110 | loss: 0.0100 | ds_loss: 0.0000 | lr: 3.1576e-06 | scale:     1.0000 | micro time: 3.809 | step time: 0.000
train | epoch   8 | Iter:   1775/  2110 | global iter:   1775/  2110 | loss: 0.0036 | ds_loss: 0.0000 | lr: 3.1398e-06 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   8 | Iter:   1776/  2110 | global iter:   1776/  2110 | loss: 0.0038 | ds_loss: 0.0000 | lr: 3.1220e-06 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1776/  2110 | global iter:   1776/  2110 | loss: 0.0076 | ds_loss: 0.0000 | lr: 3.1220e-06 | scale:     1.0000 | micro time: 3.782 | step time: 3.789
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1777/  2110 | global iter:   1777/  2110 | loss: 0.0112 | ds_loss: 0.0000 | lr: 3.1043e-06 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   8 | Iter:   1778/  2110 | global iter:   1778/  2110 | loss: 0.0176 | ds_loss: 0.0000 | lr: 3.0867e-06 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   8 | Iter:   1779/  2110 | global iter:   1779/  2110 | loss: 0.0048 | ds_loss: 0.0000 | lr: 3.0691e-06 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   8 | Iter:   1780/  2110 | global iter:   1780/  2110 | loss: 0.0055 | ds_loss: 0.0000 | lr: 3.0515e-06 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1780/  2110 | global iter:   1780/  2110 | loss: 0.0098 | ds_loss: 0.0000 | lr: 3.0515e-06 | scale:     1.0000 | micro time: 3.783 | step time: 3.789
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1781/  2110 | global iter:   1781/  2110 | loss: 0.0078 | ds_loss: 0.0000 | lr: 3.0340e-06 | scale:     1.0000 | micro time: 3.800 | step time: 0.000
train | epoch   8 | Iter:   1782/  2110 | global iter:   1782/  2110 | loss: 0.0041 | ds_loss: 0.0000 | lr: 3.0166e-06 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   8 | Iter:   1783/  2110 | global iter:   1783/  2110 | loss: 0.0085 | ds_loss: 0.0000 | lr: 2.9992e-06 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   8 | Iter:   1784/  2110 | global iter:   1784/  2110 | loss: 0.0077 | ds_loss: 0.0000 | lr: 2.9818e-06 | scale:     1.0000 | micro time: 3.820 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1784/  2110 | global iter:   1784/  2110 | loss: 0.0070 | ds_loss: 0.0000 | lr: 2.9818e-06 | scale:     1.0000 | micro time: 3.820 | step time: 3.798
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1785/  2110 | global iter:   1785/  2110 | loss: 0.0033 | ds_loss: 0.0000 | lr: 2.9645e-06 | scale:     1.0000 | micro time: 3.772 | step time: 0.000
train | epoch   8 | Iter:   1786/  2110 | global iter:   1786/  2110 | loss: 0.0016 | ds_loss: 0.0000 | lr: 2.9473e-06 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
train | epoch   8 | Iter:   1787/  2110 | global iter:   1787/  2110 | loss: 0.0031 | ds_loss: 0.0000 | lr: 2.9300e-06 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   8 | Iter:   1788/  2110 | global iter:   1788/  2110 | loss: 0.0252 | ds_loss: 0.0000 | lr: 2.9129e-06 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1788/  2110 | global iter:   1788/  2110 | loss: 0.0083 | ds_loss: 0.0000 | lr: 2.9129e-06 | scale:     1.0000 | micro time: 3.786 | step time: 3.784
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1789/  2110 | global iter:   1789/  2110 | loss: 0.0244 | ds_loss: 0.0000 | lr: 2.8958e-06 | scale:     1.0000 | micro time: 3.817 | step time: 0.000
train | epoch   8 | Iter:   1790/  2110 | global iter:   1790/  2110 | loss: 0.0104 | ds_loss: 0.0000 | lr: 2.8787e-06 | scale:     1.0000 | micro time: 3.751 | step time: 0.000
train | epoch   8 | Iter:   1791/  2110 | global iter:   1791/  2110 | loss: 0.0089 | ds_loss: 0.0000 | lr: 2.8617e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   8 | Iter:   1792/  2110 | global iter:   1792/  2110 | loss: 0.0109 | ds_loss: 0.0000 | lr: 2.8447e-06 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1792/  2110 | global iter:   1792/  2110 | loss: 0.0136 | ds_loss: 0.0000 | lr: 2.8447e-06 | scale:     1.0000 | micro time: 3.783 | step time: 3.788
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1793/  2110 | global iter:   1793/  2110 | loss: 0.0094 | ds_loss: 0.0000 | lr: 2.8278e-06 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
train | epoch   8 | Iter:   1794/  2110 | global iter:   1794/  2110 | loss: 0.0124 | ds_loss: 0.0000 | lr: 2.8110e-06 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
train | epoch   8 | Iter:   1795/  2110 | global iter:   1795/  2110 | loss: 0.0016 | ds_loss: 0.0000 | lr: 2.7941e-06 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   8 | Iter:   1796/  2110 | global iter:   1796/  2110 | loss: 0.0036 | ds_loss: 0.0000 | lr: 2.7774e-06 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1796/  2110 | global iter:   1796/  2110 | loss: 0.0068 | ds_loss: 0.0000 | lr: 2.7774e-06 | scale:     1.0000 | micro time: 3.786 | step time: 3.799
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1797/  2110 | global iter:   1797/  2110 | loss: 0.0159 | ds_loss: 0.0000 | lr: 2.7607e-06 | scale:     1.0000 | micro time: 3.774 | step time: 0.000
train | epoch   8 | Iter:   1798/  2110 | global iter:   1798/  2110 | loss: 0.0153 | ds_loss: 0.0000 | lr: 2.7440e-06 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   8 | Iter:   1799/  2110 | global iter:   1799/  2110 | loss: 0.0072 | ds_loss: 0.0000 | lr: 2.7274e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   8 | Iter:   1800/  2110 | global iter:   1800/  2110 | loss: 0.0036 | ds_loss: 0.0000 | lr: 2.7108e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1800/  2110 | global iter:   1800/  2110 | loss: 0.0105 | ds_loss: 0.0000 | lr: 2.7108e-06 | scale:     1.0000 | micro time: 3.802 | step time: 3.793
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1801/  2110 | global iter:   1801/  2110 | loss: 0.0020 | ds_loss: 0.0000 | lr: 2.6943e-06 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   8 | Iter:   1802/  2110 | global iter:   1802/  2110 | loss: 0.0081 | ds_loss: 0.0000 | lr: 2.6778e-06 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
train | epoch   8 | Iter:   1803/  2110 | global iter:   1803/  2110 | loss: 0.0017 | ds_loss: 0.0000 | lr: 2.6614e-06 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   8 | Iter:   1804/  2110 | global iter:   1804/  2110 | loss: 0.0093 | ds_loss: 0.0000 | lr: 2.6450e-06 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1804/  2110 | global iter:   1804/  2110 | loss: 0.0053 | ds_loss: 0.0000 | lr: 2.6450e-06 | scale:     1.0000 | micro time: 3.810 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1805/  2110 | global iter:   1805/  2110 | loss: 0.0070 | ds_loss: 0.0000 | lr: 2.6287e-06 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   8 | Iter:   1806/  2110 | global iter:   1806/  2110 | loss: 0.0173 | ds_loss: 0.0000 | lr: 2.6124e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   8 | Iter:   1807/  2110 | global iter:   1807/  2110 | loss: 0.0058 | ds_loss: 0.0000 | lr: 2.5962e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   8 | Iter:   1808/  2110 | global iter:   1808/  2110 | loss: 0.0114 | ds_loss: 0.0000 | lr: 2.5800e-06 | scale:     1.0000 | micro time: 3.776 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1808/  2110 | global iter:   1808/  2110 | loss: 0.0104 | ds_loss: 0.0000 | lr: 2.5800e-06 | scale:     1.0000 | micro time: 3.776 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1809/  2110 | global iter:   1809/  2110 | loss: 0.0035 | ds_loss: 0.0000 | lr: 2.5639e-06 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   8 | Iter:   1810/  2110 | global iter:   1810/  2110 | loss: 0.0182 | ds_loss: 0.0000 | lr: 2.5479e-06 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   8 | Iter:   1811/  2110 | global iter:   1811/  2110 | loss: 0.0049 | ds_loss: 0.0000 | lr: 2.5318e-06 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
train | epoch   8 | Iter:   1812/  2110 | global iter:   1812/  2110 | loss: 0.0033 | ds_loss: 0.0000 | lr: 2.5159e-06 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1812/  2110 | global iter:   1812/  2110 | loss: 0.0075 | ds_loss: 0.0000 | lr: 2.5159e-06 | scale:     1.0000 | micro time: 3.782 | step time: 3.796
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1813/  2110 | global iter:   1813/  2110 | loss: 0.0380 | ds_loss: 0.0000 | lr: 2.4999e-06 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   8 | Iter:   1814/  2110 | global iter:   1814/  2110 | loss: 0.0104 | ds_loss: 0.0000 | lr: 2.4841e-06 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   8 | Iter:   1815/  2110 | global iter:   1815/  2110 | loss: 0.0028 | ds_loss: 0.0000 | lr: 2.4682e-06 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
train | epoch   8 | Iter:   1816/  2110 | global iter:   1816/  2110 | loss: 0.0083 | ds_loss: 0.0000 | lr: 2.4525e-06 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1816/  2110 | global iter:   1816/  2110 | loss: 0.0148 | ds_loss: 0.0000 | lr: 2.4525e-06 | scale:     1.0000 | micro time: 3.789 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1817/  2110 | global iter:   1817/  2110 | loss: 0.0217 | ds_loss: 0.0000 | lr: 2.4367e-06 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
train | epoch   8 | Iter:   1818/  2110 | global iter:   1818/  2110 | loss: 0.0014 | ds_loss: 0.0000 | lr: 2.4211e-06 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   8 | Iter:   1819/  2110 | global iter:   1819/  2110 | loss: 0.0175 | ds_loss: 0.0000 | lr: 2.4055e-06 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   8 | Iter:   1820/  2110 | global iter:   1820/  2110 | loss: 0.0185 | ds_loss: 0.0000 | lr: 2.3899e-06 | scale:     1.0000 | micro time: 3.814 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1820/  2110 | global iter:   1820/  2110 | loss: 0.0148 | ds_loss: 0.0000 | lr: 2.3899e-06 | scale:     1.0000 | micro time: 3.814 | step time: 3.805
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1821/  2110 | global iter:   1821/  2110 | loss: 0.0078 | ds_loss: 0.0000 | lr: 2.3744e-06 | scale:     1.0000 | micro time: 3.816 | step time: 0.000
train | epoch   8 | Iter:   1822/  2110 | global iter:   1822/  2110 | loss: 0.0071 | ds_loss: 0.0000 | lr: 2.3589e-06 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   8 | Iter:   1823/  2110 | global iter:   1823/  2110 | loss: 0.0035 | ds_loss: 0.0000 | lr: 2.3435e-06 | scale:     1.0000 | micro time: 3.784 | step time: 0.000
train | epoch   8 | Iter:   1824/  2110 | global iter:   1824/  2110 | loss: 0.0128 | ds_loss: 0.0000 | lr: 2.3281e-06 | scale:     1.0000 | micro time: 4.202 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1824/  2110 | global iter:   1824/  2110 | loss: 0.0078 | ds_loss: 0.0000 | lr: 2.3281e-06 | scale:     1.0000 | micro time: 4.202 | step time: 3.899
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1825/  2110 | global iter:   1825/  2110 | loss: 0.0045 | ds_loss: 0.0000 | lr: 2.3128e-06 | scale:     1.0000 | micro time: 3.820 | step time: 0.000
train | epoch   8 | Iter:   1826/  2110 | global iter:   1826/  2110 | loss: 0.0051 | ds_loss: 0.0000 | lr: 2.2975e-06 | scale:     1.0000 | micro time: 3.777 | step time: 0.000
train | epoch   8 | Iter:   1827/  2110 | global iter:   1827/  2110 | loss: 0.0120 | ds_loss: 0.0000 | lr: 2.2823e-06 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   8 | Iter:   1828/  2110 | global iter:   1828/  2110 | loss: 0.0103 | ds_loss: 0.0000 | lr: 2.2671e-06 | scale:     1.0000 | micro time: 3.814 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1828/  2110 | global iter:   1828/  2110 | loss: 0.0080 | ds_loss: 0.0000 | lr: 2.2671e-06 | scale:     1.0000 | micro time: 3.814 | step time: 3.805
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1829/  2110 | global iter:   1829/  2110 | loss: 0.0147 | ds_loss: 0.0000 | lr: 2.2520e-06 | scale:     1.0000 | micro time: 3.772 | step time: 0.000
train | epoch   8 | Iter:   1830/  2110 | global iter:   1830/  2110 | loss: 0.0144 | ds_loss: 0.0000 | lr: 2.2369e-06 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   8 | Iter:   1831/  2110 | global iter:   1831/  2110 | loss: 0.0082 | ds_loss: 0.0000 | lr: 2.2219e-06 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   8 | Iter:   1832/  2110 | global iter:   1832/  2110 | loss: 0.0098 | ds_loss: 0.0000 | lr: 2.2070e-06 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1832/  2110 | global iter:   1832/  2110 | loss: 0.0118 | ds_loss: 0.0000 | lr: 2.2070e-06 | scale:     1.0000 | micro time: 3.806 | step time: 3.798
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1833/  2110 | global iter:   1833/  2110 | loss: 0.0131 | ds_loss: 0.0000 | lr: 2.1920e-06 | scale:     1.0000 | micro time: 3.765 | step time: 0.000
train | epoch   8 | Iter:   1834/  2110 | global iter:   1834/  2110 | loss: 0.0017 | ds_loss: 0.0000 | lr: 2.1772e-06 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
train | epoch   8 | Iter:   1835/  2110 | global iter:   1835/  2110 | loss: 0.0081 | ds_loss: 0.0000 | lr: 2.1624e-06 | scale:     1.0000 | micro time: 3.817 | step time: 0.000
train | epoch   8 | Iter:   1836/  2110 | global iter:   1836/  2110 | loss: 0.0060 | ds_loss: 0.0000 | lr: 2.1476e-06 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1836/  2110 | global iter:   1836/  2110 | loss: 0.0072 | ds_loss: 0.0000 | lr: 2.1476e-06 | scale:     1.0000 | micro time: 3.783 | step time: 3.788
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1837/  2110 | global iter:   1837/  2110 | loss: 0.0091 | ds_loss: 0.0000 | lr: 2.1329e-06 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
train | epoch   8 | Iter:   1838/  2110 | global iter:   1838/  2110 | loss: 0.0137 | ds_loss: 0.0000 | lr: 2.1182e-06 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   8 | Iter:   1839/  2110 | global iter:   1839/  2110 | loss: 0.0090 | ds_loss: 0.0000 | lr: 2.1036e-06 | scale:     1.0000 | micro time: 3.797 | step time: 0.000
train | epoch   8 | Iter:   1840/  2110 | global iter:   1840/  2110 | loss: 0.0136 | ds_loss: 0.0000 | lr: 2.0891e-06 | scale:     1.0000 | micro time: 4.242 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1840/  2110 | global iter:   1840/  2110 | loss: 0.0114 | ds_loss: 0.0000 | lr: 2.0891e-06 | scale:     1.0000 | micro time: 4.242 | step time: 3.902
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1841/  2110 | global iter:   1841/  2110 | loss: 0.0174 | ds_loss: 0.0000 | lr: 2.0745e-06 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   8 | Iter:   1842/  2110 | global iter:   1842/  2110 | loss: 0.0142 | ds_loss: 0.0000 | lr: 2.0601e-06 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   8 | Iter:   1843/  2110 | global iter:   1843/  2110 | loss: 0.0107 | ds_loss: 0.0000 | lr: 2.0457e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   8 | Iter:   1844/  2110 | global iter:   1844/  2110 | loss: 0.0071 | ds_loss: 0.0000 | lr: 2.0313e-06 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1844/  2110 | global iter:   1844/  2110 | loss: 0.0124 | ds_loss: 0.0000 | lr: 2.0313e-06 | scale:     1.0000 | micro time: 3.786 | step time: 3.790
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1845/  2110 | global iter:   1845/  2110 | loss: 0.0128 | ds_loss: 0.0000 | lr: 2.0170e-06 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   8 | Iter:   1846/  2110 | global iter:   1846/  2110 | loss: 0.0148 | ds_loss: 0.0000 | lr: 2.0028e-06 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   8 | Iter:   1847/  2110 | global iter:   1847/  2110 | loss: 0.0031 | ds_loss: 0.0000 | lr: 1.9886e-06 | scale:     1.0000 | micro time: 3.817 | step time: 0.000
train | epoch   8 | Iter:   1848/  2110 | global iter:   1848/  2110 | loss: 0.0067 | ds_loss: 0.0000 | lr: 1.9744e-06 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1848/  2110 | global iter:   1848/  2110 | loss: 0.0093 | ds_loss: 0.0000 | lr: 1.9744e-06 | scale:     1.0000 | micro time: 3.775 | step time: 3.787
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1849/  2110 | global iter:   1849/  2110 | loss: 0.0110 | ds_loss: 0.0000 | lr: 1.9603e-06 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   8 | Iter:   1850/  2110 | global iter:   1850/  2110 | loss: 0.0035 | ds_loss: 0.0000 | lr: 1.9463e-06 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   8 | Iter:   1851/  2110 | global iter:   1851/  2110 | loss: 0.0036 | ds_loss: 0.0000 | lr: 1.9323e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   8 | Iter:   1852/  2110 | global iter:   1852/  2110 | loss: 0.0024 | ds_loss: 0.0000 | lr: 1.9183e-06 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1852/  2110 | global iter:   1852/  2110 | loss: 0.0051 | ds_loss: 0.0000 | lr: 1.9183e-06 | scale:     1.0000 | micro time: 3.798 | step time: 3.793
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1853/  2110 | global iter:   1853/  2110 | loss: 0.0128 | ds_loss: 0.0000 | lr: 1.9044e-06 | scale:     1.0000 | micro time: 3.796 | step time: 0.000
train | epoch   8 | Iter:   1854/  2110 | global iter:   1854/  2110 | loss: 0.0027 | ds_loss: 0.0000 | lr: 1.8906e-06 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   8 | Iter:   1855/  2110 | global iter:   1855/  2110 | loss: 0.0044 | ds_loss: 0.0000 | lr: 1.8768e-06 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   8 | Iter:   1856/  2110 | global iter:   1856/  2110 | loss: 0.0059 | ds_loss: 0.0000 | lr: 1.8630e-06 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1856/  2110 | global iter:   1856/  2110 | loss: 0.0064 | ds_loss: 0.0000 | lr: 1.8630e-06 | scale:     1.0000 | micro time: 3.787 | step time: 3.789
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1857/  2110 | global iter:   1857/  2110 | loss: 0.0043 | ds_loss: 0.0000 | lr: 1.8493e-06 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   8 | Iter:   1858/  2110 | global iter:   1858/  2110 | loss: 0.0055 | ds_loss: 0.0000 | lr: 1.8357e-06 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   8 | Iter:   1859/  2110 | global iter:   1859/  2110 | loss: 0.0040 | ds_loss: 0.0000 | lr: 1.8221e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   8 | Iter:   1860/  2110 | global iter:   1860/  2110 | loss: 0.0043 | ds_loss: 0.0000 | lr: 1.8086e-06 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1860/  2110 | global iter:   1860/  2110 | loss: 0.0045 | ds_loss: 0.0000 | lr: 1.8086e-06 | scale:     1.0000 | micro time: 3.794 | step time: 3.797
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1861/  2110 | global iter:   1861/  2110 | loss: 0.0191 | ds_loss: 0.0000 | lr: 1.7951e-06 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
train | epoch   8 | Iter:   1862/  2110 | global iter:   1862/  2110 | loss: 0.0045 | ds_loss: 0.0000 | lr: 1.7817e-06 | scale:     1.0000 | micro time: 3.774 | step time: 0.000
train | epoch   8 | Iter:   1863/  2110 | global iter:   1863/  2110 | loss: 0.0141 | ds_loss: 0.0000 | lr: 1.7683e-06 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   8 | Iter:   1864/  2110 | global iter:   1864/  2110 | loss: 0.0055 | ds_loss: 0.0000 | lr: 1.7550e-06 | scale:     1.0000 | micro time: 3.812 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1864/  2110 | global iter:   1864/  2110 | loss: 0.0108 | ds_loss: 0.0000 | lr: 1.7550e-06 | scale:     1.0000 | micro time: 3.812 | step time: 3.790
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1865/  2110 | global iter:   1865/  2110 | loss: 0.0035 | ds_loss: 0.0000 | lr: 1.7417e-06 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   8 | Iter:   1866/  2110 | global iter:   1866/  2110 | loss: 0.0097 | ds_loss: 0.0000 | lr: 1.7284e-06 | scale:     1.0000 | micro time: 3.762 | step time: 0.000
train | epoch   8 | Iter:   1867/  2110 | global iter:   1867/  2110 | loss: 0.0020 | ds_loss: 0.0000 | lr: 1.7153e-06 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
train | epoch   8 | Iter:   1868/  2110 | global iter:   1868/  2110 | loss: 0.0029 | ds_loss: 0.0000 | lr: 1.7021e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1868/  2110 | global iter:   1868/  2110 | loss: 0.0045 | ds_loss: 0.0000 | lr: 1.7021e-06 | scale:     1.0000 | micro time: 3.802 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1869/  2110 | global iter:   1869/  2110 | loss: 0.0076 | ds_loss: 0.0000 | lr: 1.6891e-06 | scale:     1.0000 | micro time: 3.792 | step time: 0.000
train | epoch   8 | Iter:   1870/  2110 | global iter:   1870/  2110 | loss: 0.0170 | ds_loss: 0.0000 | lr: 1.6761e-06 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   8 | Iter:   1871/  2110 | global iter:   1871/  2110 | loss: 0.0052 | ds_loss: 0.0000 | lr: 1.6631e-06 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   8 | Iter:   1872/  2110 | global iter:   1872/  2110 | loss: 0.0073 | ds_loss: 0.0000 | lr: 1.6502e-06 | scale:     1.0000 | micro time: 3.777 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1872/  2110 | global iter:   1872/  2110 | loss: 0.0092 | ds_loss: 0.0000 | lr: 1.6502e-06 | scale:     1.0000 | micro time: 3.777 | step time: 3.787
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1873/  2110 | global iter:   1873/  2110 | loss: 0.0019 | ds_loss: 0.0000 | lr: 1.6373e-06 | scale:     1.0000 | micro time: 3.784 | step time: 0.000
train | epoch   8 | Iter:   1874/  2110 | global iter:   1874/  2110 | loss: 0.0123 | ds_loss: 0.0000 | lr: 1.6245e-06 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
train | epoch   8 | Iter:   1875/  2110 | global iter:   1875/  2110 | loss: 0.0041 | ds_loss: 0.0000 | lr: 1.6117e-06 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   8 | Iter:   1876/  2110 | global iter:   1876/  2110 | loss: 0.0093 | ds_loss: 0.0000 | lr: 1.5990e-06 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1876/  2110 | global iter:   1876/  2110 | loss: 0.0069 | ds_loss: 0.0000 | lr: 1.5990e-06 | scale:     1.0000 | micro time: 3.786 | step time: 3.797
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1877/  2110 | global iter:   1877/  2110 | loss: 0.0095 | ds_loss: 0.0000 | lr: 1.5864e-06 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   8 | Iter:   1878/  2110 | global iter:   1878/  2110 | loss: 0.0068 | ds_loss: 0.0000 | lr: 1.5738e-06 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   8 | Iter:   1879/  2110 | global iter:   1879/  2110 | loss: 0.0069 | ds_loss: 0.0000 | lr: 1.5612e-06 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   8 | Iter:   1880/  2110 | global iter:   1880/  2110 | loss: 0.0055 | ds_loss: 0.0000 | lr: 1.5487e-06 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1880/  2110 | global iter:   1880/  2110 | loss: 0.0072 | ds_loss: 0.0000 | lr: 1.5487e-06 | scale:     1.0000 | micro time: 3.788 | step time: 3.789
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1881/  2110 | global iter:   1881/  2110 | loss: 0.0145 | ds_loss: 0.0000 | lr: 1.5363e-06 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
train | epoch   8 | Iter:   1882/  2110 | global iter:   1882/  2110 | loss: 0.0046 | ds_loss: 0.0000 | lr: 1.5239e-06 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   8 | Iter:   1883/  2110 | global iter:   1883/  2110 | loss: 0.0129 | ds_loss: 0.0000 | lr: 1.5115e-06 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   8 | Iter:   1884/  2110 | global iter:   1884/  2110 | loss: 0.0131 | ds_loss: 0.0000 | lr: 1.4992e-06 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1884/  2110 | global iter:   1884/  2110 | loss: 0.0113 | ds_loss: 0.0000 | lr: 1.4992e-06 | scale:     1.0000 | micro time: 3.803 | step time: 3.795
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1885/  2110 | global iter:   1885/  2110 | loss: 0.0184 | ds_loss: 0.0000 | lr: 1.4870e-06 | scale:     1.0000 | micro time: 3.792 | step time: 0.000
train | epoch   8 | Iter:   1886/  2110 | global iter:   1886/  2110 | loss: 0.0088 | ds_loss: 0.0000 | lr: 1.4748e-06 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   8 | Iter:   1887/  2110 | global iter:   1887/  2110 | loss: 0.0148 | ds_loss: 0.0000 | lr: 1.4627e-06 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   8 | Iter:   1888/  2110 | global iter:   1888/  2110 | loss: 0.0061 | ds_loss: 0.0000 | lr: 1.4506e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1888/  2110 | global iter:   1888/  2110 | loss: 0.0120 | ds_loss: 0.0000 | lr: 1.4506e-06 | scale:     1.0000 | micro time: 3.802 | step time: 3.789
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1889/  2110 | global iter:   1889/  2110 | loss: 0.0093 | ds_loss: 0.0000 | lr: 1.4386e-06 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   8 | Iter:   1890/  2110 | global iter:   1890/  2110 | loss: 0.0102 | ds_loss: 0.0000 | lr: 1.4266e-06 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   8 | Iter:   1891/  2110 | global iter:   1891/  2110 | loss: 0.0188 | ds_loss: 0.0000 | lr: 1.4147e-06 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   8 | Iter:   1892/  2110 | global iter:   1892/  2110 | loss: 0.0069 | ds_loss: 0.0000 | lr: 1.4028e-06 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1892/  2110 | global iter:   1892/  2110 | loss: 0.0113 | ds_loss: 0.0000 | lr: 1.4028e-06 | scale:     1.0000 | micro time: 3.782 | step time: 3.787
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1893/  2110 | global iter:   1893/  2110 | loss: 0.0041 | ds_loss: 0.0000 | lr: 1.3910e-06 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   8 | Iter:   1894/  2110 | global iter:   1894/  2110 | loss: 0.0137 | ds_loss: 0.0000 | lr: 1.3792e-06 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
train | epoch   8 | Iter:   1895/  2110 | global iter:   1895/  2110 | loss: 0.0020 | ds_loss: 0.0000 | lr: 1.3675e-06 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   8 | Iter:   1896/  2110 | global iter:   1896/  2110 | loss: 0.0045 | ds_loss: 0.0000 | lr: 1.3558e-06 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1896/  2110 | global iter:   1896/  2110 | loss: 0.0061 | ds_loss: 0.0000 | lr: 1.3558e-06 | scale:     1.0000 | micro time: 3.775 | step time: 3.797
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1897/  2110 | global iter:   1897/  2110 | loss: 0.0089 | ds_loss: 0.0000 | lr: 1.3442e-06 | scale:     1.0000 | micro time: 3.758 | step time: 0.000
train | epoch   8 | Iter:   1898/  2110 | global iter:   1898/  2110 | loss: 0.0046 | ds_loss: 0.0000 | lr: 1.3326e-06 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   8 | Iter:   1899/  2110 | global iter:   1899/  2110 | loss: 0.0132 | ds_loss: 0.0000 | lr: 1.3211e-06 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   8 | Iter:   1900/  2110 | global iter:   1900/  2110 | loss: 0.0161 | ds_loss: 0.0000 | lr: 1.3097e-06 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1900/  2110 | global iter:   1900/  2110 | loss: 0.0107 | ds_loss: 0.0000 | lr: 1.3097e-06 | scale:     1.0000 | micro time: 3.791 | step time: 3.780
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1901/  2110 | global iter:   1901/  2110 | loss: 0.0042 | ds_loss: 0.0000 | lr: 1.2983e-06 | scale:     1.0000 | micro time: 3.760 | step time: 0.000
train | epoch   8 | Iter:   1902/  2110 | global iter:   1902/  2110 | loss: 0.0072 | ds_loss: 0.0000 | lr: 1.2869e-06 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   8 | Iter:   1903/  2110 | global iter:   1903/  2110 | loss: 0.0062 | ds_loss: 0.0000 | lr: 1.2756e-06 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   8 | Iter:   1904/  2110 | global iter:   1904/  2110 | loss: 0.0125 | ds_loss: 0.0000 | lr: 1.2644e-06 | scale:     1.0000 | micro time: 3.776 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1904/  2110 | global iter:   1904/  2110 | loss: 0.0075 | ds_loss: 0.0000 | lr: 1.2644e-06 | scale:     1.0000 | micro time: 3.776 | step time: 3.774
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   8 | Iter:   1905/  2110 | global iter:   1905/  2110 | loss: 0.0170 | ds_loss: 0.0000 | lr: 1.2532e-06 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
train | epoch   8 | Iter:   1906/  2110 | global iter:   1906/  2110 | loss: 0.0046 | ds_loss: 0.0000 | lr: 1.2421e-06 | scale:     1.0000 | micro time: 3.796 | step time: 0.000
train | epoch   8 | Iter:   1907/  2110 | global iter:   1907/  2110 | loss: 0.0031 | ds_loss: 0.0000 | lr: 1.2310e-06 | scale:     1.0000 | micro time: 3.817 | step time: 0.000
train | epoch   8 | Iter:   1908/  2110 | global iter:   1908/  2110 | loss: 0.0113 | ds_loss: 0.0000 | lr: 1.2200e-06 | scale:     1.0000 | micro time: 3.332 | step time: 0.000
****************************************************************************************************
train | epoch   8 | Iter:   1908/  2110 | global iter:   1908/  2110 | loss: 0.0090 | ds_loss: 0.0000 | lr: 1.2200e-06 | scale:     1.0000 | micro time: 3.332 | step time: 3.683
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
Tue Apr  8 09:06:16 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   63C    P0             55W /  250W |   29814MiB /  32768MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   50C    P0             49W /  250W |   29848MiB /  32768MiB |     83%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   54C    P0             49W /  250W |   28312MiB /  32768MiB |     54%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 09:06:16 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   63C    P0             55W /  250W |   29814MiB /  32768MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   50C    P0             49W /  250W |   29848MiB /  32768MiB |     83%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   54C    P0             49W /  250W |   28312MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 09:06:16 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   63C    P0             55W /  250W |   29814MiB /  32768MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |     25%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   50C    P0             49W /  250W |   29848MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   54C    P0             49W /  250W |   28312MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

Tue Apr  8 09:06:16 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:02:00.0 Off |                    0 |
| N/A   63C    P0             55W /  250W |   29814MiB /  32768MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-PCIE-32GB           Off |   00000000:03:00.0 Off |                    0 |
| N/A   52C    P0             50W /  250W |   29848MiB /  32768MiB |     25%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |
| N/A   50C    P0             49W /  250W |   29848MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-PCIE-32GB           Off |   00000000:82:00.0 Off |                    0 |
| N/A   54C    P0             49W /  250W |   28312MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    121792      C   ...project_distillLLM/venv/bin/python3      29810MiB |
|    1   N/A  N/A    121793      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    2   N/A  N/A    121794      C   ...project_distillLLM/venv/bin/python3      29844MiB |
|    3   N/A  N/A    121795      C   ...project_distillLLM/venv/bin/python3      28308MiB |
+-----------------------------------------------------------------------------------------+

train | epoch   9 | Iter:   1909/  2110 | global iter:   1909/  2110 | loss: 0.0097 | ds_loss: 0.0000 | lr: 1.2090e-06 | scale:     1.0000 | micro time: 4.015 | step time: 0.000
train | epoch   9 | Iter:   1910/  2110 | global iter:   1910/  2110 | loss: 0.0059 | ds_loss: 0.0000 | lr: 1.1981e-06 | scale:     1.0000 | micro time: 3.777 | step time: 0.000
train | epoch   9 | Iter:   1911/  2110 | global iter:   1911/  2110 | loss: 0.0238 | ds_loss: 0.0000 | lr: 1.1872e-06 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   9 | Iter:   1912/  2110 | global iter:   1912/  2110 | loss: 0.0089 | ds_loss: 0.0000 | lr: 1.1764e-06 | scale:     1.0000 | micro time: 3.807 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   1912/  2110 | global iter:   1912/  2110 | loss: 0.0121 | ds_loss: 0.0000 | lr: 1.1764e-06 | scale:     1.0000 | micro time: 3.807 | step time: 3.846
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   1913/  2110 | global iter:   1913/  2110 | loss: 0.0066 | ds_loss: 0.0000 | lr: 1.1656e-06 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   9 | Iter:   1914/  2110 | global iter:   1914/  2110 | loss: 0.0057 | ds_loss: 0.0000 | lr: 1.1549e-06 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   9 | Iter:   1915/  2110 | global iter:   1915/  2110 | loss: 0.0026 | ds_loss: 0.0000 | lr: 1.1442e-06 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   9 | Iter:   1916/  2110 | global iter:   1916/  2110 | loss: 0.0056 | ds_loss: 0.0000 | lr: 1.1336e-06 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   1916/  2110 | global iter:   1916/  2110 | loss: 0.0051 | ds_loss: 0.0000 | lr: 1.1336e-06 | scale:     1.0000 | micro time: 3.786 | step time: 3.786
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   1917/  2110 | global iter:   1917/  2110 | loss: 0.0046 | ds_loss: 0.0000 | lr: 1.1231e-06 | scale:     1.0000 | micro time: 3.769 | step time: 0.000
train | epoch   9 | Iter:   1918/  2110 | global iter:   1918/  2110 | loss: 0.0114 | ds_loss: 0.0000 | lr: 1.1126e-06 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   9 | Iter:   1919/  2110 | global iter:   1919/  2110 | loss: 0.0054 | ds_loss: 0.0000 | lr: 1.1021e-06 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   9 | Iter:   1920/  2110 | global iter:   1920/  2110 | loss: 0.0068 | ds_loss: 0.0000 | lr: 1.0917e-06 | scale:     1.0000 | micro time: 3.760 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   1920/  2110 | global iter:   1920/  2110 | loss: 0.0070 | ds_loss: 0.0000 | lr: 1.0917e-06 | scale:     1.0000 | micro time: 3.760 | step time: 3.775
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   1921/  2110 | global iter:   1921/  2110 | loss: 0.0050 | ds_loss: 0.0000 | lr: 1.0814e-06 | scale:     1.0000 | micro time: 3.768 | step time: 0.000
train | epoch   9 | Iter:   1922/  2110 | global iter:   1922/  2110 | loss: 0.0133 | ds_loss: 0.0000 | lr: 1.0711e-06 | scale:     1.0000 | micro time: 3.797 | step time: 0.000
train | epoch   9 | Iter:   1923/  2110 | global iter:   1923/  2110 | loss: 0.0035 | ds_loss: 0.0000 | lr: 1.0608e-06 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   9 | Iter:   1924/  2110 | global iter:   1924/  2110 | loss: 0.0034 | ds_loss: 0.0000 | lr: 1.0507e-06 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   1924/  2110 | global iter:   1924/  2110 | loss: 0.0063 | ds_loss: 0.0000 | lr: 1.0507e-06 | scale:     1.0000 | micro time: 3.798 | step time: 3.790
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   1925/  2110 | global iter:   1925/  2110 | loss: 0.0026 | ds_loss: 0.0000 | lr: 1.0405e-06 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   9 | Iter:   1926/  2110 | global iter:   1926/  2110 | loss: 0.0042 | ds_loss: 0.0000 | lr: 1.0305e-06 | scale:     1.0000 | micro time: 3.773 | step time: 0.000
train | epoch   9 | Iter:   1927/  2110 | global iter:   1927/  2110 | loss: 0.0019 | ds_loss: 0.0000 | lr: 1.0204e-06 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   9 | Iter:   1928/  2110 | global iter:   1928/  2110 | loss: 0.0079 | ds_loss: 0.0000 | lr: 1.0105e-06 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   1928/  2110 | global iter:   1928/  2110 | loss: 0.0042 | ds_loss: 0.0000 | lr: 1.0105e-06 | scale:     1.0000 | micro time: 3.795 | step time: 3.789
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   1929/  2110 | global iter:   1929/  2110 | loss: 0.0288 | ds_loss: 0.0000 | lr: 1.0005e-06 | scale:     1.0000 | micro time: 3.821 | step time: 0.000
train | epoch   9 | Iter:   1930/  2110 | global iter:   1930/  2110 | loss: 0.0060 | ds_loss: 0.0000 | lr: 9.9068e-07 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   9 | Iter:   1931/  2110 | global iter:   1931/  2110 | loss: 0.0107 | ds_loss: 0.0000 | lr: 9.8086e-07 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   9 | Iter:   1932/  2110 | global iter:   1932/  2110 | loss: 0.0032 | ds_loss: 0.0000 | lr: 9.7111e-07 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   1932/  2110 | global iter:   1932/  2110 | loss: 0.0122 | ds_loss: 0.0000 | lr: 9.7111e-07 | scale:     1.0000 | micro time: 3.802 | step time: 3.804
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   1933/  2110 | global iter:   1933/  2110 | loss: 0.0089 | ds_loss: 0.0000 | lr: 9.6140e-07 | scale:     1.0000 | micro time: 3.766 | step time: 0.000
train | epoch   9 | Iter:   1934/  2110 | global iter:   1934/  2110 | loss: 0.0089 | ds_loss: 0.0000 | lr: 9.5175e-07 | scale:     1.0000 | micro time: 3.800 | step time: 0.000
train | epoch   9 | Iter:   1935/  2110 | global iter:   1935/  2110 | loss: 0.0061 | ds_loss: 0.0000 | lr: 9.4216e-07 | scale:     1.0000 | micro time: 3.835 | step time: 0.000
train | epoch   9 | Iter:   1936/  2110 | global iter:   1936/  2110 | loss: 0.0064 | ds_loss: 0.0000 | lr: 9.3261e-07 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   1936/  2110 | global iter:   1936/  2110 | loss: 0.0076 | ds_loss: 0.0000 | lr: 9.3261e-07 | scale:     1.0000 | micro time: 3.778 | step time: 3.795
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   1937/  2110 | global iter:   1937/  2110 | loss: 0.0039 | ds_loss: 0.0000 | lr: 9.2312e-07 | scale:     1.0000 | micro time: 3.819 | step time: 0.000
train | epoch   9 | Iter:   1938/  2110 | global iter:   1938/  2110 | loss: 0.0046 | ds_loss: 0.0000 | lr: 9.1369e-07 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   9 | Iter:   1939/  2110 | global iter:   1939/  2110 | loss: 0.0085 | ds_loss: 0.0000 | lr: 9.0430e-07 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   9 | Iter:   1940/  2110 | global iter:   1940/  2110 | loss: 0.0164 | ds_loss: 0.0000 | lr: 8.9497e-07 | scale:     1.0000 | micro time: 3.807 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   1940/  2110 | global iter:   1940/  2110 | loss: 0.0084 | ds_loss: 0.0000 | lr: 8.9497e-07 | scale:     1.0000 | micro time: 3.807 | step time: 3.804
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   1941/  2110 | global iter:   1941/  2110 | loss: 0.0061 | ds_loss: 0.0000 | lr: 8.8570e-07 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   9 | Iter:   1942/  2110 | global iter:   1942/  2110 | loss: 0.0074 | ds_loss: 0.0000 | lr: 8.7648e-07 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   9 | Iter:   1943/  2110 | global iter:   1943/  2110 | loss: 0.0089 | ds_loss: 0.0000 | lr: 8.6731e-07 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   9 | Iter:   1944/  2110 | global iter:   1944/  2110 | loss: 0.0085 | ds_loss: 0.0000 | lr: 8.5819e-07 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   1944/  2110 | global iter:   1944/  2110 | loss: 0.0077 | ds_loss: 0.0000 | lr: 8.5819e-07 | scale:     1.0000 | micro time: 3.818 | step time: 3.796
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   1945/  2110 | global iter:   1945/  2110 | loss: 0.0098 | ds_loss: 0.0000 | lr: 8.4913e-07 | scale:     1.0000 | micro time: 3.777 | step time: 0.000
train | epoch   9 | Iter:   1946/  2110 | global iter:   1946/  2110 | loss: 0.0043 | ds_loss: 0.0000 | lr: 8.4012e-07 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   9 | Iter:   1947/  2110 | global iter:   1947/  2110 | loss: 0.0097 | ds_loss: 0.0000 | lr: 8.3117e-07 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   9 | Iter:   1948/  2110 | global iter:   1948/  2110 | loss: 0.0080 | ds_loss: 0.0000 | lr: 8.2227e-07 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   1948/  2110 | global iter:   1948/  2110 | loss: 0.0079 | ds_loss: 0.0000 | lr: 8.2227e-07 | scale:     1.0000 | micro time: 3.795 | step time: 3.790
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   1949/  2110 | global iter:   1949/  2110 | loss: 0.0118 | ds_loss: 0.0000 | lr: 8.1342e-07 | scale:     1.0000 | micro time: 3.773 | step time: 0.000
train | epoch   9 | Iter:   1950/  2110 | global iter:   1950/  2110 | loss: 0.0051 | ds_loss: 0.0000 | lr: 8.0463e-07 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   9 | Iter:   1951/  2110 | global iter:   1951/  2110 | loss: 0.0079 | ds_loss: 0.0000 | lr: 7.9589e-07 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   9 | Iter:   1952/  2110 | global iter:   1952/  2110 | loss: 0.0059 | ds_loss: 0.0000 | lr: 7.8720e-07 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   1952/  2110 | global iter:   1952/  2110 | loss: 0.0077 | ds_loss: 0.0000 | lr: 7.8720e-07 | scale:     1.0000 | micro time: 3.788 | step time: 3.786
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   1953/  2110 | global iter:   1953/  2110 | loss: 0.0120 | ds_loss: 0.0000 | lr: 7.7857e-07 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   9 | Iter:   1954/  2110 | global iter:   1954/  2110 | loss: 0.0132 | ds_loss: 0.0000 | lr: 7.6999e-07 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   9 | Iter:   1955/  2110 | global iter:   1955/  2110 | loss: 0.0096 | ds_loss: 0.0000 | lr: 7.6147e-07 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   9 | Iter:   1956/  2110 | global iter:   1956/  2110 | loss: 0.0067 | ds_loss: 0.0000 | lr: 7.5300e-07 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   1956/  2110 | global iter:   1956/  2110 | loss: 0.0104 | ds_loss: 0.0000 | lr: 7.5300e-07 | scale:     1.0000 | micro time: 3.798 | step time: 3.789
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   1957/  2110 | global iter:   1957/  2110 | loss: 0.0087 | ds_loss: 0.0000 | lr: 7.4458e-07 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   9 | Iter:   1958/  2110 | global iter:   1958/  2110 | loss: 0.0086 | ds_loss: 0.0000 | lr: 7.3622e-07 | scale:     1.0000 | micro time: 3.797 | step time: 0.000
train | epoch   9 | Iter:   1959/  2110 | global iter:   1959/  2110 | loss: 0.0014 | ds_loss: 0.0000 | lr: 7.2791e-07 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   9 | Iter:   1960/  2110 | global iter:   1960/  2110 | loss: 0.0109 | ds_loss: 0.0000 | lr: 7.1966e-07 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   1960/  2110 | global iter:   1960/  2110 | loss: 0.0074 | ds_loss: 0.0000 | lr: 7.1966e-07 | scale:     1.0000 | micro time: 3.794 | step time: 3.792
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   1961/  2110 | global iter:   1961/  2110 | loss: 0.0060 | ds_loss: 0.0000 | lr: 7.1146e-07 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
train | epoch   9 | Iter:   1962/  2110 | global iter:   1962/  2110 | loss: 0.0024 | ds_loss: 0.0000 | lr: 7.0331e-07 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   9 | Iter:   1963/  2110 | global iter:   1963/  2110 | loss: 0.0066 | ds_loss: 0.0000 | lr: 6.9522e-07 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   9 | Iter:   1964/  2110 | global iter:   1964/  2110 | loss: 0.0048 | ds_loss: 0.0000 | lr: 6.8718e-07 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   1964/  2110 | global iter:   1964/  2110 | loss: 0.0049 | ds_loss: 0.0000 | lr: 6.8718e-07 | scale:     1.0000 | micro time: 3.790 | step time: 3.788
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   1965/  2110 | global iter:   1965/  2110 | loss: 0.0055 | ds_loss: 0.0000 | lr: 6.7919e-07 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   9 | Iter:   1966/  2110 | global iter:   1966/  2110 | loss: 0.0033 | ds_loss: 0.0000 | lr: 6.7126e-07 | scale:     1.0000 | micro time: 3.796 | step time: 0.000
train | epoch   9 | Iter:   1967/  2110 | global iter:   1967/  2110 | loss: 0.0023 | ds_loss: 0.0000 | lr: 6.6339e-07 | scale:     1.0000 | micro time: 3.795 | step time: 0.000
train | epoch   9 | Iter:   1968/  2110 | global iter:   1968/  2110 | loss: 0.0094 | ds_loss: 0.0000 | lr: 6.5556e-07 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   1968/  2110 | global iter:   1968/  2110 | loss: 0.0051 | ds_loss: 0.0000 | lr: 6.5556e-07 | scale:     1.0000 | micro time: 3.779 | step time: 3.789
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   1969/  2110 | global iter:   1969/  2110 | loss: 0.0060 | ds_loss: 0.0000 | lr: 6.4779e-07 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   9 | Iter:   1970/  2110 | global iter:   1970/  2110 | loss: 0.0214 | ds_loss: 0.0000 | lr: 6.4008e-07 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   9 | Iter:   1971/  2110 | global iter:   1971/  2110 | loss: 0.0071 | ds_loss: 0.0000 | lr: 6.3242e-07 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   9 | Iter:   1972/  2110 | global iter:   1972/  2110 | loss: 0.0059 | ds_loss: 0.0000 | lr: 6.2481e-07 | scale:     1.0000 | micro time: 3.809 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   1972/  2110 | global iter:   1972/  2110 | loss: 0.0101 | ds_loss: 0.0000 | lr: 6.2481e-07 | scale:     1.0000 | micro time: 3.809 | step time: 3.788
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   1973/  2110 | global iter:   1973/  2110 | loss: 0.0082 | ds_loss: 0.0000 | lr: 6.1726e-07 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
train | epoch   9 | Iter:   1974/  2110 | global iter:   1974/  2110 | loss: 0.0020 | ds_loss: 0.0000 | lr: 6.0976e-07 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   9 | Iter:   1975/  2110 | global iter:   1975/  2110 | loss: 0.0078 | ds_loss: 0.0000 | lr: 6.0232e-07 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   9 | Iter:   1976/  2110 | global iter:   1976/  2110 | loss: 0.0061 | ds_loss: 0.0000 | lr: 5.9493e-07 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   1976/  2110 | global iter:   1976/  2110 | loss: 0.0060 | ds_loss: 0.0000 | lr: 5.9493e-07 | scale:     1.0000 | micro time: 3.782 | step time: 3.780
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   1977/  2110 | global iter:   1977/  2110 | loss: 0.0092 | ds_loss: 0.0000 | lr: 5.8759e-07 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   9 | Iter:   1978/  2110 | global iter:   1978/  2110 | loss: 0.0102 | ds_loss: 0.0000 | lr: 5.8031e-07 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   9 | Iter:   1979/  2110 | global iter:   1979/  2110 | loss: 0.0058 | ds_loss: 0.0000 | lr: 5.7309e-07 | scale:     1.0000 | micro time: 3.755 | step time: 0.000
train | epoch   9 | Iter:   1980/  2110 | global iter:   1980/  2110 | loss: 0.0060 | ds_loss: 0.0000 | lr: 5.6591e-07 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   1980/  2110 | global iter:   1980/  2110 | loss: 0.0078 | ds_loss: 0.0000 | lr: 5.6591e-07 | scale:     1.0000 | micro time: 3.794 | step time: 3.788
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   1981/  2110 | global iter:   1981/  2110 | loss: 0.0028 | ds_loss: 0.0000 | lr: 5.5880e-07 | scale:     1.0000 | micro time: 3.800 | step time: 0.000
train | epoch   9 | Iter:   1982/  2110 | global iter:   1982/  2110 | loss: 0.0030 | ds_loss: 0.0000 | lr: 5.5173e-07 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   9 | Iter:   1983/  2110 | global iter:   1983/  2110 | loss: 0.0074 | ds_loss: 0.0000 | lr: 5.4472e-07 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   9 | Iter:   1984/  2110 | global iter:   1984/  2110 | loss: 0.0027 | ds_loss: 0.0000 | lr: 5.3777e-07 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   1984/  2110 | global iter:   1984/  2110 | loss: 0.0040 | ds_loss: 0.0000 | lr: 5.3777e-07 | scale:     1.0000 | micro time: 3.790 | step time: 3.790
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   1985/  2110 | global iter:   1985/  2110 | loss: 0.0062 | ds_loss: 0.0000 | lr: 5.3087e-07 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   9 | Iter:   1986/  2110 | global iter:   1986/  2110 | loss: 0.0137 | ds_loss: 0.0000 | lr: 5.2402e-07 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   9 | Iter:   1987/  2110 | global iter:   1987/  2110 | loss: 0.0049 | ds_loss: 0.0000 | lr: 5.1723e-07 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   9 | Iter:   1988/  2110 | global iter:   1988/  2110 | loss: 0.0068 | ds_loss: 0.0000 | lr: 5.1049e-07 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   1988/  2110 | global iter:   1988/  2110 | loss: 0.0079 | ds_loss: 0.0000 | lr: 5.1049e-07 | scale:     1.0000 | micro time: 3.786 | step time: 3.784
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   1989/  2110 | global iter:   1989/  2110 | loss: 0.0116 | ds_loss: 0.0000 | lr: 5.0380e-07 | scale:     1.0000 | micro time: 3.773 | step time: 0.000
train | epoch   9 | Iter:   1990/  2110 | global iter:   1990/  2110 | loss: 0.0129 | ds_loss: 0.0000 | lr: 4.9718e-07 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   9 | Iter:   1991/  2110 | global iter:   1991/  2110 | loss: 0.0068 | ds_loss: 0.0000 | lr: 4.9060e-07 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   9 | Iter:   1992/  2110 | global iter:   1992/  2110 | loss: 0.0061 | ds_loss: 0.0000 | lr: 4.8408e-07 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   1992/  2110 | global iter:   1992/  2110 | loss: 0.0093 | ds_loss: 0.0000 | lr: 4.8408e-07 | scale:     1.0000 | micro time: 3.786 | step time: 3.784
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   1993/  2110 | global iter:   1993/  2110 | loss: 0.0034 | ds_loss: 0.0000 | lr: 4.7761e-07 | scale:     1.0000 | micro time: 3.784 | step time: 0.000
train | epoch   9 | Iter:   1994/  2110 | global iter:   1994/  2110 | loss: 0.0037 | ds_loss: 0.0000 | lr: 4.7120e-07 | scale:     1.0000 | micro time: 3.785 | step time: 0.000
train | epoch   9 | Iter:   1995/  2110 | global iter:   1995/  2110 | loss: 0.0134 | ds_loss: 0.0000 | lr: 4.6485e-07 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   9 | Iter:   1996/  2110 | global iter:   1996/  2110 | loss: 0.0083 | ds_loss: 0.0000 | lr: 4.5854e-07 | scale:     1.0000 | micro time: 3.807 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   1996/  2110 | global iter:   1996/  2110 | loss: 0.0072 | ds_loss: 0.0000 | lr: 4.5854e-07 | scale:     1.0000 | micro time: 3.807 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   1997/  2110 | global iter:   1997/  2110 | loss: 0.0037 | ds_loss: 0.0000 | lr: 4.5230e-07 | scale:     1.0000 | micro time: 3.770 | step time: 0.000
train | epoch   9 | Iter:   1998/  2110 | global iter:   1998/  2110 | loss: 0.0078 | ds_loss: 0.0000 | lr: 4.4610e-07 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   9 | Iter:   1999/  2110 | global iter:   1999/  2110 | loss: 0.0069 | ds_loss: 0.0000 | lr: 4.3996e-07 | scale:     1.0000 | micro time: 3.772 | step time: 0.000
train | epoch   9 | Iter:   2000/  2110 | global iter:   2000/  2110 | loss: 0.0128 | ds_loss: 0.0000 | lr: 4.3388e-07 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   2000/  2110 | global iter:   2000/  2110 | loss: 0.0078 | ds_loss: 0.0000 | lr: 4.3388e-07 | scale:     1.0000 | micro time: 3.793 | step time: 3.779
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
Model save to ./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1/2000
dp size 4
0/63
1/63
2/63
3/63
4/63
5/63
6/63
7/63
8/63
9/63
10/63
11/63
12/63
13/63
14/63
15/63
Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]Evaluating:   2%|         | 1/63 [00:21<21:43, 21.02s/it]Evaluating:   3%|         | 2/63 [00:44<22:55, 22.54s/it]Evaluating:   5%|         | 3/63 [01:05<21:48, 21.81s/it]Evaluating:   6%|         | 4/63 [01:24<20:20, 20.69s/it]Evaluating:   8%|         | 5/63 [01:46<20:36, 21.32s/it]Evaluating:  10%|         | 6/63 [02:11<21:12, 22.32s/it]Evaluating:  11%|         | 7/63 [02:39<22:45, 24.39s/it]Evaluating:  13%|        | 8/63 [03:09<23:49, 26.00s/it]Evaluating:  14%|        | 9/63 [03:24<20:27, 22.73s/it]Evaluating:  16%|        | 10/63 [03:45<19:35, 22.17s/it]Evaluating:  17%|        | 11/63 [04:07<19:07, 22.08s/it]Evaluating:  19%|        | 12/63 [04:30<18:53, 22.23s/it]Evaluating:  21%|        | 13/63 [04:52<18:27, 22.14s/it]Evaluating:  22%|       | 14/63 [05:14<18:04, 22.13s/it]Evaluating:  24%|       | 15/63 [05:35<17:29, 21.86s/it]Evaluating:  25%|     16/63
17/63
18/63
19/63
20/63
21/63
22/63
23/63
24/63
25/63
26/63
27/63
28/63
29/63
30/63
  | 16/63 [05:53<16:12, 20.70s/it]Evaluating:  27%|       | 17/63 [06:15<16:11, 21.11s/it]Evaluating:  29%|       | 18/63 [06:43<17:22, 23.17s/it]Evaluating:  30%|       | 19/63 [06:58<15:08, 20.65s/it]Evaluating:  32%|      | 20/63 [07:13<13:40, 19.07s/it]Evaluating:  33%|      | 21/63 [07:35<13:59, 20.00s/it]Evaluating:  35%|      | 22/63 [07:55<13:35, 19.89s/it]Evaluating:  37%|      | 23/63 [08:19<14:01, 21.03s/it]Evaluating:  38%|      | 24/63 [08:41<13:56, 21.45s/it]Evaluating:  40%|      | 25/63 [09:09<14:45, 23.29s/it]Evaluating:  41%|     | 26/63 [09:37<15:12, 24.66s/it]Evaluating:  43%|     | 27/63 [09:59<14:19, 23.87s/it]Evaluating:  44%|     | 28/63 [10:14<12:25, 21.29s/it]Evaluating:  46%|     | 29/63 [10:43<13:20, 23.54s/it]Evaluating:  48%|     | 30/63 [11:04<12:30, 22.75s/it]Evaluating:  49%| 31/63
32/63
33/63
34/63
35/63
36/63
Distributed index stop interation. Idx: 599 Total_length: 596
Distributed index stop interation. Idx: 598 Total_length: 596
Distributed index stop interation. Idx: 596 Total_length: 596
    | 31/63 [11:26<12:06, 22.72s/it]Evaluating:  51%|     | 32/63 [11:50<11:55, 23.07s/it]Evaluating:  52%|    | 33/63 [12:11<11:12, 22.41s/it]Evaluating:  54%|    | 34/63 [12:28<10:06, 20.93s/it]Evaluating:  56%|    | 35/63 [12:58<10:54, 23.38s/it]Evaluating:  57%|    | 36/63 [13:18<10:07, 22.50s/it]Evaluating:  59%|    | 37/63 [13:34<08:58, 20.70s/it]Evaluating:  59%|    | 37/63 [13:34<09:32, 22.03s/it]
Distributed index stop interation. Idx: 597 Total_length: 596
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1/eval/9
dev | avg_loss: 0.8007465560693998 | {'exact_match': 0.0, 'rougeL': 60.9898}
train | epoch   9 | Iter:   2001/  2110 | global iter:   2001/  2110 | loss: 0.0058 | ds_loss: 0.0000 | lr: 4.2785e-07 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   9 | Iter:   2002/  2110 | global iter:   2002/  2110 | loss: 0.0059 | ds_loss: 0.0000 | lr: 4.2187e-07 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   9 | Iter:   2003/  2110 | global iter:   2003/  2110 | loss: 0.0024 | ds_loss: 0.0000 | lr: 4.1595e-07 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
train | epoch   9 | Iter:   2004/  2110 | global iter:   2004/  2110 | loss: 0.0068 | ds_loss: 0.0000 | lr: 4.1009e-07 | scale:     1.0000 | micro time: 3.784 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   2004/  2110 | global iter:   2004/  2110 | loss: 0.0052 | ds_loss: 0.0000 | lr: 4.1009e-07 | scale:     1.0000 | micro time: 3.784 | step time: 3.786
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   2005/  2110 | global iter:   2005/  2110 | loss: 0.0067 | ds_loss: 0.0000 | lr: 4.0428e-07 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   9 | Iter:   2006/  2110 | global iter:   2006/  2110 | loss: 0.0041 | ds_loss: 0.0000 | lr: 3.9852e-07 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   9 | Iter:   2007/  2110 | global iter:   2007/  2110 | loss: 0.0039 | ds_loss: 0.0000 | lr: 3.9282e-07 | scale:     1.0000 | micro time: 3.799 | step time: 0.000
train | epoch   9 | Iter:   2008/  2110 | global iter:   2008/  2110 | loss: 0.0103 | ds_loss: 0.0000 | lr: 3.8717e-07 | scale:     1.0000 | micro time: 3.822 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   2008/  2110 | global iter:   2008/  2110 | loss: 0.0063 | ds_loss: 0.0000 | lr: 3.8717e-07 | scale:     1.0000 | micro time: 3.822 | step time: 3.804
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   2009/  2110 | global iter:   2009/  2110 | loss: 0.0049 | ds_loss: 0.0000 | lr: 3.8158e-07 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
train | epoch   9 | Iter:   2010/  2110 | global iter:   2010/  2110 | loss: 0.0019 | ds_loss: 0.0000 | lr: 3.7604e-07 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
train | epoch   9 | Iter:   2011/  2110 | global iter:   2011/  2110 | loss: 0.0040 | ds_loss: 0.0000 | lr: 3.7056e-07 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   9 | Iter:   2012/  2110 | global iter:   2012/  2110 | loss: 0.0033 | ds_loss: 0.0000 | lr: 3.6513e-07 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   2012/  2110 | global iter:   2012/  2110 | loss: 0.0035 | ds_loss: 0.0000 | lr: 3.6513e-07 | scale:     1.0000 | micro time: 3.787 | step time: 3.789
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   2013/  2110 | global iter:   2013/  2110 | loss: 0.0062 | ds_loss: 0.0000 | lr: 3.5975e-07 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   9 | Iter:   2014/  2110 | global iter:   2014/  2110 | loss: 0.0055 | ds_loss: 0.0000 | lr: 3.5444e-07 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   9 | Iter:   2015/  2110 | global iter:   2015/  2110 | loss: 0.0082 | ds_loss: 0.0000 | lr: 3.4917e-07 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   9 | Iter:   2016/  2110 | global iter:   2016/  2110 | loss: 0.0024 | ds_loss: 0.0000 | lr: 3.4396e-07 | scale:     1.0000 | micro time: 3.779 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   2016/  2110 | global iter:   2016/  2110 | loss: 0.0056 | ds_loss: 0.0000 | lr: 3.4396e-07 | scale:     1.0000 | micro time: 3.779 | step time: 3.785
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   2017/  2110 | global iter:   2017/  2110 | loss: 0.0087 | ds_loss: 0.0000 | lr: 3.3881e-07 | scale:     1.0000 | micro time: 3.807 | step time: 0.000
train | epoch   9 | Iter:   2018/  2110 | global iter:   2018/  2110 | loss: 0.0107 | ds_loss: 0.0000 | lr: 3.3371e-07 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   9 | Iter:   2019/  2110 | global iter:   2019/  2110 | loss: 0.0140 | ds_loss: 0.0000 | lr: 3.2866e-07 | scale:     1.0000 | micro time: 3.818 | step time: 0.000
train | epoch   9 | Iter:   2020/  2110 | global iter:   2020/  2110 | loss: 0.0107 | ds_loss: 0.0000 | lr: 3.2367e-07 | scale:     1.0000 | micro time: 3.771 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   2020/  2110 | global iter:   2020/  2110 | loss: 0.0110 | ds_loss: 0.0000 | lr: 3.2367e-07 | scale:     1.0000 | micro time: 3.771 | step time: 3.799
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   2021/  2110 | global iter:   2021/  2110 | loss: 0.0028 | ds_loss: 0.0000 | lr: 3.1874e-07 | scale:     1.0000 | micro time: 3.800 | step time: 0.000
train | epoch   9 | Iter:   2022/  2110 | global iter:   2022/  2110 | loss: 0.0156 | ds_loss: 0.0000 | lr: 3.1385e-07 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   9 | Iter:   2023/  2110 | global iter:   2023/  2110 | loss: 0.0037 | ds_loss: 0.0000 | lr: 3.0903e-07 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   9 | Iter:   2024/  2110 | global iter:   2024/  2110 | loss: 0.0034 | ds_loss: 0.0000 | lr: 3.0426e-07 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   2024/  2110 | global iter:   2024/  2110 | loss: 0.0064 | ds_loss: 0.0000 | lr: 3.0426e-07 | scale:     1.0000 | micro time: 3.790 | step time: 3.791
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   2025/  2110 | global iter:   2025/  2110 | loss: 0.0059 | ds_loss: 0.0000 | lr: 2.9954e-07 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   9 | Iter:   2026/  2110 | global iter:   2026/  2110 | loss: 0.0025 | ds_loss: 0.0000 | lr: 2.9488e-07 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   9 | Iter:   2027/  2110 | global iter:   2027/  2110 | loss: 0.0042 | ds_loss: 0.0000 | lr: 2.9027e-07 | scale:     1.0000 | micro time: 3.783 | step time: 0.000
train | epoch   9 | Iter:   2028/  2110 | global iter:   2028/  2110 | loss: 0.0060 | ds_loss: 0.0000 | lr: 2.8572e-07 | scale:     1.0000 | micro time: 3.765 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   2028/  2110 | global iter:   2028/  2110 | loss: 0.0046 | ds_loss: 0.0000 | lr: 2.8572e-07 | scale:     1.0000 | micro time: 3.765 | step time: 3.779
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   2029/  2110 | global iter:   2029/  2110 | loss: 0.0036 | ds_loss: 0.0000 | lr: 2.8123e-07 | scale:     1.0000 | micro time: 3.778 | step time: 0.000
train | epoch   9 | Iter:   2030/  2110 | global iter:   2030/  2110 | loss: 0.0200 | ds_loss: 0.0000 | lr: 2.7678e-07 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   9 | Iter:   2031/  2110 | global iter:   2031/  2110 | loss: 0.0056 | ds_loss: 0.0000 | lr: 2.7240e-07 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   9 | Iter:   2032/  2110 | global iter:   2032/  2110 | loss: 0.0060 | ds_loss: 0.0000 | lr: 2.6806e-07 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   2032/  2110 | global iter:   2032/  2110 | loss: 0.0088 | ds_loss: 0.0000 | lr: 2.6806e-07 | scale:     1.0000 | micro time: 3.775 | step time: 3.785
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   2033/  2110 | global iter:   2033/  2110 | loss: 0.0113 | ds_loss: 0.0000 | lr: 2.6379e-07 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   9 | Iter:   2034/  2110 | global iter:   2034/  2110 | loss: 0.0083 | ds_loss: 0.0000 | lr: 2.5957e-07 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
train | epoch   9 | Iter:   2035/  2110 | global iter:   2035/  2110 | loss: 0.0018 | ds_loss: 0.0000 | lr: 2.5540e-07 | scale:     1.0000 | micro time: 3.780 | step time: 0.000
train | epoch   9 | Iter:   2036/  2110 | global iter:   2036/  2110 | loss: 0.0030 | ds_loss: 0.0000 | lr: 2.5129e-07 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   2036/  2110 | global iter:   2036/  2110 | loss: 0.0061 | ds_loss: 0.0000 | lr: 2.5129e-07 | scale:     1.0000 | micro time: 3.798 | step time: 3.789
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   2037/  2110 | global iter:   2037/  2110 | loss: 0.0048 | ds_loss: 0.0000 | lr: 2.4723e-07 | scale:     1.0000 | micro time: 3.797 | step time: 0.000
train | epoch   9 | Iter:   2038/  2110 | global iter:   2038/  2110 | loss: 0.0059 | ds_loss: 0.0000 | lr: 2.4323e-07 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   9 | Iter:   2039/  2110 | global iter:   2039/  2110 | loss: 0.0119 | ds_loss: 0.0000 | lr: 2.3928e-07 | scale:     1.0000 | micro time: 3.774 | step time: 0.000
train | epoch   9 | Iter:   2040/  2110 | global iter:   2040/  2110 | loss: 0.0044 | ds_loss: 0.0000 | lr: 2.3539e-07 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   2040/  2110 | global iter:   2040/  2110 | loss: 0.0068 | ds_loss: 0.0000 | lr: 2.3539e-07 | scale:     1.0000 | micro time: 3.798 | step time: 3.790
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   2041/  2110 | global iter:   2041/  2110 | loss: 0.0052 | ds_loss: 0.0000 | lr: 2.3155e-07 | scale:     1.0000 | micro time: 3.769 | step time: 0.000
train | epoch   9 | Iter:   2042/  2110 | global iter:   2042/  2110 | loss: 0.0086 | ds_loss: 0.0000 | lr: 2.2777e-07 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   9 | Iter:   2043/  2110 | global iter:   2043/  2110 | loss: 0.0049 | ds_loss: 0.0000 | lr: 2.2404e-07 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
train | epoch   9 | Iter:   2044/  2110 | global iter:   2044/  2110 | loss: 0.0104 | ds_loss: 0.0000 | lr: 2.2037e-07 | scale:     1.0000 | micro time: 3.767 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   2044/  2110 | global iter:   2044/  2110 | loss: 0.0073 | ds_loss: 0.0000 | lr: 2.2037e-07 | scale:     1.0000 | micro time: 3.767 | step time: 3.777
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   2045/  2110 | global iter:   2045/  2110 | loss: 0.0141 | ds_loss: 0.0000 | lr: 2.1675e-07 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
train | epoch   9 | Iter:   2046/  2110 | global iter:   2046/  2110 | loss: 0.0193 | ds_loss: 0.0000 | lr: 2.1319e-07 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   9 | Iter:   2047/  2110 | global iter:   2047/  2110 | loss: 0.0026 | ds_loss: 0.0000 | lr: 2.0968e-07 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   9 | Iter:   2048/  2110 | global iter:   2048/  2110 | loss: 0.0186 | ds_loss: 0.0000 | lr: 2.0623e-07 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   2048/  2110 | global iter:   2048/  2110 | loss: 0.0136 | ds_loss: 0.0000 | lr: 2.0623e-07 | scale:     1.0000 | micro time: 3.806 | step time: 3.801
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   2049/  2110 | global iter:   2049/  2110 | loss: 0.0033 | ds_loss: 0.0000 | lr: 2.0283e-07 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   9 | Iter:   2050/  2110 | global iter:   2050/  2110 | loss: 0.0102 | ds_loss: 0.0000 | lr: 1.9949e-07 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   9 | Iter:   2051/  2110 | global iter:   2051/  2110 | loss: 0.0098 | ds_loss: 0.0000 | lr: 1.9621e-07 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   9 | Iter:   2052/  2110 | global iter:   2052/  2110 | loss: 0.0065 | ds_loss: 0.0000 | lr: 1.9297e-07 | scale:     1.0000 | micro time: 3.797 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   2052/  2110 | global iter:   2052/  2110 | loss: 0.0075 | ds_loss: 0.0000 | lr: 1.9297e-07 | scale:     1.0000 | micro time: 3.797 | step time: 3.784
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   2053/  2110 | global iter:   2053/  2110 | loss: 0.0020 | ds_loss: 0.0000 | lr: 1.8980e-07 | scale:     1.0000 | micro time: 3.789 | step time: 0.000
train | epoch   9 | Iter:   2054/  2110 | global iter:   2054/  2110 | loss: 0.0152 | ds_loss: 0.0000 | lr: 1.8668e-07 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   9 | Iter:   2055/  2110 | global iter:   2055/  2110 | loss: 0.0087 | ds_loss: 0.0000 | lr: 1.8361e-07 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   9 | Iter:   2056/  2110 | global iter:   2056/  2110 | loss: 0.0053 | ds_loss: 0.0000 | lr: 1.8060e-07 | scale:     1.0000 | micro time: 3.814 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   2056/  2110 | global iter:   2056/  2110 | loss: 0.0078 | ds_loss: 0.0000 | lr: 1.8060e-07 | scale:     1.0000 | micro time: 3.814 | step time: 3.798
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   2057/  2110 | global iter:   2057/  2110 | loss: 0.0070 | ds_loss: 0.0000 | lr: 1.7764e-07 | scale:     1.0000 | micro time: 3.773 | step time: 0.000
train | epoch   9 | Iter:   2058/  2110 | global iter:   2058/  2110 | loss: 0.0126 | ds_loss: 0.0000 | lr: 1.7474e-07 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   9 | Iter:   2059/  2110 | global iter:   2059/  2110 | loss: 0.0063 | ds_loss: 0.0000 | lr: 1.7190e-07 | scale:     1.0000 | micro time: 3.775 | step time: 0.000
train | epoch   9 | Iter:   2060/  2110 | global iter:   2060/  2110 | loss: 0.0053 | ds_loss: 0.0000 | lr: 1.6911e-07 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   2060/  2110 | global iter:   2060/  2110 | loss: 0.0078 | ds_loss: 0.0000 | lr: 1.6911e-07 | scale:     1.0000 | micro time: 3.794 | step time: 3.785
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   2061/  2110 | global iter:   2061/  2110 | loss: 0.0119 | ds_loss: 0.0000 | lr: 1.6637e-07 | scale:     1.0000 | micro time: 3.793 | step time: 0.000
train | epoch   9 | Iter:   2062/  2110 | global iter:   2062/  2110 | loss: 0.0043 | ds_loss: 0.0000 | lr: 1.6369e-07 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   9 | Iter:   2063/  2110 | global iter:   2063/  2110 | loss: 0.0092 | ds_loss: 0.0000 | lr: 1.6107e-07 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   9 | Iter:   2064/  2110 | global iter:   2064/  2110 | loss: 0.0030 | ds_loss: 0.0000 | lr: 1.5850e-07 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   2064/  2110 | global iter:   2064/  2110 | loss: 0.0071 | ds_loss: 0.0000 | lr: 1.5850e-07 | scale:     1.0000 | micro time: 3.787 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   2065/  2110 | global iter:   2065/  2110 | loss: 0.0113 | ds_loss: 0.0000 | lr: 1.5598e-07 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   9 | Iter:   2066/  2110 | global iter:   2066/  2110 | loss: 0.0156 | ds_loss: 0.0000 | lr: 1.5352e-07 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   9 | Iter:   2067/  2110 | global iter:   2067/  2110 | loss: 0.0025 | ds_loss: 0.0000 | lr: 1.5112e-07 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   9 | Iter:   2068/  2110 | global iter:   2068/  2110 | loss: 0.0037 | ds_loss: 0.0000 | lr: 1.4877e-07 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   2068/  2110 | global iter:   2068/  2110 | loss: 0.0083 | ds_loss: 0.0000 | lr: 1.4877e-07 | scale:     1.0000 | micro time: 3.806 | step time: 3.794
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   2069/  2110 | global iter:   2069/  2110 | loss: 0.0062 | ds_loss: 0.0000 | lr: 1.4647e-07 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   9 | Iter:   2070/  2110 | global iter:   2070/  2110 | loss: 0.0150 | ds_loss: 0.0000 | lr: 1.4424e-07 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   9 | Iter:   2071/  2110 | global iter:   2071/  2110 | loss: 0.0033 | ds_loss: 0.0000 | lr: 1.4205e-07 | scale:     1.0000 | micro time: 3.794 | step time: 0.000
train | epoch   9 | Iter:   2072/  2110 | global iter:   2072/  2110 | loss: 0.0162 | ds_loss: 0.0000 | lr: 1.3992e-07 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   2072/  2110 | global iter:   2072/  2110 | loss: 0.0102 | ds_loss: 0.0000 | lr: 1.3992e-07 | scale:     1.0000 | micro time: 3.787 | step time: 3.796
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   2073/  2110 | global iter:   2073/  2110 | loss: 0.0086 | ds_loss: 0.0000 | lr: 1.3785e-07 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
train | epoch   9 | Iter:   2074/  2110 | global iter:   2074/  2110 | loss: 0.0057 | ds_loss: 0.0000 | lr: 1.3583e-07 | scale:     1.0000 | micro time: 3.806 | step time: 0.000
train | epoch   9 | Iter:   2075/  2110 | global iter:   2075/  2110 | loss: 0.0056 | ds_loss: 0.0000 | lr: 1.3387e-07 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   9 | Iter:   2076/  2110 | global iter:   2076/  2110 | loss: 0.0035 | ds_loss: 0.0000 | lr: 1.3196e-07 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   2076/  2110 | global iter:   2076/  2110 | loss: 0.0058 | ds_loss: 0.0000 | lr: 1.3196e-07 | scale:     1.0000 | micro time: 3.802 | step time: 3.796
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   2077/  2110 | global iter:   2077/  2110 | loss: 0.0079 | ds_loss: 0.0000 | lr: 1.3011e-07 | scale:     1.0000 | micro time: 3.797 | step time: 0.000
train | epoch   9 | Iter:   2078/  2110 | global iter:   2078/  2110 | loss: 0.0055 | ds_loss: 0.0000 | lr: 1.2831e-07 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   9 | Iter:   2079/  2110 | global iter:   2079/  2110 | loss: 0.0028 | ds_loss: 0.0000 | lr: 1.2657e-07 | scale:     1.0000 | micro time: 3.803 | step time: 0.000
train | epoch   9 | Iter:   2080/  2110 | global iter:   2080/  2110 | loss: 0.0064 | ds_loss: 0.0000 | lr: 1.2489e-07 | scale:     1.0000 | micro time: 3.814 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   2080/  2110 | global iter:   2080/  2110 | loss: 0.0057 | ds_loss: 0.0000 | lr: 1.2489e-07 | scale:     1.0000 | micro time: 3.814 | step time: 3.804
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   2081/  2110 | global iter:   2081/  2110 | loss: 0.0063 | ds_loss: 0.0000 | lr: 1.2325e-07 | scale:     1.0000 | micro time: 3.791 | step time: 0.000
train | epoch   9 | Iter:   2082/  2110 | global iter:   2082/  2110 | loss: 0.0030 | ds_loss: 0.0000 | lr: 1.2168e-07 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   9 | Iter:   2083/  2110 | global iter:   2083/  2110 | loss: 0.0048 | ds_loss: 0.0000 | lr: 1.2016e-07 | scale:     1.0000 | micro time: 3.810 | step time: 0.000
train | epoch   9 | Iter:   2084/  2110 | global iter:   2084/  2110 | loss: 0.0015 | ds_loss: 0.0000 | lr: 1.1869e-07 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   2084/  2110 | global iter:   2084/  2110 | loss: 0.0039 | ds_loss: 0.0000 | lr: 1.1869e-07 | scale:     1.0000 | micro time: 3.798 | step time: 3.800
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   2085/  2110 | global iter:   2085/  2110 | loss: 0.0053 | ds_loss: 0.0000 | lr: 1.1728e-07 | scale:     1.0000 | micro time: 3.801 | step time: 0.000
train | epoch   9 | Iter:   2086/  2110 | global iter:   2086/  2110 | loss: 0.0070 | ds_loss: 0.0000 | lr: 1.1593e-07 | scale:     1.0000 | micro time: 3.776 | step time: 0.000
train | epoch   9 | Iter:   2087/  2110 | global iter:   2087/  2110 | loss: 0.0056 | ds_loss: 0.0000 | lr: 1.1463e-07 | scale:     1.0000 | micro time: 3.773 | step time: 0.000
train | epoch   9 | Iter:   2088/  2110 | global iter:   2088/  2110 | loss: 0.0019 | ds_loss: 0.0000 | lr: 1.1338e-07 | scale:     1.0000 | micro time: 3.782 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   2088/  2110 | global iter:   2088/  2110 | loss: 0.0050 | ds_loss: 0.0000 | lr: 1.1338e-07 | scale:     1.0000 | micro time: 3.782 | step time: 3.783
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   2089/  2110 | global iter:   2089/  2110 | loss: 0.0070 | ds_loss: 0.0000 | lr: 1.1219e-07 | scale:     1.0000 | micro time: 3.805 | step time: 0.000
train | epoch   9 | Iter:   2090/  2110 | global iter:   2090/  2110 | loss: 0.0093 | ds_loss: 0.0000 | lr: 1.1106e-07 | scale:     1.0000 | micro time: 3.822 | step time: 0.000
train | epoch   9 | Iter:   2091/  2110 | global iter:   2091/  2110 | loss: 0.0067 | ds_loss: 0.0000 | lr: 1.0998e-07 | scale:     1.0000 | micro time: 3.770 | step time: 0.000
train | epoch   9 | Iter:   2092/  2110 | global iter:   2092/  2110 | loss: 0.0038 | ds_loss: 0.0000 | lr: 1.0896e-07 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   2092/  2110 | global iter:   2092/  2110 | loss: 0.0067 | ds_loss: 0.0000 | lr: 1.0896e-07 | scale:     1.0000 | micro time: 3.798 | step time: 3.799
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   2093/  2110 | global iter:   2093/  2110 | loss: 0.0285 | ds_loss: 0.0000 | lr: 1.0799e-07 | scale:     1.0000 | micro time: 3.790 | step time: 0.000
train | epoch   9 | Iter:   2094/  2110 | global iter:   2094/  2110 | loss: 0.0016 | ds_loss: 0.0000 | lr: 1.0708e-07 | scale:     1.0000 | micro time: 3.786 | step time: 0.000
train | epoch   9 | Iter:   2095/  2110 | global iter:   2095/  2110 | loss: 0.0093 | ds_loss: 0.0000 | lr: 1.0622e-07 | scale:     1.0000 | micro time: 3.774 | step time: 0.000
train | epoch   9 | Iter:   2096/  2110 | global iter:   2096/  2110 | loss: 0.0037 | ds_loss: 0.0000 | lr: 1.0542e-07 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   2096/  2110 | global iter:   2096/  2110 | loss: 0.0108 | ds_loss: 0.0000 | lr: 1.0542e-07 | scale:     1.0000 | micro time: 3.802 | step time: 3.788
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   2097/  2110 | global iter:   2097/  2110 | loss: 0.0039 | ds_loss: 0.0000 | lr: 1.0467e-07 | scale:     1.0000 | micro time: 3.787 | step time: 0.000
train | epoch   9 | Iter:   2098/  2110 | global iter:   2098/  2110 | loss: 0.0037 | ds_loss: 0.0000 | lr: 1.0398e-07 | scale:     1.0000 | micro time: 3.754 | step time: 0.000
train | epoch   9 | Iter:   2099/  2110 | global iter:   2099/  2110 | loss: 0.0072 | ds_loss: 0.0000 | lr: 1.0335e-07 | scale:     1.0000 | micro time: 3.802 | step time: 0.000
train | epoch   9 | Iter:   2100/  2110 | global iter:   2100/  2110 | loss: 0.0039 | ds_loss: 0.0000 | lr: 1.0277e-07 | scale:     1.0000 | micro time: 3.788 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   2100/  2110 | global iter:   2100/  2110 | loss: 0.0047 | ds_loss: 0.0000 | lr: 1.0277e-07 | scale:     1.0000 | micro time: 3.788 | step time: 3.783
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   2101/  2110 | global iter:   2101/  2110 | loss: 0.0034 | ds_loss: 0.0000 | lr: 1.0224e-07 | scale:     1.0000 | micro time: 3.763 | step time: 0.000
train | epoch   9 | Iter:   2102/  2110 | global iter:   2102/  2110 | loss: 0.0038 | ds_loss: 0.0000 | lr: 1.0177e-07 | scale:     1.0000 | micro time: 3.798 | step time: 0.000
train | epoch   9 | Iter:   2103/  2110 | global iter:   2103/  2110 | loss: 0.0033 | ds_loss: 0.0000 | lr: 1.0136e-07 | scale:     1.0000 | micro time: 3.774 | step time: 0.000
train | epoch   9 | Iter:   2104/  2110 | global iter:   2104/  2110 | loss: 0.0021 | ds_loss: 0.0000 | lr: 1.0100e-07 | scale:     1.0000 | micro time: 3.774 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   2104/  2110 | global iter:   2104/  2110 | loss: 0.0031 | ds_loss: 0.0000 | lr: 1.0100e-07 | scale:     1.0000 | micro time: 3.774 | step time: 3.778
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   2105/  2110 | global iter:   2105/  2110 | loss: 0.0051 | ds_loss: 0.0000 | lr: 1.0069e-07 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   9 | Iter:   2106/  2110 | global iter:   2106/  2110 | loss: 0.0080 | ds_loss: 0.0000 | lr: 1.0044e-07 | scale:     1.0000 | micro time: 3.784 | step time: 0.000
train | epoch   9 | Iter:   2107/  2110 | global iter:   2107/  2110 | loss: 0.0098 | ds_loss: 0.0000 | lr: 1.0025e-07 | scale:     1.0000 | micro time: 3.781 | step time: 0.000
train | epoch   9 | Iter:   2108/  2110 | global iter:   2108/  2110 | loss: 0.0023 | ds_loss: 0.0000 | lr: 1.0011e-07 | scale:     1.0000 | micro time: 3.773 | step time: 0.000
****************************************************************************************************
train | epoch   9 | Iter:   2108/  2110 | global iter:   2108/  2110 | loss: 0.0063 | ds_loss: 0.0000 | lr: 1.0011e-07 | scale:     1.0000 | micro time: 3.773 | step time: 3.780
./results/gpt2/train/sft/gpt2-xlarge/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   9 | Iter:   2109/  2110 | global iter:   2109/  2110 | loss: 0.0061 | ds_loss: 0.0000 | lr: 1.0003e-07 | scale:     1.0000 | micro time: 3.804 | step time: 0.000
train | epoch   9 | Iter:   2110/  2110 | global iter:   2110/  2110 | loss: 0.0059 | ds_loss: 0.0000 | lr: 1.0000e-07 | scale:     1.0000 | micro time: 3.767 | step time: 0.000
[rank0]:[W408 09:33:14.658760026 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
