{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/users/ap794/final_project_distillLLM/minillm\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTER_ADDR='localhost'\n",
    "MASTER_PORT='${2-2012}'\n",
    "NNODES=1\n",
    "NODE_RANK=0\n",
    "GPUS_PER_NODE=1 #${3-16}\n",
    "\n",
    "DISTRIBUTED_ARGS=\"--nproc_per_node $GPUS_PER_NODE \\\n",
    "                  --nnodes $NNODES \\\n",
    "                  --node_rank $NODE_RANK \\\n",
    "                  --master_addr $MASTER_ADDR \\\n",
    "                  --master_port $MASTER_PORT\"\n",
    "\n",
    "# model\n",
    "# BASE_PATH='/home/MiniLLM'\n",
    "BASE_PATH='.'\n",
    "CKPT_NAME=\"gpt2-xlarge\"\n",
    "#CKPT=\"${BASE_PATH}/checkpoints/${CKPT_NAME}/\"\n",
    "CKPT=\"gpt2-xl\"\n",
    "# data\n",
    "DATA_DIR=f\"{BASE_PATH}/processed_data/dolly/full/gpt2/\"\n",
    "# hp\n",
    "BATCH_SIZE=2\n",
    "LR=0.00001\n",
    "GRAD_ACC=1\n",
    "EVAL_BATCH_SIZE=8\n",
    "# length\n",
    "MAX_LENGTH=512\n",
    "# runtime\n",
    "SAVE_PATH=f\"{BASE_PATH}/results/gpt2/train/sft\"\n",
    "# seed\n",
    "SEED=10\n",
    "SEED_ORDER=10\n",
    "\n",
    "\n",
    "# Define arguments as a dictionary\n",
    "args_dict = {\n",
    "    # Model\n",
    "    \"base_path\": BASE_PATH,\n",
    "    \"model_path\": CKPT,\n",
    "    \"ckpt_name\": CKPT_NAME,\n",
    "    \"n_gpu\": GPUS_PER_NODE,\n",
    "    \n",
    "    # Data\n",
    "    \"data_dir\": DATA_DIR,\n",
    "    \"num_workers\": 0,\n",
    "    \"dev_num\": 1000,\n",
    "\n",
    "    # Hyperparameters\n",
    "    \"lr\": LR,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"eval_batch_size\": EVAL_BATCH_SIZE,\n",
    "    \"gradient_accumulation_steps\": GRAD_ACC,\n",
    "    \"warmup_iters\": 0,\n",
    "    \"lr_decay_style\": \"cosine\",\n",
    "    \"weight_decay\": 1e-2,\n",
    "    \"clip_grad\": 1.0,\n",
    "    \"epochs\": 10,\n",
    "\n",
    "    # Sequence length\n",
    "    \"max_length\": MAX_LENGTH,\n",
    "    \"max_prompt_length\": 256,\n",
    "\n",
    "    # Runtime\n",
    "    \"do_train\": True,\n",
    "    \"do_valid\": True,\n",
    "    \"eval_gen\": True,\n",
    "    \"save_interval\": 10,\n",
    "    \"eval_interval\": 10,\n",
    "    \"log_interval\": 4,\n",
    "    \"mid_log_num\": 10,\n",
    "    \"save\": SAVE_PATH,\n",
    "\n",
    "    # Seed\n",
    "    \"seed\": SEED,\n",
    "    \"seed_order\": SEED_ORDER,\n",
    "\n",
    "    # DeepSpeed\n",
    "    \"deepspeed\": True,\n",
    "    \"deepspeed_config\": f\"{BASE_PATH}/configs/deepspeed/ds_config_zero1_fp16.json\",\n",
    "\n",
    "    # Model type\n",
    "    \"type\": \"lm\",\n",
    "\n",
    "    # Generation settings\n",
    "    \"do_sample\": True,\n",
    "    \"top_k\": 0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"temperature\": 1.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-22 09:21:03,968] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/users/ap794/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_args() got an unexpected keyword argument 'base_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#args = get_args()\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[43mget_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m initialize(args)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dist\u001b[38;5;241m.\u001b[39mget_rank() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: get_args() got an unexpected keyword argument 'base_path'"
     ]
    }
   ],
   "source": [
    "from finetune import *\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "#args = get_args()\n",
    "args = get_args(**args_dict)\n",
    "\n",
    "initialize(args)\n",
    "\n",
    "if dist.get_rank() == 0:\n",
    "    print_args(args)\n",
    "    with open(os.path.join(args.save, \"args.json\"), \"w\") as f:\n",
    "        json.dump(vars(args), f)\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "cur_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "save_rank(\"\\n\\n\" + \"=\"*30 + f\" EXP at {cur_time} \" + \"=\"*30, os.path.join(args.save, \"log.txt\"))\n",
    "\n",
    "with open(args.deepspeed_config, \"r\") as f:\n",
    "    ds_config = json.load(f)\n",
    "\n",
    "ds_config[\"gradient_accumulation_steps\"] = args.gradient_accumulation_steps\n",
    "ds_config[\"train_micro_batch_size_per_gpu\"] = args.batch_size\n",
    "ds_config[\"gradient_clipping\"] = args.clip_grad\n",
    "ds_config[\"steps_per_print\"] = 10000000\n",
    "\n",
    "if not args.do_train:\n",
    "    ds_config[\"zero_optimization\"][\"stage\"] = 0\n",
    "\n",
    "if \"fp16\" in ds_config and ds_config[\"fp16\"][\"enabled\"]:\n",
    "    args.dtype = \"torch.float16\"\n",
    "elif \"bf16\" in ds_config and ds_config[\"bf16\"][\"enabled\"]:\n",
    "    args.dtype = \"torch.bfloat16\"\n",
    "else:\n",
    "    args.dtype = \"torch.float32\"\n",
    "args.deepspeed_config = None\n",
    "\n",
    "# get the tokenizer\n",
    "tokenizer = get_tokenizer(args)\n",
    "dataset = prepare_dataset(\n",
    "    args,\n",
    "    tokenizer,\n",
    ")\n",
    "\n",
    "dp_world_size = mpu.get_data_parallel_world_size() if args.model_parallel else dist.get_world_size()\n",
    "\n",
    "if args.do_train:\n",
    "    args.train_iters_per_epoch = int(len(dataset[\"train\"]) / (args.batch_size * dp_world_size * args.gradient_accumulation_steps))\n",
    "    print_rank(\"Train iters per epoch\", args.train_iters_per_epoch)\n",
    "    if args.total_iters is None:\n",
    "        args.total_iters = args.train_iters_per_epoch * args.epochs\n",
    "    if args.epochs is None:\n",
    "        args.epochs = math.ceil(args.total_iters / args.train_iters_per_epoch)\n",
    "    print_rank(\"total_iters\", args.total_iters)\n",
    "    \n",
    "    if args.save_interval == -1:\n",
    "        args.save_interval = args.train_iters_per_epoch\n",
    "    \n",
    "    if args.eval_interval == -1:\n",
    "        args.eval_interval = args.train_iters_per_epoch\n",
    "\n",
    "model, optimizer, lr_scheduler = setup_model_and_optimizer(args, ds_config, device, set_optim=args.do_train)\n",
    "\n",
    "if args.teacher_model_type is None:\n",
    "    args.teacher_model_type = args.model_type\n",
    "\n",
    "if args.teacher_model_path is not None:\n",
    "    teacher_model = get_teacher_model(args, device)\n",
    "else:\n",
    "    teacher_model = None\n",
    "\n",
    "if args.do_train:\n",
    "    model = finetune(args, tokenizer, model, optimizer, lr_scheduler, dataset, device, teacher_model=teacher_model)\n",
    "\n",
    "if args.do_eval:\n",
    "    evaluate(args, tokenizer, model, dataset[\"test\"], \"test\", 0, device)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
