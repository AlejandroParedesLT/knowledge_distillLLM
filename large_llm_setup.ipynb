{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBQhmIsApqVM",
        "outputId": "5a3f32da-a47f-4819-9491-76c3d86e124c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'LMOps'...\n",
            "remote: Enumerating objects: 12586, done.\u001b[K\n",
            "remote: Counting objects: 100% (780/780), done.\u001b[K\n",
            "remote: Compressing objects: 100% (450/450), done.\u001b[K\n",
            "remote: Total 12586 (delta 451), reused 368 (delta 328), pack-reused 11806 (from 1)\u001b[K\n",
            "Receiving objects: 100% (12586/12586), 73.81 MiB | 8.78 MiB/s, done.\n",
            "Resolving deltas: 100% (6525/6525), done.\n",
            "Updating files: 100% (7238/7238), done.\n",
            "/content/LMOps\n",
            "/content/LMOps/minillm\n",
            "Collecting git+https://github.com/t1101675/transformers@minillm\n",
            "  Cloning https://github.com/t1101675/transformers (to revision minillm) to /tmp/pip-req-build-n2lmyxi3\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/t1101675/transformers /tmp/pip-req-build-n2lmyxi3\n",
            "  Running command git checkout -b minillm --track origin/minillm\n",
            "  Switched to a new branch 'minillm'\n",
            "  Branch 'minillm' set up to track remote branch 'minillm' from 'origin'.\n",
            "  Resolved https://github.com/t1101675/transformers to commit be0435edd0e45f491fbb66fe9fa630d458a2ace6\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.0.dev0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.0.dev0) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.0.dev0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.0.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.0.dev0) (2.32.3)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers==4.47.0.dev0)\n",
            "  Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.0.dev0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.47.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.47.0.dev0) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.47.0.dev0) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.47.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.47.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.47.0.dev0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.47.0.dev0) (2025.1.31)\n",
            "Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.47.0.dev0-py3-none-any.whl size=10055946 sha256=9f9788bb635df9266d25f0e6de07ea355f06a76a89b3ee7e7f3b96a784c51aa1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pcajyiov/wheels/ad/cb/fd/9d4c036cd896423ef1087c335b769c82caf1825a540507c0dd\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.51.1\n",
            "    Uninstalling transformers-4.51.1:\n",
            "      Successfully uninstalled transformers-4.51.1\n",
            "Successfully installed tokenizers-0.20.3 transformers-4.47.0.dev0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m111.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Collecting deepspeed\n",
            "  Downloading deepspeed-0.16.6.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from deepspeed) (0.8.1)\n",
            "Collecting hjson (from deepspeed)\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.1.0)\n",
            "Collecting ninja (from deepspeed)\n",
            "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from deepspeed) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from deepspeed) (9.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.11.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from deepspeed) (4.67.1)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.11/dist-packages (from deepspeed) (12.570.86)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (4.13.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->deepspeed) (0.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->deepspeed) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->deepspeed) (3.0.2)\n",
            "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.16.6-py3-none-any.whl size=1643403 sha256=b009f057f1ca68340dd0b213a733283e22a374a21ab75525cbf136a13ab66d20\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/95/9c/8445820a7d7ef64c6083bef3a6c84d8baf4de6a864d6cd9737\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: hjson, ninja, deepspeed\n",
            "Successfully installed deepspeed-0.16.6 hjson-3.1.0 ninja-1.11.1.4\n",
            "Collecting numerize\n",
            "  Downloading numerize-0.12.tar.gz (2.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: numerize\n",
            "  Building wheel for numerize (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for numerize: filename=numerize-0.12-py3-none-any.whl size=3153 sha256=61563afd406d4f6a44678c0fc7d2728cd98a483ea2d886f076447f03788ce2d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/49/3e/5375462d832133c3684a27f9ad763a61141a802aee4bd445d6\n",
            "Successfully built numerize\n",
            "Installing collected packages: numerize\n",
            "Successfully installed numerize-0.12\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=d0ad19ee88001599bf7d7728857995cbb8a87d02f0e97113d8b10517d891d460\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n",
            "Collecting torchtyping\n",
            "  Downloading torchtyping-0.1.5-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from torchtyping) (2.6.0+cu124)\n",
            "Collecting typeguard<3,>=2.11.1 (from torchtyping)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.0->torchtyping) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.7.0->torchtyping) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.7.0->torchtyping) (3.0.2)\n",
            "Downloading torchtyping-0.1.5-py3-none-any.whl (17 kB)\n",
            "Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, torchtyping\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.4.2\n",
            "    Uninstalling typeguard-4.4.2:\n",
            "      Successfully uninstalled typeguard-4.4.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inflect 7.5.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torchtyping-0.1.5 typeguard-2.13.3\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.30.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.47.0.dev0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.5.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2024.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.20.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.1.31)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/microsoft/LMOps.git\n",
        "\n",
        "%cd LMOps\n",
        "!rm -rf $(ls | grep -v '^minillm$')\n",
        "\n",
        "%cd minillm\n",
        "!bash install.sh\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85-IQQblq0_S"
      },
      "outputs": [],
      "source": [
        "#Setup of the sh files\n",
        "#gpt2/eval\n",
        "!sed -i 's|GPUS_PER_NODE=.*|GPUS_PER_NODE=1|' scripts/gpt2/eval/eval_main_dolly.sh\n",
        "!sed -i 's|BASE_PATH=\\${1-\"/home/MiniLLM\"}|BASE_PATH=\\${1-\"/content/LMOps/minillm\"}|' scripts/gpt2/eval/eval_main_dolly.sh\n",
        "!sed -i 's|GPUS_PER_NODE=.*|GPUS_PER_NODE=1|' scripts/gpt2/eval/eval_exposure_bias.sh\n",
        "!sed -i 's|BASE_PATH=\\${1-\"/home/MiniLLM\"}|BASE_PATH=\\${1-\"/content/LMOps/minillm\"}|' scripts/gpt2/eval/eval_exposure_bias.sh\n",
        "!sed -i 's|GPUS_PER_NODE=.*|GPUS_PER_NODE=1|' scripts/gpt2/eval/eval_main_self_inst.sh\n",
        "!sed -i 's|BASE_PATH=\\${1-\"/home/MiniLLM\"}|BASE_PATH=\\${1-\"/content/LMOps/minillm\"}|' scripts/gpt2/eval/eval_main_self_inst.sh\n",
        "!sed -i 's|GPUS_PER_NODE=.*|GPUS_PER_NODE=1|' scripts/gpt2/eval/eval_main_sinst.sh\n",
        "!sed -i 's|BASE_PATH=\\${1-\"/home/MiniLLM\"}|BASE_PATH=\\${1-\"/content/LMOps/minillm\"}|' scripts/gpt2/eval/eval_main_sinst.sh\n",
        "!sed -i 's|GPUS_PER_NODE=.*|GPUS_PER_NODE=1|' scripts/gpt2/eval/eval_main_uinst.sh\n",
        "!sed -i 's|BASE_PATH=\\${1-\"/home/MiniLLM\"}|BASE_PATH=\\${1-\"/content/LMOps/minillm\"}|' scripts/gpt2/eval/eval_main_uinst.sh\n",
        "!sed -i 's|GPUS_PER_NODE=.*|GPUS_PER_NODE=1|' scripts/gpt2/eval/eval_main_vicuna.sh\n",
        "!sed -i 's|BASE_PATH=\\${1-\"/home/MiniLLM\"}|BASE_PATH=\\${1-\"/content/LMOps/minillm\"}|' scripts/gpt2/eval/eval_main_vicuna.sh\n",
        "!sed -i 's|base_path=\\${1-\"/home/MiniLLM\"}|base_path=\\${1-\"/content/LMOps/minillm\"}|' scripts/gpt2/eval/run_eval.sh\n",
        "\n",
        "#gpt2/kd\n",
        "!sed -i 's|GPUS_PER_NODE=.*|GPUS_PER_NODE=1|' scripts/gpt2/kd/kd_base.sh\n",
        "!sed -i 's|BASE_PATH=\\${1-\"/home/MiniLLM\"}|BASE_PATH=\\${1-\"/content/LMOps/minillm\"}|' scripts/gpt2/kd/kd_base.sh\n",
        "!sed -i 's|--save-interval -1|--save-interval 2000|' scripts/gpt2/kd/kd_base.sh\n",
        "!sed -i 's|--eval-interval -1|--eval-interval 2000|' scripts/gpt2/kd/kd_base.sh\n",
        "!sed -i 's|--mid-log-num -1|--mid-log-num 10|' scripts/gpt2/kd/kd_base.sh\n",
        "!sed -i 's|TEACHER_CKPT_NAME=\"xlarge-sft\"|TEACHER_CKPT_NAME=\"large-sft\"|' scripts/gpt2/kd/kd_base.sh\n",
        "!sed -i 's|/results/gpt2/train/sft/gpt2-xlarge/|/results/gpt2/train/sft/gpt2-large/|' scripts/gpt2/kd/kd_base.sh\n",
        "!sed -i 's|--epochs 20|--epochs 5|' scripts/gpt2/kd/kd_base.sh\n",
        "\n",
        "\n",
        "#gpt2/sft_base\n",
        "!sed -i 's|GPUS_PER_NODE=.*|GPUS_PER_NODE=1|' scripts/gpt2/sft/sft_base.sh\n",
        "!sed -i 's|BASE_PATH=\\${1-\"/home/MiniLLM\"}|BASE_PATH=\\${1-\"/content/LMOps/minillm\"}|' scripts/gpt2/sft/sft_base.sh\n",
        "!sed -i 's|--save-interval -1|--save-interval 5202|' scripts/gpt2/sft/sft_base.sh\n",
        "!sed -i 's|--eval-interval -1|--eval-interval 5202|' scripts/gpt2/sft/sft_base.sh\n",
        "!sed -i 's|--mid-log-num -1|--mid-log-num 10|' scripts/gpt2/sft/sft_base.sh\n",
        "!sed -i 's|--log-interval 4|--log-interval 102|' scripts/gpt2/sft/sft_base.sh\n",
        "!sed -i 's|--epochs 20|--epochs 2|' scripts/gpt2/sft/sft_base.sh\n",
        "#gpt2/sft_large\n",
        "!sed -i 's|GPUS_PER_NODE=.*|GPUS_PER_NODE=1|' scripts/gpt2/sft/sft_large.sh\n",
        "!sed -i 's|BASE_PATH=\\${1-\"/home/MiniLLM\"}|BASE_PATH=\\${1-\"/content/LMOps/minillm\"}|' scripts/gpt2/sft/sft_large.sh\n",
        "!sed -i 's|--save-interval -1|--save-interval 5202|' scripts/gpt2/sft/sft_large.sh\n",
        "!sed -i 's|--eval-interval -1|--eval-interval 5202|' scripts/gpt2/sft/sft_large.sh\n",
        "!sed -i 's|--mid-log-num -1|--mid-log-num 10|' scripts/gpt2/sft/sft_large.sh\n",
        "!sed -i 's|--log-interval 4|--log-interval 102|' scripts/gpt2/sft/sft_large.sh\n",
        "!sed -i 's|--epochs 10|--epochs 5|' scripts/gpt2/sft/sft_large.sh\n",
        "!sed -i 's|MAX_LENGTH=512|MAX_LENGTH=512|' scripts/gpt2/sft/sft_large.sh\n",
        "!sed -i 's|BATCH_SIZE=2|BATCH_SIZE=1|' scripts/gpt2/sft/sft_large.sh\n",
        "\n",
        "\n",
        "# #gpt2/tools\n",
        "!sed -i 's|GPUS_PER_NODE=.*|GPUS_PER_NODE=1|' scripts/gpt2/tools/generate_data_seqkd.sh\n",
        "!sed -i 's|BASE_PATH=\\${1-\"/home/MiniLLM\"}|BASE_PATH=\\${1-\"/content/LMOps/minillm\"}|' scripts/gpt2/tools/generate_data_seqkd.sh\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBkQbHdZy9sP",
        "outputId": "43873a46-17e3-4838-9dd9-d46a22a28eac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\n",
            "rust_model.ot:  66% 2.30G/3.50G [02:36<01:00, 19.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  47% 1.58G/3.35G [02:44<04:13, 6.99MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  47% 1.57G/3.35G [02:45<05:08, 5.76MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  66% 2.04G/3.10G [02:46<03:03, 5.74MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  47% 1.58G/3.35G [02:46<05:12, 5.67MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  72% 2.23G/3.10G [02:40<02:39, 5.42MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  58% 1.89G/3.25G [02:41<03:57, 5.72MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  48% 1.59G/3.35G [02:45<03:48, 7.71MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  66% 2.31G/3.50G [02:41<03:28, 5.73MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  79% 2.57G/3.25G [02:47<02:01, 5.59MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  66% 2.06G/3.10G [02:46<02:11, 7.90MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  47% 1.58G/3.35G [02:46<03:45, 7.87MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  48% 1.59G/3.35G [02:46<03:46, 7.77MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  72% 2.24G/3.10G [02:40<01:56, 7.32MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  58% 1.90G/3.25G [02:41<02:56, 7.66MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  48% 1.60G/3.35G [02:45<02:54, 10.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  66% 2.32G/3.50G [02:41<02:37, 7.56MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  79% 2.58G/3.25G [02:47<01:29, 7.48MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  48% 1.60G/3.35G [02:46<02:54, 10.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  67% 2.07G/3.10G [02:47<01:42, 10.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  48% 1.59G/3.35G [02:46<02:56, 9.96MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  48% 1.61G/3.35G [02:45<02:12, 13.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  73% 2.25G/3.10G [02:40<01:27, 9.64MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  48% 1.61G/3.35G [02:46<02:07, 13.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  80% 2.59G/3.25G [02:47<01:05, 10.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  66% 2.33G/3.50G [02:41<01:57, 10.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  67% 2.08G/3.10G [02:47<01:15, 13.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  48% 1.63G/3.35G [02:45<01:41, 17.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  48% 1.60G/3.35G [02:46<02:13, 13.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  67% 2.34G/3.50G [02:41<01:25, 13.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  73% 2.26G/3.10G [02:41<01:06, 12.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  48% 1.63G/3.35G [02:47<01:36, 17.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  80% 2.60G/3.25G [02:47<00:48, 13.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  67% 2.09G/3.10G [02:47<00:56, 17.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  67% 2.35G/3.50G [02:41<01:03, 18.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  49% 1.64G/3.35G [02:45<01:20, 21.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  48% 1.61G/3.35G [02:46<01:41, 17.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  73% 2.28G/3.10G [02:41<00:50, 16.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  49% 1.64G/3.35G [02:47<01:18, 21.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  80% 2.61G/3.25G [02:47<00:37, 16.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  68% 2.10G/3.10G [02:47<00:46, 21.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  59% 1.91G/3.25G [02:42<02:31, 8.81MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  67% 2.36G/3.50G [02:42<00:53, 21.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  49% 1.65G/3.35G [02:46<01:10, 24.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  48% 1.63G/3.35G [02:47<01:26, 20.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  74% 2.29G/3.10G [02:41<00:43, 18.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  59% 1.92G/3.25G [02:44<03:06, 7.11MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  81% 2.62G/3.25G [02:51<01:20, 7.73MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  68% 2.11G/3.10G [02:50<01:58, 8.35MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  68% 2.37G/3.50G [02:45<02:14, 8.47MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  49% 1.64G/3.35G [02:50<03:24, 8.38MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  74% 2.30G/3.10G [02:44<01:35, 8.42MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  49% 1.65G/3.35G [02:50<03:34, 7.95MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  49% 1.66G/3.35G [02:49<03:20, 8.46MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  81% 2.63G/3.25G [02:51<01:00, 10.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  68% 2.12G/3.10G [02:51<01:31, 10.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  49% 1.65G/3.35G [02:50<02:38, 10.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  68% 2.38G/3.50G [02:45<01:44, 10.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  74% 2.31G/3.10G [02:44<01:13, 10.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  49% 1.66G/3.35G [02:50<02:42, 10.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  50% 1.67G/3.35G [02:49<02:32, 11.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  69% 2.13G/3.10G [02:51<01:09, 14.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  81% 2.64G/3.25G [02:51<00:47, 12.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  49% 1.66G/3.35G [02:50<02:01, 14.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  68% 2.39G/3.50G [02:45<01:20, 13.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  75% 2.32G/3.10G [02:45<00:55, 14.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  50% 1.67G/3.35G [02:51<02:07, 13.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  50% 1.68G/3.35G [02:49<02:01, 13.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  50% 1.67G/3.35G [02:50<01:38, 17.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  69% 2.14G/3.10G [02:51<00:57, 16.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  82% 2.65G/3.25G [02:52<00:38, 15.4MB/s]\u001b[A\u001b[A\n",
            "tf_model.h5:  75% 2.33G/3.10G [02:45<00:46, 16.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  69% 2.40G/3.50G [02:46<01:07, 16.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  50% 1.68G/3.35G [02:51<01:44, 16.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  50% 1.69G/3.35G [02:50<01:40, 16.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  59% 1.93G/3.25G [02:46<03:27, 6.35MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  69% 2.15G/3.10G [02:52<00:47, 19.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  50% 1.68G/3.35G [02:51<01:22, 20.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  50% 1.69G/3.35G [02:51<01:23, 19.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  82% 2.66G/3.25G [02:52<00:32, 17.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  69% 2.41G/3.50G [02:46<00:56, 19.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  76% 2.34G/3.10G [02:45<00:39, 19.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  51% 1.70G/3.35G [02:50<01:21, 20.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  50% 1.69G/3.35G [02:51<01:10, 23.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  51% 1.70G/3.35G [02:51<01:09, 23.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  82% 2.67G/3.25G [02:52<00:26, 21.3MB/s]\u001b[A\u001b[A\n",
            "tf_model.h5:  76% 2.35G/3.10G [02:46<00:33, 22.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  70% 2.16G/3.10G [02:52<00:44, 20.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  69% 2.42G/3.50G [02:46<00:48, 22.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  51% 1.71G/3.35G [02:50<01:09, 23.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  51% 1.70G/3.35G [02:51<01:02, 26.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  51% 1.71G/3.35G [02:52<01:07, 24.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  51% 1.71G/3.35G [02:52<00:53, 30.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  69% 2.43G/3.50G [02:47<00:43, 24.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  83% 2.68G/3.25G [02:53<00:24, 22.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  51% 1.72G/3.35G [02:51<01:03, 25.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  76% 2.36G/3.10G [02:46<00:31, 23.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  51% 1.72G/3.35G [02:52<00:54, 29.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  70% 2.44G/3.50G [02:47<00:38, 27.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  51% 1.72G/3.35G [02:52<00:49, 32.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  52% 1.73G/3.35G [02:51<00:57, 28.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  83% 2.69G/3.25G [02:53<00:21, 26.0MB/s]\u001b[A\u001b[A\n",
            "tf_model.h5:  77% 2.37G/3.10G [02:46<00:27, 26.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  52% 1.73G/3.35G [02:52<00:51, 31.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  70% 2.45G/3.50G [02:47<00:33, 31.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  52% 1.74G/3.35G [02:51<00:53, 30.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  52% 1.73G/3.35G [02:52<00:51, 31.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  83% 2.71G/3.25G [02:53<00:20, 27.0MB/s]\u001b[A\u001b[A\n",
            "tf_model.h5:  77% 2.38G/3.10G [02:47<00:26, 26.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  52% 1.74G/3.35G [02:53<00:49, 32.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  70% 2.17G/3.10G [02:53<01:00, 15.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  52% 1.75G/3.35G [02:51<00:46, 34.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  70% 2.46G/3.50G [02:47<00:31, 33.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  84% 2.72G/3.25G [02:53<00:17, 30.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  52% 1.74G/3.35G [02:52<00:47, 33.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  77% 2.39G/3.10G [02:47<00:22, 32.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  52% 1.75G/3.35G [02:53<00:42, 37.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  60% 1.94G/3.25G [02:48<03:32, 6.16MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  70% 2.18G/3.10G [02:56<01:51, 8.23MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  71% 2.47G/3.50G [02:50<01:41, 10.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  53% 1.76G/3.35G [02:54<02:39, 9.97MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  52% 1.75G/3.35G [02:56<03:03, 8.72MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  78% 2.40G/3.10G [02:50<01:20, 8.67MB/s]\u001b[A\n",
            "\n",
            "model.safetensors:  84% 2.73G/3.25G [02:57<01:01, 8.50MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  71% 2.49G/3.50G [02:51<01:30, 11.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  53% 1.76G/3.35G [02:56<02:58, 8.90MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  53% 1.77G/3.35G [02:55<02:20, 11.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  71% 2.19G/3.10G [02:57<01:37, 9.25MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  60% 1.95G/3.25G [02:51<04:25, 4.89MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  53% 1.76G/3.35G [02:56<02:18, 11.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  78% 2.41G/3.10G [02:50<01:00, 11.2MB/s]\u001b[A\n",
            "\n",
            "model.safetensors:  84% 2.74G/3.25G [02:57<00:47, 10.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  53% 1.77G/3.35G [02:56<02:20, 11.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  53% 1.78G/3.35G [02:55<01:52, 13.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  71% 2.50G/3.50G [02:51<01:14, 13.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  60% 1.96G/3.25G [02:51<03:18, 6.49MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  71% 2.20G/3.10G [02:57<01:17, 11.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  53% 1.77G/3.35G [02:56<01:47, 14.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  78% 2.42G/3.10G [02:51<00:45, 14.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  72% 2.51G/3.50G [02:51<00:56, 17.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  53% 1.79G/3.35G [02:55<01:26, 18.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  53% 1.78G/3.35G [02:56<01:20, 19.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  85% 2.75G/3.25G [02:57<00:35, 14.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  53% 1.78G/3.35G [02:57<01:47, 14.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  79% 2.43G/3.10G [02:51<00:37, 17.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  71% 2.21G/3.10G [02:57<01:07, 13.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  61% 1.97G/3.25G [02:52<02:46, 7.65MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  54% 1.80G/3.35G [02:59<03:35, 7.20MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  72% 2.52G/3.50G [02:55<02:18, 7.12MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  53% 1.79G/3.35G [03:00<03:32, 7.33MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  72% 2.22G/3.10G [03:01<02:04, 7.00MB/s]\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  79% 2.44G/3.10G [02:54<01:28, 7.40MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  61% 1.98G/3.25G [02:55<03:41, 5.72MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  72% 2.53G/3.50G [02:55<01:39, 9.85MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  85% 2.76G/3.25G [03:01<01:15, 6.46MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  53% 1.79G/3.35G [03:00<03:59, 6.52MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  79% 2.45G/3.10G [02:54<01:04, 9.99MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  54% 1.80G/3.35G [03:00<02:39, 9.71MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  72% 2.23G/3.10G [03:01<01:33, 9.27MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  61% 1.99G/3.25G [02:55<02:43, 7.67MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  72% 2.54G/3.50G [02:55<01:16, 12.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  54% 1.81G/3.35G [02:59<02:48, 9.12MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  85% 2.77G/3.25G [03:01<00:55, 8.61MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  54% 1.80G/3.35G [03:01<02:59, 8.62MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  80% 2.46G/3.10G [02:55<00:49, 12.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  62% 2.00G/3.25G [02:55<02:02, 10.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  54% 1.81G/3.35G [03:00<02:02, 12.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  72% 2.24G/3.10G [03:01<01:10, 12.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  73% 2.55G/3.50G [02:56<01:02, 15.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  54% 1.81G/3.35G [03:01<02:17, 11.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  86% 2.78G/3.25G [03:02<00:42, 11.0MB/s]\u001b[A\u001b[A\n",
            "tf_model.h5:  80% 2.47G/3.10G [02:55<00:39, 15.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  54% 1.82G/3.35G [03:00<02:15, 11.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  54% 1.82G/3.35G [03:01<01:37, 15.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  73% 2.56G/3.50G [02:56<00:47, 20.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  54% 1.82G/3.35G [03:01<01:44, 14.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  62% 2.01G/3.25G [02:56<01:40, 12.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  86% 2.79G/3.25G [03:02<00:32, 14.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  55% 1.84G/3.35G [03:01<01:17, 19.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  73% 2.57G/3.50G [02:56<00:37, 25.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  80% 2.49G/3.10G [02:55<00:31, 19.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  55% 1.85G/3.35G [03:00<01:21, 18.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  86% 2.80G/3.25G [03:02<00:23, 18.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  74% 2.58G/3.50G [02:56<00:30, 30.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  55% 1.85G/3.35G [03:01<01:04, 23.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  73% 2.25G/3.10G [03:02<01:06, 12.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  87% 2.81G/3.25G [03:02<00:18, 23.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  55% 1.86G/3.35G [03:01<00:55, 26.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  55% 1.86G/3.35G [03:00<01:13, 20.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  62% 2.02G/3.25G [02:56<01:27, 13.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  73% 2.26G/3.10G [03:03<01:18, 10.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  81% 2.51G/3.10G [02:59<01:05, 9.03MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  55% 1.86G/3.35G [03:06<03:38, 6.84MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  87% 2.82G/3.25G [03:07<01:10, 6.01MB/s]\u001b[A\u001b[A\n",
            "tf_model.h5:  81% 2.52G/3.10G [03:00<01:06, 8.75MB/s]\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  73% 2.28G/3.10G [03:07<02:16, 6.00MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  63% 2.03G/3.25G [03:01<03:43, 5.43MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  56% 1.87G/3.35G [03:06<03:27, 7.15MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  56% 1.87G/3.35G [03:05<03:50, 6.44MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  74% 2.59G/3.50G [03:01<02:32, 6.01MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  87% 2.83G/3.25G [03:07<00:50, 8.26MB/s]\u001b[A\u001b[A\n",
            "tf_model.h5:  82% 2.53G/3.10G [03:00<00:49, 11.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  56% 1.87G/3.35G [03:06<02:50, 8.72MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  74% 2.29G/3.10G [03:07<01:39, 8.17MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  63% 2.04G/3.25G [03:01<02:46, 7.23MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  56% 1.88G/3.35G [03:06<02:45, 8.90MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  74% 2.60G/3.50G [03:02<01:54, 7.91MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  56% 1.88G/3.35G [03:06<03:00, 8.17MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  88% 2.84G/3.25G [03:07<00:38, 10.5MB/s]\u001b[A\u001b[A\n",
            "tf_model.h5:  82% 2.54G/3.10G [03:01<00:40, 13.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  56% 1.88G/3.35G [03:07<02:15, 10.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  74% 2.30G/3.10G [03:07<01:15, 10.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  63% 2.06G/3.25G [03:02<02:01, 9.78MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  56% 1.89G/3.35G [03:07<02:05, 11.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  75% 2.61G/3.50G [03:02<01:24, 10.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  64% 2.07G/3.25G [03:02<01:28, 13.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  82% 2.55G/3.10G [03:01<00:31, 17.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  56% 1.89G/3.35G [03:07<01:44, 14.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  56% 1.89G/3.35G [03:06<02:18, 10.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  88% 2.85G/3.25G [03:08<00:29, 13.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  57% 1.90G/3.35G [03:07<01:36, 15.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  75% 2.62G/3.50G [03:02<01:01, 14.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  64% 2.08G/3.25G [03:02<01:19, 14.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  75% 2.31G/3.10G [03:08<01:09, 11.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  57% 1.90G/3.35G [03:10<04:14, 5.71MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  83% 2.56G/3.10G [03:05<01:28, 6.08MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  57% 1.90G/3.35G [03:12<04:14, 5.72MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  75% 2.63G/3.50G [03:07<02:54, 5.00MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  88% 2.86G/3.25G [03:13<01:18, 4.88MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  75% 2.32G/3.10G [03:13<02:35, 5.01MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  57% 1.91G/3.35G [03:11<03:57, 6.08MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  83% 2.57G/3.10G [03:06<01:15, 7.00MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  57% 1.91G/3.35G [03:12<03:35, 6.71MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  75% 2.64G/3.50G [03:07<02:03, 6.96MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  88% 2.87G/3.25G [03:13<00:54, 6.81MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  57% 1.91G/3.35G [03:12<04:44, 5.08MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  64% 2.09G/3.25G [03:07<03:43, 5.19MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  57% 1.92G/3.35G [03:11<02:54, 8.20MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  83% 2.58G/3.10G [03:07<00:55, 9.26MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  57% 1.92G/3.35G [03:13<02:41, 8.87MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  76% 2.65G/3.50G [03:08<01:31, 9.28MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  75% 2.33G/3.10G [03:13<01:55, 6.63MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  58% 1.93G/3.35G [03:13<02:00, 11.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  65% 2.10G/3.25G [03:08<02:46, 6.92MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  89% 2.88G/3.25G [03:14<00:41, 8.73MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  57% 1.92G/3.35G [03:13<03:35, 6.66MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  58% 1.93G/3.35G [03:12<02:15, 10.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  84% 2.59G/3.10G [03:07<00:42, 11.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  76% 2.34G/3.10G [03:13<01:25, 8.89MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  58% 1.94G/3.35G [03:13<01:32, 15.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  76% 2.66G/3.50G [03:08<01:12, 11.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  58% 1.93G/3.35G [03:13<02:40, 8.86MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  89% 2.89G/3.25G [03:14<00:30, 11.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  65% 2.11G/3.25G [03:08<02:07, 8.95MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  58% 1.94G/3.35G [03:12<01:49, 12.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  58% 1.95G/3.35G [03:13<01:12, 19.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  84% 2.60G/3.10G [03:07<00:34, 14.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  76% 2.35G/3.10G [03:14<01:05, 11.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  89% 2.90G/3.25G [03:14<00:22, 14.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  58% 1.94G/3.35G [03:13<02:01, 11.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  76% 2.67G/3.50G [03:08<00:57, 14.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  65% 2.12G/3.25G [03:08<01:35, 11.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  58% 1.95G/3.35G [03:12<01:24, 16.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  84% 2.61G/3.10G [03:08<00:26, 18.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  58% 1.96G/3.35G [03:14<01:01, 22.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  90% 2.92G/3.25G [03:14<00:17, 18.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  76% 2.36G/3.10G [03:14<00:50, 14.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  58% 1.95G/3.35G [03:13<01:35, 14.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  77% 2.68G/3.50G [03:08<00:45, 18.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  66% 2.13G/3.25G [03:08<01:11, 15.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  58% 1.96G/3.35G [03:12<01:05, 21.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  85% 2.62G/3.10G [03:08<00:20, 23.0MB/s]\u001b[A\n",
            "\n",
            "model.safetensors:  90% 2.93G/3.25G [03:14<00:13, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  59% 1.97G/3.35G [03:14<00:49, 27.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  58% 1.96G/3.35G [03:14<01:14, 18.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  77% 2.69G/3.50G [03:09<00:35, 22.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  66% 2.14G/3.25G [03:09<00:58, 19.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  77% 2.71G/3.50G [03:09<00:29, 26.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  59% 1.97G/3.35G [03:14<01:01, 22.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  90% 2.94G/3.25G [03:15<00:12, 25.3MB/s]\u001b[A\u001b[A\n",
            "tf_model.h5:  85% 2.63G/3.10G [03:08<00:19, 23.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  77% 2.37G/3.10G [03:15<00:47, 15.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  66% 2.15G/3.25G [03:12<02:24, 7.61MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  59% 1.98G/3.35G [03:18<03:03, 7.46MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  59% 1.98G/3.35G [03:16<02:32, 9.02MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  77% 2.38G/3.10G [03:19<02:00, 5.92MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  67% 2.16G/3.25G [03:13<02:20, 7.71MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  59% 1.98G/3.35G [03:18<03:38, 6.27MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  59% 1.99G/3.35G [03:19<02:47, 8.14MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  59% 1.99G/3.35G [03:17<02:27, 9.24MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  77% 2.72G/3.50G [03:14<02:05, 6.28MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  77% 2.39G/3.10G [03:19<01:27, 8.03MB/s]\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  85% 2.64G/3.10G [03:13<01:13, 6.14MB/s]\u001b[A\n",
            "\n",
            "model.safetensors:  91% 2.95G/3.25G [03:19<00:48, 6.15MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  67% 2.17G/3.25G [03:14<01:48, 9.89MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  59% 1.99G/3.35G [03:19<02:46, 8.18MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  60% 2.00G/3.35G [03:19<02:10, 10.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  60% 2.00G/3.35G [03:18<02:00, 11.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  86% 2.65G/3.10G [03:13<00:53, 8.23MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  78% 2.73G/3.50G [03:14<01:37, 8.02MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  78% 2.40G/3.10G [03:20<01:09, 9.96MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  91% 2.96G/3.25G [03:20<00:36, 7.85MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  60% 2.01G/3.35G [03:19<01:44, 12.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  67% 2.18G/3.25G [03:14<01:27, 12.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  86% 2.66G/3.10G [03:13<00:40, 10.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  60% 2.01G/3.35G [03:18<01:39, 13.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  60% 2.00G/3.35G [03:19<02:11, 10.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  78% 2.74G/3.50G [03:14<01:11, 10.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  60% 2.02G/3.35G [03:19<01:18, 16.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  78% 2.41G/3.10G [03:20<00:53, 12.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  91% 2.97G/3.25G [03:20<00:26, 10.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  67% 2.19G/3.25G [03:14<01:06, 15.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  60% 2.02G/3.35G [03:18<01:18, 16.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  60% 2.01G/3.35G [03:19<01:39, 13.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  86% 2.67G/3.10G [03:14<00:30, 13.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  78% 2.75G/3.50G [03:14<00:54, 13.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  92% 2.98G/3.25G [03:20<00:19, 14.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  61% 2.03G/3.35G [03:20<01:05, 20.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  87% 2.68G/3.10G [03:14<00:22, 18.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  61% 2.03G/3.35G [03:19<01:03, 20.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  60% 2.02G/3.35G [03:19<01:17, 17.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  79% 2.76G/3.50G [03:15<00:41, 17.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  68% 2.21G/3.25G [03:15<00:43, 23.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  92% 2.99G/3.25G [03:21<00:15, 16.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  61% 2.04G/3.35G [03:20<00:58, 22.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  78% 2.42G/3.10G [03:21<00:49, 13.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  87% 2.69G/3.10G [03:14<00:20, 19.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  61% 2.04G/3.35G [03:19<01:13, 17.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  79% 2.77G/3.50G [03:17<01:26, 8.48MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  79% 2.78G/3.50G [03:18<01:03, 11.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  79% 2.43G/3.10G [03:23<01:24, 7.89MB/s]\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  87% 2.71G/3.10G [03:17<00:43, 9.05MB/s]\u001b[A\n",
            "\n",
            "model.safetensors:  92% 3.00G/3.25G [03:23<00:30, 8.22MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  61% 2.03G/3.35G [03:22<02:47, 7.86MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  61% 2.06G/3.35G [03:22<02:17, 9.41MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  68% 2.22G/3.25G [03:18<01:49, 9.33MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  80% 2.79G/3.50G [03:22<02:05, 5.69MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  61% 2.04G/3.35G [03:26<04:24, 4.96MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  93% 3.01G/3.25G [03:27<00:47, 5.04MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  62% 2.07G/3.35G [03:26<03:56, 5.45MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  69% 2.23G/3.25G [03:22<02:54, 5.80MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  88% 2.72G/3.10G [03:21<01:13, 5.16MB/s]\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  79% 2.44G/3.10G [03:27<02:14, 4.86MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  61% 2.06G/3.35G [03:27<04:52, 4.44MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  61% 2.06G/3.35G [03:27<03:12, 6.75MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  93% 3.02G/3.25G [03:28<00:33, 6.85MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  80% 2.80G/3.50G [03:22<01:32, 7.64MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  79% 2.45G/3.10G [03:27<01:35, 6.72MB/s]\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  88% 2.73G/3.10G [03:21<00:52, 7.10MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  62% 2.08G/3.35G [03:26<02:52, 7.38MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  69% 2.24G/3.25G [03:22<02:11, 7.61MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  62% 2.07G/3.35G [03:27<03:29, 6.16MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  93% 3.03G/3.25G [03:29<00:29, 7.46MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  62% 2.07G/3.35G [03:28<02:55, 7.33MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  80% 2.46G/3.10G [03:29<01:28, 7.11MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  69% 2.25G/3.25G [03:23<02:09, 7.69MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  62% 2.09G/3.35G [03:27<02:49, 7.49MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  80% 2.81G/3.50G [03:23<01:31, 7.61MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  62% 2.08G/3.35G [03:28<03:15, 6.55MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  94% 3.04G/3.25G [03:29<00:21, 9.79MB/s]\u001b[A\u001b[A\n",
            "tf_model.h5:  88% 2.74G/3.10G [03:22<00:49, 7.22MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  62% 2.08G/3.35G [03:28<02:12, 9.62MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  80% 2.82G/3.50G [03:23<01:07, 10.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  63% 2.10G/3.35G [03:27<02:08, 9.77MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  70% 2.26G/3.25G [03:24<01:40, 9.74MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  80% 2.47G/3.10G [03:29<01:09, 8.98MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  94% 3.05G/3.25G [03:29<00:16, 12.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  62% 2.09G/3.35G [03:29<02:28, 8.51MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  62% 2.09G/3.35G [03:28<01:45, 12.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  89% 2.75G/3.10G [03:23<00:39, 8.95MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  81% 2.83G/3.50G [03:24<00:52, 12.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  80% 2.49G/3.10G [03:29<00:52, 11.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  63% 2.11G/3.35G [03:28<01:41, 12.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  70% 2.28G/3.25G [03:24<01:19, 12.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  63% 2.10G/3.35G [03:29<01:53, 11.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  94% 3.06G/3.25G [03:30<00:12, 14.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  63% 2.10G/3.35G [03:29<01:26, 14.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  89% 2.76G/3.10G [03:23<00:28, 11.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  81% 2.84G/3.50G [03:24<00:47, 13.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  81% 2.50G/3.10G [03:30<00:46, 12.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  63% 2.12G/3.35G [03:29<02:09, 9.55MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  70% 2.29G/3.25G [03:28<02:36, 6.12MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  89% 2.77G/3.10G [03:27<00:53, 6.09MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  63% 2.11G/3.35G [03:33<03:34, 5.81MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  63% 2.13G/3.35G [03:32<02:46, 7.36MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  81% 2.85G/3.50G [03:28<01:35, 6.83MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  71% 2.30G/3.25G [03:28<01:52, 8.44MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  90% 2.78G/3.10G [03:27<00:37, 8.46MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  63% 2.12G/3.35G [03:33<02:32, 8.09MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  63% 2.11G/3.35G [03:33<03:18, 6.28MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  81% 2.51G/3.10G [03:34<01:30, 6.54MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  95% 3.07G/3.25G [03:34<00:28, 6.23MB/s]\u001b[A\u001b[A\n",
            "tf_model.h5:  90% 2.79G/3.10G [03:27<00:27, 11.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  63% 2.13G/3.35G [03:33<01:55, 10.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  64% 2.15G/3.35G [03:32<01:38, 12.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  82% 2.87G/3.50G [03:28<00:55, 11.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  71% 2.31G/3.25G [03:28<01:29, 10.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  81% 2.52G/3.10G [03:34<01:06, 8.69MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  95% 3.08G/3.25G [03:34<00:19, 8.31MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  63% 2.12G/3.35G [03:33<02:29, 8.28MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  64% 2.14G/3.35G [03:33<01:27, 13.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  64% 2.16G/3.35G [03:32<01:19, 15.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  90% 2.80G/3.10G [03:28<00:21, 14.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  82% 2.88G/3.50G [03:28<00:43, 14.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  71% 2.32G/3.25G [03:28<01:06, 14.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  82% 2.53G/3.10G [03:34<00:48, 11.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  95% 3.09G/3.25G [03:34<00:13, 11.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  63% 2.13G/3.35G [03:33<01:50, 11.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  64% 2.15G/3.35G [03:34<01:04, 18.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  72% 2.33G/3.25G [03:29<00:51, 17.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  91% 2.81G/3.10G [03:28<00:17, 16.5MB/s]\u001b[A\n",
            "\n",
            "model.safetensors:  96% 3.10G/3.25G [03:35<00:10, 13.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  65% 2.17G/3.35G [03:33<01:11, 16.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  83% 2.89G/3.50G [03:29<00:38, 15.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  82% 2.54G/3.10G [03:35<00:46, 12.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  64% 2.14G/3.35G [03:34<01:51, 10.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  72% 2.34G/3.25G [03:34<02:52, 5.28MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  65% 2.18G/3.35G [03:38<03:25, 5.70MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  83% 2.90G/3.50G [03:34<01:46, 5.64MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  64% 2.15G/3.35G [03:39<03:50, 5.22MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  96% 3.11G/3.25G [03:40<00:26, 5.06MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  82% 2.55G/3.10G [03:40<01:45, 5.19MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  64% 2.16G/3.35G [03:39<03:55, 5.07MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  91% 2.82G/3.10G [03:33<00:53, 5.13MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  72% 2.35G/3.25G [03:34<02:04, 7.24MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  83% 2.92G/3.50G [03:34<01:18, 7.47MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  65% 2.19G/3.35G [03:38<02:34, 7.50MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  64% 2.16G/3.35G [03:39<02:49, 7.04MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  83% 2.56G/3.10G [03:40<01:16, 7.02MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  96% 3.12G/3.25G [03:40<00:17, 6.84MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  73% 2.36G/3.25G [03:34<01:33, 9.54MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  83% 2.57G/3.10G [03:40<00:55, 9.52MB/s]\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  91% 2.83G/3.10G [03:34<00:38, 6.86MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  65% 2.17G/3.35G [03:40<02:56, 6.71MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  65% 2.17G/3.35G [03:39<02:08, 9.20MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  97% 3.14G/3.25G [03:40<00:12, 8.96MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  83% 2.93G/3.50G [03:34<01:01, 9.39MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  73% 2.37G/3.25G [03:35<01:09, 12.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  66% 2.20G/3.35G [03:39<02:03, 9.34MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  83% 2.58G/3.10G [03:40<00:41, 12.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  92% 2.84G/3.10G [03:34<00:27, 9.19MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  65% 2.18G/3.35G [03:39<01:34, 12.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  65% 2.18G/3.35G [03:40<02:11, 8.92MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  73% 2.38G/3.25G [03:35<00:51, 16.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  97% 3.15G/3.25G [03:41<00:08, 11.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  84% 2.94G/3.50G [03:35<00:46, 12.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  66% 2.21G/3.35G [03:39<01:32, 12.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  65% 2.19G/3.35G [03:40<01:35, 12.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  65% 2.19G/3.35G [03:40<01:15, 15.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  92% 2.85G/3.10G [03:34<00:21, 11.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  74% 2.39G/3.25G [03:35<00:42, 20.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  97% 3.16G/3.25G [03:41<00:06, 14.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  84% 2.95G/3.50G [03:35<00:36, 15.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  66% 2.22G/3.35G [03:39<01:15, 15.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  84% 2.59G/3.10G [03:41<00:36, 14.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  66% 2.20G/3.35G [03:40<01:17, 14.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  66% 2.20G/3.35G [03:40<01:05, 17.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  92% 2.86G/3.10G [03:35<00:16, 13.8MB/s]\u001b[A\n",
            "\n",
            "model.safetensors:  98% 3.17G/3.25G [03:41<00:04, 16.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  74% 2.40G/3.25G [03:35<00:40, 20.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  84% 2.60G/3.10G [03:42<00:37, 13.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  84% 2.96G/3.50G [03:39<01:23, 6.58MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  66% 2.21G/3.35G [03:45<03:31, 5.38MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  93% 2.87G/3.10G [03:39<00:40, 5.48MB/s]\u001b[A\n",
            "\n",
            "model.safetensors:  98% 3.18G/3.25G [03:46<00:12, 5.78MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  85% 2.97G/3.50G [03:40<01:16, 7.02MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  93% 2.88G/3.10G [03:39<00:27, 7.64MB/s]\u001b[A\n",
            "\n",
            "model.safetensors:  98% 3.19G/3.25G [03:46<00:07, 8.02MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  85% 2.98G/3.50G [03:40<00:54, 9.63MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  67% 2.23G/3.35G [03:45<01:56, 9.59MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  67% 2.23G/3.35G [03:44<03:33, 5.25MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  74% 2.41G/3.25G [03:40<02:22, 5.87MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  66% 2.21G/3.35G [03:45<03:25, 5.56MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  84% 2.61G/3.10G [03:46<01:24, 5.76MB/s]\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  93% 2.89G/3.10G [03:40<00:20, 10.0MB/s]\u001b[A\n",
            "\n",
            "model.safetensors:  98% 3.20G/3.25G [03:46<00:04, 10.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  66% 2.22G/3.35G [03:45<02:30, 7.50MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  67% 2.24G/3.35G [03:44<02:39, 6.98MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  85% 2.99G/3.50G [03:41<00:43, 11.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  75% 2.42G/3.25G [03:41<01:47, 7.68MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  94% 2.90G/3.10G [03:40<00:14, 13.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  67% 2.24G/3.35G [03:46<01:38, 11.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  85% 2.62G/3.10G [03:46<01:02, 7.57MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  99% 3.21G/3.25G [03:47<00:02, 13.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  67% 2.25G/3.35G [03:45<01:59, 9.20MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  86% 3.00G/3.50G [03:41<00:33, 15.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  67% 2.23G/3.35G [03:46<01:55, 9.68MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  85% 2.63G/3.10G [03:46<00:46, 10.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  67% 2.25G/3.35G [03:46<01:19, 13.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  94% 2.92G/3.10G [03:40<00:11, 16.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  75% 2.43G/3.25G [03:41<01:21, 9.96MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  68% 2.26G/3.35G [03:45<01:27, 12.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  99% 3.22G/3.25G [03:47<00:01, 16.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  67% 2.24G/3.35G [03:46<01:24, 13.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  86% 3.01G/3.50G [03:41<00:25, 19.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  94% 2.93G/3.10G [03:40<00:08, 21.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  68% 2.26G/3.35G [03:46<01:01, 17.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  85% 2.64G/3.10G [03:47<00:33, 13.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  75% 2.44G/3.25G [03:41<00:59, 13.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors:  99% 3.23G/3.25G [03:47<00:00, 21.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  68% 2.28G/3.35G [03:45<01:06, 16.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  67% 2.25G/3.35G [03:46<01:05, 16.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  86% 3.02G/3.50G [03:41<00:20, 23.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  95% 2.94G/3.10G [03:40<00:06, 23.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  68% 2.28G/3.35G [03:46<00:53, 20.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  86% 2.65G/3.10G [03:47<00:27, 16.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  76% 2.45G/3.25G [03:41<00:50, 15.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  68% 2.29G/3.35G [03:50<03:07, 5.69MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors: 100% 3.24G/3.25G [03:52<00:01, 6.01MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  86% 3.03G/3.50G [03:46<01:20, 5.87MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors: 100% 3.25G/3.25G [03:52<00:00, 7.14MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "model.safetensors: 100% 3.25G/3.25G [03:52<00:00, 14.0MB/s]\n",
            "Download complete. Moving file to /content/LMOps/minillm/checkpoints/gpt2-large/model.safetensors\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  76% 2.46G/3.25G [03:46<02:20, 5.55MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  86% 2.66G/3.10G [03:52<01:18, 5.50MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  68% 2.30G/3.35G [03:50<02:23, 7.38MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  68% 2.28G/3.35G [03:51<02:23, 7.52MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  87% 3.04G/3.50G [03:46<00:57, 8.03MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  68% 2.29G/3.35G [03:51<03:02, 5.86MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  95% 2.95G/3.10G [03:46<00:26, 5.77MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  76% 2.47G/3.25G [03:46<01:42, 7.55MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  69% 2.31G/3.35G [03:50<01:47, 9.75MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'tokenizer.json' to '/content/LMOps/minillm/checkpoints/gpt2-large/.cache/huggingface/download/HgM_lKo9sdSCfRtVg7MMFS7EKqo=.4b988bccc9dc5adacd403c00b4704976196548f8.incomplete'\n",
            "\n",
            "tf_model.h5:  95% 2.96G/3.10G [03:46<00:17, 7.96MB/s]\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  86% 2.67G/3.10G [03:52<00:58, 7.28MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  68% 2.30G/3.35G [03:52<02:15, 7.78MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  69% 2.32G/3.35G [03:51<01:19, 13.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  68% 2.29G/3.35G [03:52<01:49, 9.73MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  87% 3.05G/3.50G [03:47<00:44, 10.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  77% 2.49G/3.25G [03:47<01:15, 10.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  87% 2.68G/3.10G [03:52<00:41, 10.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "tokenizer.json:   0% 0.00/1.36M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "tf_model.h5:  96% 2.97G/3.10G [03:46<00:12, 10.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  69% 2.31G/3.35G [03:52<01:42, 10.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  69% 2.33G/3.35G [03:51<01:01, 16.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  77% 2.50G/3.25G [03:47<00:57, 13.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  68% 2.30G/3.35G [03:52<01:25, 12.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  87% 3.06G/3.50G [03:47<00:34, 12.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  87% 2.69G/3.10G [03:53<00:32, 12.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  70% 2.34G/3.35G [03:51<00:49, 20.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  69% 2.32G/3.35G [03:52<01:19, 13.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  96% 2.98G/3.10G [03:46<00:09, 13.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  69% 2.31G/3.35G [03:52<01:04, 16.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  77% 2.51G/3.25G [03:47<00:44, 16.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  88% 3.07G/3.50G [03:47<00:25, 17.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 2.96MB/s]\n",
            "Download complete. Moving file to /content/LMOps/minillm/checkpoints/gpt2-large/tokenizer.json\n",
            "\n",
            "\n",
            "\n",
            "flax_model.msgpack:  87% 2.71G/3.10G [03:53<00:23, 16.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  70% 2.35G/3.35G [03:51<00:37, 26.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  69% 2.33G/3.35G [03:52<00:59, 17.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  97% 2.99G/3.10G [03:46<00:06, 16.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  88% 2.72G/3.10G [03:53<00:17, 21.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  88% 3.08G/3.50G [03:47<00:19, 21.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  69% 2.32G/3.35G [03:52<00:51, 20.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  78% 2.52G/3.25G [03:47<00:35, 20.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  70% 2.36G/3.35G [03:51<00:31, 31.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  70% 2.34G/3.35G [03:53<00:45, 22.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'tokenizer_config.json' to '/content/LMOps/minillm/checkpoints/gpt2-large/.cache/huggingface/download/vzaExXFZNBay89bvlQv-ZcI6BTg=.be4d21d94f3b4687e5a54d84bf6ab46ed0f8defd.incomplete'\n",
            "\n",
            "tf_model.h5:  97% 3.00G/3.10G [03:47<00:04, 21.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  88% 3.09G/3.50G [03:48<00:15, 26.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  69% 2.33G/3.35G [03:52<00:44, 23.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 139kB/s]\n",
            "Download complete. Moving file to /content/LMOps/minillm/checkpoints/gpt2-large/tokenizer_config.json\n",
            "\n",
            "\n",
            "\n",
            "flax_model.msgpack:  88% 2.73G/3.10G [03:53<00:15, 23.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  78% 2.53G/3.25G [03:48<00:31, 22.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  71% 2.37G/3.35G [03:52<00:31, 30.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  70% 2.35G/3.35G [03:53<00:42, 23.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  97% 3.01G/3.10G [03:47<00:03, 23.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  89% 3.10G/3.50G [03:48<00:16, 24.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'vocab.json' to '/content/LMOps/minillm/checkpoints/gpt2-large/.cache/huggingface/download/j3m-Hy6QvBddw8RXA1uSWl1AJ0c=.1f1d9aaca301414e7f6c9396df506798ff4eb9a6.incomplete'\n",
            "\n",
            "\n",
            "vocab.json:   0% 0.00/1.04M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "vocab.json: 100% 1.04M/1.04M [00:02<00:00, 378kB/s] \n",
            "Download complete. Moving file to /content/LMOps/minillm/checkpoints/gpt2-large/vocab.json\n",
            "\n",
            "\n",
            "\n",
            "flax_model.msgpack:  88% 2.74G/3.10G [03:58<00:57, 6.21MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  78% 2.54G/3.25G [03:52<01:54, 6.19MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  98% 3.02G/3.10G [03:51<00:12, 6.37MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  71% 2.38G/3.35G [03:56<02:28, 6.56MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  70% 2.36G/3.35G [03:57<02:37, 6.32MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  89% 3.11G/3.50G [03:52<01:00, 6.49MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  70% 2.34G/3.35G [03:57<02:49, 5.99MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  71% 2.39G/3.35G [03:57<02:12, 7.28MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  78% 2.55G/3.25G [03:54<01:47, 6.50MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  98% 3.03G/3.10G [03:53<00:10, 6.49MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  71% 2.37G/3.35G [04:00<02:50, 5.76MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  70% 2.35G/3.35G [04:01<03:49, 4.38MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  89% 3.12G/3.50G [03:56<01:23, 4.52MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  79% 2.56G/3.25G [03:56<02:07, 5.42MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  72% 2.40G/3.35G [04:00<02:54, 5.47MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  89% 2.76G/3.10G [04:02<01:00, 5.63MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  70% 2.36G/3.35G [04:01<02:42, 6.13MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  72% 2.41G/3.35G [04:00<02:03, 7.64MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  71% 2.38G/3.35G [04:02<02:54, 5.58MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  89% 2.77G/3.10G [04:02<00:44, 7.35MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  79% 2.57G/3.25G [03:56<01:30, 7.53MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  89% 3.14G/3.50G [03:57<00:59, 6.24MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  98% 3.04G/3.10G [03:56<00:10, 5.33MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  71% 2.37G/3.35G [04:02<02:00, 8.14MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  72% 2.42G/3.35G [04:01<01:32, 10.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  90% 2.78G/3.10G [04:02<00:34, 9.32MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  79% 2.58G/3.25G [03:57<01:07, 9.91MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  71% 2.39G/3.35G [04:02<02:08, 7.47MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  90% 3.15G/3.50G [03:57<00:43, 8.25MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  71% 2.38G/3.35G [04:02<01:29, 10.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  99% 3.05G/3.10G [03:56<00:06, 7.07MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  80% 2.59G/3.25G [03:57<00:49, 13.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  90% 2.79G/3.10G [04:03<00:25, 11.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  73% 2.43G/3.35G [04:01<01:11, 12.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  72% 2.40G/3.35G [04:02<01:36, 9.86MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  90% 3.16G/3.50G [03:57<00:31, 11.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  71% 2.39G/3.35G [04:02<01:05, 14.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  99% 3.06G/3.10G [03:56<00:03, 9.63MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  80% 2.60G/3.25G [03:57<00:37, 17.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  90% 2.80G/3.10G [04:03<00:19, 15.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  73% 2.44G/3.35G [04:01<00:53, 17.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  72% 2.41G/3.35G [04:02<01:10, 13.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  90% 3.17G/3.50G [03:57<00:23, 14.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  72% 2.40G/3.35G [04:02<00:52, 18.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5:  99% 3.07G/3.10G [03:57<00:01, 12.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  80% 2.61G/3.25G [03:57<00:30, 20.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  73% 2.45G/3.35G [04:01<00:44, 20.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  91% 2.81G/3.10G [04:03<00:16, 17.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  72% 2.42G/3.35G [04:03<00:58, 15.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  91% 3.18G/3.50G [03:58<00:22, 14.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  72% 2.41G/3.35G [04:05<01:50, 8.51MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  81% 2.62G/3.25G [04:00<01:15, 8.29MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  73% 2.43G/3.35G [04:06<01:57, 7.85MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5: 100% 3.08G/3.10G [04:00<00:01, 6.92MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  73% 2.46G/3.35G [04:04<01:47, 8.28MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  91% 3.19G/3.50G [04:00<00:37, 8.38MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  91% 2.82G/3.10G [04:06<00:33, 8.14MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  72% 2.42G/3.35G [04:05<01:29, 10.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  81% 2.63G/3.25G [04:01<00:59, 10.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  73% 2.44G/3.35G [04:06<01:33, 9.76MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  74% 2.47G/3.35G [04:05<01:26, 10.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  91% 2.83G/3.10G [04:07<00:26, 10.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  91% 3.20G/3.50G [04:01<00:29, 10.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  73% 2.43G/3.35G [04:06<01:13, 12.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5: 100% 3.09G/3.10G [04:00<00:00, 8.58MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  81% 2.64G/3.25G [04:01<00:44, 13.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  73% 2.45G/3.35G [04:06<01:08, 13.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  74% 2.49G/3.35G [04:05<01:06, 13.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  92% 2.84G/3.10G [04:07<00:20, 12.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "tf_model.h5: 100% 3.10G/3.10G [04:01<00:00, 8.57MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  92% 3.21G/3.50G [04:01<00:23, 12.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  73% 2.44G/3.35G [04:06<01:01, 14.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tf_model.h5: 100% 3.10G/3.10G [04:01<00:00, 12.8MB/s]\n",
            "Download complete. Moving file to /content/LMOps/minillm/checkpoints/gpt2-large/tf_model.h5\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  73% 2.46G/3.35G [04:07<01:01, 14.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  74% 2.50G/3.35G [04:06<01:00, 14.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  82% 2.66G/3.25G [04:02<00:32, 18.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  92% 3.22G/3.50G [04:02<00:19, 14.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  92% 2.85G/3.10G [04:07<00:17, 14.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  74% 2.47G/3.35G [04:07<00:49, 17.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  73% 2.45G/3.35G [04:07<00:54, 16.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  75% 2.51G/3.35G [04:06<00:45, 18.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  82% 2.67G/3.25G [04:02<00:26, 22.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  92% 2.86G/3.10G [04:08<00:13, 17.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  92% 3.23G/3.50G [04:02<00:15, 17.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  73% 2.46G/3.35G [04:07<00:46, 19.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  74% 2.49G/3.35G [04:07<00:42, 20.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  75% 2.52G/3.35G [04:06<00:42, 19.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  83% 2.68G/3.25G [04:02<00:24, 23.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  93% 2.87G/3.10G [04:08<00:11, 19.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  74% 2.50G/3.35G [04:08<00:35, 23.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  92% 3.24G/3.50G [04:03<00:13, 20.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  75% 2.53G/3.35G [04:07<00:32, 25.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  83% 2.69G/3.25G [04:03<00:18, 29.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  75% 2.51G/3.35G [04:08<00:27, 30.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  93% 3.25G/3.50G [04:03<00:10, 25.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  74% 2.47G/3.35G [04:08<00:43, 20.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  76% 2.54G/3.35G [04:07<00:26, 30.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  83% 2.71G/3.25G [04:03<00:15, 34.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  93% 2.88G/3.10G [04:08<00:09, 22.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  75% 2.52G/3.35G [04:08<00:24, 34.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  93% 3.26G/3.50G [04:03<00:08, 28.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  74% 2.49G/3.35G [04:08<00:37, 23.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  76% 2.55G/3.35G [04:07<00:25, 31.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  84% 2.72G/3.25G [04:03<00:15, 33.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  93% 3.27G/3.50G [04:03<00:07, 32.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  75% 2.53G/3.35G [04:08<00:24, 33.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  74% 2.50G/3.35G [04:08<00:31, 26.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  84% 2.73G/3.25G [04:03<00:13, 39.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  76% 2.56G/3.35G [04:07<00:23, 33.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  94% 3.28G/3.50G [04:03<00:06, 36.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  76% 2.54G/3.35G [04:09<00:21, 38.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  75% 2.51G/3.35G [04:08<00:26, 32.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  84% 2.74G/3.25G [04:03<00:11, 43.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  77% 2.57G/3.35G [04:08<00:21, 37.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  76% 2.55G/3.35G [04:09<00:21, 37.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  94% 3.29G/3.50G [04:04<00:05, 35.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  93% 2.89G/3.10G [04:09<00:11, 17.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  75% 2.52G/3.35G [04:09<00:27, 30.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  77% 2.58G/3.35G [04:08<00:21, 36.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  94% 3.30G/3.50G [04:04<00:05, 39.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  77% 2.59G/3.35G [04:08<00:18, 41.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  76% 2.56G/3.35G [04:09<00:21, 36.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  95% 3.31G/3.50G [04:04<00:04, 44.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  75% 2.53G/3.35G [04:09<00:24, 34.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  77% 2.57G/3.35G [04:09<00:19, 41.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  78% 2.60G/3.35G [04:08<00:16, 45.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  95% 3.32G/3.50G [04:04<00:03, 47.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  76% 2.54G/3.35G [04:09<00:21, 37.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  77% 2.58G/3.35G [04:09<00:16, 45.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  85% 2.75G/3.25G [04:04<00:20, 23.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  95% 3.33G/3.50G [04:04<00:03, 51.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  76% 2.55G/3.35G [04:09<00:18, 42.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  77% 2.59G/3.35G [04:10<00:14, 51.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  78% 2.61G/3.35G [04:08<00:18, 39.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  95% 3.34G/3.50G [04:05<00:02, 54.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  76% 2.56G/3.35G [04:09<00:16, 47.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  94% 2.90G/3.10G [04:10<00:12, 15.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  78% 2.60G/3.35G [04:10<00:13, 54.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  96% 3.36G/3.50G [04:05<00:02, 60.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  77% 2.57G/3.35G [04:10<00:14, 54.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  78% 2.61G/3.35G [04:10<00:13, 54.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  96% 3.37G/3.50G [04:05<00:02, 60.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  77% 2.58G/3.35G [04:10<00:14, 52.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  85% 2.76G/3.25G [04:05<00:23, 20.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  77% 2.59G/3.35G [04:10<00:16, 47.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  94% 2.92G/3.10G [04:11<00:12, 14.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  97% 3.39G/3.50G [04:05<00:02, 44.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  78% 2.62G/3.35G [04:11<00:24, 30.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  78% 2.62G/3.35G [04:10<00:34, 21.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  85% 2.77G/3.25G [04:06<00:28, 16.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  94% 2.93G/3.10G [04:12<00:12, 14.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  78% 2.63G/3.35G [04:12<01:17, 9.36MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  78% 2.60G/3.35G [04:13<01:24, 8.87MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  78% 2.63G/3.35G [04:14<01:19, 9.04MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  79% 2.64G/3.35G [04:13<01:03, 11.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  86% 2.78G/3.25G [04:09<00:55, 8.38MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  95% 2.94G/3.10G [04:14<00:19, 8.25MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  78% 2.61G/3.35G [04:14<01:00, 12.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  79% 2.64G/3.35G [04:14<00:57, 12.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  86% 2.79G/3.25G [04:09<00:39, 11.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  97% 3.40G/3.50G [04:09<00:10, 10.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  78% 2.62G/3.35G [04:14<00:45, 16.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  97% 3.41G/3.50G [04:09<00:07, 13.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  96% 2.96G/3.10G [04:15<00:09, 14.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  79% 2.65G/3.35G [04:14<00:44, 15.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  86% 2.80G/3.25G [04:09<00:29, 15.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  78% 2.63G/3.35G [04:14<00:35, 20.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  98% 3.42G/3.50G [04:09<00:04, 17.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  87% 2.81G/3.25G [04:09<00:22, 19.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  96% 2.97G/3.10G [04:15<00:07, 17.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  98% 3.43G/3.50G [04:09<00:03, 22.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  79% 2.64G/3.35G [04:14<00:27, 26.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  79% 2.66G/3.35G [04:14<00:36, 18.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  79% 2.66G/3.35G [04:13<00:42, 16.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  98% 3.44G/3.50G [04:09<00:02, 25.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  79% 2.65G/3.35G [04:14<00:24, 28.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  96% 2.98G/3.10G [04:15<00:06, 18.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  87% 2.82G/3.25G [04:10<00:23, 17.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  80% 2.67G/3.35G [04:14<00:48, 14.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  80% 2.67G/3.35G [04:15<00:45, 14.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  87% 2.83G/3.25G [04:13<00:49, 8.35MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  98% 3.45G/3.50G [04:13<00:06, 8.22MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  80% 2.68G/3.35G [04:17<01:19, 8.44MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  80% 2.68G/3.35G [04:18<01:22, 8.15MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  99% 3.47G/3.50G [04:13<00:02, 14.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  88% 2.86G/3.25G [04:13<00:22, 16.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  80% 2.69G/3.35G [04:17<00:58, 11.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  80% 2.69G/3.35G [04:18<00:58, 11.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot:  99% 3.48G/3.50G [04:13<00:01, 18.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  79% 2.66G/3.35G [04:18<01:28, 7.77MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  81% 2.71G/3.35G [04:17<00:44, 14.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  80% 2.67G/3.35G [04:18<01:03, 10.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  81% 2.71G/3.35G [04:18<00:44, 14.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot: 100% 3.49G/3.50G [04:13<00:00, 21.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  97% 2.99G/3.10G [04:19<00:14, 7.32MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "rust_model.ot: 100% 3.50G/3.50G [04:13<00:00, 26.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  80% 2.68G/3.35G [04:18<00:47, 14.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "rust_model.ot: 100% 3.50G/3.50G [04:14<00:00, 13.8MB/s]\n",
            "Download complete. Moving file to /content/LMOps/minillm/checkpoints/gpt2-large/rust_model.ot\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  81% 2.71G/3.35G [04:18<00:26, 24.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  98% 3.02G/3.10G [04:19<00:04, 16.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  88% 2.87G/3.25G [04:14<00:22, 16.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  99% 3.05G/3.10G [04:19<00:01, 30.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  81% 2.72G/3.35G [04:18<00:41, 15.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  81% 2.72G/3.35G [04:19<00:22, 28.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  81% 2.72G/3.35G [04:19<00:39, 16.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack:  99% 3.07G/3.10G [04:20<00:00, 38.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  89% 2.88G/3.25G [04:14<00:22, 16.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  81% 2.73G/3.35G [04:20<00:42, 14.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  81% 2.73G/3.35G [04:20<00:29, 20.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  81% 2.73G/3.35G [04:19<00:43, 14.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "flax_model.msgpack: 100% 3.10G/3.10G [04:21<00:00, 11.9MB/s]\n",
            "Download complete. Moving file to /content/LMOps/minillm/checkpoints/gpt2-large/flax_model.msgpack\n",
            "Fetching 29 files:  14% 4/29 [04:22<29:29, 70.79s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  89% 2.89G/3.25G [04:15<00:23, 14.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  82% 2.74G/3.35G [04:19<00:45, 13.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  82% 2.74G/3.35G [04:20<00:35, 17.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  82% 2.74G/3.35G [04:21<00:44, 13.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  89% 2.90G/3.25G [04:16<00:23, 14.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  82% 2.75G/3.35G [04:24<01:18, 7.74MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  82% 2.75G/3.35G [04:24<01:28, 6.87MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  82% 2.75G/3.35G [04:23<01:27, 6.89MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  90% 2.92G/3.25G [04:19<00:41, 8.05MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  82% 2.76G/3.35G [04:24<01:02, 9.54MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  83% 2.77G/3.35G [04:24<00:43, 13.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  83% 2.77G/3.35G [04:23<00:47, 12.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  90% 2.93G/3.25G [04:19<00:29, 10.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  83% 2.77G/3.35G [04:24<00:44, 13.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  90% 2.94G/3.25G [04:19<00:21, 14.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  83% 2.78G/3.35G [04:23<00:37, 15.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  83% 2.78G/3.35G [04:24<00:34, 16.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  91% 2.95G/3.25G [04:19<00:15, 18.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  91% 2.97G/3.25G [04:20<00:11, 23.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  83% 2.79G/3.35G [04:25<00:36, 15.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  83% 2.79G/3.35G [04:24<00:38, 14.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  83% 2.79G/3.35G [04:25<00:34, 16.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  92% 2.98G/3.25G [04:20<00:11, 23.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  83% 2.80G/3.35G [04:26<00:35, 15.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  83% 2.80G/3.35G [04:25<00:38, 14.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  83% 2.80G/3.35G [04:26<00:37, 14.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  92% 2.99G/3.25G [04:21<00:12, 20.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  92% 3.00G/3.25G [04:21<00:11, 21.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  84% 2.81G/3.35G [04:26<00:38, 14.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  84% 2.81G/3.35G [04:27<00:37, 14.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  84% 2.81G/3.35G [04:26<00:39, 13.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  93% 3.01G/3.25G [04:22<00:11, 21.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  84% 2.82G/3.35G [04:28<00:38, 14.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  84% 2.82G/3.35G [04:26<00:39, 13.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  84% 2.82G/3.35G [04:27<00:39, 13.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  93% 3.02G/3.25G [04:23<00:11, 19.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  93% 3.03G/3.25G [04:23<00:10, 20.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  84% 2.83G/3.35G [04:27<00:38, 13.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  84% 2.83G/3.35G [04:28<00:38, 13.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  84% 2.83G/3.35G [04:28<00:37, 13.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  94% 3.04G/3.25G [04:24<00:09, 21.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  85% 2.84G/3.35G [04:29<00:37, 13.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  85% 2.84G/3.35G [04:29<00:36, 13.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  85% 2.84G/3.35G [04:28<00:37, 13.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  94% 3.05G/3.25G [04:24<00:09, 21.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  94% 3.06G/3.25G [04:24<00:08, 21.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  85% 2.85G/3.35G [04:30<00:38, 13.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  85% 2.85G/3.35G [04:29<00:38, 13.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  85% 2.85G/3.35G [04:30<00:38, 13.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  95% 3.07G/3.25G [04:25<00:07, 22.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  95% 3.08G/3.25G [04:25<00:07, 22.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  85% 2.86G/3.35G [04:31<00:40, 12.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  85% 2.86G/3.35G [04:31<00:40, 12.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  85% 2.86G/3.35G [04:30<00:40, 12.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  95% 3.09G/3.25G [04:26<00:06, 22.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  96% 3.10G/3.25G [04:26<00:06, 22.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  86% 2.87G/3.35G [04:32<00:39, 12.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  86% 2.87G/3.35G [04:32<00:39, 12.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  86% 2.87G/3.35G [04:31<00:39, 12.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  96% 3.11G/3.25G [04:27<00:05, 22.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  96% 3.12G/3.25G [04:27<00:05, 22.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  86% 2.88G/3.35G [04:32<00:38, 12.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  86% 2.88G/3.35G [04:32<00:38, 12.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  86% 2.88G/3.35G [04:33<00:38, 12.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  97% 3.14G/3.25G [04:28<00:04, 22.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  97% 3.15G/3.25G [04:28<00:04, 22.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  86% 2.89G/3.35G [04:33<00:35, 12.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  86% 2.89G/3.35G [04:32<00:35, 12.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  86% 2.89G/3.35G [04:33<00:35, 12.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  97% 3.16G/3.25G [04:29<00:03, 22.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  98% 3.17G/3.25G [04:29<00:03, 22.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  87% 2.90G/3.35G [04:34<00:35, 12.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  87% 2.90G/3.35G [04:34<00:35, 12.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  87% 2.90G/3.35G [04:33<00:35, 12.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  98% 3.18G/3.25G [04:29<00:03, 22.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  87% 2.92G/3.35G [04:35<00:34, 12.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  87% 2.92G/3.35G [04:34<00:34, 12.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  87% 2.92G/3.35G [04:35<00:34, 12.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  98% 3.19G/3.25G [04:30<00:02, 22.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  98% 3.20G/3.25G [04:30<00:02, 22.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  99% 3.21G/3.25G [04:31<00:01, 22.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  87% 2.93G/3.35G [04:36<00:36, 11.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  87% 2.93G/3.35G [04:35<00:36, 11.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  87% 2.93G/3.35G [04:36<00:36, 11.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  99% 3.22G/3.25G [04:31<00:01, 22.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin:  99% 3.23G/3.25G [04:32<00:00, 22.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  88% 2.94G/3.35G [04:37<00:36, 11.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  88% 2.94G/3.35G [04:37<00:36, 11.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  88% 2.94G/3.35G [04:36<00:36, 11.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin: 100% 3.24G/3.25G [04:32<00:00, 22.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch_model.bin: 100% 3.25G/3.25G [04:32<00:00, 11.9MB/s]\n",
            "Download complete. Moving file to /content/LMOps/minillm/checkpoints/gpt2-large/pytorch_model.bin\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  88% 2.95G/3.35G [04:38<00:36, 11.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  88% 2.95G/3.35G [04:38<00:36, 11.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  88% 2.95G/3.35G [04:37<00:36, 11.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  88% 2.96G/3.35G [04:39<00:35, 11.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  88% 2.96G/3.35G [04:39<00:35, 11.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  88% 2.96G/3.35G [04:38<00:35, 11.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  88% 2.97G/3.35G [04:39<00:35, 10.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  88% 2.97G/3.35G [04:40<00:35, 10.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  88% 2.97G/3.35G [04:40<00:35, 10.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  89% 2.98G/3.35G [04:41<00:36, 10.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  89% 2.98G/3.35G [04:40<00:36, 10.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  89% 2.98G/3.35G [04:41<00:36, 10.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  89% 2.99G/3.35G [04:42<00:37, 9.67MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  89% 2.99G/3.35G [04:41<00:37, 9.67MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  89% 2.99G/3.35G [04:42<00:37, 9.67MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  89% 3.00G/3.35G [04:43<00:36, 9.67MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  89% 3.00G/3.35G [04:42<00:36, 9.65MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  89% 3.00G/3.35G [04:44<00:36, 9.64MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  90% 3.01G/3.35G [04:45<00:39, 8.65MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  90% 3.01G/3.35G [04:44<00:39, 8.64MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  90% 3.01G/3.35G [04:45<00:39, 8.64MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  90% 3.02G/3.35G [04:47<00:45, 7.28MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  90% 3.02G/3.35G [04:46<00:45, 7.27MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  90% 3.02G/3.35G [04:47<00:45, 7.27MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  90% 3.03G/3.35G [04:49<00:48, 6.64MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  90% 3.03G/3.35G [04:49<00:48, 6.63MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  90% 3.03G/3.35G [04:48<00:48, 6.60MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  91% 3.04G/3.35G [04:50<00:48, 6.45MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  91% 3.04G/3.35G [04:49<00:48, 6.45MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  91% 3.04G/3.35G [04:51<00:48, 6.44MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  91% 3.05G/3.35G [04:51<00:49, 6.06MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  91% 3.05G/3.35G [04:52<00:49, 6.06MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  91% 3.05G/3.35G [04:53<00:49, 6.05MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  91% 3.06G/3.35G [04:54<00:51, 5.65MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  91% 3.06G/3.35G [04:54<00:51, 5.65MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  91% 3.06G/3.35G [04:55<00:51, 5.65MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  92% 3.07G/3.35G [04:57<00:53, 5.24MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  92% 3.07G/3.35G [04:57<00:53, 5.24MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  92% 3.07G/3.35G [04:56<00:53, 5.24MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  92% 3.08G/3.35G [04:59<00:54, 4.97MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  92% 3.08G/3.35G [04:58<00:54, 4.97MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  92% 3.08G/3.35G [05:00<00:54, 4.96MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  92% 3.09G/3.35G [05:02<00:54, 4.78MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  92% 3.09G/3.35G [05:01<00:54, 4.78MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  92% 3.09G/3.35G [05:02<00:54, 4.78MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  93% 3.10G/3.35G [05:03<00:56, 4.41MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  93% 3.10G/3.35G [05:05<00:56, 4.41MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  93% 3.10G/3.35G [05:04<00:56, 4.41MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  93% 3.11G/3.35G [05:07<00:55, 4.29MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  93% 3.11G/3.35G [05:06<00:55, 4.29MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  93% 3.11G/3.35G [05:07<00:55, 4.29MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  93% 3.12G/3.35G [05:10<00:54, 4.22MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  93% 3.12G/3.35G [05:10<00:54, 4.21MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  93% 3.12G/3.35G [05:09<00:54, 4.21MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  93% 3.14G/3.35G [05:12<00:52, 4.17MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  93% 3.14G/3.35G [05:12<00:52, 4.17MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  93% 3.14G/3.35G [05:11<00:52, 4.16MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  94% 3.15G/3.35G [05:15<00:48, 4.25MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  94% 3.15G/3.35G [05:15<00:48, 4.25MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  94% 3.15G/3.35G [05:14<00:48, 4.25MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  94% 3.16G/3.35G [05:17<00:43, 4.54MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  94% 3.16G/3.35G [05:16<00:43, 4.54MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  94% 3.16G/3.35G [05:16<00:43, 4.54MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  94% 3.17G/3.35G [05:18<00:36, 5.05MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  94% 3.17G/3.35G [05:18<00:37, 5.05MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  94% 3.17G/3.35G [05:17<00:37, 5.04MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  95% 3.18G/3.35G [05:19<00:31, 5.67MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  95% 3.18G/3.35G [05:18<00:31, 5.67MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  95% 3.18G/3.35G [05:20<00:31, 5.67MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  95% 3.19G/3.35G [05:21<00:26, 6.28MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  95% 3.19G/3.35G [05:20<00:26, 6.28MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  95% 3.19G/3.35G [05:21<00:26, 6.27MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  95% 3.20G/3.35G [05:21<00:21, 7.31MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  95% 3.20G/3.35G [05:22<00:21, 7.31MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  95% 3.20G/3.35G [05:21<00:21, 7.31MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  96% 3.21G/3.35G [05:23<00:17, 8.30MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  96% 3.21G/3.35G [05:21<00:17, 8.30MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  96% 3.21G/3.35G [05:22<00:17, 8.29MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  96% 3.22G/3.35G [05:23<00:14, 9.21MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  96% 3.22G/3.35G [05:22<00:14, 9.21MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  96% 3.22G/3.35G [05:23<00:14, 9.21MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  96% 3.23G/3.35G [05:24<00:11, 10.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  96% 3.23G/3.35G [05:23<00:11, 10.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  96% 3.23G/3.35G [05:24<00:11, 10.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  97% 3.24G/3.35G [05:24<00:09, 11.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  97% 3.24G/3.35G [05:25<00:09, 11.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  97% 3.24G/3.35G [05:24<00:09, 11.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  97% 3.25G/3.35G [05:25<00:07, 12.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  97% 3.25G/3.35G [05:25<00:07, 12.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  97% 3.25G/3.35G [05:24<00:07, 12.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  97% 3.26G/3.35G [05:26<00:06, 14.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  97% 3.26G/3.35G [05:26<00:06, 14.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  97% 3.26G/3.35G [05:25<00:06, 14.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  98% 3.27G/3.35G [05:26<00:05, 15.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  98% 3.27G/3.35G [05:25<00:05, 15.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  98% 3.27G/3.35G [05:26<00:05, 15.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  98% 3.28G/3.35G [05:27<00:04, 17.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  98% 3.28G/3.35G [05:27<00:04, 17.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  98% 3.28G/3.35G [05:26<00:04, 17.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  98% 3.29G/3.35G [05:27<00:03, 18.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  98% 3.29G/3.35G [05:27<00:03, 18.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  98% 3.29G/3.35G [05:26<00:03, 18.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  98% 3.30G/3.35G [05:27<00:02, 20.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  98% 3.30G/3.35G [05:27<00:02, 20.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  98% 3.30G/3.35G [05:28<00:02, 20.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  99% 3.31G/3.35G [05:28<00:01, 21.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  99% 3.31G/3.35G [05:28<00:01, 21.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  99% 3.31G/3.35G [05:27<00:01, 21.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  99% 3.32G/3.35G [05:28<00:01, 22.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  99% 3.32G/3.35G [05:29<00:01, 22.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  99% 3.32G/3.35G [05:27<00:01, 22.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  99% 3.33G/3.35G [05:29<00:00, 22.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  99% 3.33G/3.35G [05:28<00:00, 22.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data:  99% 3.33G/3.35G [05:29<00:00, 22.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data: 100% 3.34G/3.35G [05:30<00:00, 23.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data: 100% 3.34G/3.35G [05:29<00:00, 23.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data: 100% 3.34G/3.35G [05:28<00:00, 23.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data: 100% 3.35G/3.35G [05:30<00:00, 22.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data: 100% 3.35G/3.35G [05:29<00:00, 22.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "decoder_model.onnx_data: 100% 3.35G/3.35G [05:30<00:00, 10.2MB/s]\n",
            "decoder_model.onnx_data: 100% 3.35G/3.35G [05:29<00:00, 10.2MB/s]\n",
            "decoder_model.onnx_data: 100% 3.35G/3.35G [05:30<00:00, 10.1MB/s]\n",
            "Download complete. Moving file to /content/LMOps/minillm/checkpoints/gpt2-large/onnx/decoder_model.onnx_data\n",
            "Download complete. Moving file to /content/LMOps/minillm/checkpoints/gpt2-large/onnx/decoder_model_merged.onnx_data\n",
            "Fetching 29 files:  45% 13/29 [05:31<05:49, 21.83s/it]Download complete. Moving file to /content/LMOps/minillm/checkpoints/gpt2-large/onnx/decoder_with_past_model.onnx_data\n",
            "Fetching 29 files: 100% 29/29 [05:31<00:00, 11.44s/it]\n",
            "/content/LMOps/minillm/checkpoints/gpt2-large\n"
          ]
        }
      ],
      "source": [
        "#Dolly\n",
        "#!huggingface-cli download MiniLLM/dolly --repo-type dataset --local-dir /content/LMOps/minillm/data/dolly/\n",
        "#!huggingface-cli download MiniLLM/dolly-processed --repo-type dataset --local-dir /content/LMOps/minillm/processed_data/dolly/\n",
        "\n",
        "#GPT2\n",
        "#!huggingface-cli download gpt2 --repo-type model --local-dir /content/LMOps/minillm/checkpoints/gpt2-base\n",
        "#!huggingface-cli download gpt2-xl --repo-type model --local-dir /content/LMOps/minillm/checkpoints/gpt2-xlarge\n",
        "!huggingface-cli download gpt2-large --repo-type model --local-dir /content/LMOps/minillm/checkpoints/gpt2-large\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "source = '/content/drive/MyDrive/MIDS/minillm/processed_data'\n",
        "destination ='/content/LMOps/minillm/processed_data'\n",
        "shutil.copytree(source, destination, dirs_exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "IYkBAnZpkhMX",
        "outputId": "cb739253-7b5b-4882-fc61-e4528b58a3d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/LMOps/minillm/processed_data'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIDhUYmEw6WP"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from datasets import load_dataset\n",
        "import re\n",
        "# ---------------------------\n",
        "# 1. Load the Spanish subset\n",
        "# ---------------------------\n",
        "ds = load_dataset(\"argilla/databricks-dolly-15k-curated-multilingual\")\n",
        "es_ds = ds[\"es\"]\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Split into train and validation\n",
        "#    (Here we force validation to 500 examples)\n",
        "# ---------------------------\n",
        "split = es_ds.train_test_split(test_size=500, seed=42)\n",
        "train_ds = split[\"train\"]\n",
        "valid_ds = split[\"test\"]\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Define conversion functions\n",
        "# ---------------------------\n",
        "# For TXT conversion, we use a template.\n",
        "# Here the newline token is represented as <n> (as in the original raw.txt).\n",
        "def clean_text(text):\n",
        "    \"\"\"Removes tabs, newlines, and collapses multiple spaces into a single space.\"\"\"\n",
        "    return re.sub(r'[\\t\\n\\r]+', ' ', text).strip()\n",
        "def to_txt_line(example):\n",
        "    instruction = clean_text(example[\"instruction\"])\n",
        "    context = clean_text(example[\"context\"])\n",
        "    response = clean_text(example[\"response\"])  # Asegurando que no quede fuera\n",
        "\n",
        "    if context:\n",
        "        # Template cuando hay contexto\n",
        "        template = (\n",
        "            \"A continuación se muestra una instrucción que describe una tarea, acompañada de una entrada que proporciona un contexto adicional. \"\n",
        "            \"Escribe una respuesta que complete adecuadamente la solicitud. \"\n",
        "            \"### Instruction: {instruction} \"\n",
        "            \"### Input: {context} \"\n",
        "            \"### Response: {response}\"\n",
        "        )\n",
        "    else:\n",
        "        # Template sin contexto\n",
        "        template = (\n",
        "            \"A continuación se muestra una instrucción que describe una tarea. \"\n",
        "            \"Escribe una respuesta que complete adecuadamente la solicitud. \"\n",
        "            \"### Instruction: {instruction} \"\n",
        "            \"### Response: {response}\"\n",
        "        )\n",
        "\n",
        "    return template.format(instruction=instruction, context=context, response=response)\n",
        "\n",
        "\n",
        "# For JSONL conversion, we create a JSON object with keys similar to the original file.\n",
        "def to_jsonl_entry(example):\n",
        "    if example[\"context\"].strip():\n",
        "        prompt = (\n",
        "            \"A continuación se muestra una instrucción que describe una tarea, acompañada de una entrada que proporciona un contexto adicional. \"\n",
        "            \"Escribe una respuesta que complete adecuadamente la solicitud.\\n\\n\"\n",
        "            \"### Instrucción:\\n{instruction}\\n\\n### Entrada:\\n{context}\\n\\n### Respuesta:\\n\"\n",
        "        ).format(\n",
        "            instruction=example[\"instruction\"].strip(),\n",
        "            context=example[\"context\"].strip()\n",
        "        )\n",
        "    else:\n",
        "        prompt = (\n",
        "            \"A continuación se muestra una instrucción que describe una tarea. \"\n",
        "            \"Escribe una respuesta que complete adecuadamente la solicitud.\\n\\n\"\n",
        "            \"### Instrucción:\\n{instruction}\\n\\n### Respuesta:\\n\"\n",
        "        ).format(\n",
        "            instruction=example[\"instruction\"].strip()\n",
        "        )\n",
        "\n",
        "    return {\n",
        "        \"topic\": example.get(\"category\", \"\"),\n",
        "        \"instruction\": example[\"instruction\"].strip(),\n",
        "        \"input\": example[\"context\"].strip() if example[\"context\"] else \"\",\n",
        "        \"output\": example[\"response\"].strip(),\n",
        "        \"prompt\": prompt\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Convert datasets and write files\n",
        "# ---------------------------\n",
        "def write_txt_file(dataset, filepath):\n",
        "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "        for example in dataset:\n",
        "            line = to_txt_line(example)\n",
        "            f.write(line + \"\\n\")\n",
        "\n",
        "def write_jsonl_file(dataset, filepath):\n",
        "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "        for example in dataset:\n",
        "            json_obj = to_jsonl_entry(example)\n",
        "            f.write(json.dumps(json_obj, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "# Write the training files (raw.*) and validation files (valid.*)\n",
        "write_txt_file(train_ds, \"data/dolly/raw.txt\")\n",
        "write_txt_file(valid_ds, \"data/dolly/valid.txt\")\n",
        "write_jsonl_file(train_ds, \"data/dolly/raw.jsonl\")\n",
        "write_jsonl_file(valid_ds, \"data/dolly/valid.jsonl\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z03t7vNe-3pJ",
        "outputId": "63aeccdc-6c38-4a6f-89b1-51266e072bf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-04-01 18:48:18,110] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1743533301.392383   74420 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1743533301.399006   74420 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "OK\n",
            "dtype: <class 'numpy.uint16'> split_id: 65535\n",
            "########## valid ##########\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1920 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1759 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1060 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1045 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1123 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3536 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1126 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1931 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1051 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1028 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1111 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1560 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1957 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3180 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1952 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Processed 0 documents. 1 instances. (0.0 docs/s, 2.8740744402524473e-06 MB/s).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2204 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1051 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Data num 777\n",
            "Prompt lengths. Mean: 71.24967824967825 Max: 254 Min: 35\n",
            "Response Mean: 139.1866151866152 Max: 3315 Min: 2\n",
            "########## train ##########\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1155 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1053 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1226 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1993 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1285 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1341 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1030 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2767 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1795 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2643 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1176 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1526 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1539 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2121 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1157 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1190 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2425 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1046 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Processed 0 documents. 1 instances. (0.0 docs/s, 3.0426381620153097e-06 MB/s).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1091 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (5924 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1072 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2045 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1074 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Processed 1000 documents. 762 instances. (200.45491460565697 docs/s, 0.0009567993618023997 MB/s).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1585 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1418 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1386 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1049 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2850 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1620 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1039 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1512 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Processed 3000 documents. 2326 instances. (322.0281543074094 docs/s, 0.0015360617495799632 MB/s).\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1249 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Processed 4000 documents. 3105 instances. (380.66759028287976 docs/s, 0.0018156183108351254 MB/s).\n",
            "Processed 6000 documents. 4632 instances. (461.5737106787474 docs/s, 0.0022013217908407553 MB/s).\n",
            "Processed 7000 documents. 5403 instances. (487.93084388202203 docs/s, 0.002326967946473555 MB/s).\n",
            "Processed 9000 documents. 6910 instances. (513.9654955717089 docs/s, 0.002451050771946881 MB/s).\n"
          ]
        }
      ],
      "source": [
        "#Dolly processing\n",
        "!bash scripts/gpt2/tools/process_data_dolly.sh /content/LMOps/minillm # Process Dolly Train / Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "EE4bLjggfeyF",
        "outputId": "3fef09d0-30d9-483b-943d-8b1b33100155"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "[Errno 17] File exists: '/content/drive/MyDrive/MIDS/minillm/processed_data'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-64074b10744f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/LMOps/minillm/processed_data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdestination\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/MIDS/minillm/processed_data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopytree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mcopytree\u001b[0;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m     return _copytree(entries=entries, src=src, dst=dst, symlinks=symlinks,\n\u001b[0m\u001b[1;32m    574\u001b[0m                      \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m                      \u001b[0mignore_dangling_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_dangling_symlinks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36m_copytree\u001b[0;34m(entries, src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mignored_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirs_exist_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m     \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0muse_srcentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mcopy2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcopy_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/content/drive/MyDrive/MIDS/minillm/processed_data'"
          ]
        }
      ],
      "source": [
        "# import shutil\n",
        "\n",
        "# # Copy the process data into drive\n",
        "# source = '/content/LMOps/minillm/processed_data'\n",
        "# destination = '/content/drive/MyDrive/MIDS/minillm/processed_data'\n",
        "# shutil.copytree(source, destination, dirs_exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojlvz7UcH5RP",
        "outputId": "2e8c0829-bbc7-4642-e871-599f5562f6d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Evaluating:  52% 65/125 [03:09<02:54,  2.91s/it]65/125\n",
            "Evaluating:  53% 66/125 [03:11<02:46,  2.82s/it]66/125\n",
            "Evaluating:  54% 67/125 [03:14<02:40,  2.76s/it]67/125\n",
            "Evaluating:  54% 68/125 [03:16<02:34,  2.72s/it]68/125\n",
            "Evaluating:  55% 69/125 [03:20<02:43,  2.92s/it]69/125\n",
            "Evaluating:  56% 70/125 [03:23<02:37,  2.86s/it]70/125\n",
            "Evaluating:  57% 71/125 [03:25<02:31,  2.80s/it]71/125\n",
            "Evaluating:  58% 72/125 [03:28<02:25,  2.75s/it]72/125\n",
            "Evaluating:  58% 73/125 [03:31<02:23,  2.76s/it]73/125\n",
            "Evaluating:  59% 74/125 [03:34<02:31,  2.96s/it]74/125\n",
            "Evaluating:  60% 75/125 [03:37<02:23,  2.87s/it]75/125\n",
            "Evaluating:  61% 76/125 [03:39<02:16,  2.79s/it]76/125\n",
            "Evaluating:  62% 77/125 [03:41<02:05,  2.61s/it]77/125\n",
            "Evaluating:  62% 78/125 [03:44<02:07,  2.71s/it]78/125\n",
            "Evaluating:  63% 79/125 [03:48<02:11,  2.87s/it]79/125\n",
            "Evaluating:  64% 80/125 [03:50<02:06,  2.80s/it]80/125\n",
            "Evaluating:  65% 81/125 [03:53<02:00,  2.75s/it]81/125\n",
            "Evaluating:  66% 82/125 [03:56<01:57,  2.73s/it]82/125\n",
            "Evaluating:  66% 83/125 [03:59<02:01,  2.89s/it]83/125\n",
            "Evaluating:  67% 84/125 [04:02<01:58,  2.89s/it]84/125\n",
            "Evaluating:  68% 85/125 [04:04<01:52,  2.80s/it]85/125\n",
            "Evaluating:  69% 86/125 [04:07<01:47,  2.75s/it]86/125\n",
            "Evaluating:  70% 87/125 [04:10<01:43,  2.71s/it]87/125\n",
            "Evaluating:  70% 88/125 [04:13<01:49,  2.97s/it]88/125\n",
            "Evaluating:  71% 89/125 [04:16<01:43,  2.87s/it]89/125\n",
            "Distributed index stop interation. Idx: 777 Total_length: 777\n",
            "Evaluating:  72% 90/125 [04:18<01:38,  2.80s/it]90/125\n",
            "Distributed index stop interation. Idx: 784 Total_length: 777\n",
            "Evaluating:  73% 91/125 [04:21<01:33,  2.76s/it]91/125\n",
            "Distributed index stop interation. Idx: 792 Total_length: 777\n",
            "Evaluating:  74% 92/125 [04:24<01:34,  2.85s/it]92/125\n",
            "Distributed index stop interation. Idx: 800 Total_length: 777\n",
            "Evaluating:  74% 93/125 [04:27<01:34,  2.95s/it]93/125\n",
            "Distributed index stop interation. Idx: 808 Total_length: 777\n",
            "Evaluating:  75% 94/125 [04:30<01:28,  2.85s/it]94/125\n",
            "Distributed index stop interation. Idx: 816 Total_length: 777\n",
            "Evaluating:  76% 95/125 [04:33<01:23,  2.78s/it]95/125\n",
            "Distributed index stop interation. Idx: 824 Total_length: 777\n",
            "Evaluating:  77% 96/125 [04:35<01:16,  2.64s/it]Distributed index stop interation. Idx: 832 Total_length: 77796/125\n",
            "\n",
            "Evaluating:  78% 97/125 [04:38<01:18,  2.80s/it]Distributed index stop interation. Idx: 840 Total_length: 777\n",
            "Evaluating:  78% 97/125 [04:38<01:20,  2.87s/it]\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5/eval/0\n",
            "dev | avg_loss: 2.4446581024484537 | {'exact_match': 0.0, 'rougeL': 10.7684}\n",
            "train | epoch   0 | Iter:      1/  5202 | global iter:      1/  5202 | loss: 2.9567 | ds_loss: 3.6520 | lr: 1.0000e-06 | scale:  2048.0000 | micro time: 0.550 | step time: 0.000\n",
            "train | epoch   0 | Iter:      2/  5202 | global iter:      2/  5202 | loss: 2.7510 | ds_loss: 3.9791 | lr: 1.0000e-06 | scale:  2048.0000 | micro time: 0.297 | step time: 0.000\n",
            "train | epoch   0 | Iter:      3/  5202 | global iter:      3/  5202 | loss: 1.0488 | ds_loss: 1.5198 | lr: 1.0000e-06 | scale:  2048.0000 | micro time: 0.298 | step time: 0.000\n",
            "train | epoch   0 | Iter:      4/  5202 | global iter:      4/  5202 | loss: 3.2305 | ds_loss: 3.9829 | lr: 1.0000e-06 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:      4/  5202 | global iter:      4/  5202 | loss: 2.4968 | ds_loss: 3.2835 | lr: 1.0000e-06 | scale:  2048.0000 | micro time: 0.302 | step time: 0.362\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:      5/  5202 | global iter:      5/  5202 | loss: 1.1692 | ds_loss: 1.5815 | lr: 1.0000e-06 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:      6/  5202 | global iter:      6/  5202 | loss: 1.6900 | ds_loss: 2.3969 | lr: 1.0000e-06 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:      7/  5202 | global iter:      7/  5202 | loss: 2.4237 | ds_loss: 3.4950 | lr: 1.0000e-06 | scale:  2048.0000 | micro time: 0.298 | step time: 0.000\n",
            "train | epoch   0 | Iter:      8/  5202 | global iter:      8/  5202 | loss: 2.9266 | ds_loss: 3.9633 | lr: 9.9999e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:      8/  5202 | global iter:      8/  5202 | loss: 2.0524 | ds_loss: 2.8592 | lr: 9.9999e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.301\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:      9/  5202 | global iter:      9/  5202 | loss: 3.2502 | ds_loss: 4.1235 | lr: 9.9999e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:     10/  5202 | global iter:     10/  5202 | loss: 2.2248 | ds_loss: 2.9389 | lr: 9.9999e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:     11/  5202 | global iter:     11/  5202 | loss: 2.7396 | ds_loss: 3.4040 | lr: 9.9999e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:     12/  5202 | global iter:     12/  5202 | loss: 2.4483 | ds_loss: 3.1159 | lr: 9.9999e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     12/  5202 | global iter:     12/  5202 | loss: 2.6657 | ds_loss: 3.3956 | lr: 9.9999e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.303\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     13/  5202 | global iter:     13/  5202 | loss: 2.5548 | ds_loss: 3.2992 | lr: 9.9999e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:     14/  5202 | global iter:     14/  5202 | loss: 3.2186 | ds_loss: 4.0169 | lr: 9.9998e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:     15/  5202 | global iter:     15/  5202 | loss: 2.7945 | ds_loss: 3.8377 | lr: 9.9998e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:     16/  5202 | global iter:     16/  5202 | loss: 3.2645 | ds_loss: 3.9074 | lr: 9.9998e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     16/  5202 | global iter:     16/  5202 | loss: 2.9581 | ds_loss: 3.7653 | lr: 9.9998e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.303\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     17/  5202 | global iter:     17/  5202 | loss: 2.0726 | ds_loss: 2.8699 | lr: 9.9998e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:     18/  5202 | global iter:     18/  5202 | loss: 3.1224 | ds_loss: 4.1977 | lr: 9.9997e-07 | scale:  2048.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:     19/  5202 | global iter:     19/  5202 | loss: 3.0468 | ds_loss: 3.8784 | lr: 9.9997e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:     20/  5202 | global iter:     20/  5202 | loss: 3.1338 | ds_loss: 3.6787 | lr: 9.9997e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     20/  5202 | global iter:     20/  5202 | loss: 2.8439 | ds_loss: 3.6562 | lr: 9.9997e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     21/  5202 | global iter:     21/  5202 | loss: 1.2336 | ds_loss: 1.7841 | lr: 9.9996e-07 | scale:  2048.0000 | micro time: 0.299 | step time: 0.000\n",
            "train | epoch   0 | Iter:     22/  5202 | global iter:     22/  5202 | loss: 2.0000 | ds_loss: 2.7528 | lr: 9.9996e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.000\n",
            "train | epoch   0 | Iter:     23/  5202 | global iter:     23/  5202 | loss: 2.9406 | ds_loss: 4.1290 | lr: 9.9996e-07 | scale:  2048.0000 | micro time: 0.299 | step time: 0.000\n",
            "train | epoch   0 | Iter:     24/  5202 | global iter:     24/  5202 | loss: 2.5878 | ds_loss: 3.4446 | lr: 9.9995e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     24/  5202 | global iter:     24/  5202 | loss: 2.1905 | ds_loss: 3.0276 | lr: 9.9995e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.300\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     25/  5202 | global iter:     25/  5202 | loss: 2.9997 | ds_loss: 3.6597 | lr: 9.9995e-07 | scale:  2048.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:     26/  5202 | global iter:     26/  5202 | loss: 2.4319 | ds_loss: 3.0787 | lr: 9.9994e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:     27/  5202 | global iter:     27/  5202 | loss: 0.9523 | ds_loss: 1.5645 | lr: 9.9994e-07 | scale:  2048.0000 | micro time: 0.299 | step time: 0.000\n",
            "train | epoch   0 | Iter:     28/  5202 | global iter:     28/  5202 | loss: 1.8117 | ds_loss: 2.2188 | lr: 9.9994e-07 | scale:  2048.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     28/  5202 | global iter:     28/  5202 | loss: 2.0489 | ds_loss: 2.6304 | lr: 9.9994e-07 | scale:  2048.0000 | micro time: 0.308 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     29/  5202 | global iter:     29/  5202 | loss: 2.9107 | ds_loss: 3.6672 | lr: 9.9993e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:     30/  5202 | global iter:     30/  5202 | loss: 2.9694 | ds_loss: 3.7239 | lr: 9.9993e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:     31/  5202 | global iter:     31/  5202 | loss: 1.5707 | ds_loss: 2.6446 | lr: 9.9992e-07 | scale:  2048.0000 | micro time: 0.299 | step time: 0.000\n",
            "train | epoch   0 | Iter:     32/  5202 | global iter:     32/  5202 | loss: 2.3646 | ds_loss: 2.9491 | lr: 9.9992e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     32/  5202 | global iter:     32/  5202 | loss: 2.4539 | ds_loss: 3.2462 | lr: 9.9992e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.303\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     33/  5202 | global iter:     33/  5202 | loss: 2.8430 | ds_loss: 3.3909 | lr: 9.9991e-07 | scale:  2048.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:     34/  5202 | global iter:     34/  5202 | loss: 2.8898 | ds_loss: 3.6902 | lr: 9.9991e-07 | scale:  2048.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:     35/  5202 | global iter:     35/  5202 | loss: 3.0744 | ds_loss: 4.1647 | lr: 9.9990e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:     36/  5202 | global iter:     36/  5202 | loss: 2.5740 | ds_loss: 3.5175 | lr: 9.9989e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     36/  5202 | global iter:     36/  5202 | loss: 2.8453 | ds_loss: 3.6908 | lr: 9.9989e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     37/  5202 | global iter:     37/  5202 | loss: 2.4235 | ds_loss: 3.6037 | lr: 9.9989e-07 | scale:  2048.0000 | micro time: 0.299 | step time: 0.000\n",
            "train | epoch   0 | Iter:     38/  5202 | global iter:     38/  5202 | loss: 2.6807 | ds_loss: 3.1826 | lr: 9.9988e-07 | scale:  2048.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:     39/  5202 | global iter:     39/  5202 | loss: 2.4077 | ds_loss: 3.3227 | lr: 9.9988e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:     40/  5202 | global iter:     40/  5202 | loss: 2.8499 | ds_loss: 3.8245 | lr: 9.9987e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     40/  5202 | global iter:     40/  5202 | loss: 2.5905 | ds_loss: 3.4834 | lr: 9.9987e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.303\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     41/  5202 | global iter:     41/  5202 | loss: 3.2177 | ds_loss: 3.7004 | lr: 9.9986e-07 | scale:  2048.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:     42/  5202 | global iter:     42/  5202 | loss: 2.5775 | ds_loss: 3.5895 | lr: 9.9986e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:     43/  5202 | global iter:     43/  5202 | loss: 3.1027 | ds_loss: 3.6692 | lr: 9.9985e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:     44/  5202 | global iter:     44/  5202 | loss: 1.5791 | ds_loss: 2.6873 | lr: 9.9984e-07 | scale:  2048.0000 | micro time: 0.298 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     44/  5202 | global iter:     44/  5202 | loss: 2.6192 | ds_loss: 3.4116 | lr: 9.9984e-07 | scale:  2048.0000 | micro time: 0.298 | step time: 0.303\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     45/  5202 | global iter:     45/  5202 | loss: 2.5250 | ds_loss: 3.6280 | lr: 9.9983e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:     46/  5202 | global iter:     46/  5202 | loss: 2.9265 | ds_loss: 3.6116 | lr: 9.9983e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:     47/  5202 | global iter:     47/  5202 | loss: 2.7890 | ds_loss: 3.8424 | lr: 9.9982e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.000\n",
            "train | epoch   0 | Iter:     48/  5202 | global iter:     48/  5202 | loss: 2.9160 | ds_loss: 3.6536 | lr: 9.9981e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     48/  5202 | global iter:     48/  5202 | loss: 2.7891 | ds_loss: 3.6839 | lr: 9.9981e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.302\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     49/  5202 | global iter:     49/  5202 | loss: 3.1494 | ds_loss: 3.9125 | lr: 9.9980e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:     50/  5202 | global iter:     50/  5202 | loss: 2.3707 | ds_loss: 3.1259 | lr: 9.9979e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:     51/  5202 | global iter:     51/  5202 | loss: 2.4528 | ds_loss: 3.4016 | lr: 9.9979e-07 | scale:  2048.0000 | micro time: 0.298 | step time: 0.000\n",
            "train | epoch   0 | Iter:     52/  5202 | global iter:     52/  5202 | loss: 2.7403 | ds_loss: 3.5605 | lr: 9.9978e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     52/  5202 | global iter:     52/  5202 | loss: 2.6783 | ds_loss: 3.5001 | lr: 9.9978e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.301\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     53/  5202 | global iter:     53/  5202 | loss: 2.4975 | ds_loss: 3.1798 | lr: 9.9977e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.000\n",
            "train | epoch   0 | Iter:     54/  5202 | global iter:     54/  5202 | loss: 2.6415 | ds_loss: 3.7289 | lr: 9.9976e-07 | scale:  2048.0000 | micro time: 0.299 | step time: 0.000\n",
            "train | epoch   0 | Iter:     55/  5202 | global iter:     55/  5202 | loss: 2.9339 | ds_loss: 3.6929 | lr: 9.9975e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:     56/  5202 | global iter:     56/  5202 | loss: 2.5450 | ds_loss: 3.6276 | lr: 9.9974e-07 | scale:  2048.0000 | micro time: 0.299 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     56/  5202 | global iter:     56/  5202 | loss: 2.6545 | ds_loss: 3.5573 | lr: 9.9974e-07 | scale:  2048.0000 | micro time: 0.299 | step time: 0.300\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     57/  5202 | global iter:     57/  5202 | loss: 3.3581 | ds_loss: 3.9793 | lr: 9.9973e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:     58/  5202 | global iter:     58/  5202 | loss: 2.9247 | ds_loss: 3.8656 | lr: 9.9972e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:     59/  5202 | global iter:     59/  5202 | loss: 3.3439 | ds_loss: 4.0661 | lr: 9.9971e-07 | scale:  2048.0000 | micro time: 0.298 | step time: 0.000\n",
            "train | epoch   0 | Iter:     60/  5202 | global iter:     60/  5202 | loss: 2.0383 | ds_loss: 2.6840 | lr: 9.9970e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     60/  5202 | global iter:     60/  5202 | loss: 2.9163 | ds_loss: 3.6487 | lr: 9.9970e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.301\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     61/  5202 | global iter:     61/  5202 | loss: 3.0073 | ds_loss: 3.6877 | lr: 9.9969e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:     62/  5202 | global iter:     62/  5202 | loss: 3.2194 | ds_loss: 3.9557 | lr: 9.9968e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:     63/  5202 | global iter:     63/  5202 | loss: 1.3197 | ds_loss: 2.0752 | lr: 9.9967e-07 | scale:  2048.0000 | micro time: 0.299 | step time: 0.000\n",
            "train | epoch   0 | Iter:     64/  5202 | global iter:     64/  5202 | loss: 2.3947 | ds_loss: 3.4562 | lr: 9.9966e-07 | scale:  2048.0000 | micro time: 0.298 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     64/  5202 | global iter:     64/  5202 | loss: 2.4853 | ds_loss: 3.2937 | lr: 9.9966e-07 | scale:  2048.0000 | micro time: 0.298 | step time: 0.301\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     65/  5202 | global iter:     65/  5202 | loss: 2.8198 | ds_loss: 3.4221 | lr: 9.9965e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:     66/  5202 | global iter:     66/  5202 | loss: 2.6769 | ds_loss: 3.3910 | lr: 9.9964e-07 | scale:  2048.0000 | micro time: 0.298 | step time: 0.000\n",
            "train | epoch   0 | Iter:     67/  5202 | global iter:     67/  5202 | loss: 2.7253 | ds_loss: 3.4985 | lr: 9.9963e-07 | scale:  2048.0000 | micro time: 0.299 | step time: 0.000\n",
            "train | epoch   0 | Iter:     68/  5202 | global iter:     68/  5202 | loss: 2.8206 | ds_loss: 3.3521 | lr: 9.9962e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     68/  5202 | global iter:     68/  5202 | loss: 2.7607 | ds_loss: 3.4159 | lr: 9.9962e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.301\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     69/  5202 | global iter:     69/  5202 | loss: 2.7333 | ds_loss: 3.6659 | lr: 9.9961e-07 | scale:  2048.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:     70/  5202 | global iter:     70/  5202 | loss: 2.9635 | ds_loss: 3.7148 | lr: 9.9960e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:     71/  5202 | global iter:     71/  5202 | loss: 3.1520 | ds_loss: 4.1804 | lr: 9.9959e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:     72/  5202 | global iter:     72/  5202 | loss: 2.6403 | ds_loss: 3.4845 | lr: 9.9957e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     72/  5202 | global iter:     72/  5202 | loss: 2.8723 | ds_loss: 3.7614 | lr: 9.9957e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     73/  5202 | global iter:     73/  5202 | loss: 3.1077 | ds_loss: 3.8713 | lr: 9.9956e-07 | scale:  2048.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:     74/  5202 | global iter:     74/  5202 | loss: 2.8755 | ds_loss: 4.1630 | lr: 9.9955e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.000\n",
            "train | epoch   0 | Iter:     75/  5202 | global iter:     75/  5202 | loss: 2.6798 | ds_loss: 3.2607 | lr: 9.9954e-07 | scale:  2048.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:     76/  5202 | global iter:     76/  5202 | loss: 0.3058 | ds_loss: 0.5258 | lr: 9.9953e-07 | scale:  2048.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     76/  5202 | global iter:     76/  5202 | loss: 2.2422 | ds_loss: 2.9552 | lr: 9.9953e-07 | scale:  2048.0000 | micro time: 0.307 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     77/  5202 | global iter:     77/  5202 | loss: 2.7819 | ds_loss: 3.4943 | lr: 9.9951e-07 | scale:  2048.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:     78/  5202 | global iter:     78/  5202 | loss: 3.0961 | ds_loss: 3.9279 | lr: 9.9950e-07 | scale:  2048.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:     79/  5202 | global iter:     79/  5202 | loss: 3.2450 | ds_loss: 3.7532 | lr: 9.9949e-07 | scale:  2048.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:     80/  5202 | global iter:     80/  5202 | loss: 2.5630 | ds_loss: 3.4902 | lr: 9.9947e-07 | scale:  2048.0000 | micro time: 0.298 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     80/  5202 | global iter:     80/  5202 | loss: 2.9215 | ds_loss: 3.6664 | lr: 9.9947e-07 | scale:  2048.0000 | micro time: 0.298 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     81/  5202 | global iter:     81/  5202 | loss: 2.7644 | ds_loss: 3.4885 | lr: 9.9946e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:     82/  5202 | global iter:     82/  5202 | loss: 2.0122 | ds_loss: 2.4750 | lr: 9.9945e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:     83/  5202 | global iter:     83/  5202 | loss: 2.7384 | ds_loss: 3.4600 | lr: 9.9943e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:     84/  5202 | global iter:     84/  5202 | loss: 2.2519 | ds_loss: 2.9618 | lr: 9.9942e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     84/  5202 | global iter:     84/  5202 | loss: 2.4417 | ds_loss: 3.0963 | lr: 9.9942e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.303\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     85/  5202 | global iter:     85/  5202 | loss: 2.9077 | ds_loss: 3.6339 | lr: 9.9941e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:     86/  5202 | global iter:     86/  5202 | loss: 2.1845 | ds_loss: 3.4117 | lr: 9.9939e-07 | scale:  2048.0000 | micro time: 0.296 | step time: 0.000\n",
            "train | epoch   0 | Iter:     87/  5202 | global iter:     87/  5202 | loss: 2.6416 | ds_loss: 3.2565 | lr: 9.9938e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:     88/  5202 | global iter:     88/  5202 | loss: 3.1323 | ds_loss: 4.0575 | lr: 9.9936e-07 | scale:  2048.0000 | micro time: 0.299 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     88/  5202 | global iter:     88/  5202 | loss: 2.7165 | ds_loss: 3.5899 | lr: 9.9936e-07 | scale:  2048.0000 | micro time: 0.299 | step time: 0.301\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     89/  5202 | global iter:     89/  5202 | loss: 3.0396 | ds_loss: 4.1081 | lr: 9.9935e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:     90/  5202 | global iter:     90/  5202 | loss: 3.0219 | ds_loss: 4.4774 | lr: 9.9934e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:     91/  5202 | global iter:     91/  5202 | loss: 2.6689 | ds_loss: 3.7207 | lr: 9.9932e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:     92/  5202 | global iter:     92/  5202 | loss: 2.7159 | ds_loss: 3.2890 | lr: 9.9931e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     92/  5202 | global iter:     92/  5202 | loss: 2.8616 | ds_loss: 3.8988 | lr: 9.9931e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.301\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     93/  5202 | global iter:     93/  5202 | loss: 3.0695 | ds_loss: 3.7551 | lr: 9.9929e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:     94/  5202 | global iter:     94/  5202 | loss: 2.7361 | ds_loss: 3.4178 | lr: 9.9928e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:     95/  5202 | global iter:     95/  5202 | loss: 2.9244 | ds_loss: 3.5991 | lr: 9.9926e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:     96/  5202 | global iter:     96/  5202 | loss: 1.4683 | ds_loss: 2.2126 | lr: 9.9924e-07 | scale:  2048.0000 | micro time: 0.299 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     96/  5202 | global iter:     96/  5202 | loss: 2.5496 | ds_loss: 3.2462 | lr: 9.9924e-07 | scale:  2048.0000 | micro time: 0.299 | step time: 0.302\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:     97/  5202 | global iter:     97/  5202 | loss: 2.7021 | ds_loss: 3.7039 | lr: 9.9923e-07 | scale:  2048.0000 | micro time: 0.299 | step time: 0.000\n",
            "train | epoch   0 | Iter:     98/  5202 | global iter:     98/  5202 | loss: 2.4450 | ds_loss: 3.0036 | lr: 9.9921e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:     99/  5202 | global iter:     99/  5202 | loss: 3.2385 | ds_loss: 3.8033 | lr: 9.9920e-07 | scale:  2048.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:    100/  5202 | global iter:    100/  5202 | loss: 1.8091 | ds_loss: 2.7415 | lr: 9.9918e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    100/  5202 | global iter:    100/  5202 | loss: 2.5487 | ds_loss: 3.3131 | lr: 9.9918e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.303\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    101/  5202 | global iter:    101/  5202 | loss: 2.6065 | ds_loss: 3.3826 | lr: 9.9916e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    102/  5202 | global iter:    102/  5202 | loss: 2.9714 | ds_loss: 3.7040 | lr: 9.9915e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    103/  5202 | global iter:    103/  5202 | loss: 2.9471 | ds_loss: 3.5389 | lr: 9.9913e-07 | scale:  2048.0000 | micro time: 0.316 | step time: 0.000\n",
            "train | epoch   0 | Iter:    104/  5202 | global iter:    104/  5202 | loss: 1.6921 | ds_loss: 2.5428 | lr: 9.9911e-07 | scale:  2048.0000 | micro time: 0.298 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    104/  5202 | global iter:    104/  5202 | loss: 2.5543 | ds_loss: 3.2921 | lr: 9.9911e-07 | scale:  2048.0000 | micro time: 0.298 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    105/  5202 | global iter:    105/  5202 | loss: 3.1891 | ds_loss: 3.9338 | lr: 9.9910e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    106/  5202 | global iter:    106/  5202 | loss: 0.5584 | ds_loss: 0.7975 | lr: 9.9908e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    107/  5202 | global iter:    107/  5202 | loss: 2.9789 | ds_loss: 3.8375 | lr: 9.9906e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    108/  5202 | global iter:    108/  5202 | loss: 2.3827 | ds_loss: 2.9166 | lr: 9.9904e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    108/  5202 | global iter:    108/  5202 | loss: 2.2773 | ds_loss: 2.8714 | lr: 9.9904e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.303\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    109/  5202 | global iter:    109/  5202 | loss: 3.0606 | ds_loss: 3.8519 | lr: 9.9903e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.000\n",
            "train | epoch   0 | Iter:    110/  5202 | global iter:    110/  5202 | loss: 2.2858 | ds_loss: 3.3559 | lr: 9.9901e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    111/  5202 | global iter:    111/  5202 | loss: 2.9354 | ds_loss: 3.6792 | lr: 9.9899e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.000\n",
            "train | epoch   0 | Iter:    112/  5202 | global iter:    112/  5202 | loss: 2.9331 | ds_loss: 3.7154 | lr: 9.9897e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    112/  5202 | global iter:    112/  5202 | loss: 2.8037 | ds_loss: 3.6506 | lr: 9.9897e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.301\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    113/  5202 | global iter:    113/  5202 | loss: 2.6172 | ds_loss: 3.2769 | lr: 9.9895e-07 | scale:  2048.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    114/  5202 | global iter:    114/  5202 | loss: 2.5557 | ds_loss: 3.3048 | lr: 9.9893e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    115/  5202 | global iter:    115/  5202 | loss: 2.1845 | ds_loss: 3.4520 | lr: 9.9892e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    116/  5202 | global iter:    116/  5202 | loss: 2.5869 | ds_loss: 3.2759 | lr: 9.9890e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    116/  5202 | global iter:    116/  5202 | loss: 2.4861 | ds_loss: 3.3274 | lr: 9.9890e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    117/  5202 | global iter:    117/  5202 | loss: 0.8830 | ds_loss: 1.2400 | lr: 9.9888e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.000\n",
            "train | epoch   0 | Iter:    118/  5202 | global iter:    118/  5202 | loss: 1.2087 | ds_loss: 1.7404 | lr: 9.9886e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    119/  5202 | global iter:    119/  5202 | loss: 2.9537 | ds_loss: 3.6562 | lr: 9.9884e-07 | scale:  2048.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    120/  5202 | global iter:    120/  5202 | loss: 2.8310 | ds_loss: 3.8622 | lr: 9.9882e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    120/  5202 | global iter:    120/  5202 | loss: 1.9691 | ds_loss: 2.6247 | lr: 9.9882e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    121/  5202 | global iter:    121/  5202 | loss: 2.3468 | ds_loss: 3.5185 | lr: 9.9880e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    122/  5202 | global iter:    122/  5202 | loss: 2.5048 | ds_loss: 3.4758 | lr: 9.9878e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    123/  5202 | global iter:    123/  5202 | loss: 3.1298 | ds_loss: 3.9366 | lr: 9.9876e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    124/  5202 | global iter:    124/  5202 | loss: 2.4432 | ds_loss: 3.2320 | lr: 9.9874e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    124/  5202 | global iter:    124/  5202 | loss: 2.6062 | ds_loss: 3.5407 | lr: 9.9874e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.303\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "[2025-04-16 21:03:02,450] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, but hysteresis is 4. Reducing hysteresis to 3\n",
            "train | epoch   0 | Iter:    125/  5202 | global iter:    125/  5202 | loss: 0.6944 | ds_loss: 1.0909 | lr: 9.9874e-07 | scale:  2048.0000 | micro time: 0.230 | step time: 0.000\n",
            "train | epoch   0 | Iter:    126/  5202 | global iter:    126/  5202 | loss: 2.1767 | ds_loss: 3.4771 | lr: 9.9872e-07 | scale:  2048.0000 | micro time: 0.298 | step time: 0.000\n",
            "train | epoch   0 | Iter:    127/  5202 | global iter:    127/  5202 | loss: 3.1816 | ds_loss: 4.1806 | lr: 9.9870e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    128/  5202 | global iter:    128/  5202 | loss: 1.8107 | ds_loss: 2.4485 | lr: 9.9868e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    128/  5202 | global iter:    128/  5202 | loss: 1.9658 | ds_loss: 2.7993 | lr: 9.9868e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.284\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    129/  5202 | global iter:    129/  5202 | loss: 3.1035 | ds_loss: 4.0739 | lr: 9.9866e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    130/  5202 | global iter:    130/  5202 | loss: 2.8523 | ds_loss: 3.5516 | lr: 9.9864e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    131/  5202 | global iter:    131/  5202 | loss: 3.2665 | ds_loss: 4.0225 | lr: 9.9861e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    132/  5202 | global iter:    132/  5202 | loss: 2.8645 | ds_loss: 3.7616 | lr: 9.9859e-07 | scale:  2048.0000 | micro time: 0.299 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    132/  5202 | global iter:    132/  5202 | loss: 3.0217 | ds_loss: 3.8524 | lr: 9.9859e-07 | scale:  2048.0000 | micro time: 0.299 | step time: 0.303\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    133/  5202 | global iter:    133/  5202 | loss: 2.0679 | ds_loss: 2.7133 | lr: 9.9857e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.000\n",
            "train | epoch   0 | Iter:    134/  5202 | global iter:    134/  5202 | loss: 2.9553 | ds_loss: 3.4735 | lr: 9.9855e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    135/  5202 | global iter:    135/  5202 | loss: 2.7830 | ds_loss: 3.7725 | lr: 9.9853e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.000\n",
            "train | epoch   0 | Iter:    136/  5202 | global iter:    136/  5202 | loss: 2.6928 | ds_loss: 3.3606 | lr: 9.9851e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    136/  5202 | global iter:    136/  5202 | loss: 2.6248 | ds_loss: 3.3300 | lr: 9.9851e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.302\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    137/  5202 | global iter:    137/  5202 | loss: 2.4572 | ds_loss: 3.2154 | lr: 9.9848e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    138/  5202 | global iter:    138/  5202 | loss: 2.8566 | ds_loss: 3.7086 | lr: 9.9846e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    139/  5202 | global iter:    139/  5202 | loss: 2.9588 | ds_loss: 3.6602 | lr: 9.9844e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    140/  5202 | global iter:    140/  5202 | loss: 2.1730 | ds_loss: 3.0333 | lr: 9.9842e-07 | scale:  2048.0000 | micro time: 0.298 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    140/  5202 | global iter:    140/  5202 | loss: 2.6114 | ds_loss: 3.4044 | lr: 9.9842e-07 | scale:  2048.0000 | micro time: 0.298 | step time: 0.301\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    141/  5202 | global iter:    141/  5202 | loss: 2.5540 | ds_loss: 3.3875 | lr: 9.9839e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    142/  5202 | global iter:    142/  5202 | loss: 2.9975 | ds_loss: 3.9613 | lr: 9.9837e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    143/  5202 | global iter:    143/  5202 | loss: 1.4707 | ds_loss: 1.9424 | lr: 9.9835e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    144/  5202 | global iter:    144/  5202 | loss: 3.2103 | ds_loss: 3.8642 | lr: 9.9832e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    144/  5202 | global iter:    144/  5202 | loss: 2.5581 | ds_loss: 3.2888 | lr: 9.9832e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.303\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    145/  5202 | global iter:    145/  5202 | loss: 2.9221 | ds_loss: 3.7540 | lr: 9.9830e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    146/  5202 | global iter:    146/  5202 | loss: 2.3918 | ds_loss: 3.4572 | lr: 9.9828e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    147/  5202 | global iter:    147/  5202 | loss: 2.6915 | ds_loss: 3.2538 | lr: 9.9825e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    148/  5202 | global iter:    148/  5202 | loss: 1.6978 | ds_loss: 2.8289 | lr: 9.9823e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    148/  5202 | global iter:    148/  5202 | loss: 2.4258 | ds_loss: 3.3235 | lr: 9.9823e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    149/  5202 | global iter:    149/  5202 | loss: 3.1291 | ds_loss: 3.7469 | lr: 9.9820e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    150/  5202 | global iter:    150/  5202 | loss: 3.1806 | ds_loss: 3.6957 | lr: 9.9818e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    151/  5202 | global iter:    151/  5202 | loss: 2.7091 | ds_loss: 3.2653 | lr: 9.9815e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    152/  5202 | global iter:    152/  5202 | loss: 2.6448 | ds_loss: 3.3354 | lr: 9.9813e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    152/  5202 | global iter:    152/  5202 | loss: 2.9159 | ds_loss: 3.5108 | lr: 9.9813e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.303\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    153/  5202 | global iter:    153/  5202 | loss: 2.5092 | ds_loss: 3.3304 | lr: 9.9811e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    154/  5202 | global iter:    154/  5202 | loss: 3.4182 | ds_loss: 4.0491 | lr: 9.9808e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.000\n",
            "train | epoch   0 | Iter:    155/  5202 | global iter:    155/  5202 | loss: 2.2051 | ds_loss: 2.9184 | lr: 9.9806e-07 | scale:  2048.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    156/  5202 | global iter:    156/  5202 | loss: 2.1462 | ds_loss: 2.7846 | lr: 9.9803e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    156/  5202 | global iter:    156/  5202 | loss: 2.5697 | ds_loss: 3.2706 | lr: 9.9803e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    157/  5202 | global iter:    157/  5202 | loss: 2.4572 | ds_loss: 3.1212 | lr: 9.9800e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    158/  5202 | global iter:    158/  5202 | loss: 2.5661 | ds_loss: 3.8031 | lr: 9.9798e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.000\n",
            "train | epoch   0 | Iter:    159/  5202 | global iter:    159/  5202 | loss: 2.4095 | ds_loss: 3.0218 | lr: 9.9795e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    160/  5202 | global iter:    160/  5202 | loss: 2.3341 | ds_loss: 2.9797 | lr: 9.9793e-07 | scale:  2048.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    160/  5202 | global iter:    160/  5202 | loss: 2.4417 | ds_loss: 3.2314 | lr: 9.9793e-07 | scale:  2048.0000 | micro time: 0.307 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    161/  5202 | global iter:    161/  5202 | loss: 2.5165 | ds_loss: 3.7325 | lr: 9.9790e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    162/  5202 | global iter:    162/  5202 | loss: 2.4980 | ds_loss: 3.0479 | lr: 9.9787e-07 | scale:  2048.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:    163/  5202 | global iter:    163/  5202 | loss: 2.2852 | ds_loss: 3.8492 | lr: 9.9785e-07 | scale:  2048.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    164/  5202 | global iter:    164/  5202 | loss: 2.5327 | ds_loss: 3.3581 | lr: 9.9782e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    164/  5202 | global iter:    164/  5202 | loss: 2.4581 | ds_loss: 3.4969 | lr: 9.9782e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    165/  5202 | global iter:    165/  5202 | loss: 3.1647 | ds_loss: 3.8926 | lr: 9.9779e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    166/  5202 | global iter:    166/  5202 | loss: 2.6012 | ds_loss: 3.1479 | lr: 9.9777e-07 | scale:  2048.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    167/  5202 | global iter:    167/  5202 | loss: 2.9316 | ds_loss: 3.8557 | lr: 9.9774e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    168/  5202 | global iter:    168/  5202 | loss: 2.7067 | ds_loss: 3.5533 | lr: 9.9771e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    168/  5202 | global iter:    168/  5202 | loss: 2.8510 | ds_loss: 3.6124 | lr: 9.9771e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    169/  5202 | global iter:    169/  5202 | loss: 1.6540 | ds_loss: 2.5740 | lr: 9.9769e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    170/  5202 | global iter:    170/  5202 | loss: 2.5216 | ds_loss: 3.2479 | lr: 9.9766e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    171/  5202 | global iter:    171/  5202 | loss: 2.7214 | ds_loss: 3.5622 | lr: 9.9763e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.000\n",
            "train | epoch   0 | Iter:    172/  5202 | global iter:    172/  5202 | loss: 3.0390 | ds_loss: 3.7860 | lr: 9.9760e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    172/  5202 | global iter:    172/  5202 | loss: 2.4840 | ds_loss: 3.2925 | lr: 9.9760e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.302\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    173/  5202 | global iter:    173/  5202 | loss: 2.6417 | ds_loss: 3.3865 | lr: 9.9757e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    174/  5202 | global iter:    174/  5202 | loss: 2.6812 | ds_loss: 3.2366 | lr: 9.9755e-07 | scale:  2048.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    175/  5202 | global iter:    175/  5202 | loss: 1.3600 | ds_loss: 2.0267 | lr: 9.9752e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    176/  5202 | global iter:    176/  5202 | loss: 2.4797 | ds_loss: 3.3639 | lr: 9.9749e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    176/  5202 | global iter:    176/  5202 | loss: 2.2907 | ds_loss: 3.0034 | lr: 9.9749e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    177/  5202 | global iter:    177/  5202 | loss: 2.8479 | ds_loss: 3.5834 | lr: 9.9746e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    178/  5202 | global iter:    178/  5202 | loss: 2.1658 | ds_loss: 2.6774 | lr: 9.9743e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    179/  5202 | global iter:    179/  5202 | loss: 2.5725 | ds_loss: 4.2801 | lr: 9.9740e-07 | scale:  2048.0000 | micro time: 0.299 | step time: 0.000\n",
            "train | epoch   0 | Iter:    180/  5202 | global iter:    180/  5202 | loss: 1.7858 | ds_loss: 2.7765 | lr: 9.9737e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    180/  5202 | global iter:    180/  5202 | loss: 2.3430 | ds_loss: 3.3294 | lr: 9.9737e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.302\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    181/  5202 | global iter:    181/  5202 | loss: 2.7491 | ds_loss: 3.4407 | lr: 9.9734e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    182/  5202 | global iter:    182/  5202 | loss: 2.5470 | ds_loss: 3.0577 | lr: 9.9731e-07 | scale:  2048.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    183/  5202 | global iter:    183/  5202 | loss: 2.7709 | ds_loss: 3.8430 | lr: 9.9728e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    184/  5202 | global iter:    184/  5202 | loss: 1.9041 | ds_loss: 2.6481 | lr: 9.9725e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    184/  5202 | global iter:    184/  5202 | loss: 2.4928 | ds_loss: 3.2474 | lr: 9.9725e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.303\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    185/  5202 | global iter:    185/  5202 | loss: 3.2018 | ds_loss: 3.8913 | lr: 9.9722e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    186/  5202 | global iter:    186/  5202 | loss: 2.7420 | ds_loss: 3.5633 | lr: 9.9719e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    187/  5202 | global iter:    187/  5202 | loss: 3.0554 | ds_loss: 3.8629 | lr: 9.9716e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    188/  5202 | global iter:    188/  5202 | loss: 1.8315 | ds_loss: 2.7133 | lr: 9.9713e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    188/  5202 | global iter:    188/  5202 | loss: 2.7077 | ds_loss: 3.5077 | lr: 9.9713e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.303\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    189/  5202 | global iter:    189/  5202 | loss: 3.1091 | ds_loss: 3.9224 | lr: 9.9710e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    190/  5202 | global iter:    190/  5202 | loss: 2.4899 | ds_loss: 3.4648 | lr: 9.9707e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    191/  5202 | global iter:    191/  5202 | loss: 2.8668 | ds_loss: 3.5875 | lr: 9.9704e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    192/  5202 | global iter:    192/  5202 | loss: 2.5708 | ds_loss: 3.2361 | lr: 9.9701e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    192/  5202 | global iter:    192/  5202 | loss: 2.7592 | ds_loss: 3.5527 | lr: 9.9701e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.302\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    193/  5202 | global iter:    193/  5202 | loss: 2.9988 | ds_loss: 3.9133 | lr: 9.9698e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    194/  5202 | global iter:    194/  5202 | loss: 2.7685 | ds_loss: 3.6679 | lr: 9.9695e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    195/  5202 | global iter:    195/  5202 | loss: 2.5546 | ds_loss: 3.3209 | lr: 9.9692e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    196/  5202 | global iter:    196/  5202 | loss: 2.5906 | ds_loss: 3.3160 | lr: 9.9688e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    196/  5202 | global iter:    196/  5202 | loss: 2.7281 | ds_loss: 3.5545 | lr: 9.9688e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.303\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    197/  5202 | global iter:    197/  5202 | loss: 1.7351 | ds_loss: 2.6085 | lr: 9.9685e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    198/  5202 | global iter:    198/  5202 | loss: 2.4169 | ds_loss: 3.2923 | lr: 9.9682e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    199/  5202 | global iter:    199/  5202 | loss: 2.4274 | ds_loss: 3.3047 | lr: 9.9679e-07 | scale:  2048.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    200/  5202 | global iter:    200/  5202 | loss: 2.2981 | ds_loss: 3.3486 | lr: 9.9675e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    200/  5202 | global iter:    200/  5202 | loss: 2.2194 | ds_loss: 3.1385 | lr: 9.9675e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    201/  5202 | global iter:    201/  5202 | loss: 1.8522 | ds_loss: 2.7535 | lr: 9.9672e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    202/  5202 | global iter:    202/  5202 | loss: 2.5519 | ds_loss: 3.0921 | lr: 9.9669e-07 | scale:  2048.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    203/  5202 | global iter:    203/  5202 | loss: 2.5319 | ds_loss: 3.5643 | lr: 9.9666e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    204/  5202 | global iter:    204/  5202 | loss: 2.8310 | ds_loss: 3.7009 | lr: 9.9662e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    204/  5202 | global iter:    204/  5202 | loss: 2.4418 | ds_loss: 3.2777 | lr: 9.9662e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    205/  5202 | global iter:    205/  5202 | loss: 2.6244 | ds_loss: 3.2370 | lr: 9.9659e-07 | scale:  2048.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    206/  5202 | global iter:    206/  5202 | loss: 3.0602 | ds_loss: 3.5876 | lr: 9.9656e-07 | scale:  2048.0000 | micro time: 0.320 | step time: 0.000\n",
            "train | epoch   0 | Iter:    207/  5202 | global iter:    207/  5202 | loss: 2.6325 | ds_loss: 3.3097 | lr: 9.9652e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    208/  5202 | global iter:    208/  5202 | loss: 1.9111 | ds_loss: 3.0772 | lr: 9.9649e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    208/  5202 | global iter:    208/  5202 | loss: 2.5571 | ds_loss: 3.3029 | lr: 9.9649e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    209/  5202 | global iter:    209/  5202 | loss: 2.5674 | ds_loss: 3.3008 | lr: 9.9645e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    210/  5202 | global iter:    210/  5202 | loss: 3.1193 | ds_loss: 3.7114 | lr: 9.9642e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    211/  5202 | global iter:    211/  5202 | loss: 2.8517 | ds_loss: 3.7668 | lr: 9.9639e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.000\n",
            "train | epoch   0 | Iter:    212/  5202 | global iter:    212/  5202 | loss: 1.2107 | ds_loss: 1.8074 | lr: 9.9635e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    212/  5202 | global iter:    212/  5202 | loss: 2.4373 | ds_loss: 3.1466 | lr: 9.9635e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.302\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    213/  5202 | global iter:    213/  5202 | loss: 2.5067 | ds_loss: 3.5929 | lr: 9.9632e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    214/  5202 | global iter:    214/  5202 | loss: 2.6365 | ds_loss: 3.5960 | lr: 9.9628e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.000\n",
            "train | epoch   0 | Iter:    215/  5202 | global iter:    215/  5202 | loss: 1.1856 | ds_loss: 1.7666 | lr: 9.9625e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.000\n",
            "[2025-04-16 21:03:30,461] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, but hysteresis is 3. Reducing hysteresis to 2\n",
            "train | epoch   0 | Iter:    216/  5202 | global iter:    216/  5202 | loss: 2.6335 | ds_loss: 3.6162 | lr: 9.9625e-07 | scale:  2048.0000 | micro time: 0.233 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    216/  5202 | global iter:    216/  5202 | loss: 2.2406 | ds_loss: 3.1429 | lr: 9.9625e-07 | scale:  2048.0000 | micro time: 0.233 | step time: 0.284\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    217/  5202 | global iter:    217/  5202 | loss: 2.9590 | ds_loss: 3.7847 | lr: 9.9621e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    218/  5202 | global iter:    218/  5202 | loss: 2.9415 | ds_loss: 3.3855 | lr: 9.9618e-07 | scale:  2048.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:    219/  5202 | global iter:    219/  5202 | loss: 2.8454 | ds_loss: 3.9048 | lr: 9.9614e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    220/  5202 | global iter:    220/  5202 | loss: 2.8729 | ds_loss: 4.0229 | lr: 9.9611e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    220/  5202 | global iter:    220/  5202 | loss: 2.9047 | ds_loss: 3.7745 | lr: 9.9611e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    221/  5202 | global iter:    221/  5202 | loss: 2.8354 | ds_loss: 3.9070 | lr: 9.9607e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    222/  5202 | global iter:    222/  5202 | loss: 3.0391 | ds_loss: 3.7390 | lr: 9.9603e-07 | scale:  2048.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:    223/  5202 | global iter:    223/  5202 | loss: 2.9861 | ds_loss: 3.8503 | lr: 9.9600e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    224/  5202 | global iter:    224/  5202 | loss: 2.6265 | ds_loss: 3.1652 | lr: 9.9596e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    224/  5202 | global iter:    224/  5202 | loss: 2.8718 | ds_loss: 3.6654 | lr: 9.9596e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    225/  5202 | global iter:    225/  5202 | loss: 2.0353 | ds_loss: 2.9928 | lr: 9.9593e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    226/  5202 | global iter:    226/  5202 | loss: 2.1935 | ds_loss: 2.9626 | lr: 9.9589e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    227/  5202 | global iter:    227/  5202 | loss: 2.3873 | ds_loss: 2.9448 | lr: 9.9585e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    228/  5202 | global iter:    228/  5202 | loss: 1.9841 | ds_loss: 2.5901 | lr: 9.9582e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    228/  5202 | global iter:    228/  5202 | loss: 2.1501 | ds_loss: 2.8726 | lr: 9.9582e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    229/  5202 | global iter:    229/  5202 | loss: 2.7827 | ds_loss: 3.7449 | lr: 9.9578e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    230/  5202 | global iter:    230/  5202 | loss: 2.5831 | ds_loss: 3.2760 | lr: 9.9574e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.000\n",
            "train | epoch   0 | Iter:    231/  5202 | global iter:    231/  5202 | loss: 2.7212 | ds_loss: 3.7222 | lr: 9.9570e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    232/  5202 | global iter:    232/  5202 | loss: 3.0570 | ds_loss: 3.6174 | lr: 9.9567e-07 | scale:  2048.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    232/  5202 | global iter:    232/  5202 | loss: 2.7860 | ds_loss: 3.5901 | lr: 9.9567e-07 | scale:  2048.0000 | micro time: 0.311 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    233/  5202 | global iter:    233/  5202 | loss: 2.7524 | ds_loss: 3.4636 | lr: 9.9563e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    234/  5202 | global iter:    234/  5202 | loss: 2.5636 | ds_loss: 3.4571 | lr: 9.9559e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    235/  5202 | global iter:    235/  5202 | loss: 2.8129 | ds_loss: 3.6196 | lr: 9.9555e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    236/  5202 | global iter:    236/  5202 | loss: 1.6571 | ds_loss: 2.6649 | lr: 9.9551e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    236/  5202 | global iter:    236/  5202 | loss: 2.4465 | ds_loss: 3.3013 | lr: 9.9551e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.303\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    237/  5202 | global iter:    237/  5202 | loss: 3.3055 | ds_loss: 4.3987 | lr: 9.9548e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    238/  5202 | global iter:    238/  5202 | loss: 2.7141 | ds_loss: 3.5738 | lr: 9.9544e-07 | scale:  2048.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    239/  5202 | global iter:    239/  5202 | loss: 2.9011 | ds_loss: 3.6025 | lr: 9.9540e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    240/  5202 | global iter:    240/  5202 | loss: 2.7869 | ds_loss: 3.5490 | lr: 9.9536e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    240/  5202 | global iter:    240/  5202 | loss: 2.9269 | ds_loss: 3.7810 | lr: 9.9536e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.303\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    241/  5202 | global iter:    241/  5202 | loss: 2.0649 | ds_loss: 2.8903 | lr: 9.9532e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    242/  5202 | global iter:    242/  5202 | loss: 0.5205 | ds_loss: 0.8550 | lr: 9.9528e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    243/  5202 | global iter:    243/  5202 | loss: 2.6241 | ds_loss: 3.3170 | lr: 9.9524e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    244/  5202 | global iter:    244/  5202 | loss: 2.9878 | ds_loss: 3.8945 | lr: 9.9520e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    244/  5202 | global iter:    244/  5202 | loss: 2.0493 | ds_loss: 2.7392 | lr: 9.9520e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    245/  5202 | global iter:    245/  5202 | loss: 2.7081 | ds_loss: 3.6741 | lr: 9.9516e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    246/  5202 | global iter:    246/  5202 | loss: 2.8334 | ds_loss: 3.6602 | lr: 9.9512e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    247/  5202 | global iter:    247/  5202 | loss: 2.5248 | ds_loss: 3.5707 | lr: 9.9508e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    248/  5202 | global iter:    248/  5202 | loss: 2.2641 | ds_loss: 2.9887 | lr: 9.9504e-07 | scale:  2048.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    248/  5202 | global iter:    248/  5202 | loss: 2.5826 | ds_loss: 3.4734 | lr: 9.9504e-07 | scale:  2048.0000 | micro time: 0.311 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    249/  5202 | global iter:    249/  5202 | loss: 2.5986 | ds_loss: 3.4458 | lr: 9.9500e-07 | scale:  2048.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    250/  5202 | global iter:    250/  5202 | loss: 1.9073 | ds_loss: 2.9771 | lr: 9.9496e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    251/  5202 | global iter:    251/  5202 | loss: 3.0622 | ds_loss: 4.2031 | lr: 9.9492e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    252/  5202 | global iter:    252/  5202 | loss: 2.6206 | ds_loss: 3.1714 | lr: 9.9488e-07 | scale:  2048.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    252/  5202 | global iter:    252/  5202 | loss: 2.5472 | ds_loss: 3.4494 | lr: 9.9488e-07 | scale:  2048.0000 | micro time: 0.311 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    253/  5202 | global iter:    253/  5202 | loss: 3.2500 | ds_loss: 3.9224 | lr: 9.9484e-07 | scale:  2048.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    254/  5202 | global iter:    254/  5202 | loss: 2.5377 | ds_loss: 3.3206 | lr: 9.9480e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    255/  5202 | global iter:    255/  5202 | loss: 2.9620 | ds_loss: 3.5206 | lr: 9.9476e-07 | scale:  2048.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    256/  5202 | global iter:    256/  5202 | loss: 2.6896 | ds_loss: 3.5247 | lr: 9.9472e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    256/  5202 | global iter:    256/  5202 | loss: 2.8598 | ds_loss: 3.5721 | lr: 9.9472e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    257/  5202 | global iter:    257/  5202 | loss: 1.8575 | ds_loss: 2.3693 | lr: 9.9467e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    258/  5202 | global iter:    258/  5202 | loss: 2.4405 | ds_loss: 3.1522 | lr: 9.9463e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    259/  5202 | global iter:    259/  5202 | loss: 2.7474 | ds_loss: 3.6360 | lr: 9.9459e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    260/  5202 | global iter:    260/  5202 | loss: 0.5242 | ds_loss: 0.7870 | lr: 9.9455e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    260/  5202 | global iter:    260/  5202 | loss: 1.8924 | ds_loss: 2.4861 | lr: 9.9455e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.303\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    261/  5202 | global iter:    261/  5202 | loss: 2.8992 | ds_loss: 3.6330 | lr: 9.9451e-07 | scale:  2048.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:    262/  5202 | global iter:    262/  5202 | loss: 3.7364 | ds_loss: 5.0322 | lr: 9.9446e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    263/  5202 | global iter:    263/  5202 | loss: 2.2684 | ds_loss: 2.7786 | lr: 9.9442e-07 | scale:  2048.0000 | micro time: 0.315 | step time: 0.000\n",
            "[2025-04-16 21:03:45,263] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, but hysteresis is 2. Reducing hysteresis to 1\n",
            "train | epoch   0 | Iter:    264/  5202 | global iter:    264/  5202 | loss: 2.4936 | ds_loss: 3.0948 | lr: 9.9442e-07 | scale:  2048.0000 | micro time: 0.234 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    264/  5202 | global iter:    264/  5202 | loss: 2.8494 | ds_loss: 3.6347 | lr: 9.9442e-07 | scale:  2048.0000 | micro time: 0.234 | step time: 0.290\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    265/  5202 | global iter:    265/  5202 | loss: 2.8699 | ds_loss: 3.8701 | lr: 9.9438e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    266/  5202 | global iter:    266/  5202 | loss: 1.8617 | ds_loss: 2.5462 | lr: 9.9434e-07 | scale:  2048.0000 | micro time: 0.299 | step time: 0.000\n",
            "train | epoch   0 | Iter:    267/  5202 | global iter:    267/  5202 | loss: 2.5229 | ds_loss: 3.3083 | lr: 9.9429e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    268/  5202 | global iter:    268/  5202 | loss: 2.9285 | ds_loss: 3.5946 | lr: 9.9425e-07 | scale:  2048.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    268/  5202 | global iter:    268/  5202 | loss: 2.5458 | ds_loss: 3.3298 | lr: 9.9425e-07 | scale:  2048.0000 | micro time: 0.310 | step time: 0.303\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    269/  5202 | global iter:    269/  5202 | loss: 2.5834 | ds_loss: 3.5374 | lr: 9.9421e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    270/  5202 | global iter:    270/  5202 | loss: 2.7865 | ds_loss: 3.4569 | lr: 9.9416e-07 | scale:  2048.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:    271/  5202 | global iter:    271/  5202 | loss: 2.6004 | ds_loss: 3.4167 | lr: 9.9412e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    272/  5202 | global iter:    272/  5202 | loss: 2.3933 | ds_loss: 3.5990 | lr: 9.9407e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    272/  5202 | global iter:    272/  5202 | loss: 2.5909 | ds_loss: 3.5025 | lr: 9.9407e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    273/  5202 | global iter:    273/  5202 | loss: 2.7662 | ds_loss: 3.6242 | lr: 9.9403e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    274/  5202 | global iter:    274/  5202 | loss: 2.6139 | ds_loss: 3.6715 | lr: 9.9399e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    275/  5202 | global iter:    275/  5202 | loss: 2.4550 | ds_loss: 3.4223 | lr: 9.9394e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    276/  5202 | global iter:    276/  5202 | loss: 1.9908 | ds_loss: 2.6148 | lr: 9.9390e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    276/  5202 | global iter:    276/  5202 | loss: 2.4565 | ds_loss: 3.3332 | lr: 9.9390e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.303\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    277/  5202 | global iter:    277/  5202 | loss: 2.8162 | ds_loss: 3.4660 | lr: 9.9385e-07 | scale:  2048.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:    278/  5202 | global iter:    278/  5202 | loss: 3.2180 | ds_loss: 4.1810 | lr: 9.9381e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    279/  5202 | global iter:    279/  5202 | loss: 2.3693 | ds_loss: 3.0342 | lr: 9.9376e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    280/  5202 | global iter:    280/  5202 | loss: 3.0923 | ds_loss: 3.7484 | lr: 9.9372e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    280/  5202 | global iter:    280/  5202 | loss: 2.8739 | ds_loss: 3.6074 | lr: 9.9372e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    281/  5202 | global iter:    281/  5202 | loss: 2.3469 | ds_loss: 2.8985 | lr: 9.9367e-07 | scale:  2048.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    282/  5202 | global iter:    282/  5202 | loss: 3.0460 | ds_loss: 3.6841 | lr: 9.9363e-07 | scale:  2048.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:    283/  5202 | global iter:    283/  5202 | loss: 0.9724 | ds_loss: 1.5645 | lr: 9.9358e-07 | scale:  2048.0000 | micro time: 0.299 | step time: 0.000\n",
            "train | epoch   0 | Iter:    284/  5202 | global iter:    284/  5202 | loss: 2.5202 | ds_loss: 3.4819 | lr: 9.9354e-07 | scale:  2048.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    284/  5202 | global iter:    284/  5202 | loss: 2.2214 | ds_loss: 2.9073 | lr: 9.9354e-07 | scale:  2048.0000 | micro time: 0.308 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    285/  5202 | global iter:    285/  5202 | loss: 2.6077 | ds_loss: 3.5825 | lr: 9.9349e-07 | scale:  2048.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:    286/  5202 | global iter:    286/  5202 | loss: 3.2962 | ds_loss: 3.9919 | lr: 9.9344e-07 | scale:  2048.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    287/  5202 | global iter:    287/  5202 | loss: 3.1369 | ds_loss: 4.0419 | lr: 9.9340e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    288/  5202 | global iter:    288/  5202 | loss: 3.3259 | ds_loss: 4.0291 | lr: 9.9335e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    288/  5202 | global iter:    288/  5202 | loss: 3.0917 | ds_loss: 3.9114 | lr: 9.9335e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    289/  5202 | global iter:    289/  5202 | loss: 2.8112 | ds_loss: 3.6820 | lr: 9.9330e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    290/  5202 | global iter:    290/  5202 | loss: 2.3413 | ds_loss: 3.3518 | lr: 9.9326e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    291/  5202 | global iter:    291/  5202 | loss: 1.9855 | ds_loss: 2.9019 | lr: 9.9321e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    292/  5202 | global iter:    292/  5202 | loss: 2.8152 | ds_loss: 3.5574 | lr: 9.9316e-07 | scale:  2048.0000 | micro time: 0.322 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    292/  5202 | global iter:    292/  5202 | loss: 2.4883 | ds_loss: 3.3733 | lr: 9.9316e-07 | scale:  2048.0000 | micro time: 0.322 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    293/  5202 | global iter:    293/  5202 | loss: 3.1367 | ds_loss: 3.9184 | lr: 9.9312e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    294/  5202 | global iter:    294/  5202 | loss: 2.8015 | ds_loss: 3.6773 | lr: 9.9307e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    295/  5202 | global iter:    295/  5202 | loss: 2.7571 | ds_loss: 3.4389 | lr: 9.9302e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    296/  5202 | global iter:    296/  5202 | loss: 3.1525 | ds_loss: 3.9465 | lr: 9.9297e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    296/  5202 | global iter:    296/  5202 | loss: 2.9620 | ds_loss: 3.7453 | lr: 9.9297e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.303\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    297/  5202 | global iter:    297/  5202 | loss: 0.7248 | ds_loss: 0.9788 | lr: 9.9293e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    298/  5202 | global iter:    298/  5202 | loss: 2.9257 | ds_loss: 3.5093 | lr: 9.9288e-07 | scale:  2048.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:    299/  5202 | global iter:    299/  5202 | loss: 0.9523 | ds_loss: 1.5121 | lr: 9.9283e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    300/  5202 | global iter:    300/  5202 | loss: 2.9096 | ds_loss: 3.6131 | lr: 9.9278e-07 | scale:  2048.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    300/  5202 | global iter:    300/  5202 | loss: 1.8781 | ds_loss: 2.4033 | lr: 9.9278e-07 | scale:  2048.0000 | micro time: 0.308 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    301/  5202 | global iter:    301/  5202 | loss: 2.8016 | ds_loss: 3.4553 | lr: 9.9273e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    302/  5202 | global iter:    302/  5202 | loss: 1.8569 | ds_loss: 2.7256 | lr: 9.9268e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.000\n",
            "train | epoch   0 | Iter:    303/  5202 | global iter:    303/  5202 | loss: 2.9161 | ds_loss: 3.6702 | lr: 9.9263e-07 | scale:  2048.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:    304/  5202 | global iter:    304/  5202 | loss: 2.8472 | ds_loss: 3.4678 | lr: 9.9259e-07 | scale:  2048.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    304/  5202 | global iter:    304/  5202 | loss: 2.6055 | ds_loss: 3.3297 | lr: 9.9259e-07 | scale:  2048.0000 | micro time: 0.310 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    305/  5202 | global iter:    305/  5202 | loss: 1.9273 | ds_loss: 2.9226 | lr: 9.9254e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    306/  5202 | global iter:    306/  5202 | loss: 3.0869 | ds_loss: 3.8049 | lr: 9.9249e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    307/  5202 | global iter:    307/  5202 | loss: 2.8557 | ds_loss: 3.7982 | lr: 9.9244e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    308/  5202 | global iter:    308/  5202 | loss: 2.7669 | ds_loss: 3.4478 | lr: 9.9239e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    308/  5202 | global iter:    308/  5202 | loss: 2.6592 | ds_loss: 3.4934 | lr: 9.9239e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    309/  5202 | global iter:    309/  5202 | loss: 2.9809 | ds_loss: 3.8944 | lr: 9.9234e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    310/  5202 | global iter:    310/  5202 | loss: 1.7085 | ds_loss: 2.2051 | lr: 9.9229e-07 | scale:  2048.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    311/  5202 | global iter:    311/  5202 | loss: 2.3189 | ds_loss: 3.2016 | lr: 9.9224e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    312/  5202 | global iter:    312/  5202 | loss: 2.9955 | ds_loss: 3.7060 | lr: 9.9219e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    312/  5202 | global iter:    312/  5202 | loss: 2.5010 | ds_loss: 3.2518 | lr: 9.9219e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    313/  5202 | global iter:    313/  5202 | loss: 1.9592 | ds_loss: 2.7051 | lr: 9.9214e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    314/  5202 | global iter:    314/  5202 | loss: 2.2245 | ds_loss: 2.9988 | lr: 9.9209e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    315/  5202 | global iter:    315/  5202 | loss: 2.6610 | ds_loss: 3.2651 | lr: 9.9204e-07 | scale:  2048.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    316/  5202 | global iter:    316/  5202 | loss: 2.7501 | ds_loss: 3.4893 | lr: 9.9198e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    316/  5202 | global iter:    316/  5202 | loss: 2.3987 | ds_loss: 3.1146 | lr: 9.9198e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    317/  5202 | global iter:    317/  5202 | loss: 1.7682 | ds_loss: 2.9286 | lr: 9.9193e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    318/  5202 | global iter:    318/  5202 | loss: 2.7904 | ds_loss: 3.8831 | lr: 9.9188e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.000\n",
            "train | epoch   0 | Iter:    319/  5202 | global iter:    319/  5202 | loss: 1.9278 | ds_loss: 3.0777 | lr: 9.9183e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.000\n",
            "train | epoch   0 | Iter:    320/  5202 | global iter:    320/  5202 | loss: 1.6649 | ds_loss: 2.5615 | lr: 9.9178e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    320/  5202 | global iter:    320/  5202 | loss: 2.0378 | ds_loss: 3.1127 | lr: 9.9178e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.301\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    321/  5202 | global iter:    321/  5202 | loss: 2.8770 | ds_loss: 3.8139 | lr: 9.9173e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    322/  5202 | global iter:    322/  5202 | loss: 3.1449 | ds_loss: 4.0875 | lr: 9.9168e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    323/  5202 | global iter:    323/  5202 | loss: 2.9538 | ds_loss: 3.6756 | lr: 9.9162e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    324/  5202 | global iter:    324/  5202 | loss: 2.8655 | ds_loss: 3.6338 | lr: 9.9157e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    324/  5202 | global iter:    324/  5202 | loss: 2.9603 | ds_loss: 3.8027 | lr: 9.9157e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    325/  5202 | global iter:    325/  5202 | loss: 2.2227 | ds_loss: 3.3731 | lr: 9.9152e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    326/  5202 | global iter:    326/  5202 | loss: 3.0380 | ds_loss: 3.7908 | lr: 9.9147e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    327/  5202 | global iter:    327/  5202 | loss: 3.0445 | ds_loss: 3.8016 | lr: 9.9141e-07 | scale:  2048.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    328/  5202 | global iter:    328/  5202 | loss: 2.7795 | ds_loss: 3.4867 | lr: 9.9136e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    328/  5202 | global iter:    328/  5202 | loss: 2.7712 | ds_loss: 3.6130 | lr: 9.9136e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    329/  5202 | global iter:    329/  5202 | loss: 2.8166 | ds_loss: 3.5038 | lr: 9.9131e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    330/  5202 | global iter:    330/  5202 | loss: 1.7419 | ds_loss: 2.7347 | lr: 9.9125e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    331/  5202 | global iter:    331/  5202 | loss: 2.4498 | ds_loss: 3.6505 | lr: 9.9120e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    332/  5202 | global iter:    332/  5202 | loss: 3.2959 | ds_loss: 3.9330 | lr: 9.9115e-07 | scale:  2048.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    332/  5202 | global iter:    332/  5202 | loss: 2.5760 | ds_loss: 3.4555 | lr: 9.9115e-07 | scale:  2048.0000 | micro time: 0.310 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    333/  5202 | global iter:    333/  5202 | loss: 2.8788 | ds_loss: 3.5678 | lr: 9.9109e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    334/  5202 | global iter:    334/  5202 | loss: 2.8525 | ds_loss: 3.6553 | lr: 9.9104e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    335/  5202 | global iter:    335/  5202 | loss: 2.8596 | ds_loss: 3.6453 | lr: 9.9099e-07 | scale:  2048.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:    336/  5202 | global iter:    336/  5202 | loss: 2.5079 | ds_loss: 3.4281 | lr: 9.9093e-07 | scale:  2048.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    336/  5202 | global iter:    336/  5202 | loss: 2.7747 | ds_loss: 3.5741 | lr: 9.9093e-07 | scale:  2048.0000 | micro time: 0.309 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    337/  5202 | global iter:    337/  5202 | loss: 2.5851 | ds_loss: 3.4581 | lr: 9.9088e-07 | scale:  2048.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    338/  5202 | global iter:    338/  5202 | loss: 2.5734 | ds_loss: 3.2174 | lr: 9.9082e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    339/  5202 | global iter:    339/  5202 | loss: 2.8385 | ds_loss: 3.6772 | lr: 9.9077e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    340/  5202 | global iter:    340/  5202 | loss: 2.1478 | ds_loss: 2.7586 | lr: 9.9071e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    340/  5202 | global iter:    340/  5202 | loss: 2.5362 | ds_loss: 3.2778 | lr: 9.9071e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    341/  5202 | global iter:    341/  5202 | loss: 2.2781 | ds_loss: 3.0397 | lr: 9.9066e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    342/  5202 | global iter:    342/  5202 | loss: 2.4972 | ds_loss: 3.6916 | lr: 9.9060e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    343/  5202 | global iter:    343/  5202 | loss: 2.5655 | ds_loss: 3.4915 | lr: 9.9055e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    344/  5202 | global iter:    344/  5202 | loss: 2.9485 | ds_loss: 3.7168 | lr: 9.9049e-07 | scale:  2048.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    344/  5202 | global iter:    344/  5202 | loss: 2.5723 | ds_loss: 3.4849 | lr: 9.9049e-07 | scale:  2048.0000 | micro time: 0.308 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    345/  5202 | global iter:    345/  5202 | loss: 3.1724 | ds_loss: 3.9700 | lr: 9.9044e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    346/  5202 | global iter:    346/  5202 | loss: 2.4925 | ds_loss: 3.4344 | lr: 9.9038e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    347/  5202 | global iter:    347/  5202 | loss: 2.9403 | ds_loss: 3.9912 | lr: 9.9032e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    348/  5202 | global iter:    348/  5202 | loss: 2.7811 | ds_loss: 3.6752 | lr: 9.9027e-07 | scale:  2048.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    348/  5202 | global iter:    348/  5202 | loss: 2.8466 | ds_loss: 3.7677 | lr: 9.9027e-07 | scale:  2048.0000 | micro time: 0.307 | step time: 0.303\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    349/  5202 | global iter:    349/  5202 | loss: 1.5282 | ds_loss: 2.1684 | lr: 9.9021e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    350/  5202 | global iter:    350/  5202 | loss: 2.3297 | ds_loss: 3.3331 | lr: 9.9016e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    351/  5202 | global iter:    351/  5202 | loss: 2.2751 | ds_loss: 3.0264 | lr: 9.9010e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    352/  5202 | global iter:    352/  5202 | loss: 1.5471 | ds_loss: 2.5446 | lr: 9.9004e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    352/  5202 | global iter:    352/  5202 | loss: 1.9200 | ds_loss: 2.7681 | lr: 9.9004e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    353/  5202 | global iter:    353/  5202 | loss: 2.9346 | ds_loss: 3.6381 | lr: 9.8998e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    354/  5202 | global iter:    354/  5202 | loss: 3.0518 | ds_loss: 3.7714 | lr: 9.8993e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    355/  5202 | global iter:    355/  5202 | loss: 2.4035 | ds_loss: 3.1520 | lr: 9.8987e-07 | scale:  2048.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    356/  5202 | global iter:    356/  5202 | loss: 1.3935 | ds_loss: 1.7460 | lr: 9.8981e-07 | scale:  2048.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    356/  5202 | global iter:    356/  5202 | loss: 2.4458 | ds_loss: 3.0769 | lr: 9.8981e-07 | scale:  2048.0000 | micro time: 0.308 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    357/  5202 | global iter:    357/  5202 | loss: 2.4901 | ds_loss: 3.1566 | lr: 9.8976e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    358/  5202 | global iter:    358/  5202 | loss: 1.8530 | ds_loss: 2.4972 | lr: 9.8970e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    359/  5202 | global iter:    359/  5202 | loss: 1.2382 | ds_loss: 1.9363 | lr: 9.8964e-07 | scale:  2048.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    360/  5202 | global iter:    360/  5202 | loss: 3.4221 | ds_loss: 4.0977 | lr: 9.8958e-07 | scale:  2048.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    360/  5202 | global iter:    360/  5202 | loss: 2.2509 | ds_loss: 2.9220 | lr: 9.8958e-07 | scale:  2048.0000 | micro time: 0.307 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    361/  5202 | global iter:    361/  5202 | loss: 1.3085 | ds_loss: 1.8763 | lr: 9.8952e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    362/  5202 | global iter:    362/  5202 | loss: 3.0818 | ds_loss: 3.7979 | lr: 9.8947e-07 | scale:  2048.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    363/  5202 | global iter:    363/  5202 | loss: 2.6464 | ds_loss: 3.6737 | lr: 9.8941e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    364/  5202 | global iter:    364/  5202 | loss: 2.8729 | ds_loss: 3.4932 | lr: 9.8935e-07 | scale:  2048.0000 | micro time: 0.312 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    364/  5202 | global iter:    364/  5202 | loss: 2.4774 | ds_loss: 3.2103 | lr: 9.8935e-07 | scale:  2048.0000 | micro time: 0.312 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    365/  5202 | global iter:    365/  5202 | loss: 1.8602 | ds_loss: 2.9701 | lr: 9.8929e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    366/  5202 | global iter:    366/  5202 | loss: 2.9879 | ds_loss: 3.6893 | lr: 9.8923e-07 | scale:  2048.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:    367/  5202 | global iter:    367/  5202 | loss: 1.4026 | ds_loss: 1.8601 | lr: 9.8917e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    368/  5202 | global iter:    368/  5202 | loss: 2.5676 | ds_loss: 3.7593 | lr: 9.8911e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    368/  5202 | global iter:    368/  5202 | loss: 2.2046 | ds_loss: 3.0697 | lr: 9.8911e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    369/  5202 | global iter:    369/  5202 | loss: 1.5651 | ds_loss: 2.0147 | lr: 9.8905e-07 | scale:  2048.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    370/  5202 | global iter:    370/  5202 | loss: 0.9929 | ds_loss: 1.5170 | lr: 9.8899e-07 | scale:  2048.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    371/  5202 | global iter:    371/  5202 | loss: 3.1059 | ds_loss: 3.9656 | lr: 9.8893e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    372/  5202 | global iter:    372/  5202 | loss: 2.7619 | ds_loss: 3.6468 | lr: 9.8887e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    372/  5202 | global iter:    372/  5202 | loss: 2.1064 | ds_loss: 2.7860 | lr: 9.8887e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    373/  5202 | global iter:    373/  5202 | loss: 2.4949 | ds_loss: 3.4455 | lr: 9.8881e-07 | scale:  2048.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    374/  5202 | global iter:    374/  5202 | loss: 2.4830 | ds_loss: 3.4776 | lr: 9.8875e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    375/  5202 | global iter:    375/  5202 | loss: 2.6373 | ds_loss: 3.2505 | lr: 9.8869e-07 | scale:  2048.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    376/  5202 | global iter:    376/  5202 | loss: 2.9781 | ds_loss: 3.9179 | lr: 9.8863e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    376/  5202 | global iter:    376/  5202 | loss: 2.6483 | ds_loss: 3.5229 | lr: 9.8863e-07 | scale:  2048.0000 | micro time: 0.300 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    377/  5202 | global iter:    377/  5202 | loss: 2.8136 | ds_loss: 3.4908 | lr: 9.8857e-07 | scale:  2048.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    378/  5202 | global iter:    378/  5202 | loss: 2.7574 | ds_loss: 3.5787 | lr: 9.8851e-07 | scale:  2048.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    379/  5202 | global iter:    379/  5202 | loss: 1.7646 | ds_loss: 2.4884 | lr: 9.8845e-07 | scale:  2048.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:    380/  5202 | global iter:    380/  5202 | loss: 2.6858 | ds_loss: 3.4172 | lr: 9.8839e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    380/  5202 | global iter:    380/  5202 | loss: 2.5053 | ds_loss: 3.2438 | lr: 9.8839e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    381/  5202 | global iter:    381/  5202 | loss: 2.4776 | ds_loss: 4.0204 | lr: 9.8833e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    382/  5202 | global iter:    382/  5202 | loss: 2.7134 | ds_loss: 3.5398 | lr: 9.8826e-07 | scale:  2048.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    383/  5202 | global iter:    383/  5202 | loss: 2.8997 | ds_loss: 3.6441 | lr: 9.8820e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    384/  5202 | global iter:    384/  5202 | loss: 2.9921 | ds_loss: 3.7874 | lr: 9.8814e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    384/  5202 | global iter:    384/  5202 | loss: 2.7707 | ds_loss: 3.7479 | lr: 9.8814e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    385/  5202 | global iter:    385/  5202 | loss: 2.7247 | ds_loss: 3.3950 | lr: 9.8808e-07 | scale:  2048.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    386/  5202 | global iter:    386/  5202 | loss: 3.0261 | ds_loss: 3.6275 | lr: 9.8802e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    387/  5202 | global iter:    387/  5202 | loss: 2.4378 | ds_loss: 3.3555 | lr: 9.8795e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    388/  5202 | global iter:    388/  5202 | loss: 3.1603 | ds_loss: 4.5769 | lr: 9.8789e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    388/  5202 | global iter:    388/  5202 | loss: 2.8372 | ds_loss: 3.7387 | lr: 9.8789e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    389/  5202 | global iter:    389/  5202 | loss: 3.2421 | ds_loss: 4.4300 | lr: 9.8783e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    390/  5202 | global iter:    390/  5202 | loss: 2.4205 | ds_loss: 3.0456 | lr: 9.8777e-07 | scale:  2048.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    391/  5202 | global iter:    391/  5202 | loss: 1.3140 | ds_loss: 1.8876 | lr: 9.8770e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    392/  5202 | global iter:    392/  5202 | loss: 1.8001 | ds_loss: 2.6244 | lr: 9.8764e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    392/  5202 | global iter:    392/  5202 | loss: 2.1942 | ds_loss: 2.9969 | lr: 9.8764e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    393/  5202 | global iter:    393/  5202 | loss: 2.7102 | ds_loss: 3.7098 | lr: 9.8758e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    394/  5202 | global iter:    394/  5202 | loss: 2.8640 | ds_loss: 3.5461 | lr: 9.8751e-07 | scale:  2048.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    395/  5202 | global iter:    395/  5202 | loss: 2.9817 | ds_loss: 4.4985 | lr: 9.8745e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    396/  5202 | global iter:    396/  5202 | loss: 3.0154 | ds_loss: 3.7669 | lr: 9.8739e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    396/  5202 | global iter:    396/  5202 | loss: 2.8929 | ds_loss: 3.8803 | lr: 9.8739e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    397/  5202 | global iter:    397/  5202 | loss: 3.1383 | ds_loss: 3.9013 | lr: 9.8732e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    398/  5202 | global iter:    398/  5202 | loss: 2.8401 | ds_loss: 3.4874 | lr: 9.8726e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    399/  5202 | global iter:    399/  5202 | loss: 2.9096 | ds_loss: 3.8674 | lr: 9.8719e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    400/  5202 | global iter:    400/  5202 | loss: 2.6805 | ds_loss: 3.2461 | lr: 9.8713e-07 | scale:  2048.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    400/  5202 | global iter:    400/  5202 | loss: 2.8921 | ds_loss: 3.6256 | lr: 9.8713e-07 | scale:  2048.0000 | micro time: 0.311 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    401/  5202 | global iter:    401/  5202 | loss: 2.5474 | ds_loss: 3.4566 | lr: 9.8706e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    402/  5202 | global iter:    402/  5202 | loss: 3.2509 | ds_loss: 3.9440 | lr: 9.8700e-07 | scale:  2048.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:    403/  5202 | global iter:    403/  5202 | loss: 2.8241 | ds_loss: 3.7053 | lr: 9.8693e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    404/  5202 | global iter:    404/  5202 | loss: 2.4544 | ds_loss: 3.4902 | lr: 9.8687e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    404/  5202 | global iter:    404/  5202 | loss: 2.7692 | ds_loss: 3.6490 | lr: 9.8687e-07 | scale:  2048.0000 | micro time: 0.301 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    405/  5202 | global iter:    405/  5202 | loss: 2.4816 | ds_loss: 3.6869 | lr: 9.8680e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    406/  5202 | global iter:    406/  5202 | loss: 2.9873 | ds_loss: 3.7239 | lr: 9.8674e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    407/  5202 | global iter:    407/  5202 | loss: 2.4956 | ds_loss: 3.2032 | lr: 9.8667e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    408/  5202 | global iter:    408/  5202 | loss: 2.5863 | ds_loss: 3.4153 | lr: 9.8661e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    408/  5202 | global iter:    408/  5202 | loss: 2.6377 | ds_loss: 3.5073 | lr: 9.8661e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    409/  5202 | global iter:    409/  5202 | loss: 1.9587 | ds_loss: 2.7757 | lr: 9.8654e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    410/  5202 | global iter:    410/  5202 | loss: 2.7155 | ds_loss: 3.4466 | lr: 9.8647e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    411/  5202 | global iter:    411/  5202 | loss: 1.6283 | ds_loss: 2.2683 | lr: 9.8641e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    412/  5202 | global iter:    412/  5202 | loss: 2.5682 | ds_loss: 3.8588 | lr: 9.8634e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    412/  5202 | global iter:    412/  5202 | loss: 2.2177 | ds_loss: 3.0873 | lr: 9.8634e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    413/  5202 | global iter:    413/  5202 | loss: 1.8915 | ds_loss: 2.6053 | lr: 9.8628e-07 | scale:  2048.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:    414/  5202 | global iter:    414/  5202 | loss: 3.1910 | ds_loss: 3.7507 | lr: 9.8621e-07 | scale:  2048.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:    415/  5202 | global iter:    415/  5202 | loss: 2.9891 | ds_loss: 3.6522 | lr: 9.8614e-07 | scale:  2048.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:    416/  5202 | global iter:    416/  5202 | loss: 3.1124 | ds_loss: 4.1776 | lr: 9.8608e-07 | scale:  2048.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    416/  5202 | global iter:    416/  5202 | loss: 2.7960 | ds_loss: 3.5464 | lr: 9.8608e-07 | scale:  2048.0000 | micro time: 0.308 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    417/  5202 | global iter:    417/  5202 | loss: 3.0389 | ds_loss: 4.2116 | lr: 9.8601e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    418/  5202 | global iter:    418/  5202 | loss: 3.1173 | ds_loss: 3.8926 | lr: 9.8594e-07 | scale:  2048.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:    419/  5202 | global iter:    419/  5202 | loss: 1.6341 | ds_loss: 2.4885 | lr: 9.8587e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    420/  5202 | global iter:    420/  5202 | loss: 1.4671 | ds_loss: 1.9189 | lr: 9.8581e-07 | scale:  2048.0000 | micro time: 0.316 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    420/  5202 | global iter:    420/  5202 | loss: 2.3143 | ds_loss: 3.1279 | lr: 9.8581e-07 | scale:  2048.0000 | micro time: 0.316 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    421/  5202 | global iter:    421/  5202 | loss: 1.8926 | ds_loss: 2.3931 | lr: 9.8574e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    422/  5202 | global iter:    422/  5202 | loss: 3.1915 | ds_loss: 3.8029 | lr: 9.8567e-07 | scale:  2048.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    423/  5202 | global iter:    423/  5202 | loss: 2.9116 | ds_loss: 3.6430 | lr: 9.8560e-07 | scale:  2048.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    424/  5202 | global iter:    424/  5202 | loss: 2.7113 | ds_loss: 3.6187 | lr: 9.8553e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    424/  5202 | global iter:    424/  5202 | loss: 2.6768 | ds_loss: 3.3644 | lr: 9.8553e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    425/  5202 | global iter:    425/  5202 | loss: 2.8291 | ds_loss: 3.4706 | lr: 9.8547e-07 | scale:  2048.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    426/  5202 | global iter:    426/  5202 | loss: 3.0057 | ds_loss: 3.7093 | lr: 9.8540e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    427/  5202 | global iter:    427/  5202 | loss: 2.3369 | ds_loss: 2.9830 | lr: 9.8533e-07 | scale:  2048.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:    428/  5202 | global iter:    428/  5202 | loss: 2.8970 | ds_loss: 3.7690 | lr: 9.8526e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    428/  5202 | global iter:    428/  5202 | loss: 2.7672 | ds_loss: 3.4830 | lr: 9.8526e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    429/  5202 | global iter:    429/  5202 | loss: 2.7500 | ds_loss: 3.7418 | lr: 9.8519e-07 | scale:  2048.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    430/  5202 | global iter:    430/  5202 | loss: 3.3388 | ds_loss: 4.1517 | lr: 9.8512e-07 | scale:  2048.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    431/  5202 | global iter:    431/  5202 | loss: 2.6164 | ds_loss: 4.3372 | lr: 9.8505e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    432/  5202 | global iter:    432/  5202 | loss: 2.7961 | ds_loss: 3.7095 | lr: 9.8498e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    432/  5202 | global iter:    432/  5202 | loss: 2.8753 | ds_loss: 3.9851 | lr: 9.8498e-07 | scale:  2048.0000 | micro time: 0.303 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    433/  5202 | global iter:    433/  5202 | loss: 1.6129 | ds_loss: 2.3966 | lr: 9.8491e-07 | scale:  2048.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    434/  5202 | global iter:    434/  5202 | loss: 2.7237 | ds_loss: 3.6391 | lr: 9.8484e-07 | scale:  2048.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    435/  5202 | global iter:    435/  5202 | loss: 3.0712 | ds_loss: 3.7594 | lr: 9.8477e-07 | scale:  2048.0000 | micro time: 0.308 | step time: 0.000\n",
            "[2025-04-16 21:04:38,667] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, reducing to 1024\n",
            "train | epoch   0 | Iter:    436/  5202 | global iter:    436/  5202 | loss: 2.9383 | ds_loss: 3.9031 | lr: 9.8477e-07 | scale:  1024.0000 | micro time: 0.236 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    436/  5202 | global iter:    436/  5202 | loss: 2.5865 | ds_loss: 3.4246 | lr: 9.8477e-07 | scale:  1024.0000 | micro time: 0.236 | step time: 0.288\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    437/  5202 | global iter:    437/  5202 | loss: 2.8884 | ds_loss: 3.6122 | lr: 9.8470e-07 | scale:  1024.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    438/  5202 | global iter:    438/  5202 | loss: 2.2677 | ds_loss: 3.3478 | lr: 9.8463e-07 | scale:  1024.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    439/  5202 | global iter:    439/  5202 | loss: 2.2122 | ds_loss: 3.6982 | lr: 9.8456e-07 | scale:  1024.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    440/  5202 | global iter:    440/  5202 | loss: 2.7979 | ds_loss: 3.3577 | lr: 9.8449e-07 | scale:  1024.0000 | micro time: 0.312 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    440/  5202 | global iter:    440/  5202 | loss: 2.5416 | ds_loss: 3.5040 | lr: 9.8449e-07 | scale:  1024.0000 | micro time: 0.312 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    441/  5202 | global iter:    441/  5202 | loss: 3.0009 | ds_loss: 3.8731 | lr: 9.8442e-07 | scale:  1024.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    442/  5202 | global iter:    442/  5202 | loss: 1.5559 | ds_loss: 2.4460 | lr: 9.8435e-07 | scale:  1024.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    443/  5202 | global iter:    443/  5202 | loss: 1.7999 | ds_loss: 2.1807 | lr: 9.8428e-07 | scale:  1024.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    444/  5202 | global iter:    444/  5202 | loss: 2.5372 | ds_loss: 3.0323 | lr: 9.8421e-07 | scale:  1024.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    444/  5202 | global iter:    444/  5202 | loss: 2.2234 | ds_loss: 2.8830 | lr: 9.8421e-07 | scale:  1024.0000 | micro time: 0.308 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    445/  5202 | global iter:    445/  5202 | loss: 2.8945 | ds_loss: 3.6006 | lr: 9.8413e-07 | scale:  1024.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    446/  5202 | global iter:    446/  5202 | loss: 1.7813 | ds_loss: 2.6318 | lr: 9.8406e-07 | scale:  1024.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    447/  5202 | global iter:    447/  5202 | loss: 2.5978 | ds_loss: 3.3750 | lr: 9.8399e-07 | scale:  1024.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    448/  5202 | global iter:    448/  5202 | loss: 2.3524 | ds_loss: 3.0164 | lr: 9.8392e-07 | scale:  1024.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    448/  5202 | global iter:    448/  5202 | loss: 2.4065 | ds_loss: 3.1559 | lr: 9.8392e-07 | scale:  1024.0000 | micro time: 0.305 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    449/  5202 | global iter:    449/  5202 | loss: 2.6497 | ds_loss: 3.3727 | lr: 9.8385e-07 | scale:  1024.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    450/  5202 | global iter:    450/  5202 | loss: 2.8538 | ds_loss: 3.4091 | lr: 9.8378e-07 | scale:  1024.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    451/  5202 | global iter:    451/  5202 | loss: 2.8997 | ds_loss: 3.8086 | lr: 9.8370e-07 | scale:  1024.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    452/  5202 | global iter:    452/  5202 | loss: 0.5151 | ds_loss: 0.7283 | lr: 9.8363e-07 | scale:  1024.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    452/  5202 | global iter:    452/  5202 | loss: 2.2295 | ds_loss: 2.8297 | lr: 9.8363e-07 | scale:  1024.0000 | micro time: 0.308 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    453/  5202 | global iter:    453/  5202 | loss: 2.9305 | ds_loss: 3.6331 | lr: 9.8356e-07 | scale:  1024.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    454/  5202 | global iter:    454/  5202 | loss: 0.8975 | ds_loss: 1.5095 | lr: 9.8348e-07 | scale:  1024.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    455/  5202 | global iter:    455/  5202 | loss: 3.2448 | ds_loss: 3.8684 | lr: 9.8341e-07 | scale:  1024.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:    456/  5202 | global iter:    456/  5202 | loss: 2.6265 | ds_loss: 3.6944 | lr: 9.8334e-07 | scale:  1024.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    456/  5202 | global iter:    456/  5202 | loss: 2.4248 | ds_loss: 3.1764 | lr: 9.8334e-07 | scale:  1024.0000 | micro time: 0.306 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    457/  5202 | global iter:    457/  5202 | loss: 2.8300 | ds_loss: 3.9265 | lr: 9.8326e-07 | scale:  1024.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    458/  5202 | global iter:    458/  5202 | loss: 2.7455 | ds_loss: 3.2993 | lr: 9.8319e-07 | scale:  1024.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:    459/  5202 | global iter:    459/  5202 | loss: 2.5179 | ds_loss: 3.6341 | lr: 9.8312e-07 | scale:  1024.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    460/  5202 | global iter:    460/  5202 | loss: 3.4250 | ds_loss: 4.2087 | lr: 9.8304e-07 | scale:  1024.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    460/  5202 | global iter:    460/  5202 | loss: 2.8796 | ds_loss: 3.7671 | lr: 9.8304e-07 | scale:  1024.0000 | micro time: 0.308 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    461/  5202 | global iter:    461/  5202 | loss: 2.5733 | ds_loss: 3.8236 | lr: 9.8297e-07 | scale:  1024.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    462/  5202 | global iter:    462/  5202 | loss: 2.9195 | ds_loss: 3.5533 | lr: 9.8290e-07 | scale:  1024.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:    463/  5202 | global iter:    463/  5202 | loss: 2.7974 | ds_loss: 3.4657 | lr: 9.8282e-07 | scale:  1024.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    464/  5202 | global iter:    464/  5202 | loss: 1.4218 | ds_loss: 2.2404 | lr: 9.8275e-07 | scale:  1024.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    464/  5202 | global iter:    464/  5202 | loss: 2.4280 | ds_loss: 3.2708 | lr: 9.8275e-07 | scale:  1024.0000 | micro time: 0.307 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    465/  5202 | global iter:    465/  5202 | loss: 2.0323 | ds_loss: 3.1035 | lr: 9.8267e-07 | scale:  1024.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    466/  5202 | global iter:    466/  5202 | loss: 2.1333 | ds_loss: 2.7681 | lr: 9.8260e-07 | scale:  1024.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    467/  5202 | global iter:    467/  5202 | loss: 2.4017 | ds_loss: 3.0667 | lr: 9.8252e-07 | scale:  1024.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    468/  5202 | global iter:    468/  5202 | loss: 2.5723 | ds_loss: 3.1127 | lr: 9.8245e-07 | scale:  1024.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    468/  5202 | global iter:    468/  5202 | loss: 2.2849 | ds_loss: 3.0127 | lr: 9.8245e-07 | scale:  1024.0000 | micro time: 0.303 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    469/  5202 | global iter:    469/  5202 | loss: 2.7811 | ds_loss: 3.6205 | lr: 9.8237e-07 | scale:  1024.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    470/  5202 | global iter:    470/  5202 | loss: 2.4904 | ds_loss: 3.5714 | lr: 9.8230e-07 | scale:  1024.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    471/  5202 | global iter:    471/  5202 | loss: 3.0637 | ds_loss: 3.9369 | lr: 9.8222e-07 | scale:  1024.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    472/  5202 | global iter:    472/  5202 | loss: 2.8794 | ds_loss: 3.6519 | lr: 9.8215e-07 | scale:  1024.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    472/  5202 | global iter:    472/  5202 | loss: 2.8037 | ds_loss: 3.6952 | lr: 9.8215e-07 | scale:  1024.0000 | micro time: 0.306 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    473/  5202 | global iter:    473/  5202 | loss: 2.6487 | ds_loss: 3.2970 | lr: 9.8207e-07 | scale:  1024.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    474/  5202 | global iter:    474/  5202 | loss: 2.8107 | ds_loss: 3.8982 | lr: 9.8199e-07 | scale:  1024.0000 | micro time: 0.296 | step time: 0.000\n",
            "train | epoch   0 | Iter:    475/  5202 | global iter:    475/  5202 | loss: 1.9754 | ds_loss: 2.7175 | lr: 9.8192e-07 | scale:  1024.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    476/  5202 | global iter:    476/  5202 | loss: 2.0823 | ds_loss: 3.1073 | lr: 9.8184e-07 | scale:  1024.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    476/  5202 | global iter:    476/  5202 | loss: 2.3793 | ds_loss: 3.2550 | lr: 9.8184e-07 | scale:  1024.0000 | micro time: 0.304 | step time: 0.303\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    477/  5202 | global iter:    477/  5202 | loss: 2.8304 | ds_loss: 3.8862 | lr: 9.8176e-07 | scale:  1024.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    478/  5202 | global iter:    478/  5202 | loss: 2.6437 | ds_loss: 3.4350 | lr: 9.8169e-07 | scale:  1024.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    479/  5202 | global iter:    479/  5202 | loss: 1.8689 | ds_loss: 2.4851 | lr: 9.8161e-07 | scale:  1024.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    480/  5202 | global iter:    480/  5202 | loss: 2.7307 | ds_loss: 3.2799 | lr: 9.8153e-07 | scale:  1024.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    480/  5202 | global iter:    480/  5202 | loss: 2.5184 | ds_loss: 3.2715 | lr: 9.8153e-07 | scale:  1024.0000 | micro time: 0.307 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    481/  5202 | global iter:    481/  5202 | loss: 2.8278 | ds_loss: 3.4173 | lr: 9.8146e-07 | scale:  1024.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    482/  5202 | global iter:    482/  5202 | loss: 3.1577 | ds_loss: 3.8931 | lr: 9.8138e-07 | scale:  1024.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:    483/  5202 | global iter:    483/  5202 | loss: 3.1420 | ds_loss: 3.9620 | lr: 9.8130e-07 | scale:  1024.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    484/  5202 | global iter:    484/  5202 | loss: 2.6017 | ds_loss: 3.4448 | lr: 9.8122e-07 | scale:  1024.0000 | micro time: 0.302 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    484/  5202 | global iter:    484/  5202 | loss: 2.9323 | ds_loss: 3.6793 | lr: 9.8122e-07 | scale:  1024.0000 | micro time: 0.302 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    485/  5202 | global iter:    485/  5202 | loss: 2.7359 | ds_loss: 3.8523 | lr: 9.8115e-07 | scale:  1024.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    486/  5202 | global iter:    486/  5202 | loss: 3.2424 | ds_loss: 3.8795 | lr: 9.8107e-07 | scale:  1024.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:    487/  5202 | global iter:    487/  5202 | loss: 2.7805 | ds_loss: 3.5212 | lr: 9.8099e-07 | scale:  1024.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    488/  5202 | global iter:    488/  5202 | loss: 2.5282 | ds_loss: 3.3148 | lr: 9.8091e-07 | scale:  1024.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    488/  5202 | global iter:    488/  5202 | loss: 2.8218 | ds_loss: 3.6420 | lr: 9.8091e-07 | scale:  1024.0000 | micro time: 0.307 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    489/  5202 | global iter:    489/  5202 | loss: 3.2951 | ds_loss: 3.8864 | lr: 9.8083e-07 | scale:  1024.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:    490/  5202 | global iter:    490/  5202 | loss: 2.6265 | ds_loss: 3.6909 | lr: 9.8076e-07 | scale:  1024.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    491/  5202 | global iter:    491/  5202 | loss: 1.7860 | ds_loss: 2.4948 | lr: 9.8068e-07 | scale:  1024.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    492/  5202 | global iter:    492/  5202 | loss: 1.4755 | ds_loss: 2.2426 | lr: 9.8060e-07 | scale:  1024.0000 | micro time: 0.302 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    492/  5202 | global iter:    492/  5202 | loss: 2.2958 | ds_loss: 3.0787 | lr: 9.8060e-07 | scale:  1024.0000 | micro time: 0.302 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    493/  5202 | global iter:    493/  5202 | loss: 2.8107 | ds_loss: 3.3203 | lr: 9.8052e-07 | scale:  1024.0000 | micro time: 0.322 | step time: 0.000\n",
            "train | epoch   0 | Iter:    494/  5202 | global iter:    494/  5202 | loss: 2.4171 | ds_loss: 3.4929 | lr: 9.8044e-07 | scale:  1024.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    495/  5202 | global iter:    495/  5202 | loss: 1.5082 | ds_loss: 2.3683 | lr: 9.8036e-07 | scale:  1024.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    496/  5202 | global iter:    496/  5202 | loss: 2.6547 | ds_loss: 3.3431 | lr: 9.8028e-07 | scale:  1024.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    496/  5202 | global iter:    496/  5202 | loss: 2.3477 | ds_loss: 3.1312 | lr: 9.8028e-07 | scale:  1024.0000 | micro time: 0.308 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    497/  5202 | global iter:    497/  5202 | loss: 2.7401 | ds_loss: 3.5396 | lr: 9.8020e-07 | scale:  1024.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    498/  5202 | global iter:    498/  5202 | loss: 2.6139 | ds_loss: 3.4734 | lr: 9.8012e-07 | scale:  1024.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    499/  5202 | global iter:    499/  5202 | loss: 3.3074 | ds_loss: 3.9233 | lr: 9.8004e-07 | scale:  1024.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:    500/  5202 | global iter:    500/  5202 | loss: 2.6516 | ds_loss: 3.7652 | lr: 9.7996e-07 | scale:  1024.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    500/  5202 | global iter:    500/  5202 | loss: 2.8283 | ds_loss: 3.6754 | lr: 9.7996e-07 | scale:  1024.0000 | micro time: 0.307 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    501/  5202 | global iter:    501/  5202 | loss: 2.4755 | ds_loss: 3.3307 | lr: 9.7988e-07 | scale:  1024.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    502/  5202 | global iter:    502/  5202 | loss: 2.0421 | ds_loss: 3.1379 | lr: 9.7980e-07 | scale:  1024.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    503/  5202 | global iter:    503/  5202 | loss: 0.8957 | ds_loss: 1.2466 | lr: 9.7972e-07 | scale:  1024.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    504/  5202 | global iter:    504/  5202 | loss: 2.6160 | ds_loss: 3.6131 | lr: 9.7964e-07 | scale:  1024.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    504/  5202 | global iter:    504/  5202 | loss: 2.0073 | ds_loss: 2.8321 | lr: 9.7964e-07 | scale:  1024.0000 | micro time: 0.304 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    505/  5202 | global iter:    505/  5202 | loss: 2.9972 | ds_loss: 3.9517 | lr: 9.7956e-07 | scale:  1024.0000 | micro time: 0.316 | step time: 0.000\n",
            "train | epoch   0 | Iter:    506/  5202 | global iter:    506/  5202 | loss: 2.6944 | ds_loss: 3.4083 | lr: 9.7948e-07 | scale:  1024.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    507/  5202 | global iter:    507/  5202 | loss: 2.4663 | ds_loss: 3.5538 | lr: 9.7940e-07 | scale:  1024.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    508/  5202 | global iter:    508/  5202 | loss: 2.9734 | ds_loss: 3.6497 | lr: 9.7932e-07 | scale:  1024.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    508/  5202 | global iter:    508/  5202 | loss: 2.7828 | ds_loss: 3.6409 | lr: 9.7932e-07 | scale:  1024.0000 | micro time: 0.309 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    509/  5202 | global iter:    509/  5202 | loss: 2.7822 | ds_loss: 3.4009 | lr: 9.7923e-07 | scale:  1024.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:    510/  5202 | global iter:    510/  5202 | loss: 2.7746 | ds_loss: 3.4853 | lr: 9.7915e-07 | scale:  1024.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    511/  5202 | global iter:    511/  5202 | loss: 2.1681 | ds_loss: 2.7013 | lr: 9.7907e-07 | scale:  1024.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    512/  5202 | global iter:    512/  5202 | loss: 1.8401 | ds_loss: 2.2433 | lr: 9.7899e-07 | scale:  1024.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    512/  5202 | global iter:    512/  5202 | loss: 2.3912 | ds_loss: 2.9577 | lr: 9.7899e-07 | scale:  1024.0000 | micro time: 0.311 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    513/  5202 | global iter:    513/  5202 | loss: 2.0257 | ds_loss: 2.9712 | lr: 9.7891e-07 | scale:  1024.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    514/  5202 | global iter:    514/  5202 | loss: 2.6704 | ds_loss: 3.9356 | lr: 9.7882e-07 | scale:  1024.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    515/  5202 | global iter:    515/  5202 | loss: 2.3228 | ds_loss: 2.9218 | lr: 9.7874e-07 | scale:  1024.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    516/  5202 | global iter:    516/  5202 | loss: 2.6850 | ds_loss: 3.9828 | lr: 9.7866e-07 | scale:  1024.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    516/  5202 | global iter:    516/  5202 | loss: 2.4260 | ds_loss: 3.4529 | lr: 9.7866e-07 | scale:  1024.0000 | micro time: 0.308 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    517/  5202 | global iter:    517/  5202 | loss: 2.0149 | ds_loss: 2.8625 | lr: 9.7858e-07 | scale:  1024.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    518/  5202 | global iter:    518/  5202 | loss: 1.4467 | ds_loss: 2.3970 | lr: 9.7849e-07 | scale:  1024.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    519/  5202 | global iter:    519/  5202 | loss: 3.2889 | ds_loss: 4.0011 | lr: 9.7841e-07 | scale:  1024.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    520/  5202 | global iter:    520/  5202 | loss: 1.5830 | ds_loss: 2.1242 | lr: 9.7833e-07 | scale:  1024.0000 | micro time: 0.302 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    520/  5202 | global iter:    520/  5202 | loss: 2.0834 | ds_loss: 2.8462 | lr: 9.7833e-07 | scale:  1024.0000 | micro time: 0.302 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    521/  5202 | global iter:    521/  5202 | loss: 3.0310 | ds_loss: 5.2335 | lr: 9.7824e-07 | scale:  1024.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    522/  5202 | global iter:    522/  5202 | loss: 3.0561 | ds_loss: 3.7804 | lr: 9.7816e-07 | scale:  1024.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    523/  5202 | global iter:    523/  5202 | loss: 2.3077 | ds_loss: 3.2161 | lr: 9.7808e-07 | scale:  1024.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    524/  5202 | global iter:    524/  5202 | loss: 2.5071 | ds_loss: 3.6481 | lr: 9.7799e-07 | scale:  1024.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    524/  5202 | global iter:    524/  5202 | loss: 2.7255 | ds_loss: 3.9695 | lr: 9.7799e-07 | scale:  1024.0000 | micro time: 0.303 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    525/  5202 | global iter:    525/  5202 | loss: 2.2377 | ds_loss: 3.2824 | lr: 9.7791e-07 | scale:  1024.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    526/  5202 | global iter:    526/  5202 | loss: 2.7570 | ds_loss: 3.4804 | lr: 9.7782e-07 | scale:  1024.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    527/  5202 | global iter:    527/  5202 | loss: 2.3388 | ds_loss: 3.6379 | lr: 9.7774e-07 | scale:  1024.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    528/  5202 | global iter:    528/  5202 | loss: 3.2114 | ds_loss: 3.9680 | lr: 9.7766e-07 | scale:  1024.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    528/  5202 | global iter:    528/  5202 | loss: 2.6362 | ds_loss: 3.5922 | lr: 9.7766e-07 | scale:  1024.0000 | micro time: 0.306 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    529/  5202 | global iter:    529/  5202 | loss: 2.6031 | ds_loss: 3.5892 | lr: 9.7757e-07 | scale:  1024.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    530/  5202 | global iter:    530/  5202 | loss: 2.9508 | ds_loss: 4.0215 | lr: 9.7749e-07 | scale:  1024.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    531/  5202 | global iter:    531/  5202 | loss: 3.1118 | ds_loss: 3.7520 | lr: 9.7740e-07 | scale:  1024.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    532/  5202 | global iter:    532/  5202 | loss: 2.8804 | ds_loss: 3.9226 | lr: 9.7732e-07 | scale:  1024.0000 | micro time: 0.302 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    532/  5202 | global iter:    532/  5202 | loss: 2.8865 | ds_loss: 3.8213 | lr: 9.7732e-07 | scale:  1024.0000 | micro time: 0.302 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    533/  5202 | global iter:    533/  5202 | loss: 3.1730 | ds_loss: 3.9971 | lr: 9.7723e-07 | scale:  1024.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:    534/  5202 | global iter:    534/  5202 | loss: 2.2057 | ds_loss: 3.6295 | lr: 9.7714e-07 | scale:  1024.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    535/  5202 | global iter:    535/  5202 | loss: 0.9733 | ds_loss: 1.5758 | lr: 9.7706e-07 | scale:  1024.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    536/  5202 | global iter:    536/  5202 | loss: 2.0809 | ds_loss: 2.9258 | lr: 9.7697e-07 | scale:  1024.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    536/  5202 | global iter:    536/  5202 | loss: 2.1082 | ds_loss: 3.0320 | lr: 9.7697e-07 | scale:  1024.0000 | micro time: 0.303 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    537/  5202 | global iter:    537/  5202 | loss: 2.7567 | ds_loss: 3.3853 | lr: 9.7689e-07 | scale:  1024.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    538/  5202 | global iter:    538/  5202 | loss: 2.8934 | ds_loss: 3.5367 | lr: 9.7680e-07 | scale:  1024.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:    539/  5202 | global iter:    539/  5202 | loss: 2.8879 | ds_loss: 3.6128 | lr: 9.7672e-07 | scale:  1024.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:    540/  5202 | global iter:    540/  5202 | loss: 3.1582 | ds_loss: 3.8633 | lr: 9.7663e-07 | scale:  1024.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    540/  5202 | global iter:    540/  5202 | loss: 2.9240 | ds_loss: 3.5996 | lr: 9.7663e-07 | scale:  1024.0000 | micro time: 0.307 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    541/  5202 | global iter:    541/  5202 | loss: 1.8707 | ds_loss: 2.6105 | lr: 9.7654e-07 | scale:  1024.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    542/  5202 | global iter:    542/  5202 | loss: 3.0817 | ds_loss: 3.6406 | lr: 9.7646e-07 | scale:  1024.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    543/  5202 | global iter:    543/  5202 | loss: 2.8861 | ds_loss: 3.6699 | lr: 9.7637e-07 | scale:  1024.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    544/  5202 | global iter:    544/  5202 | loss: 2.9954 | ds_loss: 4.0501 | lr: 9.7628e-07 | scale:  1024.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    544/  5202 | global iter:    544/  5202 | loss: 2.7085 | ds_loss: 3.4928 | lr: 9.7628e-07 | scale:  1024.0000 | micro time: 0.306 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    545/  5202 | global iter:    545/  5202 | loss: 2.5036 | ds_loss: 3.4816 | lr: 9.7619e-07 | scale:  1024.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    546/  5202 | global iter:    546/  5202 | loss: 1.6585 | ds_loss: 2.5834 | lr: 9.7611e-07 | scale:  1024.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    547/  5202 | global iter:    547/  5202 | loss: 2.8678 | ds_loss: 4.0898 | lr: 9.7602e-07 | scale:  1024.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    548/  5202 | global iter:    548/  5202 | loss: 2.9833 | ds_loss: 3.6830 | lr: 9.7593e-07 | scale:  1024.0000 | micro time: 0.317 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    548/  5202 | global iter:    548/  5202 | loss: 2.5033 | ds_loss: 3.4594 | lr: 9.7593e-07 | scale:  1024.0000 | micro time: 0.317 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    549/  5202 | global iter:    549/  5202 | loss: 1.6749 | ds_loss: 2.1239 | lr: 9.7584e-07 | scale:  1024.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:    550/  5202 | global iter:    550/  5202 | loss: 2.7995 | ds_loss: 3.7715 | lr: 9.7576e-07 | scale:  1024.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    551/  5202 | global iter:    551/  5202 | loss: 2.2677 | ds_loss: 3.1557 | lr: 9.7567e-07 | scale:  1024.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    552/  5202 | global iter:    552/  5202 | loss: 2.7710 | ds_loss: 3.3368 | lr: 9.7558e-07 | scale:  1024.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    552/  5202 | global iter:    552/  5202 | loss: 2.3783 | ds_loss: 3.0970 | lr: 9.7558e-07 | scale:  1024.0000 | micro time: 0.310 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    553/  5202 | global iter:    553/  5202 | loss: 2.4123 | ds_loss: 2.8556 | lr: 9.7549e-07 | scale:  1024.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:    554/  5202 | global iter:    554/  5202 | loss: 3.0632 | ds_loss: 3.6087 | lr: 9.7540e-07 | scale:  1024.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:    555/  5202 | global iter:    555/  5202 | loss: 2.6558 | ds_loss: 3.3506 | lr: 9.7531e-07 | scale:  1024.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:    556/  5202 | global iter:    556/  5202 | loss: 3.0606 | ds_loss: 3.9577 | lr: 9.7523e-07 | scale:  1024.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    556/  5202 | global iter:    556/  5202 | loss: 2.7980 | ds_loss: 3.4432 | lr: 9.7523e-07 | scale:  1024.0000 | micro time: 0.308 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    557/  5202 | global iter:    557/  5202 | loss: 2.8639 | ds_loss: 3.5168 | lr: 9.7514e-07 | scale:  1024.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:    558/  5202 | global iter:    558/  5202 | loss: 2.6338 | ds_loss: 3.4882 | lr: 9.7505e-07 | scale:  1024.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    559/  5202 | global iter:    559/  5202 | loss: 2.1196 | ds_loss: 3.0694 | lr: 9.7496e-07 | scale:  1024.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    560/  5202 | global iter:    560/  5202 | loss: 2.6712 | ds_loss: 3.4990 | lr: 9.7487e-07 | scale:  1024.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    560/  5202 | global iter:    560/  5202 | loss: 2.5721 | ds_loss: 3.3933 | lr: 9.7487e-07 | scale:  1024.0000 | micro time: 0.305 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    561/  5202 | global iter:    561/  5202 | loss: 1.6681 | ds_loss: 2.6341 | lr: 9.7478e-07 | scale:  1024.0000 | micro time: 0.300 | step time: 0.000\n",
            "train | epoch   0 | Iter:    562/  5202 | global iter:    562/  5202 | loss: 2.3514 | ds_loss: 3.3888 | lr: 9.7469e-07 | scale:  1024.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    563/  5202 | global iter:    563/  5202 | loss: 2.4992 | ds_loss: 3.4466 | lr: 9.7460e-07 | scale:  1024.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    564/  5202 | global iter:    564/  5202 | loss: 2.7538 | ds_loss: 3.6045 | lr: 9.7451e-07 | scale:  1024.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    564/  5202 | global iter:    564/  5202 | loss: 2.3181 | ds_loss: 3.2685 | lr: 9.7451e-07 | scale:  1024.0000 | micro time: 0.303 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    565/  5202 | global iter:    565/  5202 | loss: 1.2329 | ds_loss: 2.0886 | lr: 9.7442e-07 | scale:  1024.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    566/  5202 | global iter:    566/  5202 | loss: 2.4443 | ds_loss: 3.2501 | lr: 9.7433e-07 | scale:  1024.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    567/  5202 | global iter:    567/  5202 | loss: 2.8986 | ds_loss: 3.8926 | lr: 9.7424e-07 | scale:  1024.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    568/  5202 | global iter:    568/  5202 | loss: 2.7074 | ds_loss: 3.4085 | lr: 9.7415e-07 | scale:  1024.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    568/  5202 | global iter:    568/  5202 | loss: 2.3208 | ds_loss: 3.1599 | lr: 9.7415e-07 | scale:  1024.0000 | micro time: 0.309 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    569/  5202 | global iter:    569/  5202 | loss: 2.8465 | ds_loss: 3.5656 | lr: 9.7406e-07 | scale:  1024.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    570/  5202 | global iter:    570/  5202 | loss: 1.6340 | ds_loss: 2.4739 | lr: 9.7397e-07 | scale:  1024.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    571/  5202 | global iter:    571/  5202 | loss: 2.7937 | ds_loss: 3.7632 | lr: 9.7387e-07 | scale:  1024.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    572/  5202 | global iter:    572/  5202 | loss: 2.6399 | ds_loss: 3.8433 | lr: 9.7378e-07 | scale:  1024.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    572/  5202 | global iter:    572/  5202 | loss: 2.4785 | ds_loss: 3.4115 | lr: 9.7378e-07 | scale:  1024.0000 | micro time: 0.304 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    573/  5202 | global iter:    573/  5202 | loss: 2.6275 | ds_loss: 3.5348 | lr: 9.7369e-07 | scale:  1024.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    574/  5202 | global iter:    574/  5202 | loss: 0.8478 | ds_loss: 1.2022 | lr: 9.7360e-07 | scale:  1024.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    575/  5202 | global iter:    575/  5202 | loss: 3.0088 | ds_loss: 4.3020 | lr: 9.7351e-07 | scale:  1024.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    576/  5202 | global iter:    576/  5202 | loss: 2.2546 | ds_loss: 3.4596 | lr: 9.7342e-07 | scale:  1024.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    576/  5202 | global iter:    576/  5202 | loss: 2.1847 | ds_loss: 3.1247 | lr: 9.7342e-07 | scale:  1024.0000 | micro time: 0.307 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    577/  5202 | global iter:    577/  5202 | loss: 2.3915 | ds_loss: 3.1087 | lr: 9.7332e-07 | scale:  1024.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    578/  5202 | global iter:    578/  5202 | loss: 2.5929 | ds_loss: 3.6675 | lr: 9.7323e-07 | scale:  1024.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    579/  5202 | global iter:    579/  5202 | loss: 2.7518 | ds_loss: 3.6861 | lr: 9.7314e-07 | scale:  1024.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    580/  5202 | global iter:    580/  5202 | loss: 2.5246 | ds_loss: 3.4932 | lr: 9.7305e-07 | scale:  1024.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    580/  5202 | global iter:    580/  5202 | loss: 2.5652 | ds_loss: 3.4889 | lr: 9.7305e-07 | scale:  1024.0000 | micro time: 0.305 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    581/  5202 | global iter:    581/  5202 | loss: 2.4600 | ds_loss: 3.2553 | lr: 9.7295e-07 | scale:  1024.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    582/  5202 | global iter:    582/  5202 | loss: 2.8540 | ds_loss: 3.8055 | lr: 9.7286e-07 | scale:  1024.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    583/  5202 | global iter:    583/  5202 | loss: 3.1695 | ds_loss: 3.7398 | lr: 9.7277e-07 | scale:  1024.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:    584/  5202 | global iter:    584/  5202 | loss: 2.7554 | ds_loss: 3.4463 | lr: 9.7268e-07 | scale:  1024.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    584/  5202 | global iter:    584/  5202 | loss: 2.8097 | ds_loss: 3.5617 | lr: 9.7268e-07 | scale:  1024.0000 | micro time: 0.311 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    585/  5202 | global iter:    585/  5202 | loss: 1.6931 | ds_loss: 2.5118 | lr: 9.7258e-07 | scale:  1024.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:    586/  5202 | global iter:    586/  5202 | loss: 2.1269 | ds_loss: 2.9496 | lr: 9.7249e-07 | scale:  1024.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    587/  5202 | global iter:    587/  5202 | loss: 2.7842 | ds_loss: 3.6486 | lr: 9.7240e-07 | scale:  1024.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:    588/  5202 | global iter:    588/  5202 | loss: 2.7578 | ds_loss: 3.8190 | lr: 9.7230e-07 | scale:  1024.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    588/  5202 | global iter:    588/  5202 | loss: 2.3405 | ds_loss: 3.2323 | lr: 9.7230e-07 | scale:  1024.0000 | micro time: 0.309 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    589/  5202 | global iter:    589/  5202 | loss: 3.0084 | ds_loss: 3.8910 | lr: 9.7221e-07 | scale:  1024.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    590/  5202 | global iter:    590/  5202 | loss: 2.7634 | ds_loss: 3.3242 | lr: 9.7211e-07 | scale:  1024.0000 | micro time: 0.317 | step time: 0.000\n",
            "train | epoch   0 | Iter:    591/  5202 | global iter:    591/  5202 | loss: 2.5001 | ds_loss: 3.3661 | lr: 9.7202e-07 | scale:  1024.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    592/  5202 | global iter:    592/  5202 | loss: 2.1468 | ds_loss: 3.3076 | lr: 9.7192e-07 | scale:  1024.0000 | micro time: 0.314 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    592/  5202 | global iter:    592/  5202 | loss: 2.6046 | ds_loss: 3.4722 | lr: 9.7192e-07 | scale:  1024.0000 | micro time: 0.314 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    593/  5202 | global iter:    593/  5202 | loss: 2.6185 | ds_loss: 3.2627 | lr: 9.7183e-07 | scale:  1024.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    594/  5202 | global iter:    594/  5202 | loss: 1.6914 | ds_loss: 2.4419 | lr: 9.7174e-07 | scale:  1024.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    595/  5202 | global iter:    595/  5202 | loss: 1.7679 | ds_loss: 2.7231 | lr: 9.7164e-07 | scale:  1024.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    596/  5202 | global iter:    596/  5202 | loss: 3.0047 | ds_loss: 3.6296 | lr: 9.7155e-07 | scale:  1024.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    596/  5202 | global iter:    596/  5202 | loss: 2.2706 | ds_loss: 3.0143 | lr: 9.7155e-07 | scale:  1024.0000 | micro time: 0.307 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    597/  5202 | global iter:    597/  5202 | loss: 2.9304 | ds_loss: 3.5115 | lr: 9.7145e-07 | scale:  1024.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    598/  5202 | global iter:    598/  5202 | loss: 2.8791 | ds_loss: 3.6166 | lr: 9.7135e-07 | scale:  1024.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    599/  5202 | global iter:    599/  5202 | loss: 1.9099 | ds_loss: 3.1696 | lr: 9.7126e-07 | scale:  1024.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    600/  5202 | global iter:    600/  5202 | loss: 2.7233 | ds_loss: 3.6882 | lr: 9.7116e-07 | scale:  1024.0000 | micro time: 0.302 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    600/  5202 | global iter:    600/  5202 | loss: 2.6107 | ds_loss: 3.4965 | lr: 9.7116e-07 | scale:  1024.0000 | micro time: 0.302 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    601/  5202 | global iter:    601/  5202 | loss: 2.2937 | ds_loss: 3.6977 | lr: 9.7107e-07 | scale:  1024.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    602/  5202 | global iter:    602/  5202 | loss: 1.9561 | ds_loss: 3.0314 | lr: 9.7097e-07 | scale:  1024.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    603/  5202 | global iter:    603/  5202 | loss: 2.0428 | ds_loss: 2.8627 | lr: 9.7088e-07 | scale:  1024.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    604/  5202 | global iter:    604/  5202 | loss: 2.5131 | ds_loss: 3.2374 | lr: 9.7078e-07 | scale:  1024.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    604/  5202 | global iter:    604/  5202 | loss: 2.2014 | ds_loss: 3.2073 | lr: 9.7078e-07 | scale:  1024.0000 | micro time: 0.306 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    605/  5202 | global iter:    605/  5202 | loss: 2.9944 | ds_loss: 3.5800 | lr: 9.7068e-07 | scale:  1024.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:    606/  5202 | global iter:    606/  5202 | loss: 2.5831 | ds_loss: 3.5017 | lr: 9.7059e-07 | scale:  1024.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    607/  5202 | global iter:    607/  5202 | loss: 2.3405 | ds_loss: 3.2373 | lr: 9.7049e-07 | scale:  1024.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    608/  5202 | global iter:    608/  5202 | loss: 3.0286 | ds_loss: 3.6637 | lr: 9.7039e-07 | scale:  1024.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    608/  5202 | global iter:    608/  5202 | loss: 2.7366 | ds_loss: 3.4957 | lr: 9.7039e-07 | scale:  1024.0000 | micro time: 0.310 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    609/  5202 | global iter:    609/  5202 | loss: 1.1957 | ds_loss: 1.8302 | lr: 9.7030e-07 | scale:  1024.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:    610/  5202 | global iter:    610/  5202 | loss: 2.5110 | ds_loss: 3.1570 | lr: 9.7020e-07 | scale:  1024.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:    611/  5202 | global iter:    611/  5202 | loss: 3.0104 | ds_loss: 3.7784 | lr: 9.7010e-07 | scale:  1024.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    612/  5202 | global iter:    612/  5202 | loss: 3.1465 | ds_loss: 4.2887 | lr: 9.7000e-07 | scale:  1024.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    612/  5202 | global iter:    612/  5202 | loss: 2.4659 | ds_loss: 3.2636 | lr: 9.7000e-07 | scale:  1024.0000 | micro time: 0.303 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    613/  5202 | global iter:    613/  5202 | loss: 2.7464 | ds_loss: 3.4951 | lr: 9.6991e-07 | scale:  1024.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    614/  5202 | global iter:    614/  5202 | loss: 3.0253 | ds_loss: 3.4952 | lr: 9.6981e-07 | scale:  1024.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:    615/  5202 | global iter:    615/  5202 | loss: 2.6530 | ds_loss: 3.5125 | lr: 9.6971e-07 | scale:  1024.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    616/  5202 | global iter:    616/  5202 | loss: 1.7782 | ds_loss: 2.7195 | lr: 9.6961e-07 | scale:  1024.0000 | micro time: 0.302 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    616/  5202 | global iter:    616/  5202 | loss: 2.5507 | ds_loss: 3.3056 | lr: 9.6961e-07 | scale:  1024.0000 | micro time: 0.302 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    617/  5202 | global iter:    617/  5202 | loss: 2.0540 | ds_loss: 2.9768 | lr: 9.6951e-07 | scale:  1024.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    618/  5202 | global iter:    618/  5202 | loss: 1.8287 | ds_loss: 2.6742 | lr: 9.6942e-07 | scale:  1024.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    619/  5202 | global iter:    619/  5202 | loss: 2.5053 | ds_loss: 3.1617 | lr: 9.6932e-07 | scale:  1024.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:    620/  5202 | global iter:    620/  5202 | loss: 2.4476 | ds_loss: 3.2963 | lr: 9.6922e-07 | scale:  1024.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    620/  5202 | global iter:    620/  5202 | loss: 2.2089 | ds_loss: 3.0273 | lr: 9.6922e-07 | scale:  1024.0000 | micro time: 0.303 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    621/  5202 | global iter:    621/  5202 | loss: 2.7929 | ds_loss: 3.9480 | lr: 9.6912e-07 | scale:  1024.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    622/  5202 | global iter:    622/  5202 | loss: 3.1236 | ds_loss: 3.7785 | lr: 9.6902e-07 | scale:  1024.0000 | micro time: 0.319 | step time: 0.000\n",
            "train | epoch   0 | Iter:    623/  5202 | global iter:    623/  5202 | loss: 2.3997 | ds_loss: 3.2462 | lr: 9.6892e-07 | scale:  1024.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    624/  5202 | global iter:    624/  5202 | loss: 2.8441 | ds_loss: 3.5223 | lr: 9.6882e-07 | scale:  1024.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    624/  5202 | global iter:    624/  5202 | loss: 2.7901 | ds_loss: 3.6238 | lr: 9.6882e-07 | scale:  1024.0000 | micro time: 0.307 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    625/  5202 | global iter:    625/  5202 | loss: 2.6460 | ds_loss: 3.4061 | lr: 9.6872e-07 | scale:  1024.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    626/  5202 | global iter:    626/  5202 | loss: 1.1031 | ds_loss: 1.4573 | lr: 9.6862e-07 | scale:  1024.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:    627/  5202 | global iter:    627/  5202 | loss: 2.5191 | ds_loss: 3.6261 | lr: 9.6852e-07 | scale:  1024.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    628/  5202 | global iter:    628/  5202 | loss: 2.7689 | ds_loss: 3.5749 | lr: 9.6842e-07 | scale:  1024.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    628/  5202 | global iter:    628/  5202 | loss: 2.2593 | ds_loss: 3.0161 | lr: 9.6842e-07 | scale:  1024.0000 | micro time: 0.311 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    629/  5202 | global iter:    629/  5202 | loss: 2.6097 | ds_loss: 3.4628 | lr: 9.6832e-07 | scale:  1024.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    630/  5202 | global iter:    630/  5202 | loss: 3.1204 | ds_loss: 3.7665 | lr: 9.6822e-07 | scale:  1024.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:    631/  5202 | global iter:    631/  5202 | loss: 2.7805 | ds_loss: 3.2466 | lr: 9.6812e-07 | scale:  1024.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:    632/  5202 | global iter:    632/  5202 | loss: 2.7388 | ds_loss: 3.5245 | lr: 9.6802e-07 | scale:  1024.0000 | micro time: 0.316 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    632/  5202 | global iter:    632/  5202 | loss: 2.8123 | ds_loss: 3.5001 | lr: 9.6802e-07 | scale:  1024.0000 | micro time: 0.316 | step time: 0.312\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    633/  5202 | global iter:    633/  5202 | loss: 3.2094 | ds_loss: 3.9348 | lr: 9.6792e-07 | scale:  1024.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:    634/  5202 | global iter:    634/  5202 | loss: 3.1552 | ds_loss: 3.7997 | lr: 9.6782e-07 | scale:  1024.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:    635/  5202 | global iter:    635/  5202 | loss: 2.9005 | ds_loss: 3.5346 | lr: 9.6772e-07 | scale:  1024.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:    636/  5202 | global iter:    636/  5202 | loss: 2.9035 | ds_loss: 3.6625 | lr: 9.6762e-07 | scale:  1024.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    636/  5202 | global iter:    636/  5202 | loss: 3.0422 | ds_loss: 3.7329 | lr: 9.6762e-07 | scale:  1024.0000 | micro time: 0.310 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    637/  5202 | global iter:    637/  5202 | loss: 2.7288 | ds_loss: 3.4083 | lr: 9.6752e-07 | scale:  1024.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    638/  5202 | global iter:    638/  5202 | loss: 2.9680 | ds_loss: 3.5500 | lr: 9.6742e-07 | scale:  1024.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    639/  5202 | global iter:    639/  5202 | loss: 2.5920 | ds_loss: 3.4029 | lr: 9.6731e-07 | scale:  1024.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    640/  5202 | global iter:    640/  5202 | loss: 2.1018 | ds_loss: 3.4436 | lr: 9.6721e-07 | scale:  1024.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    640/  5202 | global iter:    640/  5202 | loss: 2.5977 | ds_loss: 3.4512 | lr: 9.6721e-07 | scale:  1024.0000 | micro time: 0.305 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    641/  5202 | global iter:    641/  5202 | loss: 2.8930 | ds_loss: 3.7855 | lr: 9.6711e-07 | scale:  1024.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    642/  5202 | global iter:    642/  5202 | loss: 2.6864 | ds_loss: 3.2418 | lr: 9.6701e-07 | scale:  1024.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:    643/  5202 | global iter:    643/  5202 | loss: 2.2039 | ds_loss: 3.1776 | lr: 9.6691e-07 | scale:  1024.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    644/  5202 | global iter:    644/  5202 | loss: 2.3387 | ds_loss: 3.3126 | lr: 9.6680e-07 | scale:  1024.0000 | micro time: 0.301 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    644/  5202 | global iter:    644/  5202 | loss: 2.5305 | ds_loss: 3.3794 | lr: 9.6680e-07 | scale:  1024.0000 | micro time: 0.301 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    645/  5202 | global iter:    645/  5202 | loss: 2.8282 | ds_loss: 3.5356 | lr: 9.6670e-07 | scale:  1024.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    646/  5202 | global iter:    646/  5202 | loss: 3.0478 | ds_loss: 4.0758 | lr: 9.6660e-07 | scale:  1024.0000 | micro time: 0.305 | step time: 0.000\n",
            "[2025-04-16 21:05:44,471] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1024, reducing to 512\n",
            "train | epoch   0 | Iter:    647/  5202 | global iter:    647/  5202 | loss: 3.1253 | ds_loss: 3.8994 | lr: 9.6660e-07 | scale:   512.0000 | micro time: 0.241 | step time: 0.000\n",
            "train | epoch   0 | Iter:    648/  5202 | global iter:    648/  5202 | loss: 2.1954 | ds_loss: 3.2281 | lr: 9.6650e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    648/  5202 | global iter:    648/  5202 | loss: 2.7992 | ds_loss: 3.6847 | lr: 9.6650e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.289\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    649/  5202 | global iter:    649/  5202 | loss: 2.4306 | ds_loss: 3.1201 | lr: 9.6639e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    650/  5202 | global iter:    650/  5202 | loss: 3.0960 | ds_loss: 3.9041 | lr: 9.6629e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    651/  5202 | global iter:    651/  5202 | loss: 2.5188 | ds_loss: 3.2556 | lr: 9.6619e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:    652/  5202 | global iter:    652/  5202 | loss: 1.1792 | ds_loss: 1.6478 | lr: 9.6608e-07 | scale:   512.0000 | micro time: 0.302 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    652/  5202 | global iter:    652/  5202 | loss: 2.3062 | ds_loss: 2.9819 | lr: 9.6608e-07 | scale:   512.0000 | micro time: 0.302 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    653/  5202 | global iter:    653/  5202 | loss: 2.6766 | ds_loss: 3.5025 | lr: 9.6598e-07 | scale:   512.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    654/  5202 | global iter:    654/  5202 | loss: 3.0364 | ds_loss: 3.6151 | lr: 9.6588e-07 | scale:   512.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:    655/  5202 | global iter:    655/  5202 | loss: 1.0489 | ds_loss: 1.6141 | lr: 9.6577e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    656/  5202 | global iter:    656/  5202 | loss: 2.9152 | ds_loss: 3.7965 | lr: 9.6567e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    656/  5202 | global iter:    656/  5202 | loss: 2.4193 | ds_loss: 3.1321 | lr: 9.6567e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    657/  5202 | global iter:    657/  5202 | loss: 3.0386 | ds_loss: 3.6076 | lr: 9.6556e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    658/  5202 | global iter:    658/  5202 | loss: 2.9126 | ds_loss: 3.7028 | lr: 9.6546e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    659/  5202 | global iter:    659/  5202 | loss: 2.7816 | ds_loss: 3.8961 | lr: 9.6535e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    660/  5202 | global iter:    660/  5202 | loss: 2.9320 | ds_loss: 3.7679 | lr: 9.6525e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    660/  5202 | global iter:    660/  5202 | loss: 2.9162 | ds_loss: 3.7436 | lr: 9.6525e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    661/  5202 | global iter:    661/  5202 | loss: 2.5168 | ds_loss: 3.4025 | lr: 9.6515e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    662/  5202 | global iter:    662/  5202 | loss: 1.9627 | ds_loss: 2.5985 | lr: 9.6504e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    663/  5202 | global iter:    663/  5202 | loss: 3.0915 | ds_loss: 3.9211 | lr: 9.6494e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    664/  5202 | global iter:    664/  5202 | loss: 3.0045 | ds_loss: 3.6457 | lr: 9.6483e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    664/  5202 | global iter:    664/  5202 | loss: 2.6439 | ds_loss: 3.3920 | lr: 9.6483e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    665/  5202 | global iter:    665/  5202 | loss: 2.7479 | ds_loss: 3.5313 | lr: 9.6472e-07 | scale:   512.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:    666/  5202 | global iter:    666/  5202 | loss: 2.9482 | ds_loss: 3.5427 | lr: 9.6462e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    667/  5202 | global iter:    667/  5202 | loss: 3.5999 | ds_loss: 4.3402 | lr: 9.6451e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    668/  5202 | global iter:    668/  5202 | loss: 1.6312 | ds_loss: 2.4450 | lr: 9.6441e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    668/  5202 | global iter:    668/  5202 | loss: 2.7318 | ds_loss: 3.4648 | lr: 9.6441e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    669/  5202 | global iter:    669/  5202 | loss: 1.6982 | ds_loss: 2.3826 | lr: 9.6430e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    670/  5202 | global iter:    670/  5202 | loss: 2.7282 | ds_loss: 3.4279 | lr: 9.6420e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:    671/  5202 | global iter:    671/  5202 | loss: 2.9434 | ds_loss: 3.6846 | lr: 9.6409e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:    672/  5202 | global iter:    672/  5202 | loss: 1.7119 | ds_loss: 2.3117 | lr: 9.6398e-07 | scale:   512.0000 | micro time: 0.314 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    672/  5202 | global iter:    672/  5202 | loss: 2.2704 | ds_loss: 2.9517 | lr: 9.6398e-07 | scale:   512.0000 | micro time: 0.314 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    673/  5202 | global iter:    673/  5202 | loss: 2.7725 | ds_loss: 3.5910 | lr: 9.6388e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    674/  5202 | global iter:    674/  5202 | loss: 2.5448 | ds_loss: 3.2905 | lr: 9.6377e-07 | scale:   512.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:    675/  5202 | global iter:    675/  5202 | loss: 2.3457 | ds_loss: 2.9665 | lr: 9.6366e-07 | scale:   512.0000 | micro time: 0.320 | step time: 0.000\n",
            "train | epoch   0 | Iter:    676/  5202 | global iter:    676/  5202 | loss: 2.3013 | ds_loss: 3.4552 | lr: 9.6356e-07 | scale:   512.0000 | micro time: 0.312 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    676/  5202 | global iter:    676/  5202 | loss: 2.4911 | ds_loss: 3.3258 | lr: 9.6356e-07 | scale:   512.0000 | micro time: 0.312 | step time: 0.313\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    677/  5202 | global iter:    677/  5202 | loss: 1.5642 | ds_loss: 2.0360 | lr: 9.6345e-07 | scale:   512.0000 | micro time: 0.316 | step time: 0.000\n",
            "train | epoch   0 | Iter:    678/  5202 | global iter:    678/  5202 | loss: 1.4468 | ds_loss: 2.1645 | lr: 9.6334e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    679/  5202 | global iter:    679/  5202 | loss: 2.6557 | ds_loss: 3.4206 | lr: 9.6323e-07 | scale:   512.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:    680/  5202 | global iter:    680/  5202 | loss: 2.8495 | ds_loss: 3.6650 | lr: 9.6313e-07 | scale:   512.0000 | micro time: 0.316 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    680/  5202 | global iter:    680/  5202 | loss: 2.1291 | ds_loss: 2.8215 | lr: 9.6313e-07 | scale:   512.0000 | micro time: 0.316 | step time: 0.313\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    681/  5202 | global iter:    681/  5202 | loss: 1.2014 | ds_loss: 1.5852 | lr: 9.6302e-07 | scale:   512.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:    682/  5202 | global iter:    682/  5202 | loss: 2.6800 | ds_loss: 4.0323 | lr: 9.6291e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    683/  5202 | global iter:    683/  5202 | loss: 3.2125 | ds_loss: 3.7211 | lr: 9.6280e-07 | scale:   512.0000 | micro time: 0.321 | step time: 0.000\n",
            "train | epoch   0 | Iter:    684/  5202 | global iter:    684/  5202 | loss: 2.6103 | ds_loss: 4.0149 | lr: 9.6269e-07 | scale:   512.0000 | micro time: 0.318 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    684/  5202 | global iter:    684/  5202 | loss: 2.4260 | ds_loss: 3.3384 | lr: 9.6269e-07 | scale:   512.0000 | micro time: 0.318 | step time: 0.314\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    685/  5202 | global iter:    685/  5202 | loss: 3.1403 | ds_loss: 3.8956 | lr: 9.6258e-07 | scale:   512.0000 | micro time: 0.318 | step time: 0.000\n",
            "train | epoch   0 | Iter:    686/  5202 | global iter:    686/  5202 | loss: 2.7909 | ds_loss: 3.3955 | lr: 9.6248e-07 | scale:   512.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:    687/  5202 | global iter:    687/  5202 | loss: 2.8236 | ds_loss: 3.5008 | lr: 9.6237e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    688/  5202 | global iter:    688/  5202 | loss: 3.1131 | ds_loss: 3.7447 | lr: 9.6226e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    688/  5202 | global iter:    688/  5202 | loss: 2.9670 | ds_loss: 3.6342 | lr: 9.6226e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.312\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    689/  5202 | global iter:    689/  5202 | loss: 1.7010 | ds_loss: 2.5269 | lr: 9.6215e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    690/  5202 | global iter:    690/  5202 | loss: 2.1137 | ds_loss: 2.8639 | lr: 9.6204e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    691/  5202 | global iter:    691/  5202 | loss: 2.4523 | ds_loss: 3.1480 | lr: 9.6193e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    692/  5202 | global iter:    692/  5202 | loss: 2.7904 | ds_loss: 3.7204 | lr: 9.6182e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    692/  5202 | global iter:    692/  5202 | loss: 2.2643 | ds_loss: 3.0648 | lr: 9.6182e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    693/  5202 | global iter:    693/  5202 | loss: 2.5235 | ds_loss: 3.7162 | lr: 9.6171e-07 | scale:   512.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    694/  5202 | global iter:    694/  5202 | loss: 2.8872 | ds_loss: 3.6305 | lr: 9.6160e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:    695/  5202 | global iter:    695/  5202 | loss: 3.3558 | ds_loss: 4.0233 | lr: 9.6149e-07 | scale:   512.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:    696/  5202 | global iter:    696/  5202 | loss: 3.3522 | ds_loss: 3.9528 | lr: 9.6138e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    696/  5202 | global iter:    696/  5202 | loss: 3.0297 | ds_loss: 3.8307 | lr: 9.6138e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    697/  5202 | global iter:    697/  5202 | loss: 2.2757 | ds_loss: 3.2585 | lr: 9.6127e-07 | scale:   512.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    698/  5202 | global iter:    698/  5202 | loss: 2.6712 | ds_loss: 3.5745 | lr: 9.6116e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    699/  5202 | global iter:    699/  5202 | loss: 3.3147 | ds_loss: 4.0691 | lr: 9.6105e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    700/  5202 | global iter:    700/  5202 | loss: 2.8097 | ds_loss: 3.5229 | lr: 9.6094e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    700/  5202 | global iter:    700/  5202 | loss: 2.7678 | ds_loss: 3.6062 | lr: 9.6094e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    701/  5202 | global iter:    701/  5202 | loss: 2.9548 | ds_loss: 3.7123 | lr: 9.6083e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    702/  5202 | global iter:    702/  5202 | loss: 3.0053 | ds_loss: 3.7838 | lr: 9.6072e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    703/  5202 | global iter:    703/  5202 | loss: 1.8496 | ds_loss: 2.7985 | lr: 9.6061e-07 | scale:   512.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    704/  5202 | global iter:    704/  5202 | loss: 2.6021 | ds_loss: 3.3034 | lr: 9.6050e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    704/  5202 | global iter:    704/  5202 | loss: 2.6030 | ds_loss: 3.3995 | lr: 9.6050e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    705/  5202 | global iter:    705/  5202 | loss: 2.0320 | ds_loss: 3.0342 | lr: 9.6038e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    706/  5202 | global iter:    706/  5202 | loss: 2.7165 | ds_loss: 3.5212 | lr: 9.6027e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    707/  5202 | global iter:    707/  5202 | loss: 2.2346 | ds_loss: 2.7659 | lr: 9.6016e-07 | scale:   512.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:    708/  5202 | global iter:    708/  5202 | loss: 2.5803 | ds_loss: 3.2835 | lr: 9.6005e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    708/  5202 | global iter:    708/  5202 | loss: 2.3909 | ds_loss: 3.1512 | lr: 9.6005e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    709/  5202 | global iter:    709/  5202 | loss: 1.4688 | ds_loss: 2.1621 | lr: 9.5994e-07 | scale:   512.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:    710/  5202 | global iter:    710/  5202 | loss: 2.9384 | ds_loss: 3.6212 | lr: 9.5983e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    711/  5202 | global iter:    711/  5202 | loss: 1.8880 | ds_loss: 3.1929 | lr: 9.5971e-07 | scale:   512.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    712/  5202 | global iter:    712/  5202 | loss: 2.0466 | ds_loss: 3.5394 | lr: 9.5960e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    712/  5202 | global iter:    712/  5202 | loss: 2.0854 | ds_loss: 3.1289 | lr: 9.5960e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    713/  5202 | global iter:    713/  5202 | loss: 3.4318 | ds_loss: 3.9875 | lr: 9.5949e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    714/  5202 | global iter:    714/  5202 | loss: 2.7320 | ds_loss: 3.4284 | lr: 9.5938e-07 | scale:   512.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:    715/  5202 | global iter:    715/  5202 | loss: 2.9505 | ds_loss: 3.8028 | lr: 9.5926e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    716/  5202 | global iter:    716/  5202 | loss: 1.7793 | ds_loss: 2.7680 | lr: 9.5915e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    716/  5202 | global iter:    716/  5202 | loss: 2.7234 | ds_loss: 3.4967 | lr: 9.5915e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    717/  5202 | global iter:    717/  5202 | loss: 2.8708 | ds_loss: 3.4212 | lr: 9.5904e-07 | scale:   512.0000 | micro time: 0.317 | step time: 0.000\n",
            "train | epoch   0 | Iter:    718/  5202 | global iter:    718/  5202 | loss: 2.0767 | ds_loss: 2.8945 | lr: 9.5892e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    719/  5202 | global iter:    719/  5202 | loss: 1.6125 | ds_loss: 2.1749 | lr: 9.5881e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    720/  5202 | global iter:    720/  5202 | loss: 2.6569 | ds_loss: 3.7053 | lr: 9.5870e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    720/  5202 | global iter:    720/  5202 | loss: 2.3042 | ds_loss: 3.0490 | lr: 9.5870e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    721/  5202 | global iter:    721/  5202 | loss: 0.8321 | ds_loss: 1.2655 | lr: 9.5858e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    722/  5202 | global iter:    722/  5202 | loss: 2.8377 | ds_loss: 3.7082 | lr: 9.5847e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:    723/  5202 | global iter:    723/  5202 | loss: 2.7243 | ds_loss: 3.9089 | lr: 9.5835e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    724/  5202 | global iter:    724/  5202 | loss: 2.2465 | ds_loss: 3.0917 | lr: 9.5824e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    724/  5202 | global iter:    724/  5202 | loss: 2.1602 | ds_loss: 2.9936 | lr: 9.5824e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    725/  5202 | global iter:    725/  5202 | loss: 3.3172 | ds_loss: 4.1046 | lr: 9.5813e-07 | scale:   512.0000 | micro time: 0.320 | step time: 0.000\n",
            "train | epoch   0 | Iter:    726/  5202 | global iter:    726/  5202 | loss: 2.7756 | ds_loss: 3.2378 | lr: 9.5801e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    727/  5202 | global iter:    727/  5202 | loss: 2.6938 | ds_loss: 3.2518 | lr: 9.5790e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    728/  5202 | global iter:    728/  5202 | loss: 2.8427 | ds_loss: 3.5604 | lr: 9.5778e-07 | scale:   512.0000 | micro time: 0.315 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    728/  5202 | global iter:    728/  5202 | loss: 2.9073 | ds_loss: 3.5386 | lr: 9.5778e-07 | scale:   512.0000 | micro time: 0.315 | step time: 0.313\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    729/  5202 | global iter:    729/  5202 | loss: 2.7838 | ds_loss: 3.8106 | lr: 9.5767e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    730/  5202 | global iter:    730/  5202 | loss: 1.8640 | ds_loss: 2.9951 | lr: 9.5755e-07 | scale:   512.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    731/  5202 | global iter:    731/  5202 | loss: 3.1015 | ds_loss: 3.6831 | lr: 9.5744e-07 | scale:   512.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:    732/  5202 | global iter:    732/  5202 | loss: 3.0765 | ds_loss: 3.8286 | lr: 9.5732e-07 | scale:   512.0000 | micro time: 0.314 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    732/  5202 | global iter:    732/  5202 | loss: 2.7065 | ds_loss: 3.5794 | lr: 9.5732e-07 | scale:   512.0000 | micro time: 0.314 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    733/  5202 | global iter:    733/  5202 | loss: 3.0634 | ds_loss: 3.6228 | lr: 9.5720e-07 | scale:   512.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:    734/  5202 | global iter:    734/  5202 | loss: 2.7997 | ds_loss: 3.3778 | lr: 9.5709e-07 | scale:   512.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:    735/  5202 | global iter:    735/  5202 | loss: 1.2608 | ds_loss: 1.8773 | lr: 9.5697e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    736/  5202 | global iter:    736/  5202 | loss: 2.9371 | ds_loss: 3.6344 | lr: 9.5686e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    736/  5202 | global iter:    736/  5202 | loss: 2.5153 | ds_loss: 3.1281 | lr: 9.5686e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    737/  5202 | global iter:    737/  5202 | loss: 2.2288 | ds_loss: 3.0563 | lr: 9.5674e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    738/  5202 | global iter:    738/  5202 | loss: 3.2047 | ds_loss: 3.8833 | lr: 9.5662e-07 | scale:   512.0000 | micro time: 0.317 | step time: 0.000\n",
            "train | epoch   0 | Iter:    739/  5202 | global iter:    739/  5202 | loss: 3.0780 | ds_loss: 3.7894 | lr: 9.5651e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    740/  5202 | global iter:    740/  5202 | loss: 3.2176 | ds_loss: 4.5252 | lr: 9.5639e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    740/  5202 | global iter:    740/  5202 | loss: 2.9323 | ds_loss: 3.8135 | lr: 9.5639e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    741/  5202 | global iter:    741/  5202 | loss: 1.5390 | ds_loss: 2.0283 | lr: 9.5627e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    742/  5202 | global iter:    742/  5202 | loss: 2.4060 | ds_loss: 3.2217 | lr: 9.5616e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:    743/  5202 | global iter:    743/  5202 | loss: 3.0101 | ds_loss: 3.6305 | lr: 9.5604e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:    744/  5202 | global iter:    744/  5202 | loss: 3.1517 | ds_loss: 4.1006 | lr: 9.5592e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    744/  5202 | global iter:    744/  5202 | loss: 2.5267 | ds_loss: 3.2453 | lr: 9.5592e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    745/  5202 | global iter:    745/  5202 | loss: 2.5661 | ds_loss: 3.3994 | lr: 9.5581e-07 | scale:   512.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:    746/  5202 | global iter:    746/  5202 | loss: 2.8459 | ds_loss: 3.5379 | lr: 9.5569e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    747/  5202 | global iter:    747/  5202 | loss: 2.3271 | ds_loss: 3.2767 | lr: 9.5557e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    748/  5202 | global iter:    748/  5202 | loss: 2.2397 | ds_loss: 2.8546 | lr: 9.5545e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    748/  5202 | global iter:    748/  5202 | loss: 2.4947 | ds_loss: 3.2671 | lr: 9.5545e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    749/  5202 | global iter:    749/  5202 | loss: 1.5342 | ds_loss: 2.1133 | lr: 9.5533e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    750/  5202 | global iter:    750/  5202 | loss: 2.9788 | ds_loss: 3.6124 | lr: 9.5522e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    751/  5202 | global iter:    751/  5202 | loss: 3.3058 | ds_loss: 4.3590 | lr: 9.5510e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    752/  5202 | global iter:    752/  5202 | loss: 3.5762 | ds_loss: 4.2737 | lr: 9.5498e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    752/  5202 | global iter:    752/  5202 | loss: 2.8487 | ds_loss: 3.5896 | lr: 9.5498e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    753/  5202 | global iter:    753/  5202 | loss: 3.1580 | ds_loss: 3.8914 | lr: 9.5486e-07 | scale:   512.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:    754/  5202 | global iter:    754/  5202 | loss: 2.7748 | ds_loss: 3.8461 | lr: 9.5474e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:    755/  5202 | global iter:    755/  5202 | loss: 2.2700 | ds_loss: 3.0220 | lr: 9.5462e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    756/  5202 | global iter:    756/  5202 | loss: 2.5426 | ds_loss: 3.4348 | lr: 9.5450e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    756/  5202 | global iter:    756/  5202 | loss: 2.6863 | ds_loss: 3.5486 | lr: 9.5450e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    757/  5202 | global iter:    757/  5202 | loss: 2.8938 | ds_loss: 3.7911 | lr: 9.5439e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    758/  5202 | global iter:    758/  5202 | loss: 3.4542 | ds_loss: 4.0432 | lr: 9.5427e-07 | scale:   512.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:    759/  5202 | global iter:    759/  5202 | loss: 3.0336 | ds_loss: 3.6514 | lr: 9.5415e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    760/  5202 | global iter:    760/  5202 | loss: 3.1039 | ds_loss: 3.7799 | lr: 9.5403e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    760/  5202 | global iter:    760/  5202 | loss: 3.1214 | ds_loss: 3.8164 | lr: 9.5403e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    761/  5202 | global iter:    761/  5202 | loss: 3.1050 | ds_loss: 3.9035 | lr: 9.5391e-07 | scale:   512.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:    762/  5202 | global iter:    762/  5202 | loss: 2.2750 | ds_loss: 2.9319 | lr: 9.5379e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    763/  5202 | global iter:    763/  5202 | loss: 2.8323 | ds_loss: 3.7478 | lr: 9.5367e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    764/  5202 | global iter:    764/  5202 | loss: 1.6594 | ds_loss: 2.0155 | lr: 9.5355e-07 | scale:   512.0000 | micro time: 0.314 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    764/  5202 | global iter:    764/  5202 | loss: 2.4679 | ds_loss: 3.1497 | lr: 9.5355e-07 | scale:   512.0000 | micro time: 0.314 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    765/  5202 | global iter:    765/  5202 | loss: 2.0476 | ds_loss: 2.6816 | lr: 9.5343e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:    766/  5202 | global iter:    766/  5202 | loss: 2.8251 | ds_loss: 3.5685 | lr: 9.5331e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:    767/  5202 | global iter:    767/  5202 | loss: 2.5705 | ds_loss: 3.2496 | lr: 9.5319e-07 | scale:   512.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:    768/  5202 | global iter:    768/  5202 | loss: 2.8416 | ds_loss: 3.6107 | lr: 9.5307e-07 | scale:   512.0000 | micro time: 0.325 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    768/  5202 | global iter:    768/  5202 | loss: 2.5712 | ds_loss: 3.2776 | lr: 9.5307e-07 | scale:   512.0000 | micro time: 0.325 | step time: 0.314\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    769/  5202 | global iter:    769/  5202 | loss: 2.6461 | ds_loss: 3.3190 | lr: 9.5294e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    770/  5202 | global iter:    770/  5202 | loss: 2.7499 | ds_loss: 3.3321 | lr: 9.5282e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    771/  5202 | global iter:    771/  5202 | loss: 2.9213 | ds_loss: 3.6157 | lr: 9.5270e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:    772/  5202 | global iter:    772/  5202 | loss: 2.9961 | ds_loss: 3.7274 | lr: 9.5258e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    772/  5202 | global iter:    772/  5202 | loss: 2.8284 | ds_loss: 3.4986 | lr: 9.5258e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    773/  5202 | global iter:    773/  5202 | loss: 2.8307 | ds_loss: 3.4586 | lr: 9.5246e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    774/  5202 | global iter:    774/  5202 | loss: 2.6136 | ds_loss: 3.6074 | lr: 9.5234e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    775/  5202 | global iter:    775/  5202 | loss: 2.7309 | ds_loss: 3.5340 | lr: 9.5222e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    776/  5202 | global iter:    776/  5202 | loss: 2.5773 | ds_loss: 3.4557 | lr: 9.5209e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    776/  5202 | global iter:    776/  5202 | loss: 2.6881 | ds_loss: 3.5139 | lr: 9.5209e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    777/  5202 | global iter:    777/  5202 | loss: 3.0483 | ds_loss: 3.9078 | lr: 9.5197e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    778/  5202 | global iter:    778/  5202 | loss: 2.0482 | ds_loss: 2.8628 | lr: 9.5185e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    779/  5202 | global iter:    779/  5202 | loss: 2.8787 | ds_loss: 3.6308 | lr: 9.5173e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:    780/  5202 | global iter:    780/  5202 | loss: 2.6556 | ds_loss: 3.7871 | lr: 9.5160e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    780/  5202 | global iter:    780/  5202 | loss: 2.6577 | ds_loss: 3.5471 | lr: 9.5160e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    781/  5202 | global iter:    781/  5202 | loss: 2.9423 | ds_loss: 3.8425 | lr: 9.5148e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    782/  5202 | global iter:    782/  5202 | loss: 2.8302 | ds_loss: 3.6476 | lr: 9.5136e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    783/  5202 | global iter:    783/  5202 | loss: 2.7227 | ds_loss: 3.4451 | lr: 9.5124e-07 | scale:   512.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:    784/  5202 | global iter:    784/  5202 | loss: 3.2876 | ds_loss: 4.0857 | lr: 9.5111e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    784/  5202 | global iter:    784/  5202 | loss: 2.9457 | ds_loss: 3.7552 | lr: 9.5111e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    785/  5202 | global iter:    785/  5202 | loss: 2.4160 | ds_loss: 3.3787 | lr: 9.5099e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    786/  5202 | global iter:    786/  5202 | loss: 1.7541 | ds_loss: 2.5815 | lr: 9.5087e-07 | scale:   512.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    787/  5202 | global iter:    787/  5202 | loss: 1.4056 | ds_loss: 2.5010 | lr: 9.5074e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    788/  5202 | global iter:    788/  5202 | loss: 2.6399 | ds_loss: 4.0316 | lr: 9.5062e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    788/  5202 | global iter:    788/  5202 | loss: 2.0539 | ds_loss: 3.1232 | lr: 9.5062e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.303\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    789/  5202 | global iter:    789/  5202 | loss: 2.9392 | ds_loss: 4.7082 | lr: 9.5050e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    790/  5202 | global iter:    790/  5202 | loss: 2.5560 | ds_loss: 3.4488 | lr: 9.5037e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    791/  5202 | global iter:    791/  5202 | loss: 1.1789 | ds_loss: 1.8546 | lr: 9.5025e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    792/  5202 | global iter:    792/  5202 | loss: 2.9348 | ds_loss: 3.8172 | lr: 9.5012e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    792/  5202 | global iter:    792/  5202 | loss: 2.4022 | ds_loss: 3.4572 | lr: 9.5012e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    793/  5202 | global iter:    793/  5202 | loss: 3.0451 | ds_loss: 4.1752 | lr: 9.5000e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    794/  5202 | global iter:    794/  5202 | loss: 2.3051 | ds_loss: 3.4259 | lr: 9.4987e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    795/  5202 | global iter:    795/  5202 | loss: 3.0112 | ds_loss: 3.6322 | lr: 9.4975e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:    796/  5202 | global iter:    796/  5202 | loss: 2.7142 | ds_loss: 3.7584 | lr: 9.4962e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    796/  5202 | global iter:    796/  5202 | loss: 2.7689 | ds_loss: 3.7479 | lr: 9.4962e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    797/  5202 | global iter:    797/  5202 | loss: 1.4578 | ds_loss: 2.1391 | lr: 9.4950e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    798/  5202 | global iter:    798/  5202 | loss: 2.8694 | ds_loss: 3.5087 | lr: 9.4937e-07 | scale:   512.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:    799/  5202 | global iter:    799/  5202 | loss: 3.1647 | ds_loss: 3.8170 | lr: 9.4925e-07 | scale:   512.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:    800/  5202 | global iter:    800/  5202 | loss: 1.7356 | ds_loss: 2.3514 | lr: 9.4912e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    800/  5202 | global iter:    800/  5202 | loss: 2.3069 | ds_loss: 2.9541 | lr: 9.4912e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    801/  5202 | global iter:    801/  5202 | loss: 2.1975 | ds_loss: 3.5279 | lr: 9.4900e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    802/  5202 | global iter:    802/  5202 | loss: 1.9368 | ds_loss: 3.1221 | lr: 9.4887e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    803/  5202 | global iter:    803/  5202 | loss: 1.4267 | ds_loss: 1.9725 | lr: 9.4875e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    804/  5202 | global iter:    804/  5202 | loss: 2.5421 | ds_loss: 3.5446 | lr: 9.4862e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    804/  5202 | global iter:    804/  5202 | loss: 2.0257 | ds_loss: 3.0418 | lr: 9.4862e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    805/  5202 | global iter:    805/  5202 | loss: 0.6645 | ds_loss: 1.0097 | lr: 9.4849e-07 | scale:   512.0000 | micro time: 0.317 | step time: 0.000\n",
            "train | epoch   0 | Iter:    806/  5202 | global iter:    806/  5202 | loss: 2.5462 | ds_loss: 3.2034 | lr: 9.4837e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    807/  5202 | global iter:    807/  5202 | loss: 3.0926 | ds_loss: 4.1480 | lr: 9.4824e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    808/  5202 | global iter:    808/  5202 | loss: 2.4839 | ds_loss: 3.2473 | lr: 9.4811e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    808/  5202 | global iter:    808/  5202 | loss: 2.1968 | ds_loss: 2.9021 | lr: 9.4811e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    809/  5202 | global iter:    809/  5202 | loss: 3.2331 | ds_loss: 3.8541 | lr: 9.4799e-07 | scale:   512.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:    810/  5202 | global iter:    810/  5202 | loss: 2.8573 | ds_loss: 3.7502 | lr: 9.4786e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    811/  5202 | global iter:    811/  5202 | loss: 2.7047 | ds_loss: 3.5536 | lr: 9.4773e-07 | scale:   512.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:    812/  5202 | global iter:    812/  5202 | loss: 1.8061 | ds_loss: 2.2825 | lr: 9.4761e-07 | scale:   512.0000 | micro time: 0.312 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    812/  5202 | global iter:    812/  5202 | loss: 2.6503 | ds_loss: 3.3601 | lr: 9.4761e-07 | scale:   512.0000 | micro time: 0.312 | step time: 0.312\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    813/  5202 | global iter:    813/  5202 | loss: 1.8911 | ds_loss: 2.4063 | lr: 9.4748e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    814/  5202 | global iter:    814/  5202 | loss: 1.8659 | ds_loss: 2.5018 | lr: 9.4735e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    815/  5202 | global iter:    815/  5202 | loss: 2.7400 | ds_loss: 3.7187 | lr: 9.4722e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    816/  5202 | global iter:    816/  5202 | loss: 2.6774 | ds_loss: 3.5273 | lr: 9.4710e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    816/  5202 | global iter:    816/  5202 | loss: 2.2936 | ds_loss: 3.0385 | lr: 9.4710e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    817/  5202 | global iter:    817/  5202 | loss: 1.8119 | ds_loss: 2.3700 | lr: 9.4697e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    818/  5202 | global iter:    818/  5202 | loss: 2.6713 | ds_loss: 3.7269 | lr: 9.4684e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    819/  5202 | global iter:    819/  5202 | loss: 1.8082 | ds_loss: 2.5614 | lr: 9.4671e-07 | scale:   512.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:    820/  5202 | global iter:    820/  5202 | loss: 3.3161 | ds_loss: 3.9788 | lr: 9.4658e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    820/  5202 | global iter:    820/  5202 | loss: 2.4019 | ds_loss: 3.1593 | lr: 9.4658e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    821/  5202 | global iter:    821/  5202 | loss: 1.8086 | ds_loss: 2.8436 | lr: 9.4646e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    822/  5202 | global iter:    822/  5202 | loss: 1.9668 | ds_loss: 2.5296 | lr: 9.4633e-07 | scale:   512.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:    823/  5202 | global iter:    823/  5202 | loss: 2.7437 | ds_loss: 3.4078 | lr: 9.4620e-07 | scale:   512.0000 | micro time: 0.319 | step time: 0.000\n",
            "train | epoch   0 | Iter:    824/  5202 | global iter:    824/  5202 | loss: 2.7270 | ds_loss: 3.5314 | lr: 9.4607e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    824/  5202 | global iter:    824/  5202 | loss: 2.3115 | ds_loss: 3.0781 | lr: 9.4607e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    825/  5202 | global iter:    825/  5202 | loss: 2.6956 | ds_loss: 3.6715 | lr: 9.4594e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    826/  5202 | global iter:    826/  5202 | loss: 1.6950 | ds_loss: 2.5036 | lr: 9.4581e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    827/  5202 | global iter:    827/  5202 | loss: 3.0863 | ds_loss: 3.8326 | lr: 9.4568e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    828/  5202 | global iter:    828/  5202 | loss: 2.7225 | ds_loss: 3.9577 | lr: 9.4555e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    828/  5202 | global iter:    828/  5202 | loss: 2.5498 | ds_loss: 3.4913 | lr: 9.4555e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    829/  5202 | global iter:    829/  5202 | loss: 2.2722 | ds_loss: 3.1446 | lr: 9.4542e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    830/  5202 | global iter:    830/  5202 | loss: 3.0616 | ds_loss: 3.7464 | lr: 9.4529e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    831/  5202 | global iter:    831/  5202 | loss: 2.7455 | ds_loss: 3.6829 | lr: 9.4516e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    832/  5202 | global iter:    832/  5202 | loss: 2.8805 | ds_loss: 3.8795 | lr: 9.4503e-07 | scale:   512.0000 | micro time: 0.302 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    832/  5202 | global iter:    832/  5202 | loss: 2.7400 | ds_loss: 3.6134 | lr: 9.4503e-07 | scale:   512.0000 | micro time: 0.302 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    833/  5202 | global iter:    833/  5202 | loss: 1.2942 | ds_loss: 1.9359 | lr: 9.4490e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    834/  5202 | global iter:    834/  5202 | loss: 2.6698 | ds_loss: 3.3545 | lr: 9.4477e-07 | scale:   512.0000 | micro time: 0.326 | step time: 0.000\n",
            "train | epoch   0 | Iter:    835/  5202 | global iter:    835/  5202 | loss: 2.4599 | ds_loss: 3.2057 | lr: 9.4464e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    836/  5202 | global iter:    836/  5202 | loss: 3.0389 | ds_loss: 3.6891 | lr: 9.4451e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    836/  5202 | global iter:    836/  5202 | loss: 2.3657 | ds_loss: 3.0463 | lr: 9.4451e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    837/  5202 | global iter:    837/  5202 | loss: 2.8882 | ds_loss: 3.6248 | lr: 9.4438e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    838/  5202 | global iter:    838/  5202 | loss: 2.7471 | ds_loss: 3.8859 | lr: 9.4425e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    839/  5202 | global iter:    839/  5202 | loss: 2.9917 | ds_loss: 3.8496 | lr: 9.4412e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    840/  5202 | global iter:    840/  5202 | loss: 2.8927 | ds_loss: 3.6757 | lr: 9.4399e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    840/  5202 | global iter:    840/  5202 | loss: 2.8799 | ds_loss: 3.7590 | lr: 9.4399e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    841/  5202 | global iter:    841/  5202 | loss: 2.6414 | ds_loss: 3.2837 | lr: 9.4386e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    842/  5202 | global iter:    842/  5202 | loss: 2.5460 | ds_loss: 3.8319 | lr: 9.4372e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    843/  5202 | global iter:    843/  5202 | loss: 2.4164 | ds_loss: 2.9684 | lr: 9.4359e-07 | scale:   512.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:    844/  5202 | global iter:    844/  5202 | loss: 3.0993 | ds_loss: 3.5983 | lr: 9.4346e-07 | scale:   512.0000 | micro time: 0.317 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    844/  5202 | global iter:    844/  5202 | loss: 2.6758 | ds_loss: 3.4206 | lr: 9.4346e-07 | scale:   512.0000 | micro time: 0.317 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    845/  5202 | global iter:    845/  5202 | loss: 3.1203 | ds_loss: 4.0469 | lr: 9.4333e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    846/  5202 | global iter:    846/  5202 | loss: 3.1411 | ds_loss: 3.8010 | lr: 9.4320e-07 | scale:   512.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:    847/  5202 | global iter:    847/  5202 | loss: 3.3088 | ds_loss: 4.6646 | lr: 9.4306e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    848/  5202 | global iter:    848/  5202 | loss: 3.0707 | ds_loss: 3.7619 | lr: 9.4293e-07 | scale:   512.0000 | micro time: 0.315 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    848/  5202 | global iter:    848/  5202 | loss: 3.1602 | ds_loss: 4.0686 | lr: 9.4293e-07 | scale:   512.0000 | micro time: 0.315 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    849/  5202 | global iter:    849/  5202 | loss: 2.9172 | ds_loss: 3.8777 | lr: 9.4280e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    850/  5202 | global iter:    850/  5202 | loss: 2.7330 | ds_loss: 3.3459 | lr: 9.4267e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    851/  5202 | global iter:    851/  5202 | loss: 3.0222 | ds_loss: 3.6761 | lr: 9.4253e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    852/  5202 | global iter:    852/  5202 | loss: 3.2687 | ds_loss: 4.7624 | lr: 9.4240e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    852/  5202 | global iter:    852/  5202 | loss: 2.9853 | ds_loss: 3.9155 | lr: 9.4240e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    853/  5202 | global iter:    853/  5202 | loss: 2.7863 | ds_loss: 3.6507 | lr: 9.4227e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    854/  5202 | global iter:    854/  5202 | loss: 2.7190 | ds_loss: 3.4419 | lr: 9.4213e-07 | scale:   512.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:    855/  5202 | global iter:    855/  5202 | loss: 3.0831 | ds_loss: 3.8675 | lr: 9.4200e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    856/  5202 | global iter:    856/  5202 | loss: 2.1341 | ds_loss: 2.6994 | lr: 9.4187e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    856/  5202 | global iter:    856/  5202 | loss: 2.6806 | ds_loss: 3.4149 | lr: 9.4187e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    857/  5202 | global iter:    857/  5202 | loss: 2.4854 | ds_loss: 3.5554 | lr: 9.4173e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    858/  5202 | global iter:    858/  5202 | loss: 2.9009 | ds_loss: 3.7916 | lr: 9.4160e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    859/  5202 | global iter:    859/  5202 | loss: 2.0824 | ds_loss: 2.7978 | lr: 9.4147e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    860/  5202 | global iter:    860/  5202 | loss: 2.8726 | ds_loss: 3.6896 | lr: 9.4133e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    860/  5202 | global iter:    860/  5202 | loss: 2.5853 | ds_loss: 3.4586 | lr: 9.4133e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    861/  5202 | global iter:    861/  5202 | loss: 2.6904 | ds_loss: 3.3924 | lr: 9.4120e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    862/  5202 | global iter:    862/  5202 | loss: 2.6526 | ds_loss: 3.9887 | lr: 9.4106e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    863/  5202 | global iter:    863/  5202 | loss: 3.3493 | ds_loss: 3.9565 | lr: 9.4093e-07 | scale:   512.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:    864/  5202 | global iter:    864/  5202 | loss: 2.5051 | ds_loss: 3.6669 | lr: 9.4079e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    864/  5202 | global iter:    864/  5202 | loss: 2.7994 | ds_loss: 3.7511 | lr: 9.4079e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    865/  5202 | global iter:    865/  5202 | loss: 2.8158 | ds_loss: 3.7106 | lr: 9.4066e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    866/  5202 | global iter:    866/  5202 | loss: 2.9126 | ds_loss: 3.9202 | lr: 9.4052e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    867/  5202 | global iter:    867/  5202 | loss: 2.6297 | ds_loss: 3.6884 | lr: 9.4039e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    868/  5202 | global iter:    868/  5202 | loss: 2.8172 | ds_loss: 3.6221 | lr: 9.4025e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    868/  5202 | global iter:    868/  5202 | loss: 2.7938 | ds_loss: 3.7353 | lr: 9.4025e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    869/  5202 | global iter:    869/  5202 | loss: 2.3900 | ds_loss: 3.2633 | lr: 9.4012e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    870/  5202 | global iter:    870/  5202 | loss: 3.1377 | ds_loss: 3.7344 | lr: 9.3998e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    871/  5202 | global iter:    871/  5202 | loss: 2.1748 | ds_loss: 3.3909 | lr: 9.3985e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    872/  5202 | global iter:    872/  5202 | loss: 2.7068 | ds_loss: 3.5615 | lr: 9.3971e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    872/  5202 | global iter:    872/  5202 | loss: 2.6024 | ds_loss: 3.4875 | lr: 9.3971e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    873/  5202 | global iter:    873/  5202 | loss: 2.4413 | ds_loss: 3.4207 | lr: 9.3958e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    874/  5202 | global iter:    874/  5202 | loss: 3.0511 | ds_loss: 3.7207 | lr: 9.3944e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    875/  5202 | global iter:    875/  5202 | loss: 2.9697 | ds_loss: 4.1385 | lr: 9.3930e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    876/  5202 | global iter:    876/  5202 | loss: 2.4164 | ds_loss: 3.4643 | lr: 9.3917e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    876/  5202 | global iter:    876/  5202 | loss: 2.7196 | ds_loss: 3.6861 | lr: 9.3917e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    877/  5202 | global iter:    877/  5202 | loss: 2.2310 | ds_loss: 2.9906 | lr: 9.3903e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    878/  5202 | global iter:    878/  5202 | loss: 2.7307 | ds_loss: 3.7147 | lr: 9.3889e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    879/  5202 | global iter:    879/  5202 | loss: 2.7172 | ds_loss: 3.5088 | lr: 9.3876e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    880/  5202 | global iter:    880/  5202 | loss: 1.4124 | ds_loss: 2.3596 | lr: 9.3862e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    880/  5202 | global iter:    880/  5202 | loss: 2.2728 | ds_loss: 3.1434 | lr: 9.3862e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    881/  5202 | global iter:    881/  5202 | loss: 2.6513 | ds_loss: 3.8510 | lr: 9.3848e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    882/  5202 | global iter:    882/  5202 | loss: 2.4335 | ds_loss: 3.3238 | lr: 9.3835e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    883/  5202 | global iter:    883/  5202 | loss: 3.1768 | ds_loss: 3.8712 | lr: 9.3821e-07 | scale:   512.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:    884/  5202 | global iter:    884/  5202 | loss: 2.2406 | ds_loss: 3.1061 | lr: 9.3807e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    884/  5202 | global iter:    884/  5202 | loss: 2.6256 | ds_loss: 3.5380 | lr: 9.3807e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    885/  5202 | global iter:    885/  5202 | loss: 2.5382 | ds_loss: 3.7230 | lr: 9.3793e-07 | scale:   512.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:    886/  5202 | global iter:    886/  5202 | loss: 2.4714 | ds_loss: 3.2007 | lr: 9.3780e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    887/  5202 | global iter:    887/  5202 | loss: 2.9916 | ds_loss: 3.7966 | lr: 9.3766e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    888/  5202 | global iter:    888/  5202 | loss: 2.7556 | ds_loss: 3.7939 | lr: 9.3752e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    888/  5202 | global iter:    888/  5202 | loss: 2.6892 | ds_loss: 3.6285 | lr: 9.3752e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    889/  5202 | global iter:    889/  5202 | loss: 3.0687 | ds_loss: 4.2427 | lr: 9.3738e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    890/  5202 | global iter:    890/  5202 | loss: 3.0362 | ds_loss: 3.9370 | lr: 9.3724e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    891/  5202 | global iter:    891/  5202 | loss: 2.8660 | ds_loss: 3.4857 | lr: 9.3710e-07 | scale:   512.0000 | micro time: 0.316 | step time: 0.000\n",
            "train | epoch   0 | Iter:    892/  5202 | global iter:    892/  5202 | loss: 2.9205 | ds_loss: 4.2290 | lr: 9.3697e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    892/  5202 | global iter:    892/  5202 | loss: 2.9728 | ds_loss: 3.9736 | lr: 9.3697e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    893/  5202 | global iter:    893/  5202 | loss: 3.1016 | ds_loss: 4.3589 | lr: 9.3683e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    894/  5202 | global iter:    894/  5202 | loss: 3.0648 | ds_loss: 3.7435 | lr: 9.3669e-07 | scale:   512.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:    895/  5202 | global iter:    895/  5202 | loss: 2.2200 | ds_loss: 3.3466 | lr: 9.3655e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    896/  5202 | global iter:    896/  5202 | loss: 1.3014 | ds_loss: 1.9807 | lr: 9.3641e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    896/  5202 | global iter:    896/  5202 | loss: 2.4220 | ds_loss: 3.3574 | lr: 9.3641e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    897/  5202 | global iter:    897/  5202 | loss: 2.3557 | ds_loss: 3.0984 | lr: 9.3627e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    898/  5202 | global iter:    898/  5202 | loss: 2.6081 | ds_loss: 3.6990 | lr: 9.3613e-07 | scale:   512.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    899/  5202 | global iter:    899/  5202 | loss: 1.1575 | ds_loss: 1.7566 | lr: 9.3599e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    900/  5202 | global iter:    900/  5202 | loss: 3.0650 | ds_loss: 3.8342 | lr: 9.3585e-07 | scale:   512.0000 | micro time: 0.314 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    900/  5202 | global iter:    900/  5202 | loss: 2.2966 | ds_loss: 3.0970 | lr: 9.3585e-07 | scale:   512.0000 | micro time: 0.314 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    901/  5202 | global iter:    901/  5202 | loss: 2.9554 | ds_loss: 3.6609 | lr: 9.3571e-07 | scale:   512.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:    902/  5202 | global iter:    902/  5202 | loss: 2.6217 | ds_loss: 3.3706 | lr: 9.3557e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    903/  5202 | global iter:    903/  5202 | loss: 1.3807 | ds_loss: 1.9775 | lr: 9.3543e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    904/  5202 | global iter:    904/  5202 | loss: 2.4702 | ds_loss: 3.0922 | lr: 9.3529e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    904/  5202 | global iter:    904/  5202 | loss: 2.3570 | ds_loss: 3.0253 | lr: 9.3529e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    905/  5202 | global iter:    905/  5202 | loss: 0.9718 | ds_loss: 1.3333 | lr: 9.3515e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    906/  5202 | global iter:    906/  5202 | loss: 2.6913 | ds_loss: 3.7045 | lr: 9.3501e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    907/  5202 | global iter:    907/  5202 | loss: 2.6508 | ds_loss: 4.4656 | lr: 9.3487e-07 | scale:   512.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    908/  5202 | global iter:    908/  5202 | loss: 2.8038 | ds_loss: 3.4262 | lr: 9.3473e-07 | scale:   512.0000 | micro time: 0.316 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    908/  5202 | global iter:    908/  5202 | loss: 2.2794 | ds_loss: 3.2324 | lr: 9.3473e-07 | scale:   512.0000 | micro time: 0.316 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    909/  5202 | global iter:    909/  5202 | loss: 2.5296 | ds_loss: 3.5330 | lr: 9.3459e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    910/  5202 | global iter:    910/  5202 | loss: 3.0514 | ds_loss: 3.8784 | lr: 9.3445e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    911/  5202 | global iter:    911/  5202 | loss: 1.3881 | ds_loss: 1.9807 | lr: 9.3430e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    912/  5202 | global iter:    912/  5202 | loss: 2.1636 | ds_loss: 2.7577 | lr: 9.3416e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    912/  5202 | global iter:    912/  5202 | loss: 2.2832 | ds_loss: 3.0375 | lr: 9.3416e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    913/  5202 | global iter:    913/  5202 | loss: 2.2453 | ds_loss: 3.1460 | lr: 9.3402e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    914/  5202 | global iter:    914/  5202 | loss: 3.0530 | ds_loss: 3.9821 | lr: 9.3388e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    915/  5202 | global iter:    915/  5202 | loss: 2.6306 | ds_loss: 3.4101 | lr: 9.3374e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    916/  5202 | global iter:    916/  5202 | loss: 2.3544 | ds_loss: 3.7002 | lr: 9.3360e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    916/  5202 | global iter:    916/  5202 | loss: 2.5708 | ds_loss: 3.5596 | lr: 9.3360e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    917/  5202 | global iter:    917/  5202 | loss: 3.1493 | ds_loss: 3.8883 | lr: 9.3345e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    918/  5202 | global iter:    918/  5202 | loss: 2.8963 | ds_loss: 3.7728 | lr: 9.3331e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    919/  5202 | global iter:    919/  5202 | loss: 1.1345 | ds_loss: 1.8651 | lr: 9.3317e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    920/  5202 | global iter:    920/  5202 | loss: 2.5961 | ds_loss: 3.3165 | lr: 9.3303e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    920/  5202 | global iter:    920/  5202 | loss: 2.4440 | ds_loss: 3.2107 | lr: 9.3303e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    921/  5202 | global iter:    921/  5202 | loss: 2.5566 | ds_loss: 3.4117 | lr: 9.3288e-07 | scale:   512.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:    922/  5202 | global iter:    922/  5202 | loss: 2.7740 | ds_loss: 3.6417 | lr: 9.3274e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    923/  5202 | global iter:    923/  5202 | loss: 3.1780 | ds_loss: 4.0736 | lr: 9.3260e-07 | scale:   512.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:    924/  5202 | global iter:    924/  5202 | loss: 2.7896 | ds_loss: 3.5332 | lr: 9.3245e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    924/  5202 | global iter:    924/  5202 | loss: 2.8246 | ds_loss: 3.6651 | lr: 9.3245e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    925/  5202 | global iter:    925/  5202 | loss: 3.0585 | ds_loss: 3.6839 | lr: 9.3231e-07 | scale:   512.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:    926/  5202 | global iter:    926/  5202 | loss: 2.0520 | ds_loss: 3.1075 | lr: 9.3217e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    927/  5202 | global iter:    927/  5202 | loss: 2.5260 | ds_loss: 3.0734 | lr: 9.3202e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    928/  5202 | global iter:    928/  5202 | loss: 3.0237 | ds_loss: 3.8108 | lr: 9.3188e-07 | scale:   512.0000 | micro time: 0.313 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    928/  5202 | global iter:    928/  5202 | loss: 2.6651 | ds_loss: 3.4189 | lr: 9.3188e-07 | scale:   512.0000 | micro time: 0.313 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    929/  5202 | global iter:    929/  5202 | loss: 2.9048 | ds_loss: 3.5465 | lr: 9.3174e-07 | scale:   512.0000 | micro time: 0.318 | step time: 0.000\n",
            "train | epoch   0 | Iter:    930/  5202 | global iter:    930/  5202 | loss: 2.2103 | ds_loss: 2.9492 | lr: 9.3159e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    931/  5202 | global iter:    931/  5202 | loss: 2.8067 | ds_loss: 3.5338 | lr: 9.3145e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    932/  5202 | global iter:    932/  5202 | loss: 3.0102 | ds_loss: 3.9075 | lr: 9.3130e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    932/  5202 | global iter:    932/  5202 | loss: 2.7330 | ds_loss: 3.4843 | lr: 9.3130e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    933/  5202 | global iter:    933/  5202 | loss: 2.9383 | ds_loss: 3.7071 | lr: 9.3116e-07 | scale:   512.0000 | micro time: 0.317 | step time: 0.000\n",
            "train | epoch   0 | Iter:    934/  5202 | global iter:    934/  5202 | loss: 3.1193 | ds_loss: 3.7131 | lr: 9.3102e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    935/  5202 | global iter:    935/  5202 | loss: 2.9776 | ds_loss: 3.7156 | lr: 9.3087e-07 | scale:   512.0000 | micro time: 0.316 | step time: 0.000\n",
            "train | epoch   0 | Iter:    936/  5202 | global iter:    936/  5202 | loss: 0.9482 | ds_loss: 1.5036 | lr: 9.3073e-07 | scale:   512.0000 | micro time: 0.315 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    936/  5202 | global iter:    936/  5202 | loss: 2.4959 | ds_loss: 3.1598 | lr: 9.3073e-07 | scale:   512.0000 | micro time: 0.315 | step time: 0.313\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    937/  5202 | global iter:    937/  5202 | loss: 2.8968 | ds_loss: 3.7317 | lr: 9.3058e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    938/  5202 | global iter:    938/  5202 | loss: 2.6809 | ds_loss: 3.6232 | lr: 9.3044e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    939/  5202 | global iter:    939/  5202 | loss: 2.3165 | ds_loss: 2.9668 | lr: 9.3029e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    940/  5202 | global iter:    940/  5202 | loss: 2.2162 | ds_loss: 2.9841 | lr: 9.3015e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    940/  5202 | global iter:    940/  5202 | loss: 2.5276 | ds_loss: 3.3264 | lr: 9.3015e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    941/  5202 | global iter:    941/  5202 | loss: 2.7437 | ds_loss: 3.8308 | lr: 9.3000e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    942/  5202 | global iter:    942/  5202 | loss: 3.3212 | ds_loss: 4.0048 | lr: 9.2985e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:    943/  5202 | global iter:    943/  5202 | loss: 3.1674 | ds_loss: 3.9713 | lr: 9.2971e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    944/  5202 | global iter:    944/  5202 | loss: 2.8386 | ds_loss: 3.5227 | lr: 9.2956e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    944/  5202 | global iter:    944/  5202 | loss: 3.0178 | ds_loss: 3.8324 | lr: 9.2956e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    945/  5202 | global iter:    945/  5202 | loss: 2.8621 | ds_loss: 3.8346 | lr: 9.2942e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:    946/  5202 | global iter:    946/  5202 | loss: 2.7381 | ds_loss: 3.4185 | lr: 9.2927e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    947/  5202 | global iter:    947/  5202 | loss: 2.7864 | ds_loss: 3.4693 | lr: 9.2912e-07 | scale:   512.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:    948/  5202 | global iter:    948/  5202 | loss: 2.7494 | ds_loss: 3.5237 | lr: 9.2898e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    948/  5202 | global iter:    948/  5202 | loss: 2.7840 | ds_loss: 3.5615 | lr: 9.2898e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    949/  5202 | global iter:    949/  5202 | loss: 2.1970 | ds_loss: 3.0865 | lr: 9.2883e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    950/  5202 | global iter:    950/  5202 | loss: 1.4519 | ds_loss: 2.0921 | lr: 9.2868e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    951/  5202 | global iter:    951/  5202 | loss: 2.1974 | ds_loss: 3.5133 | lr: 9.2854e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    952/  5202 | global iter:    952/  5202 | loss: 3.0346 | ds_loss: 3.7208 | lr: 9.2839e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    952/  5202 | global iter:    952/  5202 | loss: 2.2202 | ds_loss: 3.1032 | lr: 9.2839e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    953/  5202 | global iter:    953/  5202 | loss: 2.5740 | ds_loss: 3.4980 | lr: 9.2824e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    954/  5202 | global iter:    954/  5202 | loss: 3.0554 | ds_loss: 3.6311 | lr: 9.2810e-07 | scale:   512.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:    955/  5202 | global iter:    955/  5202 | loss: 2.5200 | ds_loss: 3.2029 | lr: 9.2795e-07 | scale:   512.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:    956/  5202 | global iter:    956/  5202 | loss: 2.8164 | ds_loss: 3.4313 | lr: 9.2780e-07 | scale:   512.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    956/  5202 | global iter:    956/  5202 | loss: 2.7414 | ds_loss: 3.4408 | lr: 9.2780e-07 | scale:   512.0000 | micro time: 0.311 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    957/  5202 | global iter:    957/  5202 | loss: 2.6525 | ds_loss: 3.3960 | lr: 9.2765e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    958/  5202 | global iter:    958/  5202 | loss: 2.6572 | ds_loss: 3.6490 | lr: 9.2751e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    959/  5202 | global iter:    959/  5202 | loss: 3.2930 | ds_loss: 3.9975 | lr: 9.2736e-07 | scale:   512.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:    960/  5202 | global iter:    960/  5202 | loss: 2.3182 | ds_loss: 3.4040 | lr: 9.2721e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    960/  5202 | global iter:    960/  5202 | loss: 2.7302 | ds_loss: 3.6116 | lr: 9.2721e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    961/  5202 | global iter:    961/  5202 | loss: 1.3083 | ds_loss: 1.7806 | lr: 9.2706e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    962/  5202 | global iter:    962/  5202 | loss: 2.7056 | ds_loss: 3.7965 | lr: 9.2691e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    963/  5202 | global iter:    963/  5202 | loss: 2.2982 | ds_loss: 3.0007 | lr: 9.2676e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    964/  5202 | global iter:    964/  5202 | loss: 3.1497 | ds_loss: 3.8599 | lr: 9.2662e-07 | scale:   512.0000 | micro time: 0.313 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    964/  5202 | global iter:    964/  5202 | loss: 2.3654 | ds_loss: 3.1094 | lr: 9.2662e-07 | scale:   512.0000 | micro time: 0.313 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    965/  5202 | global iter:    965/  5202 | loss: 2.0797 | ds_loss: 2.9378 | lr: 9.2647e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    966/  5202 | global iter:    966/  5202 | loss: 2.7334 | ds_loss: 3.5691 | lr: 9.2632e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    967/  5202 | global iter:    967/  5202 | loss: 3.0179 | ds_loss: 3.7550 | lr: 9.2617e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:    968/  5202 | global iter:    968/  5202 | loss: 2.9352 | ds_loss: 3.6111 | lr: 9.2602e-07 | scale:   512.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    968/  5202 | global iter:    968/  5202 | loss: 2.6915 | ds_loss: 3.4683 | lr: 9.2602e-07 | scale:   512.0000 | micro time: 0.311 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    969/  5202 | global iter:    969/  5202 | loss: 2.6146 | ds_loss: 3.5489 | lr: 9.2587e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:    970/  5202 | global iter:    970/  5202 | loss: 1.5956 | ds_loss: 2.1423 | lr: 9.2572e-07 | scale:   512.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:    971/  5202 | global iter:    971/  5202 | loss: 2.6443 | ds_loss: 3.4542 | lr: 9.2557e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    972/  5202 | global iter:    972/  5202 | loss: 2.2101 | ds_loss: 2.9317 | lr: 9.2542e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    972/  5202 | global iter:    972/  5202 | loss: 2.2662 | ds_loss: 3.0193 | lr: 9.2542e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    973/  5202 | global iter:    973/  5202 | loss: 1.7669 | ds_loss: 3.0278 | lr: 9.2527e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    974/  5202 | global iter:    974/  5202 | loss: 3.1481 | ds_loss: 3.7300 | lr: 9.2512e-07 | scale:   512.0000 | micro time: 0.322 | step time: 0.000\n",
            "train | epoch   0 | Iter:    975/  5202 | global iter:    975/  5202 | loss: 2.5061 | ds_loss: 3.9312 | lr: 9.2497e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    976/  5202 | global iter:    976/  5202 | loss: 3.1303 | ds_loss: 3.7617 | lr: 9.2482e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    976/  5202 | global iter:    976/  5202 | loss: 2.6379 | ds_loss: 3.6127 | lr: 9.2482e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    977/  5202 | global iter:    977/  5202 | loss: 2.8448 | ds_loss: 3.5622 | lr: 9.2467e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    978/  5202 | global iter:    978/  5202 | loss: 2.3844 | ds_loss: 3.4031 | lr: 9.2452e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    979/  5202 | global iter:    979/  5202 | loss: 2.9889 | ds_loss: 4.1369 | lr: 9.2437e-07 | scale:   512.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:    980/  5202 | global iter:    980/  5202 | loss: 3.0546 | ds_loss: 3.7835 | lr: 9.2422e-07 | scale:   512.0000 | micro time: 0.313 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    980/  5202 | global iter:    980/  5202 | loss: 2.8182 | ds_loss: 3.7214 | lr: 9.2422e-07 | scale:   512.0000 | micro time: 0.313 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    981/  5202 | global iter:    981/  5202 | loss: 2.9263 | ds_loss: 3.6139 | lr: 9.2407e-07 | scale:   512.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:    982/  5202 | global iter:    982/  5202 | loss: 1.7580 | ds_loss: 2.5949 | lr: 9.2392e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    983/  5202 | global iter:    983/  5202 | loss: 2.6238 | ds_loss: 3.9881 | lr: 9.2376e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:    984/  5202 | global iter:    984/  5202 | loss: 2.6290 | ds_loss: 3.2513 | lr: 9.2361e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    984/  5202 | global iter:    984/  5202 | loss: 2.4843 | ds_loss: 3.3621 | lr: 9.2361e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    985/  5202 | global iter:    985/  5202 | loss: 2.3828 | ds_loss: 3.3887 | lr: 9.2346e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:    986/  5202 | global iter:    986/  5202 | loss: 2.4205 | ds_loss: 3.1506 | lr: 9.2331e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    987/  5202 | global iter:    987/  5202 | loss: 2.0624 | ds_loss: 2.5276 | lr: 9.2316e-07 | scale:   512.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:    988/  5202 | global iter:    988/  5202 | loss: 2.6973 | ds_loss: 3.3645 | lr: 9.2301e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    988/  5202 | global iter:    988/  5202 | loss: 2.3907 | ds_loss: 3.1079 | lr: 9.2301e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    989/  5202 | global iter:    989/  5202 | loss: 2.9641 | ds_loss: 3.7048 | lr: 9.2285e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:    990/  5202 | global iter:    990/  5202 | loss: 2.8182 | ds_loss: 3.9109 | lr: 9.2270e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:    991/  5202 | global iter:    991/  5202 | loss: 2.5899 | ds_loss: 3.5505 | lr: 9.2255e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:    992/  5202 | global iter:    992/  5202 | loss: 1.5959 | ds_loss: 2.0539 | lr: 9.2240e-07 | scale:   512.0000 | micro time: 0.312 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    992/  5202 | global iter:    992/  5202 | loss: 2.4920 | ds_loss: 3.3050 | lr: 9.2240e-07 | scale:   512.0000 | micro time: 0.312 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    993/  5202 | global iter:    993/  5202 | loss: 2.2228 | ds_loss: 3.1275 | lr: 9.2224e-07 | scale:   512.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:    994/  5202 | global iter:    994/  5202 | loss: 2.4352 | ds_loss: 3.6644 | lr: 9.2209e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    995/  5202 | global iter:    995/  5202 | loss: 2.0277 | ds_loss: 2.5936 | lr: 9.2194e-07 | scale:   512.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:    996/  5202 | global iter:    996/  5202 | loss: 2.7977 | ds_loss: 3.5538 | lr: 9.2179e-07 | scale:   512.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    996/  5202 | global iter:    996/  5202 | loss: 2.3708 | ds_loss: 3.2348 | lr: 9.2179e-07 | scale:   512.0000 | micro time: 0.311 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:    997/  5202 | global iter:    997/  5202 | loss: 1.1062 | ds_loss: 1.5181 | lr: 9.2163e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:    998/  5202 | global iter:    998/  5202 | loss: 2.4485 | ds_loss: 3.2774 | lr: 9.2148e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:    999/  5202 | global iter:    999/  5202 | loss: 2.5026 | ds_loss: 3.6425 | lr: 9.2133e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1000/  5202 | global iter:   1000/  5202 | loss: 2.6348 | ds_loss: 3.4957 | lr: 9.2117e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1000/  5202 | global iter:   1000/  5202 | loss: 2.1730 | ds_loss: 2.9834 | lr: 9.2117e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1001/  5202 | global iter:   1001/  5202 | loss: 2.0272 | ds_loss: 2.7966 | lr: 9.2102e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1002/  5202 | global iter:   1002/  5202 | loss: 3.0816 | ds_loss: 3.5954 | lr: 9.2086e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1003/  5202 | global iter:   1003/  5202 | loss: 2.6988 | ds_loss: 3.6970 | lr: 9.2071e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1004/  5202 | global iter:   1004/  5202 | loss: 2.4429 | ds_loss: 3.9505 | lr: 9.2056e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1004/  5202 | global iter:   1004/  5202 | loss: 2.5626 | ds_loss: 3.5099 | lr: 9.2056e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1005/  5202 | global iter:   1005/  5202 | loss: 1.9860 | ds_loss: 2.5984 | lr: 9.2040e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1006/  5202 | global iter:   1006/  5202 | loss: 2.6744 | ds_loss: 3.4403 | lr: 9.2025e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1007/  5202 | global iter:   1007/  5202 | loss: 2.8985 | ds_loss: 3.7896 | lr: 9.2009e-07 | scale:   512.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1008/  5202 | global iter:   1008/  5202 | loss: 2.8407 | ds_loss: 3.4457 | lr: 9.1994e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1008/  5202 | global iter:   1008/  5202 | loss: 2.5999 | ds_loss: 3.3185 | lr: 9.1994e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1009/  5202 | global iter:   1009/  5202 | loss: 2.6417 | ds_loss: 3.5166 | lr: 9.1978e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1010/  5202 | global iter:   1010/  5202 | loss: 2.4621 | ds_loss: 3.1150 | lr: 9.1963e-07 | scale:   512.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1011/  5202 | global iter:   1011/  5202 | loss: 2.6090 | ds_loss: 3.9293 | lr: 9.1947e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1012/  5202 | global iter:   1012/  5202 | loss: 3.1073 | ds_loss: 3.7400 | lr: 9.1932e-07 | scale:   512.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1012/  5202 | global iter:   1012/  5202 | loss: 2.7050 | ds_loss: 3.5752 | lr: 9.1932e-07 | scale:   512.0000 | micro time: 0.311 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1013/  5202 | global iter:   1013/  5202 | loss: 2.6115 | ds_loss: 3.7046 | lr: 9.1916e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1014/  5202 | global iter:   1014/  5202 | loss: 2.7866 | ds_loss: 3.3747 | lr: 9.1901e-07 | scale:   512.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1015/  5202 | global iter:   1015/  5202 | loss: 2.7268 | ds_loss: 3.4600 | lr: 9.1885e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1016/  5202 | global iter:   1016/  5202 | loss: 2.7197 | ds_loss: 3.5715 | lr: 9.1870e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1016/  5202 | global iter:   1016/  5202 | loss: 2.7111 | ds_loss: 3.5277 | lr: 9.1870e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1017/  5202 | global iter:   1017/  5202 | loss: 1.8337 | ds_loss: 2.8165 | lr: 9.1854e-07 | scale:   512.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1018/  5202 | global iter:   1018/  5202 | loss: 3.5101 | ds_loss: 5.1787 | lr: 9.1838e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1019/  5202 | global iter:   1019/  5202 | loss: 1.8299 | ds_loss: 2.9765 | lr: 9.1823e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1020/  5202 | global iter:   1020/  5202 | loss: 1.3999 | ds_loss: 2.2211 | lr: 9.1807e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1020/  5202 | global iter:   1020/  5202 | loss: 2.1434 | ds_loss: 3.2982 | lr: 9.1807e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1021/  5202 | global iter:   1021/  5202 | loss: 2.3048 | ds_loss: 3.3288 | lr: 9.1792e-07 | scale:   512.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1022/  5202 | global iter:   1022/  5202 | loss: 2.9755 | ds_loss: 3.8065 | lr: 9.1776e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1023/  5202 | global iter:   1023/  5202 | loss: 3.1128 | ds_loss: 3.8072 | lr: 9.1760e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1024/  5202 | global iter:   1024/  5202 | loss: 2.9884 | ds_loss: 3.6950 | lr: 9.1745e-07 | scale:   512.0000 | micro time: 0.318 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1024/  5202 | global iter:   1024/  5202 | loss: 2.8454 | ds_loss: 3.6594 | lr: 9.1745e-07 | scale:   512.0000 | micro time: 0.318 | step time: 0.312\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1025/  5202 | global iter:   1025/  5202 | loss: 2.9524 | ds_loss: 3.6180 | lr: 9.1729e-07 | scale:   512.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1026/  5202 | global iter:   1026/  5202 | loss: 2.4131 | ds_loss: 3.1731 | lr: 9.1713e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1027/  5202 | global iter:   1027/  5202 | loss: 2.5493 | ds_loss: 3.2911 | lr: 9.1697e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1028/  5202 | global iter:   1028/  5202 | loss: 1.9677 | ds_loss: 2.5699 | lr: 9.1682e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1028/  5202 | global iter:   1028/  5202 | loss: 2.4706 | ds_loss: 3.1630 | lr: 9.1682e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1029/  5202 | global iter:   1029/  5202 | loss: 2.6775 | ds_loss: 3.4354 | lr: 9.1666e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1030/  5202 | global iter:   1030/  5202 | loss: 1.8093 | ds_loss: 2.4370 | lr: 9.1650e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1031/  5202 | global iter:   1031/  5202 | loss: 1.5806 | ds_loss: 2.3721 | lr: 9.1634e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1032/  5202 | global iter:   1032/  5202 | loss: 2.8864 | ds_loss: 3.6712 | lr: 9.1619e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1032/  5202 | global iter:   1032/  5202 | loss: 2.2384 | ds_loss: 2.9789 | lr: 9.1619e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1033/  5202 | global iter:   1033/  5202 | loss: 2.2046 | ds_loss: 3.0383 | lr: 9.1603e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1034/  5202 | global iter:   1034/  5202 | loss: 2.5637 | ds_loss: 3.3167 | lr: 9.1587e-07 | scale:   512.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1035/  5202 | global iter:   1035/  5202 | loss: 2.8222 | ds_loss: 3.8096 | lr: 9.1571e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1036/  5202 | global iter:   1036/  5202 | loss: 2.6129 | ds_loss: 3.5144 | lr: 9.1555e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1036/  5202 | global iter:   1036/  5202 | loss: 2.5508 | ds_loss: 3.4197 | lr: 9.1555e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1037/  5202 | global iter:   1037/  5202 | loss: 2.5899 | ds_loss: 3.3435 | lr: 9.1539e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1038/  5202 | global iter:   1038/  5202 | loss: 3.1046 | ds_loss: 3.7668 | lr: 9.1524e-07 | scale:   512.0000 | micro time: 0.317 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1039/  5202 | global iter:   1039/  5202 | loss: 2.6573 | ds_loss: 3.3607 | lr: 9.1508e-07 | scale:   512.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1040/  5202 | global iter:   1040/  5202 | loss: 2.3204 | ds_loss: 2.9110 | lr: 9.1492e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1040/  5202 | global iter:   1040/  5202 | loss: 2.6681 | ds_loss: 3.3455 | lr: 9.1492e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1041/  5202 | global iter:   1041/  5202 | loss: 3.0078 | ds_loss: 3.7451 | lr: 9.1476e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1042/  5202 | global iter:   1042/  5202 | loss: 1.3342 | ds_loss: 2.1913 | lr: 9.1460e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1043/  5202 | global iter:   1043/  5202 | loss: 2.2284 | ds_loss: 2.9620 | lr: 9.1444e-07 | scale:   512.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1044/  5202 | global iter:   1044/  5202 | loss: 2.7418 | ds_loss: 3.4752 | lr: 9.1428e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1044/  5202 | global iter:   1044/  5202 | loss: 2.3280 | ds_loss: 3.0934 | lr: 9.1428e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1045/  5202 | global iter:   1045/  5202 | loss: 2.8037 | ds_loss: 3.6036 | lr: 9.1412e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1046/  5202 | global iter:   1046/  5202 | loss: 3.0915 | ds_loss: 3.7251 | lr: 9.1396e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1047/  5202 | global iter:   1047/  5202 | loss: 2.5506 | ds_loss: 3.3158 | lr: 9.1380e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1048/  5202 | global iter:   1048/  5202 | loss: 2.8977 | ds_loss: 3.8135 | lr: 9.1364e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1048/  5202 | global iter:   1048/  5202 | loss: 2.8359 | ds_loss: 3.6145 | lr: 9.1364e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1049/  5202 | global iter:   1049/  5202 | loss: 3.1018 | ds_loss: 3.8281 | lr: 9.1348e-07 | scale:   512.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1050/  5202 | global iter:   1050/  5202 | loss: 1.5964 | ds_loss: 2.2360 | lr: 9.1332e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1051/  5202 | global iter:   1051/  5202 | loss: 2.0991 | ds_loss: 3.0460 | lr: 9.1316e-07 | scale:   512.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1052/  5202 | global iter:   1052/  5202 | loss: 2.1490 | ds_loss: 2.8847 | lr: 9.1300e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1052/  5202 | global iter:   1052/  5202 | loss: 2.2366 | ds_loss: 2.9987 | lr: 9.1300e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1053/  5202 | global iter:   1053/  5202 | loss: 3.3222 | ds_loss: 4.0481 | lr: 9.1284e-07 | scale:   512.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1054/  5202 | global iter:   1054/  5202 | loss: 1.9789 | ds_loss: 2.6231 | lr: 9.1268e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1055/  5202 | global iter:   1055/  5202 | loss: 1.5053 | ds_loss: 2.2892 | lr: 9.1252e-07 | scale:   512.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1056/  5202 | global iter:   1056/  5202 | loss: 1.8767 | ds_loss: 2.8571 | lr: 9.1236e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1056/  5202 | global iter:   1056/  5202 | loss: 2.1708 | ds_loss: 2.9544 | lr: 9.1236e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1057/  5202 | global iter:   1057/  5202 | loss: 3.1795 | ds_loss: 3.9922 | lr: 9.1220e-07 | scale:   512.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1058/  5202 | global iter:   1058/  5202 | loss: 3.1173 | ds_loss: 4.0901 | lr: 9.1203e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1059/  5202 | global iter:   1059/  5202 | loss: 0.3351 | ds_loss: 0.5402 | lr: 9.1187e-07 | scale:   512.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1060/  5202 | global iter:   1060/  5202 | loss: 3.3960 | ds_loss: 4.0258 | lr: 9.1171e-07 | scale:   512.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1060/  5202 | global iter:   1060/  5202 | loss: 2.5070 | ds_loss: 3.1621 | lr: 9.1171e-07 | scale:   512.0000 | micro time: 0.311 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1061/  5202 | global iter:   1061/  5202 | loss: 2.7490 | ds_loss: 3.4318 | lr: 9.1155e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1062/  5202 | global iter:   1062/  5202 | loss: 2.8678 | ds_loss: 3.4949 | lr: 9.1139e-07 | scale:   512.0000 | micro time: 0.316 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1063/  5202 | global iter:   1063/  5202 | loss: 2.8220 | ds_loss: 3.7754 | lr: 9.1123e-07 | scale:   512.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1064/  5202 | global iter:   1064/  5202 | loss: 2.8426 | ds_loss: 3.4745 | lr: 9.1106e-07 | scale:   512.0000 | micro time: 0.314 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1064/  5202 | global iter:   1064/  5202 | loss: 2.8204 | ds_loss: 3.5441 | lr: 9.1106e-07 | scale:   512.0000 | micro time: 0.314 | step time: 0.312\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1065/  5202 | global iter:   1065/  5202 | loss: 2.7552 | ds_loss: 3.8590 | lr: 9.1090e-07 | scale:   512.0000 | micro time: 0.310 | step time: 0.000\n",
            "[2025-04-16 21:07:55,760] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 512, reducing to 256\n",
            "train | epoch   0 | Iter:   1066/  5202 | global iter:   1066/  5202 | loss: 2.8662 | ds_loss: 3.5869 | lr: 9.1090e-07 | scale:   256.0000 | micro time: 0.235 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1067/  5202 | global iter:   1067/  5202 | loss: 2.7190 | ds_loss: 3.7261 | lr: 9.1074e-07 | scale:   256.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1068/  5202 | global iter:   1068/  5202 | loss: 1.7014 | ds_loss: 2.9263 | lr: 9.1058e-07 | scale:   256.0000 | micro time: 0.302 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1068/  5202 | global iter:   1068/  5202 | loss: 2.5104 | ds_loss: 3.5246 | lr: 9.1058e-07 | scale:   256.0000 | micro time: 0.302 | step time: 0.289\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1069/  5202 | global iter:   1069/  5202 | loss: 2.8576 | ds_loss: 3.6095 | lr: 9.1041e-07 | scale:   256.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1070/  5202 | global iter:   1070/  5202 | loss: 2.9863 | ds_loss: 3.6639 | lr: 9.1025e-07 | scale:   256.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1071/  5202 | global iter:   1071/  5202 | loss: 2.7855 | ds_loss: 3.5089 | lr: 9.1009e-07 | scale:   256.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1072/  5202 | global iter:   1072/  5202 | loss: 2.9573 | ds_loss: 3.8410 | lr: 9.0992e-07 | scale:   256.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1072/  5202 | global iter:   1072/  5202 | loss: 2.8967 | ds_loss: 3.6558 | lr: 9.0992e-07 | scale:   256.0000 | micro time: 0.307 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1073/  5202 | global iter:   1073/  5202 | loss: 2.9805 | ds_loss: 3.8015 | lr: 9.0976e-07 | scale:   256.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1074/  5202 | global iter:   1074/  5202 | loss: 2.8572 | ds_loss: 3.5325 | lr: 9.0960e-07 | scale:   256.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1075/  5202 | global iter:   1075/  5202 | loss: 2.8897 | ds_loss: 3.6533 | lr: 9.0944e-07 | scale:   256.0000 | micro time: 0.314 | step time: 0.000\n",
            "[2025-04-16 21:07:58,820] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 256, reducing to 128\n",
            "train | epoch   0 | Iter:   1076/  5202 | global iter:   1076/  5202 | loss: 3.1745 | ds_loss: 4.3562 | lr: 9.0944e-07 | scale:   128.0000 | micro time: 0.234 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1076/  5202 | global iter:   1076/  5202 | loss: 2.9755 | ds_loss: 3.8359 | lr: 9.0944e-07 | scale:   128.0000 | micro time: 0.234 | step time: 0.292\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1077/  5202 | global iter:   1077/  5202 | loss: 2.5388 | ds_loss: 3.2874 | lr: 9.0927e-07 | scale:   128.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1078/  5202 | global iter:   1078/  5202 | loss: 1.9239 | ds_loss: 2.8957 | lr: 9.0911e-07 | scale:   128.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1079/  5202 | global iter:   1079/  5202 | loss: 2.8326 | ds_loss: 3.6542 | lr: 9.0894e-07 | scale:   128.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1080/  5202 | global iter:   1080/  5202 | loss: 1.6828 | ds_loss: 2.5258 | lr: 9.0878e-07 | scale:   128.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1080/  5202 | global iter:   1080/  5202 | loss: 2.2445 | ds_loss: 3.0907 | lr: 9.0878e-07 | scale:   128.0000 | micro time: 0.305 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1081/  5202 | global iter:   1081/  5202 | loss: 2.1760 | ds_loss: 3.0948 | lr: 9.0862e-07 | scale:   128.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1082/  5202 | global iter:   1082/  5202 | loss: 3.1035 | ds_loss: 3.8103 | lr: 9.0845e-07 | scale:   128.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1083/  5202 | global iter:   1083/  5202 | loss: 2.2978 | ds_loss: 3.1477 | lr: 9.0829e-07 | scale:   128.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1084/  5202 | global iter:   1084/  5202 | loss: 2.5738 | ds_loss: 3.4943 | lr: 9.0812e-07 | scale:   128.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1084/  5202 | global iter:   1084/  5202 | loss: 2.5378 | ds_loss: 3.3868 | lr: 9.0812e-07 | scale:   128.0000 | micro time: 0.307 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1085/  5202 | global iter:   1085/  5202 | loss: 1.9107 | ds_loss: 2.2944 | lr: 9.0796e-07 | scale:   128.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1086/  5202 | global iter:   1086/  5202 | loss: 2.9679 | ds_loss: 3.6567 | lr: 9.0779e-07 | scale:   128.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1087/  5202 | global iter:   1087/  5202 | loss: 2.9047 | ds_loss: 3.4554 | lr: 9.0763e-07 | scale:   128.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1088/  5202 | global iter:   1088/  5202 | loss: 1.2009 | ds_loss: 1.8892 | lr: 9.0746e-07 | scale:   128.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1088/  5202 | global iter:   1088/  5202 | loss: 2.2461 | ds_loss: 2.8239 | lr: 9.0746e-07 | scale:   128.0000 | micro time: 0.303 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1089/  5202 | global iter:   1089/  5202 | loss: 3.0154 | ds_loss: 3.7999 | lr: 9.0730e-07 | scale:   128.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1090/  5202 | global iter:   1090/  5202 | loss: 3.2461 | ds_loss: 3.7761 | lr: 9.0713e-07 | scale:   128.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1091/  5202 | global iter:   1091/  5202 | loss: 2.5870 | ds_loss: 3.4357 | lr: 9.0697e-07 | scale:   128.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1092/  5202 | global iter:   1092/  5202 | loss: 2.1722 | ds_loss: 3.3850 | lr: 9.0680e-07 | scale:   128.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1092/  5202 | global iter:   1092/  5202 | loss: 2.7552 | ds_loss: 3.5992 | lr: 9.0680e-07 | scale:   128.0000 | micro time: 0.307 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1093/  5202 | global iter:   1093/  5202 | loss: 2.5482 | ds_loss: 3.6754 | lr: 9.0664e-07 | scale:   128.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1094/  5202 | global iter:   1094/  5202 | loss: 1.2326 | ds_loss: 1.7656 | lr: 9.0647e-07 | scale:   128.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1095/  5202 | global iter:   1095/  5202 | loss: 3.0672 | ds_loss: 4.2515 | lr: 9.0630e-07 | scale:   128.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1096/  5202 | global iter:   1096/  5202 | loss: 2.7555 | ds_loss: 3.2482 | lr: 9.0614e-07 | scale:   128.0000 | micro time: 0.313 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1096/  5202 | global iter:   1096/  5202 | loss: 2.4008 | ds_loss: 3.2352 | lr: 9.0614e-07 | scale:   128.0000 | micro time: 0.313 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1097/  5202 | global iter:   1097/  5202 | loss: 2.7459 | ds_loss: 3.4464 | lr: 9.0597e-07 | scale:   128.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1098/  5202 | global iter:   1098/  5202 | loss: 2.4067 | ds_loss: 3.4863 | lr: 9.0581e-07 | scale:   128.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1099/  5202 | global iter:   1099/  5202 | loss: 2.5142 | ds_loss: 3.5280 | lr: 9.0564e-07 | scale:   128.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1100/  5202 | global iter:   1100/  5202 | loss: 2.7769 | ds_loss: 3.4852 | lr: 9.0547e-07 | scale:   128.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1100/  5202 | global iter:   1100/  5202 | loss: 2.6110 | ds_loss: 3.4865 | lr: 9.0547e-07 | scale:   128.0000 | micro time: 0.310 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1101/  5202 | global iter:   1101/  5202 | loss: 2.6668 | ds_loss: 3.3512 | lr: 9.0531e-07 | scale:   128.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1102/  5202 | global iter:   1102/  5202 | loss: 3.0863 | ds_loss: 3.8687 | lr: 9.0514e-07 | scale:   128.0000 | micro time: 0.317 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1103/  5202 | global iter:   1103/  5202 | loss: 2.7996 | ds_loss: 3.5357 | lr: 9.0497e-07 | scale:   128.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1104/  5202 | global iter:   1104/  5202 | loss: 2.4623 | ds_loss: 3.1998 | lr: 9.0481e-07 | scale:   128.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1104/  5202 | global iter:   1104/  5202 | loss: 2.7538 | ds_loss: 3.4888 | lr: 9.0481e-07 | scale:   128.0000 | micro time: 0.307 | step time: 0.312\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1105/  5202 | global iter:   1105/  5202 | loss: 1.6670 | ds_loss: 2.5423 | lr: 9.0464e-07 | scale:   128.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1106/  5202 | global iter:   1106/  5202 | loss: 2.7841 | ds_loss: 3.5111 | lr: 9.0447e-07 | scale:   128.0000 | micro time: 0.318 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1107/  5202 | global iter:   1107/  5202 | loss: 2.1132 | ds_loss: 2.8200 | lr: 9.0430e-07 | scale:   128.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1108/  5202 | global iter:   1108/  5202 | loss: 2.4737 | ds_loss: 3.3902 | lr: 9.0414e-07 | scale:   128.0000 | micro time: 0.314 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1108/  5202 | global iter:   1108/  5202 | loss: 2.2595 | ds_loss: 3.0659 | lr: 9.0414e-07 | scale:   128.0000 | micro time: 0.314 | step time: 0.313\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1109/  5202 | global iter:   1109/  5202 | loss: 2.6776 | ds_loss: 3.8495 | lr: 9.0397e-07 | scale:   128.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1110/  5202 | global iter:   1110/  5202 | loss: 2.4526 | ds_loss: 3.4644 | lr: 9.0380e-07 | scale:   128.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1111/  5202 | global iter:   1111/  5202 | loss: 2.5852 | ds_loss: 3.3939 | lr: 9.0363e-07 | scale:   128.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1112/  5202 | global iter:   1112/  5202 | loss: 2.8643 | ds_loss: 3.6169 | lr: 9.0346e-07 | scale:   128.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1112/  5202 | global iter:   1112/  5202 | loss: 2.6449 | ds_loss: 3.5812 | lr: 9.0346e-07 | scale:   128.0000 | micro time: 0.308 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1113/  5202 | global iter:   1113/  5202 | loss: 2.7519 | ds_loss: 3.4355 | lr: 9.0330e-07 | scale:   128.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1114/  5202 | global iter:   1114/  5202 | loss: 2.4197 | ds_loss: 3.3977 | lr: 9.0313e-07 | scale:   128.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1115/  5202 | global iter:   1115/  5202 | loss: 2.7731 | ds_loss: 3.6566 | lr: 9.0296e-07 | scale:   128.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1116/  5202 | global iter:   1116/  5202 | loss: 2.5927 | ds_loss: 3.3680 | lr: 9.0279e-07 | scale:   128.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1116/  5202 | global iter:   1116/  5202 | loss: 2.6344 | ds_loss: 3.4645 | lr: 9.0279e-07 | scale:   128.0000 | micro time: 0.305 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1117/  5202 | global iter:   1117/  5202 | loss: 2.3077 | ds_loss: 3.6342 | lr: 9.0262e-07 | scale:   128.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1118/  5202 | global iter:   1118/  5202 | loss: 1.6810 | ds_loss: 2.4787 | lr: 9.0245e-07 | scale:   128.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1119/  5202 | global iter:   1119/  5202 | loss: 2.6409 | ds_loss: 3.3832 | lr: 9.0228e-07 | scale:   128.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1120/  5202 | global iter:   1120/  5202 | loss: 1.6083 | ds_loss: 2.5060 | lr: 9.0211e-07 | scale:   128.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1120/  5202 | global iter:   1120/  5202 | loss: 2.0595 | ds_loss: 3.0005 | lr: 9.0211e-07 | scale:   128.0000 | micro time: 0.309 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1121/  5202 | global iter:   1121/  5202 | loss: 2.7570 | ds_loss: 3.6218 | lr: 9.0195e-07 | scale:   128.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1122/  5202 | global iter:   1122/  5202 | loss: 3.3702 | ds_loss: 4.0185 | lr: 9.0178e-07 | scale:   128.0000 | micro time: 0.316 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1123/  5202 | global iter:   1123/  5202 | loss: 2.5613 | ds_loss: 3.4786 | lr: 9.0161e-07 | scale:   128.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1124/  5202 | global iter:   1124/  5202 | loss: 2.8048 | ds_loss: 3.8192 | lr: 9.0144e-07 | scale:   128.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1124/  5202 | global iter:   1124/  5202 | loss: 2.8734 | ds_loss: 3.7345 | lr: 9.0144e-07 | scale:   128.0000 | micro time: 0.308 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1125/  5202 | global iter:   1125/  5202 | loss: 2.5239 | ds_loss: 3.3796 | lr: 9.0127e-07 | scale:   128.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1126/  5202 | global iter:   1126/  5202 | loss: 2.6070 | ds_loss: 3.3726 | lr: 9.0110e-07 | scale:   128.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1127/  5202 | global iter:   1127/  5202 | loss: 2.5202 | ds_loss: 3.3856 | lr: 9.0093e-07 | scale:   128.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1128/  5202 | global iter:   1128/  5202 | loss: 2.9116 | ds_loss: 3.6634 | lr: 9.0076e-07 | scale:   128.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1128/  5202 | global iter:   1128/  5202 | loss: 2.6407 | ds_loss: 3.4503 | lr: 9.0076e-07 | scale:   128.0000 | micro time: 0.310 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1129/  5202 | global iter:   1129/  5202 | loss: 3.0573 | ds_loss: 3.8446 | lr: 9.0059e-07 | scale:   128.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1130/  5202 | global iter:   1130/  5202 | loss: 2.8797 | ds_loss: 3.6462 | lr: 9.0042e-07 | scale:   128.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1131/  5202 | global iter:   1131/  5202 | loss: 2.0904 | ds_loss: 3.1845 | lr: 9.0025e-07 | scale:   128.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1132/  5202 | global iter:   1132/  5202 | loss: 2.9317 | ds_loss: 3.8017 | lr: 9.0007e-07 | scale:   128.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1132/  5202 | global iter:   1132/  5202 | loss: 2.7398 | ds_loss: 3.6193 | lr: 9.0007e-07 | scale:   128.0000 | micro time: 0.310 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1133/  5202 | global iter:   1133/  5202 | loss: 1.8656 | ds_loss: 2.8010 | lr: 8.9990e-07 | scale:   128.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1134/  5202 | global iter:   1134/  5202 | loss: 2.8235 | ds_loss: 3.4069 | lr: 8.9973e-07 | scale:   128.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1135/  5202 | global iter:   1135/  5202 | loss: 2.6276 | ds_loss: 3.2314 | lr: 8.9956e-07 | scale:   128.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1136/  5202 | global iter:   1136/  5202 | loss: 1.5209 | ds_loss: 1.9721 | lr: 8.9939e-07 | scale:   128.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1136/  5202 | global iter:   1136/  5202 | loss: 2.2094 | ds_loss: 2.8529 | lr: 8.9939e-07 | scale:   128.0000 | micro time: 0.307 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1137/  5202 | global iter:   1137/  5202 | loss: 0.8860 | ds_loss: 1.3646 | lr: 8.9922e-07 | scale:   128.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1138/  5202 | global iter:   1138/  5202 | loss: 2.7628 | ds_loss: 3.6868 | lr: 8.9905e-07 | scale:   128.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1139/  5202 | global iter:   1139/  5202 | loss: 1.4398 | ds_loss: 2.2119 | lr: 8.9888e-07 | scale:   128.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1140/  5202 | global iter:   1140/  5202 | loss: 1.9463 | ds_loss: 2.9684 | lr: 8.9870e-07 | scale:   128.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1140/  5202 | global iter:   1140/  5202 | loss: 1.7587 | ds_loss: 2.5579 | lr: 8.9870e-07 | scale:   128.0000 | micro time: 0.305 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1141/  5202 | global iter:   1141/  5202 | loss: 2.8268 | ds_loss: 3.6751 | lr: 8.9853e-07 | scale:   128.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1142/  5202 | global iter:   1142/  5202 | loss: 2.5917 | ds_loss: 3.3289 | lr: 8.9836e-07 | scale:   128.0000 | micro time: 0.319 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1143/  5202 | global iter:   1143/  5202 | loss: 2.3686 | ds_loss: 3.7596 | lr: 8.9819e-07 | scale:   128.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1144/  5202 | global iter:   1144/  5202 | loss: 2.7334 | ds_loss: 3.3893 | lr: 8.9802e-07 | scale:   128.0000 | micro time: 0.314 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1144/  5202 | global iter:   1144/  5202 | loss: 2.6301 | ds_loss: 3.5382 | lr: 8.9802e-07 | scale:   128.0000 | micro time: 0.314 | step time: 0.313\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1145/  5202 | global iter:   1145/  5202 | loss: 2.7778 | ds_loss: 3.5785 | lr: 8.9784e-07 | scale:   128.0000 | micro time: 0.318 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1146/  5202 | global iter:   1146/  5202 | loss: 2.8622 | ds_loss: 3.5704 | lr: 8.9767e-07 | scale:   128.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1147/  5202 | global iter:   1147/  5202 | loss: 1.7854 | ds_loss: 3.0768 | lr: 8.9750e-07 | scale:   128.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1148/  5202 | global iter:   1148/  5202 | loss: 1.5136 | ds_loss: 2.1447 | lr: 8.9733e-07 | scale:   128.0000 | micro time: 0.312 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1148/  5202 | global iter:   1148/  5202 | loss: 2.2347 | ds_loss: 3.0926 | lr: 8.9733e-07 | scale:   128.0000 | micro time: 0.312 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1149/  5202 | global iter:   1149/  5202 | loss: 1.0072 | ds_loss: 1.6442 | lr: 8.9715e-07 | scale:   128.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1150/  5202 | global iter:   1150/  5202 | loss: 2.8218 | ds_loss: 3.5653 | lr: 8.9698e-07 | scale:   128.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1151/  5202 | global iter:   1151/  5202 | loss: 1.5071 | ds_loss: 2.1450 | lr: 8.9681e-07 | scale:   128.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1152/  5202 | global iter:   1152/  5202 | loss: 1.9362 | ds_loss: 2.9172 | lr: 8.9663e-07 | scale:   128.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1152/  5202 | global iter:   1152/  5202 | loss: 1.8181 | ds_loss: 2.5679 | lr: 8.9663e-07 | scale:   128.0000 | micro time: 0.306 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1153/  5202 | global iter:   1153/  5202 | loss: 2.9847 | ds_loss: 3.6194 | lr: 8.9646e-07 | scale:   128.0000 | micro time: 0.318 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1154/  5202 | global iter:   1154/  5202 | loss: 3.3921 | ds_loss: 4.3659 | lr: 8.9629e-07 | scale:   128.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1155/  5202 | global iter:   1155/  5202 | loss: 1.3366 | ds_loss: 2.1135 | lr: 8.9611e-07 | scale:   128.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1156/  5202 | global iter:   1156/  5202 | loss: 1.4743 | ds_loss: 2.3001 | lr: 8.9594e-07 | scale:   128.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1156/  5202 | global iter:   1156/  5202 | loss: 2.2969 | ds_loss: 3.0997 | lr: 8.9594e-07 | scale:   128.0000 | micro time: 0.303 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1157/  5202 | global iter:   1157/  5202 | loss: 2.2586 | ds_loss: 2.9231 | lr: 8.9577e-07 | scale:   128.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1158/  5202 | global iter:   1158/  5202 | loss: 3.0463 | ds_loss: 3.8882 | lr: 8.9559e-07 | scale:   128.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1159/  5202 | global iter:   1159/  5202 | loss: 2.0557 | ds_loss: 3.3285 | lr: 8.9542e-07 | scale:   128.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1160/  5202 | global iter:   1160/  5202 | loss: 1.8273 | ds_loss: 2.2261 | lr: 8.9524e-07 | scale:   128.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1160/  5202 | global iter:   1160/  5202 | loss: 2.2970 | ds_loss: 3.0915 | lr: 8.9524e-07 | scale:   128.0000 | micro time: 0.310 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1161/  5202 | global iter:   1161/  5202 | loss: 2.0785 | ds_loss: 3.1815 | lr: 8.9507e-07 | scale:   128.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1162/  5202 | global iter:   1162/  5202 | loss: 2.3664 | ds_loss: 3.2770 | lr: 8.9489e-07 | scale:   128.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1163/  5202 | global iter:   1163/  5202 | loss: 1.9589 | ds_loss: 2.6207 | lr: 8.9472e-07 | scale:   128.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1164/  5202 | global iter:   1164/  5202 | loss: 1.8215 | ds_loss: 2.8391 | lr: 8.9455e-07 | scale:   128.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1164/  5202 | global iter:   1164/  5202 | loss: 2.0563 | ds_loss: 2.9796 | lr: 8.9455e-07 | scale:   128.0000 | micro time: 0.303 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1165/  5202 | global iter:   1165/  5202 | loss: 3.3163 | ds_loss: 4.0786 | lr: 8.9437e-07 | scale:   128.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1166/  5202 | global iter:   1166/  5202 | loss: 2.8811 | ds_loss: 3.5770 | lr: 8.9420e-07 | scale:   128.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1167/  5202 | global iter:   1167/  5202 | loss: 2.6631 | ds_loss: 3.4464 | lr: 8.9402e-07 | scale:   128.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1168/  5202 | global iter:   1168/  5202 | loss: 2.5271 | ds_loss: 3.9189 | lr: 8.9384e-07 | scale:   128.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1168/  5202 | global iter:   1168/  5202 | loss: 2.8469 | ds_loss: 3.7552 | lr: 8.9384e-07 | scale:   128.0000 | micro time: 0.307 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1169/  5202 | global iter:   1169/  5202 | loss: 1.1212 | ds_loss: 1.6802 | lr: 8.9367e-07 | scale:   128.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1170/  5202 | global iter:   1170/  5202 | loss: 3.0188 | ds_loss: 4.0475 | lr: 8.9349e-07 | scale:   128.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1171/  5202 | global iter:   1171/  5202 | loss: 2.9192 | ds_loss: 3.8461 | lr: 8.9332e-07 | scale:   128.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1172/  5202 | global iter:   1172/  5202 | loss: 2.4605 | ds_loss: 3.1724 | lr: 8.9314e-07 | scale:   128.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1172/  5202 | global iter:   1172/  5202 | loss: 2.3799 | ds_loss: 3.1865 | lr: 8.9314e-07 | scale:   128.0000 | micro time: 0.307 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1173/  5202 | global iter:   1173/  5202 | loss: 2.5327 | ds_loss: 3.5029 | lr: 8.9297e-07 | scale:   128.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1174/  5202 | global iter:   1174/  5202 | loss: 2.8289 | ds_loss: 3.6992 | lr: 8.9279e-07 | scale:   128.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1175/  5202 | global iter:   1175/  5202 | loss: 1.5466 | ds_loss: 2.2820 | lr: 8.9261e-07 | scale:   128.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1176/  5202 | global iter:   1176/  5202 | loss: 2.6829 | ds_loss: 3.1769 | lr: 8.9244e-07 | scale:   128.0000 | micro time: 0.312 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1176/  5202 | global iter:   1176/  5202 | loss: 2.3978 | ds_loss: 3.1652 | lr: 8.9244e-07 | scale:   128.0000 | micro time: 0.312 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1177/  5202 | global iter:   1177/  5202 | loss: 3.0564 | ds_loss: 3.7137 | lr: 8.9226e-07 | scale:   128.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1178/  5202 | global iter:   1178/  5202 | loss: 3.0746 | ds_loss: 3.6805 | lr: 8.9209e-07 | scale:   128.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1179/  5202 | global iter:   1179/  5202 | loss: 2.7460 | ds_loss: 3.5050 | lr: 8.9191e-07 | scale:   128.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1180/  5202 | global iter:   1180/  5202 | loss: 1.0887 | ds_loss: 1.5167 | lr: 8.9173e-07 | scale:   128.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1180/  5202 | global iter:   1180/  5202 | loss: 2.4915 | ds_loss: 3.1039 | lr: 8.9173e-07 | scale:   128.0000 | micro time: 0.308 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1181/  5202 | global iter:   1181/  5202 | loss: 1.1113 | ds_loss: 1.5721 | lr: 8.9156e-07 | scale:   128.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1182/  5202 | global iter:   1182/  5202 | loss: 3.3229 | ds_loss: 3.9199 | lr: 8.9138e-07 | scale:   128.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1183/  5202 | global iter:   1183/  5202 | loss: 2.7145 | ds_loss: 3.4526 | lr: 8.9120e-07 | scale:   128.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1184/  5202 | global iter:   1184/  5202 | loss: 2.6624 | ds_loss: 4.1409 | lr: 8.9102e-07 | scale:   128.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1184/  5202 | global iter:   1184/  5202 | loss: 2.4528 | ds_loss: 3.2714 | lr: 8.9102e-07 | scale:   128.0000 | micro time: 0.305 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1185/  5202 | global iter:   1185/  5202 | loss: 2.3924 | ds_loss: 3.3598 | lr: 8.9085e-07 | scale:   128.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1186/  5202 | global iter:   1186/  5202 | loss: 3.0797 | ds_loss: 3.7693 | lr: 8.9067e-07 | scale:   128.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1187/  5202 | global iter:   1187/  5202 | loss: 2.9760 | ds_loss: 3.7412 | lr: 8.9049e-07 | scale:   128.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1188/  5202 | global iter:   1188/  5202 | loss: 2.1122 | ds_loss: 2.9135 | lr: 8.9031e-07 | scale:   128.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1188/  5202 | global iter:   1188/  5202 | loss: 2.6401 | ds_loss: 3.4459 | lr: 8.9031e-07 | scale:   128.0000 | micro time: 0.309 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1189/  5202 | global iter:   1189/  5202 | loss: 2.1244 | ds_loss: 3.3188 | lr: 8.9014e-07 | scale:   128.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1190/  5202 | global iter:   1190/  5202 | loss: 2.4981 | ds_loss: 3.1043 | lr: 8.8996e-07 | scale:   128.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1191/  5202 | global iter:   1191/  5202 | loss: 2.2593 | ds_loss: 2.6840 | lr: 8.8978e-07 | scale:   128.0000 | micro time: 0.321 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1192/  5202 | global iter:   1192/  5202 | loss: 2.1628 | ds_loss: 3.0945 | lr: 8.8960e-07 | scale:   128.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1192/  5202 | global iter:   1192/  5202 | loss: 2.2611 | ds_loss: 3.0504 | lr: 8.8960e-07 | scale:   128.0000 | micro time: 0.308 | step time: 0.312\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1193/  5202 | global iter:   1193/  5202 | loss: 2.2091 | ds_loss: 2.9688 | lr: 8.8942e-07 | scale:   128.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1194/  5202 | global iter:   1194/  5202 | loss: 2.9100 | ds_loss: 4.0074 | lr: 8.8924e-07 | scale:   128.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1195/  5202 | global iter:   1195/  5202 | loss: 2.4301 | ds_loss: 3.2894 | lr: 8.8907e-07 | scale:   128.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1196/  5202 | global iter:   1196/  5202 | loss: 2.9082 | ds_loss: 3.6012 | lr: 8.8889e-07 | scale:   128.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1196/  5202 | global iter:   1196/  5202 | loss: 2.6144 | ds_loss: 3.4667 | lr: 8.8889e-07 | scale:   128.0000 | micro time: 0.309 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1197/  5202 | global iter:   1197/  5202 | loss: 2.8439 | ds_loss: 3.6095 | lr: 8.8871e-07 | scale:   128.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1198/  5202 | global iter:   1198/  5202 | loss: 2.6096 | ds_loss: 3.3194 | lr: 8.8853e-07 | scale:   128.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1199/  5202 | global iter:   1199/  5202 | loss: 2.7772 | ds_loss: 3.6216 | lr: 8.8835e-07 | scale:   128.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1200/  5202 | global iter:   1200/  5202 | loss: 2.5337 | ds_loss: 3.2120 | lr: 8.8817e-07 | scale:   128.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1200/  5202 | global iter:   1200/  5202 | loss: 2.6911 | ds_loss: 3.4406 | lr: 8.8817e-07 | scale:   128.0000 | micro time: 0.304 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1201/  5202 | global iter:   1201/  5202 | loss: 2.7258 | ds_loss: 3.5051 | lr: 8.8799e-07 | scale:   128.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1202/  5202 | global iter:   1202/  5202 | loss: 2.5870 | ds_loss: 3.4146 | lr: 8.8781e-07 | scale:   128.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1203/  5202 | global iter:   1203/  5202 | loss: 2.6808 | ds_loss: 4.3157 | lr: 8.8763e-07 | scale:   128.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1204/  5202 | global iter:   1204/  5202 | loss: 1.9298 | ds_loss: 2.8722 | lr: 8.8745e-07 | scale:   128.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1204/  5202 | global iter:   1204/  5202 | loss: 2.4808 | ds_loss: 3.5269 | lr: 8.8745e-07 | scale:   128.0000 | micro time: 0.307 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1205/  5202 | global iter:   1205/  5202 | loss: 2.6442 | ds_loss: 3.6146 | lr: 8.8727e-07 | scale:   128.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1206/  5202 | global iter:   1206/  5202 | loss: 1.0286 | ds_loss: 1.5937 | lr: 8.8709e-07 | scale:   128.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1207/  5202 | global iter:   1207/  5202 | loss: 1.5114 | ds_loss: 2.2403 | lr: 8.8691e-07 | scale:   128.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1208/  5202 | global iter:   1208/  5202 | loss: 1.9010 | ds_loss: 2.6350 | lr: 8.8673e-07 | scale:   128.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1208/  5202 | global iter:   1208/  5202 | loss: 1.7713 | ds_loss: 2.5209 | lr: 8.8673e-07 | scale:   128.0000 | micro time: 0.306 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1209/  5202 | global iter:   1209/  5202 | loss: 2.6978 | ds_loss: 3.4402 | lr: 8.8655e-07 | scale:   128.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1210/  5202 | global iter:   1210/  5202 | loss: 2.7432 | ds_loss: 3.3743 | lr: 8.8637e-07 | scale:   128.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1211/  5202 | global iter:   1211/  5202 | loss: 3.0728 | ds_loss: 3.8820 | lr: 8.8619e-07 | scale:   128.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1212/  5202 | global iter:   1212/  5202 | loss: 2.7432 | ds_loss: 3.7610 | lr: 8.8601e-07 | scale:   128.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1212/  5202 | global iter:   1212/  5202 | loss: 2.8142 | ds_loss: 3.6144 | lr: 8.8601e-07 | scale:   128.0000 | micro time: 0.306 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1213/  5202 | global iter:   1213/  5202 | loss: 2.7699 | ds_loss: 3.4767 | lr: 8.8583e-07 | scale:   128.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1214/  5202 | global iter:   1214/  5202 | loss: 2.9997 | ds_loss: 3.7121 | lr: 8.8565e-07 | scale:   128.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1215/  5202 | global iter:   1215/  5202 | loss: 2.8841 | ds_loss: 3.9363 | lr: 8.8547e-07 | scale:   128.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1216/  5202 | global iter:   1216/  5202 | loss: 3.1830 | ds_loss: 3.7424 | lr: 8.8529e-07 | scale:   128.0000 | micro time: 0.312 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1216/  5202 | global iter:   1216/  5202 | loss: 2.9592 | ds_loss: 3.7169 | lr: 8.8529e-07 | scale:   128.0000 | micro time: 0.312 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1217/  5202 | global iter:   1217/  5202 | loss: 2.8295 | ds_loss: 3.5190 | lr: 8.8511e-07 | scale:   128.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1218/  5202 | global iter:   1218/  5202 | loss: 2.9828 | ds_loss: 4.0506 | lr: 8.8492e-07 | scale:   128.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1219/  5202 | global iter:   1219/  5202 | loss: 2.7691 | ds_loss: 3.7693 | lr: 8.8474e-07 | scale:   128.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1220/  5202 | global iter:   1220/  5202 | loss: 2.3728 | ds_loss: 3.0943 | lr: 8.8456e-07 | scale:   128.0000 | micro time: 0.313 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1220/  5202 | global iter:   1220/  5202 | loss: 2.7385 | ds_loss: 3.6083 | lr: 8.8456e-07 | scale:   128.0000 | micro time: 0.313 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1221/  5202 | global iter:   1221/  5202 | loss: 2.8218 | ds_loss: 3.9112 | lr: 8.8438e-07 | scale:   128.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1222/  5202 | global iter:   1222/  5202 | loss: 2.4042 | ds_loss: 3.0176 | lr: 8.8420e-07 | scale:   128.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1223/  5202 | global iter:   1223/  5202 | loss: 2.9232 | ds_loss: 3.5551 | lr: 8.8401e-07 | scale:   128.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1224/  5202 | global iter:   1224/  5202 | loss: 3.5626 | ds_loss: 4.7802 | lr: 8.8383e-07 | scale:   128.0000 | micro time: 0.301 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1224/  5202 | global iter:   1224/  5202 | loss: 2.9279 | ds_loss: 3.8160 | lr: 8.8383e-07 | scale:   128.0000 | micro time: 0.301 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1225/  5202 | global iter:   1225/  5202 | loss: 2.0257 | ds_loss: 2.8063 | lr: 8.8365e-07 | scale:   128.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1226/  5202 | global iter:   1226/  5202 | loss: 1.2192 | ds_loss: 1.8971 | lr: 8.8347e-07 | scale:   128.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1227/  5202 | global iter:   1227/  5202 | loss: 2.4663 | ds_loss: 3.2419 | lr: 8.8329e-07 | scale:   128.0000 | micro time: 0.318 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1228/  5202 | global iter:   1228/  5202 | loss: 1.3032 | ds_loss: 1.7288 | lr: 8.8310e-07 | scale:   128.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1228/  5202 | global iter:   1228/  5202 | loss: 1.7536 | ds_loss: 2.4185 | lr: 8.8310e-07 | scale:   128.0000 | micro time: 0.306 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1229/  5202 | global iter:   1229/  5202 | loss: 2.0410 | ds_loss: 3.3059 | lr: 8.8292e-07 | scale:   128.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1230/  5202 | global iter:   1230/  5202 | loss: 2.3846 | ds_loss: 3.2550 | lr: 8.8274e-07 | scale:   128.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1231/  5202 | global iter:   1231/  5202 | loss: 1.2494 | ds_loss: 1.7518 | lr: 8.8255e-07 | scale:   128.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1232/  5202 | global iter:   1232/  5202 | loss: 2.8175 | ds_loss: 3.6880 | lr: 8.8237e-07 | scale:   128.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1232/  5202 | global iter:   1232/  5202 | loss: 2.1231 | ds_loss: 3.0002 | lr: 8.8237e-07 | scale:   128.0000 | micro time: 0.308 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1233/  5202 | global iter:   1233/  5202 | loss: 2.2891 | ds_loss: 3.0790 | lr: 8.8219e-07 | scale:   128.0000 | micro time: 0.319 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1234/  5202 | global iter:   1234/  5202 | loss: 2.4759 | ds_loss: 3.5822 | lr: 8.8200e-07 | scale:   128.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1235/  5202 | global iter:   1235/  5202 | loss: 2.6210 | ds_loss: 3.3199 | lr: 8.8182e-07 | scale:   128.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1236/  5202 | global iter:   1236/  5202 | loss: 2.6421 | ds_loss: 3.4814 | lr: 8.8164e-07 | scale:   128.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1236/  5202 | global iter:   1236/  5202 | loss: 2.5071 | ds_loss: 3.3656 | lr: 8.8164e-07 | scale:   128.0000 | micro time: 0.310 | step time: 0.312\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1237/  5202 | global iter:   1237/  5202 | loss: 2.2402 | ds_loss: 3.2677 | lr: 8.8145e-07 | scale:   128.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1238/  5202 | global iter:   1238/  5202 | loss: 2.4403 | ds_loss: 3.3165 | lr: 8.8127e-07 | scale:   128.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1239/  5202 | global iter:   1239/  5202 | loss: 2.3948 | ds_loss: 3.0040 | lr: 8.8109e-07 | scale:   128.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1240/  5202 | global iter:   1240/  5202 | loss: 2.6931 | ds_loss: 3.5046 | lr: 8.8090e-07 | scale:   128.0000 | micro time: 0.314 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1240/  5202 | global iter:   1240/  5202 | loss: 2.4421 | ds_loss: 3.2732 | lr: 8.8090e-07 | scale:   128.0000 | micro time: 0.314 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1241/  5202 | global iter:   1241/  5202 | loss: 2.9455 | ds_loss: 3.6723 | lr: 8.8072e-07 | scale:   128.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1242/  5202 | global iter:   1242/  5202 | loss: 2.6676 | ds_loss: 3.5250 | lr: 8.8053e-07 | scale:   128.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1243/  5202 | global iter:   1243/  5202 | loss: 2.3110 | ds_loss: 3.1067 | lr: 8.8035e-07 | scale:   128.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1244/  5202 | global iter:   1244/  5202 | loss: 2.6650 | ds_loss: 3.2729 | lr: 8.8016e-07 | scale:   128.0000 | micro time: 0.313 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1244/  5202 | global iter:   1244/  5202 | loss: 2.6473 | ds_loss: 3.3942 | lr: 8.8016e-07 | scale:   128.0000 | micro time: 0.313 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1245/  5202 | global iter:   1245/  5202 | loss: 2.4616 | ds_loss: 4.1918 | lr: 8.7998e-07 | scale:   128.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1246/  5202 | global iter:   1246/  5202 | loss: 3.0005 | ds_loss: 3.6778 | lr: 8.7979e-07 | scale:   128.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1247/  5202 | global iter:   1247/  5202 | loss: 2.9905 | ds_loss: 4.1766 | lr: 8.7961e-07 | scale:   128.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1248/  5202 | global iter:   1248/  5202 | loss: 2.9717 | ds_loss: 3.7438 | lr: 8.7942e-07 | scale:   128.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1248/  5202 | global iter:   1248/  5202 | loss: 2.8561 | ds_loss: 3.9475 | lr: 8.7942e-07 | scale:   128.0000 | micro time: 0.311 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1249/  5202 | global iter:   1249/  5202 | loss: 2.3298 | ds_loss: 3.2979 | lr: 8.7924e-07 | scale:   128.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1250/  5202 | global iter:   1250/  5202 | loss: 2.8259 | ds_loss: 3.6076 | lr: 8.7905e-07 | scale:   128.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1251/  5202 | global iter:   1251/  5202 | loss: 2.8203 | ds_loss: 3.6046 | lr: 8.7887e-07 | scale:   128.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1252/  5202 | global iter:   1252/  5202 | loss: 2.1591 | ds_loss: 2.8678 | lr: 8.7868e-07 | scale:   128.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1252/  5202 | global iter:   1252/  5202 | loss: 2.5338 | ds_loss: 3.3445 | lr: 8.7868e-07 | scale:   128.0000 | micro time: 0.310 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1253/  5202 | global iter:   1253/  5202 | loss: 2.3794 | ds_loss: 3.2906 | lr: 8.7850e-07 | scale:   128.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1254/  5202 | global iter:   1254/  5202 | loss: 2.5977 | ds_loss: 3.3438 | lr: 8.7831e-07 | scale:   128.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1255/  5202 | global iter:   1255/  5202 | loss: 2.2293 | ds_loss: 3.3952 | lr: 8.7813e-07 | scale:   128.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1256/  5202 | global iter:   1256/  5202 | loss: 2.6027 | ds_loss: 3.3580 | lr: 8.7794e-07 | scale:   128.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1256/  5202 | global iter:   1256/  5202 | loss: 2.4523 | ds_loss: 3.3469 | lr: 8.7794e-07 | scale:   128.0000 | micro time: 0.311 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1257/  5202 | global iter:   1257/  5202 | loss: 2.6641 | ds_loss: 3.3336 | lr: 8.7775e-07 | scale:   128.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1258/  5202 | global iter:   1258/  5202 | loss: 1.6832 | ds_loss: 2.1373 | lr: 8.7757e-07 | scale:   128.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1259/  5202 | global iter:   1259/  5202 | loss: 2.8606 | ds_loss: 3.3934 | lr: 8.7738e-07 | scale:   128.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1260/  5202 | global iter:   1260/  5202 | loss: 2.6555 | ds_loss: 3.5682 | lr: 8.7719e-07 | scale:   128.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1260/  5202 | global iter:   1260/  5202 | loss: 2.4658 | ds_loss: 3.1081 | lr: 8.7719e-07 | scale:   128.0000 | micro time: 0.307 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1261/  5202 | global iter:   1261/  5202 | loss: 2.8021 | ds_loss: 3.4177 | lr: 8.7701e-07 | scale:   128.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1262/  5202 | global iter:   1262/  5202 | loss: 3.0510 | ds_loss: 3.7492 | lr: 8.7682e-07 | scale:   128.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1263/  5202 | global iter:   1263/  5202 | loss: 1.5878 | ds_loss: 2.6257 | lr: 8.7663e-07 | scale:   128.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1264/  5202 | global iter:   1264/  5202 | loss: 3.2840 | ds_loss: 3.8962 | lr: 8.7645e-07 | scale:   128.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1264/  5202 | global iter:   1264/  5202 | loss: 2.6812 | ds_loss: 3.4222 | lr: 8.7645e-07 | scale:   128.0000 | micro time: 0.307 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1265/  5202 | global iter:   1265/  5202 | loss: 2.5689 | ds_loss: 3.0842 | lr: 8.7626e-07 | scale:   128.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1266/  5202 | global iter:   1266/  5202 | loss: 3.0221 | ds_loss: 3.9829 | lr: 8.7607e-07 | scale:   128.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1267/  5202 | global iter:   1267/  5202 | loss: 2.9109 | ds_loss: 3.5692 | lr: 8.7588e-07 | scale:   128.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1268/  5202 | global iter:   1268/  5202 | loss: 2.8185 | ds_loss: 3.8045 | lr: 8.7570e-07 | scale:   128.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1268/  5202 | global iter:   1268/  5202 | loss: 2.8301 | ds_loss: 3.6102 | lr: 8.7570e-07 | scale:   128.0000 | micro time: 0.308 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1269/  5202 | global iter:   1269/  5202 | loss: 3.0772 | ds_loss: 3.9676 | lr: 8.7551e-07 | scale:   128.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1270/  5202 | global iter:   1270/  5202 | loss: 2.7462 | ds_loss: 3.5467 | lr: 8.7532e-07 | scale:   128.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1271/  5202 | global iter:   1271/  5202 | loss: 2.1512 | ds_loss: 3.0132 | lr: 8.7513e-07 | scale:   128.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1272/  5202 | global iter:   1272/  5202 | loss: 1.6659 | ds_loss: 2.7109 | lr: 8.7495e-07 | scale:   128.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1272/  5202 | global iter:   1272/  5202 | loss: 2.4101 | ds_loss: 3.3096 | lr: 8.7495e-07 | scale:   128.0000 | micro time: 0.308 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1273/  5202 | global iter:   1273/  5202 | loss: 2.5234 | ds_loss: 3.2018 | lr: 8.7476e-07 | scale:   128.0000 | micro time: 0.318 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1274/  5202 | global iter:   1274/  5202 | loss: 2.4470 | ds_loss: 3.1253 | lr: 8.7457e-07 | scale:   128.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1275/  5202 | global iter:   1275/  5202 | loss: 2.4687 | ds_loss: 3.2920 | lr: 8.7438e-07 | scale:   128.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1276/  5202 | global iter:   1276/  5202 | loss: 2.9166 | ds_loss: 4.4130 | lr: 8.7419e-07 | scale:   128.0000 | micro time: 0.312 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1276/  5202 | global iter:   1276/  5202 | loss: 2.5889 | ds_loss: 3.5080 | lr: 8.7419e-07 | scale:   128.0000 | micro time: 0.312 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1277/  5202 | global iter:   1277/  5202 | loss: 3.0147 | ds_loss: 3.6093 | lr: 8.7400e-07 | scale:   128.0000 | micro time: 0.318 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1278/  5202 | global iter:   1278/  5202 | loss: 2.4938 | ds_loss: 3.2990 | lr: 8.7382e-07 | scale:   128.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1279/  5202 | global iter:   1279/  5202 | loss: 2.5357 | ds_loss: 3.4882 | lr: 8.7363e-07 | scale:   128.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1280/  5202 | global iter:   1280/  5202 | loss: 3.1437 | ds_loss: 3.8846 | lr: 8.7344e-07 | scale:   128.0000 | micro time: 0.314 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1280/  5202 | global iter:   1280/  5202 | loss: 2.7970 | ds_loss: 3.5703 | lr: 8.7344e-07 | scale:   128.0000 | micro time: 0.314 | step time: 0.312\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1281/  5202 | global iter:   1281/  5202 | loss: 2.5124 | ds_loss: 3.2654 | lr: 8.7325e-07 | scale:   128.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1282/  5202 | global iter:   1282/  5202 | loss: 3.2792 | ds_loss: 3.9805 | lr: 8.7306e-07 | scale:   128.0000 | micro time: 0.312 | step time: 0.000\n",
            "[2025-04-16 21:09:03,729] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 128, reducing to 64\n",
            "train | epoch   0 | Iter:   1283/  5202 | global iter:   1283/  5202 | loss: 2.6996 | ds_loss: 3.4072 | lr: 8.7306e-07 | scale:    64.0000 | micro time: 0.240 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1284/  5202 | global iter:   1284/  5202 | loss: 2.3590 | ds_loss: 3.7734 | lr: 8.7287e-07 | scale:    64.0000 | micro time: 0.302 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1284/  5202 | global iter:   1284/  5202 | loss: 2.7125 | ds_loss: 3.6066 | lr: 8.7287e-07 | scale:    64.0000 | micro time: 0.302 | step time: 0.289\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1285/  5202 | global iter:   1285/  5202 | loss: 3.1220 | ds_loss: 3.6671 | lr: 8.7268e-07 | scale:    64.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1286/  5202 | global iter:   1286/  5202 | loss: 2.1506 | ds_loss: 2.8752 | lr: 8.7249e-07 | scale:    64.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1287/  5202 | global iter:   1287/  5202 | loss: 2.6679 | ds_loss: 3.5948 | lr: 8.7230e-07 | scale:    64.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1288/  5202 | global iter:   1288/  5202 | loss: 1.5203 | ds_loss: 2.2301 | lr: 8.7211e-07 | scale:    64.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1288/  5202 | global iter:   1288/  5202 | loss: 2.3652 | ds_loss: 3.0918 | lr: 8.7211e-07 | scale:    64.0000 | micro time: 0.305 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1289/  5202 | global iter:   1289/  5202 | loss: 2.4617 | ds_loss: 3.5405 | lr: 8.7192e-07 | scale:    64.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1290/  5202 | global iter:   1290/  5202 | loss: 1.0471 | ds_loss: 1.6562 | lr: 8.7173e-07 | scale:    64.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1291/  5202 | global iter:   1291/  5202 | loss: 2.8518 | ds_loss: 3.4520 | lr: 8.7154e-07 | scale:    64.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1292/  5202 | global iter:   1292/  5202 | loss: 2.9640 | ds_loss: 3.6705 | lr: 8.7135e-07 | scale:    64.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1292/  5202 | global iter:   1292/  5202 | loss: 2.3312 | ds_loss: 3.0798 | lr: 8.7135e-07 | scale:    64.0000 | micro time: 0.310 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1293/  5202 | global iter:   1293/  5202 | loss: 2.8108 | ds_loss: 3.5409 | lr: 8.7116e-07 | scale:    64.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1294/  5202 | global iter:   1294/  5202 | loss: 2.5985 | ds_loss: 3.6191 | lr: 8.7097e-07 | scale:    64.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1295/  5202 | global iter:   1295/  5202 | loss: 2.7398 | ds_loss: 3.6679 | lr: 8.7078e-07 | scale:    64.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1296/  5202 | global iter:   1296/  5202 | loss: 3.1908 | ds_loss: 4.0263 | lr: 8.7059e-07 | scale:    64.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1296/  5202 | global iter:   1296/  5202 | loss: 2.8350 | ds_loss: 3.7135 | lr: 8.7059e-07 | scale:    64.0000 | micro time: 0.311 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1297/  5202 | global iter:   1297/  5202 | loss: 2.9144 | ds_loss: 3.7845 | lr: 8.7040e-07 | scale:    64.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1298/  5202 | global iter:   1298/  5202 | loss: 2.8374 | ds_loss: 3.4805 | lr: 8.7021e-07 | scale:    64.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1299/  5202 | global iter:   1299/  5202 | loss: 2.2145 | ds_loss: 3.3045 | lr: 8.7002e-07 | scale:    64.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1300/  5202 | global iter:   1300/  5202 | loss: 2.3930 | ds_loss: 3.1331 | lr: 8.6983e-07 | scale:    64.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1300/  5202 | global iter:   1300/  5202 | loss: 2.5898 | ds_loss: 3.4257 | lr: 8.6983e-07 | scale:    64.0000 | micro time: 0.307 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1301/  5202 | global iter:   1301/  5202 | loss: 2.9337 | ds_loss: 3.9610 | lr: 8.6964e-07 | scale:    64.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1302/  5202 | global iter:   1302/  5202 | loss: 2.1918 | ds_loss: 2.8465 | lr: 8.6944e-07 | scale:    64.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1303/  5202 | global iter:   1303/  5202 | loss: 2.1655 | ds_loss: 2.7149 | lr: 8.6925e-07 | scale:    64.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1304/  5202 | global iter:   1304/  5202 | loss: 2.5929 | ds_loss: 4.0566 | lr: 8.6906e-07 | scale:    64.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1304/  5202 | global iter:   1304/  5202 | loss: 2.4710 | ds_loss: 3.3947 | lr: 8.6906e-07 | scale:    64.0000 | micro time: 0.303 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1305/  5202 | global iter:   1305/  5202 | loss: 2.4625 | ds_loss: 3.6320 | lr: 8.6887e-07 | scale:    64.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1306/  5202 | global iter:   1306/  5202 | loss: 3.7769 | ds_loss: 4.4360 | lr: 8.6868e-07 | scale:    64.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1307/  5202 | global iter:   1307/  5202 | loss: 3.2202 | ds_loss: 3.9325 | lr: 8.6849e-07 | scale:    64.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1308/  5202 | global iter:   1308/  5202 | loss: 3.0008 | ds_loss: 4.3979 | lr: 8.6829e-07 | scale:    64.0000 | micro time: 0.302 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1308/  5202 | global iter:   1308/  5202 | loss: 3.1151 | ds_loss: 4.0996 | lr: 8.6829e-07 | scale:    64.0000 | micro time: 0.302 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1309/  5202 | global iter:   1309/  5202 | loss: 3.3547 | ds_loss: 3.8785 | lr: 8.6810e-07 | scale:    64.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1310/  5202 | global iter:   1310/  5202 | loss: 3.0487 | ds_loss: 3.8754 | lr: 8.6791e-07 | scale:    64.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1311/  5202 | global iter:   1311/  5202 | loss: 2.6737 | ds_loss: 3.7279 | lr: 8.6772e-07 | scale:    64.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1312/  5202 | global iter:   1312/  5202 | loss: 2.9200 | ds_loss: 4.3896 | lr: 8.6752e-07 | scale:    64.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1312/  5202 | global iter:   1312/  5202 | loss: 2.9993 | ds_loss: 3.9679 | lr: 8.6752e-07 | scale:    64.0000 | micro time: 0.310 | step time: 0.312\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1313/  5202 | global iter:   1313/  5202 | loss: 2.6919 | ds_loss: 3.4727 | lr: 8.6733e-07 | scale:    64.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1314/  5202 | global iter:   1314/  5202 | loss: 2.8587 | ds_loss: 3.5739 | lr: 8.6714e-07 | scale:    64.0000 | micro time: 0.312 | step time: 0.000\n",
            "[2025-04-16 21:09:13,692] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 64, reducing to 32\n",
            "train | epoch   0 | Iter:   1315/  5202 | global iter:   1315/  5202 | loss: 2.4994 | ds_loss: 3.5681 | lr: 8.6714e-07 | scale:    32.0000 | micro time: 0.240 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1316/  5202 | global iter:   1316/  5202 | loss: 2.8017 | ds_loss: 3.8246 | lr: 8.6695e-07 | scale:    32.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1316/  5202 | global iter:   1316/  5202 | loss: 2.7129 | ds_loss: 3.6098 | lr: 8.6695e-07 | scale:    32.0000 | micro time: 0.306 | step time: 0.292\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1317/  5202 | global iter:   1317/  5202 | loss: 2.8430 | ds_loss: 3.7778 | lr: 8.6675e-07 | scale:    32.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1318/  5202 | global iter:   1318/  5202 | loss: 3.1044 | ds_loss: 3.6674 | lr: 8.6656e-07 | scale:    32.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1319/  5202 | global iter:   1319/  5202 | loss: 2.5762 | ds_loss: 3.1942 | lr: 8.6637e-07 | scale:    32.0000 | micro time: 0.317 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1320/  5202 | global iter:   1320/  5202 | loss: 1.4213 | ds_loss: 2.4193 | lr: 8.6617e-07 | scale:    32.0000 | micro time: 0.313 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1320/  5202 | global iter:   1320/  5202 | loss: 2.4862 | ds_loss: 3.2647 | lr: 8.6617e-07 | scale:    32.0000 | micro time: 0.313 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1321/  5202 | global iter:   1321/  5202 | loss: 3.1823 | ds_loss: 4.2961 | lr: 8.6598e-07 | scale:    32.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1322/  5202 | global iter:   1322/  5202 | loss: 3.0090 | ds_loss: 3.9786 | lr: 8.6579e-07 | scale:    32.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1323/  5202 | global iter:   1323/  5202 | loss: 2.5838 | ds_loss: 3.3513 | lr: 8.6559e-07 | scale:    32.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1324/  5202 | global iter:   1324/  5202 | loss: 2.8182 | ds_loss: 3.4768 | lr: 8.6540e-07 | scale:    32.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1324/  5202 | global iter:   1324/  5202 | loss: 2.8983 | ds_loss: 3.7757 | lr: 8.6540e-07 | scale:    32.0000 | micro time: 0.309 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1325/  5202 | global iter:   1325/  5202 | loss: 1.7677 | ds_loss: 2.5428 | lr: 8.6521e-07 | scale:    32.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1326/  5202 | global iter:   1326/  5202 | loss: 2.8175 | ds_loss: 3.5313 | lr: 8.6501e-07 | scale:    32.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1327/  5202 | global iter:   1327/  5202 | loss: 2.7311 | ds_loss: 3.3200 | lr: 8.6482e-07 | scale:    32.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1328/  5202 | global iter:   1328/  5202 | loss: 2.4189 | ds_loss: 3.4072 | lr: 8.6462e-07 | scale:    32.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1328/  5202 | global iter:   1328/  5202 | loss: 2.4338 | ds_loss: 3.2003 | lr: 8.6462e-07 | scale:    32.0000 | micro time: 0.304 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1329/  5202 | global iter:   1329/  5202 | loss: 3.0209 | ds_loss: 3.8108 | lr: 8.6443e-07 | scale:    32.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1330/  5202 | global iter:   1330/  5202 | loss: 2.0393 | ds_loss: 2.8432 | lr: 8.6423e-07 | scale:    32.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1331/  5202 | global iter:   1331/  5202 | loss: 2.7278 | ds_loss: 3.9706 | lr: 8.6404e-07 | scale:    32.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1332/  5202 | global iter:   1332/  5202 | loss: 1.2291 | ds_loss: 1.7774 | lr: 8.6385e-07 | scale:    32.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1332/  5202 | global iter:   1332/  5202 | loss: 2.2543 | ds_loss: 3.1005 | lr: 8.6385e-07 | scale:    32.0000 | micro time: 0.307 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1333/  5202 | global iter:   1333/  5202 | loss: 2.8085 | ds_loss: 3.4713 | lr: 8.6365e-07 | scale:    32.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1334/  5202 | global iter:   1334/  5202 | loss: 2.9662 | ds_loss: 3.6290 | lr: 8.6346e-07 | scale:    32.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1335/  5202 | global iter:   1335/  5202 | loss: 2.3258 | ds_loss: 3.6689 | lr: 8.6326e-07 | scale:    32.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1336/  5202 | global iter:   1336/  5202 | loss: 3.1952 | ds_loss: 3.7498 | lr: 8.6307e-07 | scale:    32.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1336/  5202 | global iter:   1336/  5202 | loss: 2.8239 | ds_loss: 3.6298 | lr: 8.6307e-07 | scale:    32.0000 | micro time: 0.310 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1337/  5202 | global iter:   1337/  5202 | loss: 1.6443 | ds_loss: 2.5546 | lr: 8.6287e-07 | scale:    32.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1338/  5202 | global iter:   1338/  5202 | loss: 2.5561 | ds_loss: 3.3598 | lr: 8.6267e-07 | scale:    32.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1339/  5202 | global iter:   1339/  5202 | loss: 2.2660 | ds_loss: 3.2914 | lr: 8.6248e-07 | scale:    32.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1340/  5202 | global iter:   1340/  5202 | loss: 2.9166 | ds_loss: 3.7193 | lr: 8.6228e-07 | scale:    32.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1340/  5202 | global iter:   1340/  5202 | loss: 2.3458 | ds_loss: 3.2313 | lr: 8.6228e-07 | scale:    32.0000 | micro time: 0.311 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1341/  5202 | global iter:   1341/  5202 | loss: 2.7432 | ds_loss: 3.8212 | lr: 8.6209e-07 | scale:    32.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1342/  5202 | global iter:   1342/  5202 | loss: 2.4335 | ds_loss: 3.1907 | lr: 8.6189e-07 | scale:    32.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1343/  5202 | global iter:   1343/  5202 | loss: 2.1722 | ds_loss: 3.4124 | lr: 8.6170e-07 | scale:    32.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1344/  5202 | global iter:   1344/  5202 | loss: 2.9268 | ds_loss: 3.6802 | lr: 8.6150e-07 | scale:    32.0000 | micro time: 0.312 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1344/  5202 | global iter:   1344/  5202 | loss: 2.5689 | ds_loss: 3.5261 | lr: 8.6150e-07 | scale:    32.0000 | micro time: 0.312 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1345/  5202 | global iter:   1345/  5202 | loss: 3.0779 | ds_loss: 3.7270 | lr: 8.6130e-07 | scale:    32.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1346/  5202 | global iter:   1346/  5202 | loss: 2.3624 | ds_loss: 3.2522 | lr: 8.6111e-07 | scale:    32.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1347/  5202 | global iter:   1347/  5202 | loss: 3.1776 | ds_loss: 3.8814 | lr: 8.6091e-07 | scale:    32.0000 | micro time: 0.317 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1348/  5202 | global iter:   1348/  5202 | loss: 1.1534 | ds_loss: 1.5739 | lr: 8.6071e-07 | scale:    32.0000 | micro time: 0.315 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1348/  5202 | global iter:   1348/  5202 | loss: 2.4428 | ds_loss: 3.1086 | lr: 8.6071e-07 | scale:    32.0000 | micro time: 0.315 | step time: 0.313\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1349/  5202 | global iter:   1349/  5202 | loss: 2.4885 | ds_loss: 3.0512 | lr: 8.6052e-07 | scale:    32.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1350/  5202 | global iter:   1350/  5202 | loss: 3.1344 | ds_loss: 3.8531 | lr: 8.6032e-07 | scale:    32.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1351/  5202 | global iter:   1351/  5202 | loss: 2.4907 | ds_loss: 3.2988 | lr: 8.6012e-07 | scale:    32.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1352/  5202 | global iter:   1352/  5202 | loss: 0.9555 | ds_loss: 1.3700 | lr: 8.5993e-07 | scale:    32.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1352/  5202 | global iter:   1352/  5202 | loss: 2.2673 | ds_loss: 2.8933 | lr: 8.5993e-07 | scale:    32.0000 | micro time: 0.304 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1353/  5202 | global iter:   1353/  5202 | loss: 1.9721 | ds_loss: 2.8807 | lr: 8.5973e-07 | scale:    32.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1354/  5202 | global iter:   1354/  5202 | loss: 2.8451 | ds_loss: 3.6260 | lr: 8.5953e-07 | scale:    32.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1355/  5202 | global iter:   1355/  5202 | loss: 3.0091 | ds_loss: 3.8278 | lr: 8.5934e-07 | scale:    32.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1356/  5202 | global iter:   1356/  5202 | loss: 2.7172 | ds_loss: 3.5361 | lr: 8.5914e-07 | scale:    32.0000 | micro time: 0.315 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1356/  5202 | global iter:   1356/  5202 | loss: 2.6359 | ds_loss: 3.4677 | lr: 8.5914e-07 | scale:    32.0000 | micro time: 0.315 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1357/  5202 | global iter:   1357/  5202 | loss: 2.4266 | ds_loss: 3.2378 | lr: 8.5894e-07 | scale:    32.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1358/  5202 | global iter:   1358/  5202 | loss: 3.0611 | ds_loss: 3.9857 | lr: 8.5874e-07 | scale:    32.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1359/  5202 | global iter:   1359/  5202 | loss: 2.9901 | ds_loss: 3.7814 | lr: 8.5855e-07 | scale:    32.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1360/  5202 | global iter:   1360/  5202 | loss: 2.4310 | ds_loss: 3.3120 | lr: 8.5835e-07 | scale:    32.0000 | micro time: 0.312 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1360/  5202 | global iter:   1360/  5202 | loss: 2.7272 | ds_loss: 3.5792 | lr: 8.5835e-07 | scale:    32.0000 | micro time: 0.312 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1361/  5202 | global iter:   1361/  5202 | loss: 1.4324 | ds_loss: 2.5369 | lr: 8.5815e-07 | scale:    32.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1362/  5202 | global iter:   1362/  5202 | loss: 2.0345 | ds_loss: 2.6538 | lr: 8.5795e-07 | scale:    32.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1363/  5202 | global iter:   1363/  5202 | loss: 1.9459 | ds_loss: 3.1204 | lr: 8.5775e-07 | scale:    32.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1364/  5202 | global iter:   1364/  5202 | loss: 1.6168 | ds_loss: 2.0955 | lr: 8.5755e-07 | scale:    32.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1364/  5202 | global iter:   1364/  5202 | loss: 1.7574 | ds_loss: 2.6016 | lr: 8.5755e-07 | scale:    32.0000 | micro time: 0.311 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1365/  5202 | global iter:   1365/  5202 | loss: 2.8955 | ds_loss: 3.7513 | lr: 8.5736e-07 | scale:    32.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1366/  5202 | global iter:   1366/  5202 | loss: 2.7382 | ds_loss: 3.4343 | lr: 8.5716e-07 | scale:    32.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1367/  5202 | global iter:   1367/  5202 | loss: 1.8265 | ds_loss: 2.8442 | lr: 8.5696e-07 | scale:    32.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1368/  5202 | global iter:   1368/  5202 | loss: 2.9210 | ds_loss: 3.7208 | lr: 8.5676e-07 | scale:    32.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1368/  5202 | global iter:   1368/  5202 | loss: 2.5953 | ds_loss: 3.4377 | lr: 8.5676e-07 | scale:    32.0000 | micro time: 0.303 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1369/  5202 | global iter:   1369/  5202 | loss: 2.6581 | ds_loss: 3.5848 | lr: 8.5656e-07 | scale:    32.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1370/  5202 | global iter:   1370/  5202 | loss: 1.7161 | ds_loss: 2.7408 | lr: 8.5636e-07 | scale:    32.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1371/  5202 | global iter:   1371/  5202 | loss: 2.8782 | ds_loss: 3.9046 | lr: 8.5616e-07 | scale:    32.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1372/  5202 | global iter:   1372/  5202 | loss: 2.8450 | ds_loss: 3.6286 | lr: 8.5596e-07 | scale:    32.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1372/  5202 | global iter:   1372/  5202 | loss: 2.5243 | ds_loss: 3.4647 | lr: 8.5596e-07 | scale:    32.0000 | micro time: 0.310 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1373/  5202 | global iter:   1373/  5202 | loss: 1.6084 | ds_loss: 2.3414 | lr: 8.5576e-07 | scale:    32.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1374/  5202 | global iter:   1374/  5202 | loss: 2.5813 | ds_loss: 3.8015 | lr: 8.5557e-07 | scale:    32.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1375/  5202 | global iter:   1375/  5202 | loss: 2.8401 | ds_loss: 3.5122 | lr: 8.5537e-07 | scale:    32.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1376/  5202 | global iter:   1376/  5202 | loss: 2.5245 | ds_loss: 3.7974 | lr: 8.5517e-07 | scale:    32.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1376/  5202 | global iter:   1376/  5202 | loss: 2.3886 | ds_loss: 3.3631 | lr: 8.5517e-07 | scale:    32.0000 | micro time: 0.304 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1377/  5202 | global iter:   1377/  5202 | loss: 2.8510 | ds_loss: 3.6806 | lr: 8.5497e-07 | scale:    32.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1378/  5202 | global iter:   1378/  5202 | loss: 2.9110 | ds_loss: 3.7438 | lr: 8.5477e-07 | scale:    32.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1379/  5202 | global iter:   1379/  5202 | loss: 2.3237 | ds_loss: 2.9782 | lr: 8.5457e-07 | scale:    32.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1380/  5202 | global iter:   1380/  5202 | loss: 2.6037 | ds_loss: 4.3173 | lr: 8.5437e-07 | scale:    32.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1380/  5202 | global iter:   1380/  5202 | loss: 2.6724 | ds_loss: 3.6800 | lr: 8.5437e-07 | scale:    32.0000 | micro time: 0.303 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1381/  5202 | global iter:   1381/  5202 | loss: 3.2587 | ds_loss: 3.7653 | lr: 8.5417e-07 | scale:    32.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1382/  5202 | global iter:   1382/  5202 | loss: 3.0003 | ds_loss: 3.6330 | lr: 8.5397e-07 | scale:    32.0000 | micro time: 0.316 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1383/  5202 | global iter:   1383/  5202 | loss: 2.9851 | ds_loss: 4.0976 | lr: 8.5377e-07 | scale:    32.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1384/  5202 | global iter:   1384/  5202 | loss: 2.6201 | ds_loss: 3.3114 | lr: 8.5356e-07 | scale:    32.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1384/  5202 | global iter:   1384/  5202 | loss: 2.9661 | ds_loss: 3.7018 | lr: 8.5356e-07 | scale:    32.0000 | micro time: 0.311 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1385/  5202 | global iter:   1385/  5202 | loss: 1.9351 | ds_loss: 3.3474 | lr: 8.5336e-07 | scale:    32.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1386/  5202 | global iter:   1386/  5202 | loss: 2.9418 | ds_loss: 3.5816 | lr: 8.5316e-07 | scale:    32.0000 | micro time: 0.317 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1387/  5202 | global iter:   1387/  5202 | loss: 2.6971 | ds_loss: 3.5283 | lr: 8.5296e-07 | scale:    32.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1388/  5202 | global iter:   1388/  5202 | loss: 2.5264 | ds_loss: 3.1586 | lr: 8.5276e-07 | scale:    32.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1388/  5202 | global iter:   1388/  5202 | loss: 2.5251 | ds_loss: 3.4040 | lr: 8.5276e-07 | scale:    32.0000 | micro time: 0.308 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1389/  5202 | global iter:   1389/  5202 | loss: 2.1250 | ds_loss: 3.2616 | lr: 8.5256e-07 | scale:    32.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1390/  5202 | global iter:   1390/  5202 | loss: 2.6973 | ds_loss: 3.3399 | lr: 8.5236e-07 | scale:    32.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1391/  5202 | global iter:   1391/  5202 | loss: 2.7878 | ds_loss: 3.7596 | lr: 8.5216e-07 | scale:    32.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1392/  5202 | global iter:   1392/  5202 | loss: 2.8328 | ds_loss: 3.6352 | lr: 8.5196e-07 | scale:    32.0000 | micro time: 0.314 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1392/  5202 | global iter:   1392/  5202 | loss: 2.6107 | ds_loss: 3.4991 | lr: 8.5196e-07 | scale:    32.0000 | micro time: 0.314 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1393/  5202 | global iter:   1393/  5202 | loss: 1.6755 | ds_loss: 2.2553 | lr: 8.5175e-07 | scale:    32.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1394/  5202 | global iter:   1394/  5202 | loss: 2.5948 | ds_loss: 3.3879 | lr: 8.5155e-07 | scale:    32.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1395/  5202 | global iter:   1395/  5202 | loss: 3.2078 | ds_loss: 4.0768 | lr: 8.5135e-07 | scale:    32.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1396/  5202 | global iter:   1396/  5202 | loss: 3.2940 | ds_loss: 4.1861 | lr: 8.5115e-07 | scale:    32.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1396/  5202 | global iter:   1396/  5202 | loss: 2.6930 | ds_loss: 3.4765 | lr: 8.5115e-07 | scale:    32.0000 | micro time: 0.310 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1397/  5202 | global iter:   1397/  5202 | loss: 2.9338 | ds_loss: 3.7311 | lr: 8.5095e-07 | scale:    32.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1398/  5202 | global iter:   1398/  5202 | loss: 3.2473 | ds_loss: 4.0993 | lr: 8.5075e-07 | scale:    32.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1399/  5202 | global iter:   1399/  5202 | loss: 2.8493 | ds_loss: 3.4956 | lr: 8.5054e-07 | scale:    32.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1400/  5202 | global iter:   1400/  5202 | loss: 2.4144 | ds_loss: 2.9698 | lr: 8.5034e-07 | scale:    32.0000 | micro time: 0.317 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1400/  5202 | global iter:   1400/  5202 | loss: 2.8612 | ds_loss: 3.5739 | lr: 8.5034e-07 | scale:    32.0000 | micro time: 0.317 | step time: 0.313\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1401/  5202 | global iter:   1401/  5202 | loss: 2.6361 | ds_loss: 3.3416 | lr: 8.5014e-07 | scale:    32.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1402/  5202 | global iter:   1402/  5202 | loss: 3.1188 | ds_loss: 3.9039 | lr: 8.4994e-07 | scale:    32.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1403/  5202 | global iter:   1403/  5202 | loss: 2.8961 | ds_loss: 3.6944 | lr: 8.4973e-07 | scale:    32.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1404/  5202 | global iter:   1404/  5202 | loss: 2.7351 | ds_loss: 3.6522 | lr: 8.4953e-07 | scale:    32.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1404/  5202 | global iter:   1404/  5202 | loss: 2.8466 | ds_loss: 3.6480 | lr: 8.4953e-07 | scale:    32.0000 | micro time: 0.307 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1405/  5202 | global iter:   1405/  5202 | loss: 2.7317 | ds_loss: 3.9131 | lr: 8.4933e-07 | scale:    32.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1406/  5202 | global iter:   1406/  5202 | loss: 3.0890 | ds_loss: 3.9416 | lr: 8.4912e-07 | scale:    32.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1407/  5202 | global iter:   1407/  5202 | loss: 2.7750 | ds_loss: 3.3920 | lr: 8.4892e-07 | scale:    32.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1408/  5202 | global iter:   1408/  5202 | loss: 2.5343 | ds_loss: 3.4260 | lr: 8.4872e-07 | scale:    32.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1408/  5202 | global iter:   1408/  5202 | loss: 2.7825 | ds_loss: 3.6682 | lr: 8.4872e-07 | scale:    32.0000 | micro time: 0.303 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1409/  5202 | global iter:   1409/  5202 | loss: 2.7259 | ds_loss: 3.3146 | lr: 8.4851e-07 | scale:    32.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1410/  5202 | global iter:   1410/  5202 | loss: 2.8412 | ds_loss: 4.0812 | lr: 8.4831e-07 | scale:    32.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1411/  5202 | global iter:   1411/  5202 | loss: 2.0290 | ds_loss: 3.1118 | lr: 8.4811e-07 | scale:    32.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1412/  5202 | global iter:   1412/  5202 | loss: 2.6969 | ds_loss: 3.5512 | lr: 8.4790e-07 | scale:    32.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1412/  5202 | global iter:   1412/  5202 | loss: 2.5732 | ds_loss: 3.5147 | lr: 8.4790e-07 | scale:    32.0000 | micro time: 0.309 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1413/  5202 | global iter:   1413/  5202 | loss: 2.8804 | ds_loss: 3.6691 | lr: 8.4770e-07 | scale:    32.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1414/  5202 | global iter:   1414/  5202 | loss: 2.4466 | ds_loss: 3.2588 | lr: 8.4750e-07 | scale:    32.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1415/  5202 | global iter:   1415/  5202 | loss: 2.7727 | ds_loss: 3.5706 | lr: 8.4729e-07 | scale:    32.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1416/  5202 | global iter:   1416/  5202 | loss: 2.7515 | ds_loss: 3.7709 | lr: 8.4709e-07 | scale:    32.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1416/  5202 | global iter:   1416/  5202 | loss: 2.7128 | ds_loss: 3.5674 | lr: 8.4709e-07 | scale:    32.0000 | micro time: 0.309 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1417/  5202 | global iter:   1417/  5202 | loss: 2.3646 | ds_loss: 2.9997 | lr: 8.4688e-07 | scale:    32.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1418/  5202 | global iter:   1418/  5202 | loss: 2.8701 | ds_loss: 3.9170 | lr: 8.4668e-07 | scale:    32.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1419/  5202 | global iter:   1419/  5202 | loss: 2.1550 | ds_loss: 3.3901 | lr: 8.4648e-07 | scale:    32.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1420/  5202 | global iter:   1420/  5202 | loss: 2.9787 | ds_loss: 3.7736 | lr: 8.4627e-07 | scale:    32.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1420/  5202 | global iter:   1420/  5202 | loss: 2.5921 | ds_loss: 3.5201 | lr: 8.4627e-07 | scale:    32.0000 | micro time: 0.305 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1421/  5202 | global iter:   1421/  5202 | loss: 2.8061 | ds_loss: 3.6744 | lr: 8.4607e-07 | scale:    32.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1422/  5202 | global iter:   1422/  5202 | loss: 2.8831 | ds_loss: 3.5538 | lr: 8.4586e-07 | scale:    32.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1423/  5202 | global iter:   1423/  5202 | loss: 2.8380 | ds_loss: 4.1193 | lr: 8.4566e-07 | scale:    32.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1424/  5202 | global iter:   1424/  5202 | loss: 2.4411 | ds_loss: 3.9363 | lr: 8.4545e-07 | scale:    32.0000 | micro time: 0.301 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1424/  5202 | global iter:   1424/  5202 | loss: 2.7421 | ds_loss: 3.8209 | lr: 8.4545e-07 | scale:    32.0000 | micro time: 0.301 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1425/  5202 | global iter:   1425/  5202 | loss: 2.3080 | ds_loss: 3.3630 | lr: 8.4525e-07 | scale:    32.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1426/  5202 | global iter:   1426/  5202 | loss: 2.4850 | ds_loss: 3.6011 | lr: 8.4504e-07 | scale:    32.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1427/  5202 | global iter:   1427/  5202 | loss: 0.8350 | ds_loss: 1.3415 | lr: 8.4484e-07 | scale:    32.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1428/  5202 | global iter:   1428/  5202 | loss: 2.0867 | ds_loss: 3.1855 | lr: 8.4463e-07 | scale:    32.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1428/  5202 | global iter:   1428/  5202 | loss: 1.9287 | ds_loss: 2.8728 | lr: 8.4463e-07 | scale:    32.0000 | micro time: 0.304 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1429/  5202 | global iter:   1429/  5202 | loss: 3.0852 | ds_loss: 3.7336 | lr: 8.4443e-07 | scale:    32.0000 | micro time: 0.319 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1430/  5202 | global iter:   1430/  5202 | loss: 2.6099 | ds_loss: 3.5021 | lr: 8.4422e-07 | scale:    32.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1431/  5202 | global iter:   1431/  5202 | loss: 2.7932 | ds_loss: 3.4938 | lr: 8.4401e-07 | scale:    32.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1432/  5202 | global iter:   1432/  5202 | loss: 2.0734 | ds_loss: 2.9332 | lr: 8.4381e-07 | scale:    32.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1432/  5202 | global iter:   1432/  5202 | loss: 2.6404 | ds_loss: 3.4157 | lr: 8.4381e-07 | scale:    32.0000 | micro time: 0.306 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1433/  5202 | global iter:   1433/  5202 | loss: 2.7545 | ds_loss: 3.3683 | lr: 8.4360e-07 | scale:    32.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1434/  5202 | global iter:   1434/  5202 | loss: 2.4059 | ds_loss: 3.5619 | lr: 8.4340e-07 | scale:    32.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1435/  5202 | global iter:   1435/  5202 | loss: 2.9220 | ds_loss: 3.7851 | lr: 8.4319e-07 | scale:    32.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1436/  5202 | global iter:   1436/  5202 | loss: 2.7407 | ds_loss: 3.4319 | lr: 8.4298e-07 | scale:    32.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1436/  5202 | global iter:   1436/  5202 | loss: 2.7058 | ds_loss: 3.5368 | lr: 8.4298e-07 | scale:    32.0000 | micro time: 0.308 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1437/  5202 | global iter:   1437/  5202 | loss: 2.2779 | ds_loss: 3.3887 | lr: 8.4278e-07 | scale:    32.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1438/  5202 | global iter:   1438/  5202 | loss: 2.5449 | ds_loss: 3.6514 | lr: 8.4257e-07 | scale:    32.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1439/  5202 | global iter:   1439/  5202 | loss: 2.5764 | ds_loss: 3.2192 | lr: 8.4237e-07 | scale:    32.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1440/  5202 | global iter:   1440/  5202 | loss: 2.8674 | ds_loss: 3.5757 | lr: 8.4216e-07 | scale:    32.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1440/  5202 | global iter:   1440/  5202 | loss: 2.5667 | ds_loss: 3.4587 | lr: 8.4216e-07 | scale:    32.0000 | micro time: 0.307 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1441/  5202 | global iter:   1441/  5202 | loss: 2.7171 | ds_loss: 3.4853 | lr: 8.4195e-07 | scale:    32.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1442/  5202 | global iter:   1442/  5202 | loss: 2.8731 | ds_loss: 3.4315 | lr: 8.4175e-07 | scale:    32.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1443/  5202 | global iter:   1443/  5202 | loss: 2.9657 | ds_loss: 4.1660 | lr: 8.4154e-07 | scale:    32.0000 | micro time: 0.307 | step time: 0.000\n",
            "[2025-04-16 21:09:54,100] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32, reducing to 16\n",
            "train | epoch   0 | Iter:   1444/  5202 | global iter:   1444/  5202 | loss: 2.8352 | ds_loss: 3.9566 | lr: 8.4154e-07 | scale:    16.0000 | micro time: 0.233 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1444/  5202 | global iter:   1444/  5202 | loss: 2.8478 | ds_loss: 3.7598 | lr: 8.4154e-07 | scale:    16.0000 | micro time: 0.233 | step time: 0.291\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1445/  5202 | global iter:   1445/  5202 | loss: 1.8089 | ds_loss: 2.9072 | lr: 8.4133e-07 | scale:    16.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1446/  5202 | global iter:   1446/  5202 | loss: 3.6540 | ds_loss: 4.2190 | lr: 8.4112e-07 | scale:    16.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1447/  5202 | global iter:   1447/  5202 | loss: 2.9802 | ds_loss: 4.2602 | lr: 8.4092e-07 | scale:    16.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1448/  5202 | global iter:   1448/  5202 | loss: 1.4034 | ds_loss: 1.8928 | lr: 8.4071e-07 | scale:    16.0000 | micro time: 0.316 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1448/  5202 | global iter:   1448/  5202 | loss: 2.4616 | ds_loss: 3.3198 | lr: 8.4071e-07 | scale:    16.0000 | micro time: 0.316 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1449/  5202 | global iter:   1449/  5202 | loss: 2.1323 | ds_loss: 3.1348 | lr: 8.4050e-07 | scale:    16.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1450/  5202 | global iter:   1450/  5202 | loss: 1.9595 | ds_loss: 2.7293 | lr: 8.4029e-07 | scale:    16.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1451/  5202 | global iter:   1451/  5202 | loss: 2.1614 | ds_loss: 2.9724 | lr: 8.4009e-07 | scale:    16.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1452/  5202 | global iter:   1452/  5202 | loss: 2.6939 | ds_loss: 3.5525 | lr: 8.3988e-07 | scale:    16.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1452/  5202 | global iter:   1452/  5202 | loss: 2.2368 | ds_loss: 3.0972 | lr: 8.3988e-07 | scale:    16.0000 | micro time: 0.308 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1453/  5202 | global iter:   1453/  5202 | loss: 2.3427 | ds_loss: 3.1898 | lr: 8.3967e-07 | scale:    16.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1454/  5202 | global iter:   1454/  5202 | loss: 2.6011 | ds_loss: 3.3376 | lr: 8.3946e-07 | scale:    16.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1455/  5202 | global iter:   1455/  5202 | loss: 2.6806 | ds_loss: 3.6169 | lr: 8.3925e-07 | scale:    16.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1456/  5202 | global iter:   1456/  5202 | loss: 2.8265 | ds_loss: 3.5958 | lr: 8.3905e-07 | scale:    16.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1456/  5202 | global iter:   1456/  5202 | loss: 2.6127 | ds_loss: 3.4350 | lr: 8.3905e-07 | scale:    16.0000 | micro time: 0.310 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1457/  5202 | global iter:   1457/  5202 | loss: 3.0518 | ds_loss: 3.9192 | lr: 8.3884e-07 | scale:    16.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1458/  5202 | global iter:   1458/  5202 | loss: 0.8581 | ds_loss: 1.3990 | lr: 8.3863e-07 | scale:    16.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1459/  5202 | global iter:   1459/  5202 | loss: 2.7480 | ds_loss: 3.5885 | lr: 8.3842e-07 | scale:    16.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1460/  5202 | global iter:   1460/  5202 | loss: 3.0902 | ds_loss: 4.3059 | lr: 8.3821e-07 | scale:    16.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1460/  5202 | global iter:   1460/  5202 | loss: 2.4370 | ds_loss: 3.3032 | lr: 8.3821e-07 | scale:    16.0000 | micro time: 0.308 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1461/  5202 | global iter:   1461/  5202 | loss: 2.5434 | ds_loss: 3.4671 | lr: 8.3800e-07 | scale:    16.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1462/  5202 | global iter:   1462/  5202 | loss: 2.8967 | ds_loss: 3.7349 | lr: 8.3779e-07 | scale:    16.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1463/  5202 | global iter:   1463/  5202 | loss: 2.1414 | ds_loss: 2.9569 | lr: 8.3759e-07 | scale:    16.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1464/  5202 | global iter:   1464/  5202 | loss: 3.0153 | ds_loss: 3.7100 | lr: 8.3738e-07 | scale:    16.0000 | micro time: 0.313 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1464/  5202 | global iter:   1464/  5202 | loss: 2.6492 | ds_loss: 3.4672 | lr: 8.3738e-07 | scale:    16.0000 | micro time: 0.313 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1465/  5202 | global iter:   1465/  5202 | loss: 2.5073 | ds_loss: 3.6920 | lr: 8.3717e-07 | scale:    16.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1466/  5202 | global iter:   1466/  5202 | loss: 2.3368 | ds_loss: 3.6040 | lr: 8.3696e-07 | scale:    16.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1467/  5202 | global iter:   1467/  5202 | loss: 2.4648 | ds_loss: 3.0216 | lr: 8.3675e-07 | scale:    16.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1468/  5202 | global iter:   1468/  5202 | loss: 2.9060 | ds_loss: 3.6313 | lr: 8.3654e-07 | scale:    16.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1468/  5202 | global iter:   1468/  5202 | loss: 2.5537 | ds_loss: 3.4872 | lr: 8.3654e-07 | scale:    16.0000 | micro time: 0.311 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1469/  5202 | global iter:   1469/  5202 | loss: 2.1893 | ds_loss: 2.9080 | lr: 8.3633e-07 | scale:    16.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1470/  5202 | global iter:   1470/  5202 | loss: 2.5307 | ds_loss: 3.3012 | lr: 8.3612e-07 | scale:    16.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1471/  5202 | global iter:   1471/  5202 | loss: 2.0778 | ds_loss: 2.9329 | lr: 8.3591e-07 | scale:    16.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1472/  5202 | global iter:   1472/  5202 | loss: 2.5664 | ds_loss: 3.6768 | lr: 8.3570e-07 | scale:    16.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1472/  5202 | global iter:   1472/  5202 | loss: 2.3410 | ds_loss: 3.2047 | lr: 8.3570e-07 | scale:    16.0000 | micro time: 0.304 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1473/  5202 | global iter:   1473/  5202 | loss: 2.5155 | ds_loss: 3.5809 | lr: 8.3549e-07 | scale:    16.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1474/  5202 | global iter:   1474/  5202 | loss: 1.7350 | ds_loss: 2.5840 | lr: 8.3528e-07 | scale:    16.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1475/  5202 | global iter:   1475/  5202 | loss: 2.9056 | ds_loss: 3.6652 | lr: 8.3507e-07 | scale:    16.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1476/  5202 | global iter:   1476/  5202 | loss: 2.3597 | ds_loss: 2.9800 | lr: 8.3486e-07 | scale:    16.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1476/  5202 | global iter:   1476/  5202 | loss: 2.3789 | ds_loss: 3.2025 | lr: 8.3486e-07 | scale:    16.0000 | micro time: 0.307 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1477/  5202 | global iter:   1477/  5202 | loss: 2.5879 | ds_loss: 3.4478 | lr: 8.3465e-07 | scale:    16.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1478/  5202 | global iter:   1478/  5202 | loss: 1.4878 | ds_loss: 2.0973 | lr: 8.3444e-07 | scale:    16.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1479/  5202 | global iter:   1479/  5202 | loss: 2.8504 | ds_loss: 3.4799 | lr: 8.3423e-07 | scale:    16.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1480/  5202 | global iter:   1480/  5202 | loss: 2.3440 | ds_loss: 3.2598 | lr: 8.3402e-07 | scale:    16.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1480/  5202 | global iter:   1480/  5202 | loss: 2.3175 | ds_loss: 3.0712 | lr: 8.3402e-07 | scale:    16.0000 | micro time: 0.307 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1481/  5202 | global iter:   1481/  5202 | loss: 2.7605 | ds_loss: 3.5435 | lr: 8.3381e-07 | scale:    16.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1482/  5202 | global iter:   1482/  5202 | loss: 2.3094 | ds_loss: 3.1012 | lr: 8.3360e-07 | scale:    16.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1483/  5202 | global iter:   1483/  5202 | loss: 3.0162 | ds_loss: 3.9443 | lr: 8.3338e-07 | scale:    16.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1484/  5202 | global iter:   1484/  5202 | loss: 3.3602 | ds_loss: 3.8944 | lr: 8.3317e-07 | scale:    16.0000 | micro time: 0.319 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1484/  5202 | global iter:   1484/  5202 | loss: 2.8616 | ds_loss: 3.6209 | lr: 8.3317e-07 | scale:    16.0000 | micro time: 0.319 | step time: 0.312\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1485/  5202 | global iter:   1485/  5202 | loss: 2.1335 | ds_loss: 2.9397 | lr: 8.3296e-07 | scale:    16.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1486/  5202 | global iter:   1486/  5202 | loss: 1.8708 | ds_loss: 2.4203 | lr: 8.3275e-07 | scale:    16.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1487/  5202 | global iter:   1487/  5202 | loss: 2.4794 | ds_loss: 3.1402 | lr: 8.3254e-07 | scale:    16.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1488/  5202 | global iter:   1488/  5202 | loss: 1.5029 | ds_loss: 2.2318 | lr: 8.3233e-07 | scale:    16.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1488/  5202 | global iter:   1488/  5202 | loss: 1.9966 | ds_loss: 2.6830 | lr: 8.3233e-07 | scale:    16.0000 | micro time: 0.311 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1489/  5202 | global iter:   1489/  5202 | loss: 2.9691 | ds_loss: 3.7948 | lr: 8.3212e-07 | scale:    16.0000 | micro time: 0.321 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1490/  5202 | global iter:   1490/  5202 | loss: 3.0817 | ds_loss: 4.0556 | lr: 8.3190e-07 | scale:    16.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1491/  5202 | global iter:   1491/  5202 | loss: 2.9587 | ds_loss: 3.5812 | lr: 8.3169e-07 | scale:    16.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1492/  5202 | global iter:   1492/  5202 | loss: 2.3389 | ds_loss: 2.9532 | lr: 8.3148e-07 | scale:    16.0000 | micro time: 0.313 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1492/  5202 | global iter:   1492/  5202 | loss: 2.8371 | ds_loss: 3.5962 | lr: 8.3148e-07 | scale:    16.0000 | micro time: 0.313 | step time: 0.315\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1493/  5202 | global iter:   1493/  5202 | loss: 1.8606 | ds_loss: 2.6702 | lr: 8.3127e-07 | scale:    16.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1494/  5202 | global iter:   1494/  5202 | loss: 3.1251 | ds_loss: 3.9082 | lr: 8.3106e-07 | scale:    16.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1495/  5202 | global iter:   1495/  5202 | loss: 2.9954 | ds_loss: 3.7314 | lr: 8.3084e-07 | scale:    16.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1496/  5202 | global iter:   1496/  5202 | loss: 2.1600 | ds_loss: 3.1992 | lr: 8.3063e-07 | scale:    16.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1496/  5202 | global iter:   1496/  5202 | loss: 2.5353 | ds_loss: 3.3773 | lr: 8.3063e-07 | scale:    16.0000 | micro time: 0.303 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1497/  5202 | global iter:   1497/  5202 | loss: 2.7315 | ds_loss: 3.4135 | lr: 8.3042e-07 | scale:    16.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1498/  5202 | global iter:   1498/  5202 | loss: 1.2681 | ds_loss: 2.0170 | lr: 8.3021e-07 | scale:    16.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1499/  5202 | global iter:   1499/  5202 | loss: 2.7699 | ds_loss: 4.3124 | lr: 8.2999e-07 | scale:    16.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1500/  5202 | global iter:   1500/  5202 | loss: 3.1568 | ds_loss: 3.7671 | lr: 8.2978e-07 | scale:    16.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1500/  5202 | global iter:   1500/  5202 | loss: 2.4815 | ds_loss: 3.3775 | lr: 8.2978e-07 | scale:    16.0000 | micro time: 0.309 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1501/  5202 | global iter:   1501/  5202 | loss: 2.2027 | ds_loss: 2.9896 | lr: 8.2957e-07 | scale:    16.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1502/  5202 | global iter:   1502/  5202 | loss: 2.5883 | ds_loss: 3.4199 | lr: 8.2935e-07 | scale:    16.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1503/  5202 | global iter:   1503/  5202 | loss: 2.3129 | ds_loss: 2.7933 | lr: 8.2914e-07 | scale:    16.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1504/  5202 | global iter:   1504/  5202 | loss: 2.8604 | ds_loss: 3.4694 | lr: 8.2893e-07 | scale:    16.0000 | micro time: 0.314 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1504/  5202 | global iter:   1504/  5202 | loss: 2.4911 | ds_loss: 3.1681 | lr: 8.2893e-07 | scale:    16.0000 | micro time: 0.314 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1505/  5202 | global iter:   1505/  5202 | loss: 2.8127 | ds_loss: 3.4384 | lr: 8.2871e-07 | scale:    16.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1506/  5202 | global iter:   1506/  5202 | loss: 2.1417 | ds_loss: 2.9581 | lr: 8.2850e-07 | scale:    16.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1507/  5202 | global iter:   1507/  5202 | loss: 2.8092 | ds_loss: 3.5863 | lr: 8.2829e-07 | scale:    16.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1508/  5202 | global iter:   1508/  5202 | loss: 2.3721 | ds_loss: 3.5176 | lr: 8.2807e-07 | scale:    16.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1508/  5202 | global iter:   1508/  5202 | loss: 2.5339 | ds_loss: 3.3751 | lr: 8.2807e-07 | scale:    16.0000 | micro time: 0.304 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1509/  5202 | global iter:   1509/  5202 | loss: 2.4450 | ds_loss: 3.1018 | lr: 8.2786e-07 | scale:    16.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1510/  5202 | global iter:   1510/  5202 | loss: 2.7483 | ds_loss: 4.1204 | lr: 8.2765e-07 | scale:    16.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1511/  5202 | global iter:   1511/  5202 | loss: 2.9476 | ds_loss: 4.1574 | lr: 8.2743e-07 | scale:    16.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1512/  5202 | global iter:   1512/  5202 | loss: 1.5199 | ds_loss: 2.2553 | lr: 8.2722e-07 | scale:    16.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1512/  5202 | global iter:   1512/  5202 | loss: 2.4152 | ds_loss: 3.4087 | lr: 8.2722e-07 | scale:    16.0000 | micro time: 0.307 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1513/  5202 | global iter:   1513/  5202 | loss: 2.4520 | ds_loss: 3.0311 | lr: 8.2700e-07 | scale:    16.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1514/  5202 | global iter:   1514/  5202 | loss: 2.0469 | ds_loss: 3.1774 | lr: 8.2679e-07 | scale:    16.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1515/  5202 | global iter:   1515/  5202 | loss: 2.7949 | ds_loss: 3.4794 | lr: 8.2658e-07 | scale:    16.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1516/  5202 | global iter:   1516/  5202 | loss: 2.7921 | ds_loss: 3.6145 | lr: 8.2636e-07 | scale:    16.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1516/  5202 | global iter:   1516/  5202 | loss: 2.5215 | ds_loss: 3.3256 | lr: 8.2636e-07 | scale:    16.0000 | micro time: 0.305 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1517/  5202 | global iter:   1517/  5202 | loss: 1.4976 | ds_loss: 2.2189 | lr: 8.2615e-07 | scale:    16.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1518/  5202 | global iter:   1518/  5202 | loss: 2.5285 | ds_loss: 3.3933 | lr: 8.2593e-07 | scale:    16.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1519/  5202 | global iter:   1519/  5202 | loss: 2.5097 | ds_loss: 3.9201 | lr: 8.2572e-07 | scale:    16.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1520/  5202 | global iter:   1520/  5202 | loss: 2.4110 | ds_loss: 3.3265 | lr: 8.2550e-07 | scale:    16.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1520/  5202 | global iter:   1520/  5202 | loss: 2.2367 | ds_loss: 3.2147 | lr: 8.2550e-07 | scale:    16.0000 | micro time: 0.308 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1521/  5202 | global iter:   1521/  5202 | loss: 2.1948 | ds_loss: 2.9502 | lr: 8.2529e-07 | scale:    16.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1522/  5202 | global iter:   1522/  5202 | loss: 2.7552 | ds_loss: 3.5018 | lr: 8.2507e-07 | scale:    16.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1523/  5202 | global iter:   1523/  5202 | loss: 2.3962 | ds_loss: 3.0239 | lr: 8.2486e-07 | scale:    16.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1524/  5202 | global iter:   1524/  5202 | loss: 2.5552 | ds_loss: 3.8976 | lr: 8.2464e-07 | scale:    16.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1524/  5202 | global iter:   1524/  5202 | loss: 2.4754 | ds_loss: 3.3434 | lr: 8.2464e-07 | scale:    16.0000 | micro time: 0.309 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1525/  5202 | global iter:   1525/  5202 | loss: 2.2062 | ds_loss: 2.9908 | lr: 8.2443e-07 | scale:    16.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1526/  5202 | global iter:   1526/  5202 | loss: 1.9699 | ds_loss: 2.8048 | lr: 8.2421e-07 | scale:    16.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1527/  5202 | global iter:   1527/  5202 | loss: 2.3894 | ds_loss: 3.2957 | lr: 8.2400e-07 | scale:    16.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1528/  5202 | global iter:   1528/  5202 | loss: 2.4849 | ds_loss: 3.6089 | lr: 8.2378e-07 | scale:    16.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1528/  5202 | global iter:   1528/  5202 | loss: 2.2626 | ds_loss: 3.1751 | lr: 8.2378e-07 | scale:    16.0000 | micro time: 0.309 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1529/  5202 | global iter:   1529/  5202 | loss: 3.0234 | ds_loss: 3.6605 | lr: 8.2357e-07 | scale:    16.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1530/  5202 | global iter:   1530/  5202 | loss: 2.8897 | ds_loss: 3.8517 | lr: 8.2335e-07 | scale:    16.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1531/  5202 | global iter:   1531/  5202 | loss: 2.6474 | ds_loss: 3.0674 | lr: 8.2313e-07 | scale:    16.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1532/  5202 | global iter:   1532/  5202 | loss: 1.2394 | ds_loss: 1.9852 | lr: 8.2292e-07 | scale:    16.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1532/  5202 | global iter:   1532/  5202 | loss: 2.4500 | ds_loss: 3.1412 | lr: 8.2292e-07 | scale:    16.0000 | micro time: 0.311 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1533/  5202 | global iter:   1533/  5202 | loss: 2.6365 | ds_loss: 3.6693 | lr: 8.2270e-07 | scale:    16.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1534/  5202 | global iter:   1534/  5202 | loss: 2.3769 | ds_loss: 3.2490 | lr: 8.2249e-07 | scale:    16.0000 | micro time: 0.316 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1535/  5202 | global iter:   1535/  5202 | loss: 2.2947 | ds_loss: 3.3088 | lr: 8.2227e-07 | scale:    16.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1536/  5202 | global iter:   1536/  5202 | loss: 2.6046 | ds_loss: 3.3014 | lr: 8.2205e-07 | scale:    16.0000 | micro time: 0.312 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1536/  5202 | global iter:   1536/  5202 | loss: 2.4782 | ds_loss: 3.3821 | lr: 8.2205e-07 | scale:    16.0000 | micro time: 0.312 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1537/  5202 | global iter:   1537/  5202 | loss: 2.7746 | ds_loss: 3.4644 | lr: 8.2184e-07 | scale:    16.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1538/  5202 | global iter:   1538/  5202 | loss: 2.7922 | ds_loss: 4.3091 | lr: 8.2162e-07 | scale:    16.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1539/  5202 | global iter:   1539/  5202 | loss: 2.2529 | ds_loss: 3.0080 | lr: 8.2140e-07 | scale:    16.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1540/  5202 | global iter:   1540/  5202 | loss: 3.0750 | ds_loss: 3.6947 | lr: 8.2119e-07 | scale:    16.0000 | micro time: 0.313 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1540/  5202 | global iter:   1540/  5202 | loss: 2.7237 | ds_loss: 3.6190 | lr: 8.2119e-07 | scale:    16.0000 | micro time: 0.313 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1541/  5202 | global iter:   1541/  5202 | loss: 2.9233 | ds_loss: 3.6650 | lr: 8.2097e-07 | scale:    16.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1542/  5202 | global iter:   1542/  5202 | loss: 3.1493 | ds_loss: 3.9255 | lr: 8.2075e-07 | scale:    16.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1543/  5202 | global iter:   1543/  5202 | loss: 2.7045 | ds_loss: 3.6593 | lr: 8.2053e-07 | scale:    16.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1544/  5202 | global iter:   1544/  5202 | loss: 3.2899 | ds_loss: 4.0269 | lr: 8.2032e-07 | scale:    16.0000 | micro time: 0.314 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1544/  5202 | global iter:   1544/  5202 | loss: 3.0167 | ds_loss: 3.8192 | lr: 8.2032e-07 | scale:    16.0000 | micro time: 0.314 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1545/  5202 | global iter:   1545/  5202 | loss: 2.9640 | ds_loss: 3.6809 | lr: 8.2010e-07 | scale:    16.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1546/  5202 | global iter:   1546/  5202 | loss: 2.6315 | ds_loss: 3.3095 | lr: 8.1988e-07 | scale:    16.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1547/  5202 | global iter:   1547/  5202 | loss: 1.3333 | ds_loss: 1.9311 | lr: 8.1967e-07 | scale:    16.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1548/  5202 | global iter:   1548/  5202 | loss: 2.5400 | ds_loss: 3.2535 | lr: 8.1945e-07 | scale:    16.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1548/  5202 | global iter:   1548/  5202 | loss: 2.3672 | ds_loss: 3.0437 | lr: 8.1945e-07 | scale:    16.0000 | micro time: 0.307 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1549/  5202 | global iter:   1549/  5202 | loss: 2.7192 | ds_loss: 3.5154 | lr: 8.1923e-07 | scale:    16.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1550/  5202 | global iter:   1550/  5202 | loss: 1.1180 | ds_loss: 1.5672 | lr: 8.1901e-07 | scale:    16.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1551/  5202 | global iter:   1551/  5202 | loss: 2.6530 | ds_loss: 3.4414 | lr: 8.1879e-07 | scale:    16.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1552/  5202 | global iter:   1552/  5202 | loss: 2.4217 | ds_loss: 3.3191 | lr: 8.1858e-07 | scale:    16.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1552/  5202 | global iter:   1552/  5202 | loss: 2.2280 | ds_loss: 2.9608 | lr: 8.1858e-07 | scale:    16.0000 | micro time: 0.306 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1553/  5202 | global iter:   1553/  5202 | loss: 3.0352 | ds_loss: 3.8240 | lr: 8.1836e-07 | scale:    16.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1554/  5202 | global iter:   1554/  5202 | loss: 3.3160 | ds_loss: 4.0056 | lr: 8.1814e-07 | scale:    16.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1555/  5202 | global iter:   1555/  5202 | loss: 2.7127 | ds_loss: 3.7315 | lr: 8.1792e-07 | scale:    16.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1556/  5202 | global iter:   1556/  5202 | loss: 2.3415 | ds_loss: 3.3129 | lr: 8.1770e-07 | scale:    16.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1556/  5202 | global iter:   1556/  5202 | loss: 2.8513 | ds_loss: 3.7185 | lr: 8.1770e-07 | scale:    16.0000 | micro time: 0.308 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1557/  5202 | global iter:   1557/  5202 | loss: 2.8094 | ds_loss: 3.9888 | lr: 8.1748e-07 | scale:    16.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1558/  5202 | global iter:   1558/  5202 | loss: 2.8177 | ds_loss: 3.6991 | lr: 8.1727e-07 | scale:    16.0000 | micro time: 0.317 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1559/  5202 | global iter:   1559/  5202 | loss: 3.1584 | ds_loss: 3.7218 | lr: 8.1705e-07 | scale:    16.0000 | micro time: 0.316 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1560/  5202 | global iter:   1560/  5202 | loss: 2.9785 | ds_loss: 3.5484 | lr: 8.1683e-07 | scale:    16.0000 | micro time: 0.313 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1560/  5202 | global iter:   1560/  5202 | loss: 2.9410 | ds_loss: 3.7395 | lr: 8.1683e-07 | scale:    16.0000 | micro time: 0.313 | step time: 0.312\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1561/  5202 | global iter:   1561/  5202 | loss: 0.9811 | ds_loss: 1.4463 | lr: 8.1661e-07 | scale:    16.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1562/  5202 | global iter:   1562/  5202 | loss: 2.2760 | ds_loss: 3.5026 | lr: 8.1639e-07 | scale:    16.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1563/  5202 | global iter:   1563/  5202 | loss: 3.1361 | ds_loss: 3.7686 | lr: 8.1617e-07 | scale:    16.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1564/  5202 | global iter:   1564/  5202 | loss: 2.7098 | ds_loss: 3.5025 | lr: 8.1595e-07 | scale:    16.0000 | micro time: 0.312 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1564/  5202 | global iter:   1564/  5202 | loss: 2.2758 | ds_loss: 3.0550 | lr: 8.1595e-07 | scale:    16.0000 | micro time: 0.312 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1565/  5202 | global iter:   1565/  5202 | loss: 2.2035 | ds_loss: 3.0283 | lr: 8.1573e-07 | scale:    16.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1566/  5202 | global iter:   1566/  5202 | loss: 2.9802 | ds_loss: 3.7298 | lr: 8.1551e-07 | scale:    16.0000 | micro time: 0.320 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1567/  5202 | global iter:   1567/  5202 | loss: 2.8890 | ds_loss: 3.8592 | lr: 8.1529e-07 | scale:    16.0000 | micro time: 0.317 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1568/  5202 | global iter:   1568/  5202 | loss: 3.3157 | ds_loss: 5.1869 | lr: 8.1507e-07 | scale:    16.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1568/  5202 | global iter:   1568/  5202 | loss: 2.8471 | ds_loss: 3.9511 | lr: 8.1507e-07 | scale:    16.0000 | micro time: 0.307 | step time: 0.313\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1569/  5202 | global iter:   1569/  5202 | loss: 2.8106 | ds_loss: 3.5337 | lr: 8.1486e-07 | scale:    16.0000 | micro time: 0.317 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1570/  5202 | global iter:   1570/  5202 | loss: 2.9416 | ds_loss: 3.5640 | lr: 8.1464e-07 | scale:    16.0000 | micro time: 0.316 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1571/  5202 | global iter:   1571/  5202 | loss: 2.0082 | ds_loss: 2.8023 | lr: 8.1442e-07 | scale:    16.0000 | micro time: 0.319 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1572/  5202 | global iter:   1572/  5202 | loss: 2.8766 | ds_loss: 3.5642 | lr: 8.1420e-07 | scale:    16.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1572/  5202 | global iter:   1572/  5202 | loss: 2.6593 | ds_loss: 3.3661 | lr: 8.1420e-07 | scale:    16.0000 | micro time: 0.311 | step time: 0.316\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1573/  5202 | global iter:   1573/  5202 | loss: 2.9216 | ds_loss: 3.8917 | lr: 8.1398e-07 | scale:    16.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1574/  5202 | global iter:   1574/  5202 | loss: 2.8307 | ds_loss: 3.5597 | lr: 8.1376e-07 | scale:    16.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1575/  5202 | global iter:   1575/  5202 | loss: 2.6892 | ds_loss: 3.3711 | lr: 8.1354e-07 | scale:    16.0000 | micro time: 0.320 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1576/  5202 | global iter:   1576/  5202 | loss: 2.0174 | ds_loss: 2.4888 | lr: 8.1331e-07 | scale:    16.0000 | micro time: 0.314 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1576/  5202 | global iter:   1576/  5202 | loss: 2.6147 | ds_loss: 3.3278 | lr: 8.1331e-07 | scale:    16.0000 | micro time: 0.314 | step time: 0.313\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1577/  5202 | global iter:   1577/  5202 | loss: 2.3349 | ds_loss: 3.0183 | lr: 8.1309e-07 | scale:    16.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1578/  5202 | global iter:   1578/  5202 | loss: 2.9997 | ds_loss: 3.7466 | lr: 8.1287e-07 | scale:    16.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1579/  5202 | global iter:   1579/  5202 | loss: 2.7571 | ds_loss: 3.4720 | lr: 8.1265e-07 | scale:    16.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1580/  5202 | global iter:   1580/  5202 | loss: 3.0087 | ds_loss: 3.9728 | lr: 8.1243e-07 | scale:    16.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1580/  5202 | global iter:   1580/  5202 | loss: 2.7751 | ds_loss: 3.5524 | lr: 8.1243e-07 | scale:    16.0000 | micro time: 0.311 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1581/  5202 | global iter:   1581/  5202 | loss: 2.7880 | ds_loss: 3.8079 | lr: 8.1221e-07 | scale:    16.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1582/  5202 | global iter:   1582/  5202 | loss: 2.4038 | ds_loss: 3.0447 | lr: 8.1199e-07 | scale:    16.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1583/  5202 | global iter:   1583/  5202 | loss: 1.0412 | ds_loss: 1.5279 | lr: 8.1177e-07 | scale:    16.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1584/  5202 | global iter:   1584/  5202 | loss: 2.6635 | ds_loss: 3.4559 | lr: 8.1155e-07 | scale:    16.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1584/  5202 | global iter:   1584/  5202 | loss: 2.2241 | ds_loss: 2.9591 | lr: 8.1155e-07 | scale:    16.0000 | micro time: 0.309 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1585/  5202 | global iter:   1585/  5202 | loss: 3.2045 | ds_loss: 4.1486 | lr: 8.1133e-07 | scale:    16.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1586/  5202 | global iter:   1586/  5202 | loss: 2.7224 | ds_loss: 3.3250 | lr: 8.1111e-07 | scale:    16.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1587/  5202 | global iter:   1587/  5202 | loss: 1.7035 | ds_loss: 2.6136 | lr: 8.1088e-07 | scale:    16.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1588/  5202 | global iter:   1588/  5202 | loss: 3.1308 | ds_loss: 3.8194 | lr: 8.1066e-07 | scale:    16.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1588/  5202 | global iter:   1588/  5202 | loss: 2.6903 | ds_loss: 3.4766 | lr: 8.1066e-07 | scale:    16.0000 | micro time: 0.309 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1589/  5202 | global iter:   1589/  5202 | loss: 2.6222 | ds_loss: 4.3982 | lr: 8.1044e-07 | scale:    16.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1590/  5202 | global iter:   1590/  5202 | loss: 2.5926 | ds_loss: 3.5789 | lr: 8.1022e-07 | scale:    16.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1591/  5202 | global iter:   1591/  5202 | loss: 3.2160 | ds_loss: 4.1913 | lr: 8.1000e-07 | scale:    16.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1592/  5202 | global iter:   1592/  5202 | loss: 1.4897 | ds_loss: 2.1022 | lr: 8.0978e-07 | scale:    16.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1592/  5202 | global iter:   1592/  5202 | loss: 2.4801 | ds_loss: 3.5677 | lr: 8.0978e-07 | scale:    16.0000 | micro time: 0.308 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1593/  5202 | global iter:   1593/  5202 | loss: 2.3933 | ds_loss: 2.8894 | lr: 8.0955e-07 | scale:    16.0000 | micro time: 0.317 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1594/  5202 | global iter:   1594/  5202 | loss: 2.5965 | ds_loss: 3.3874 | lr: 8.0933e-07 | scale:    16.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1595/  5202 | global iter:   1595/  5202 | loss: 1.7939 | ds_loss: 2.5384 | lr: 8.0911e-07 | scale:    16.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1596/  5202 | global iter:   1596/  5202 | loss: 2.8485 | ds_loss: 3.9643 | lr: 8.0889e-07 | scale:    16.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1596/  5202 | global iter:   1596/  5202 | loss: 2.4081 | ds_loss: 3.1949 | lr: 8.0889e-07 | scale:    16.0000 | micro time: 0.305 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1597/  5202 | global iter:   1597/  5202 | loss: 3.0040 | ds_loss: 3.9222 | lr: 8.0867e-07 | scale:    16.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1598/  5202 | global iter:   1598/  5202 | loss: 3.0609 | ds_loss: 3.6450 | lr: 8.0844e-07 | scale:    16.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1599/  5202 | global iter:   1599/  5202 | loss: 3.1467 | ds_loss: 4.1532 | lr: 8.0822e-07 | scale:    16.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1600/  5202 | global iter:   1600/  5202 | loss: 3.1061 | ds_loss: 3.6244 | lr: 8.0800e-07 | scale:    16.0000 | micro time: 0.319 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1600/  5202 | global iter:   1600/  5202 | loss: 3.0794 | ds_loss: 3.8362 | lr: 8.0800e-07 | scale:    16.0000 | micro time: 0.319 | step time: 0.312\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1601/  5202 | global iter:   1601/  5202 | loss: 1.5950 | ds_loss: 2.5040 | lr: 8.0778e-07 | scale:    16.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1602/  5202 | global iter:   1602/  5202 | loss: 2.6876 | ds_loss: 3.3292 | lr: 8.0755e-07 | scale:    16.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1603/  5202 | global iter:   1603/  5202 | loss: 2.8037 | ds_loss: 3.2898 | lr: 8.0733e-07 | scale:    16.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1604/  5202 | global iter:   1604/  5202 | loss: 2.6819 | ds_loss: 3.4905 | lr: 8.0711e-07 | scale:    16.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1604/  5202 | global iter:   1604/  5202 | loss: 2.4420 | ds_loss: 3.1534 | lr: 8.0711e-07 | scale:    16.0000 | micro time: 0.310 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1605/  5202 | global iter:   1605/  5202 | loss: 2.8951 | ds_loss: 3.4982 | lr: 8.0688e-07 | scale:    16.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1606/  5202 | global iter:   1606/  5202 | loss: 2.8541 | ds_loss: 3.6562 | lr: 8.0666e-07 | scale:    16.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1607/  5202 | global iter:   1607/  5202 | loss: 2.9275 | ds_loss: 3.9152 | lr: 8.0644e-07 | scale:    16.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1608/  5202 | global iter:   1608/  5202 | loss: 3.5802 | ds_loss: 5.1164 | lr: 8.0621e-07 | scale:    16.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1608/  5202 | global iter:   1608/  5202 | loss: 3.0643 | ds_loss: 4.0465 | lr: 8.0621e-07 | scale:    16.0000 | micro time: 0.304 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1609/  5202 | global iter:   1609/  5202 | loss: 2.7634 | ds_loss: 3.8269 | lr: 8.0599e-07 | scale:    16.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1610/  5202 | global iter:   1610/  5202 | loss: 3.0079 | ds_loss: 3.8418 | lr: 8.0577e-07 | scale:    16.0000 | micro time: 0.325 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1611/  5202 | global iter:   1611/  5202 | loss: 2.7169 | ds_loss: 3.4832 | lr: 8.0554e-07 | scale:    16.0000 | micro time: 0.318 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1612/  5202 | global iter:   1612/  5202 | loss: 2.8951 | ds_loss: 3.7159 | lr: 8.0532e-07 | scale:    16.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1612/  5202 | global iter:   1612/  5202 | loss: 2.8458 | ds_loss: 3.7169 | lr: 8.0532e-07 | scale:    16.0000 | micro time: 0.306 | step time: 0.315\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1613/  5202 | global iter:   1613/  5202 | loss: 2.5388 | ds_loss: 3.6019 | lr: 8.0510e-07 | scale:    16.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1614/  5202 | global iter:   1614/  5202 | loss: 1.7173 | ds_loss: 2.3851 | lr: 8.0487e-07 | scale:    16.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1615/  5202 | global iter:   1615/  5202 | loss: 1.7908 | ds_loss: 2.5739 | lr: 8.0465e-07 | scale:    16.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1616/  5202 | global iter:   1616/  5202 | loss: 2.6366 | ds_loss: 3.6671 | lr: 8.0442e-07 | scale:    16.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1616/  5202 | global iter:   1616/  5202 | loss: 2.1709 | ds_loss: 3.0570 | lr: 8.0442e-07 | scale:    16.0000 | micro time: 0.305 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1617/  5202 | global iter:   1617/  5202 | loss: 2.2774 | ds_loss: 3.1145 | lr: 8.0420e-07 | scale:    16.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1618/  5202 | global iter:   1618/  5202 | loss: 2.7371 | ds_loss: 3.6614 | lr: 8.0397e-07 | scale:    16.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1619/  5202 | global iter:   1619/  5202 | loss: 2.8048 | ds_loss: 3.4854 | lr: 8.0375e-07 | scale:    16.0000 | micro time: 0.316 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1620/  5202 | global iter:   1620/  5202 | loss: 2.6088 | ds_loss: 3.5708 | lr: 8.0353e-07 | scale:    16.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1620/  5202 | global iter:   1620/  5202 | loss: 2.6070 | ds_loss: 3.4580 | lr: 8.0353e-07 | scale:    16.0000 | micro time: 0.309 | step time: 0.312\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1621/  5202 | global iter:   1621/  5202 | loss: 3.1440 | ds_loss: 3.9096 | lr: 8.0330e-07 | scale:    16.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1622/  5202 | global iter:   1622/  5202 | loss: 2.3471 | ds_loss: 3.3674 | lr: 8.0308e-07 | scale:    16.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1623/  5202 | global iter:   1623/  5202 | loss: 2.8373 | ds_loss: 3.4367 | lr: 8.0285e-07 | scale:    16.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1624/  5202 | global iter:   1624/  5202 | loss: 2.7832 | ds_loss: 3.4951 | lr: 8.0263e-07 | scale:    16.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1624/  5202 | global iter:   1624/  5202 | loss: 2.7779 | ds_loss: 3.5522 | lr: 8.0263e-07 | scale:    16.0000 | micro time: 0.309 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1625/  5202 | global iter:   1625/  5202 | loss: 2.8361 | ds_loss: 3.6509 | lr: 8.0240e-07 | scale:    16.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1626/  5202 | global iter:   1626/  5202 | loss: 3.0380 | ds_loss: 3.6169 | lr: 8.0218e-07 | scale:    16.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1627/  5202 | global iter:   1627/  5202 | loss: 2.8910 | ds_loss: 3.5951 | lr: 8.0195e-07 | scale:    16.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1628/  5202 | global iter:   1628/  5202 | loss: 2.7387 | ds_loss: 3.3195 | lr: 8.0173e-07 | scale:    16.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1628/  5202 | global iter:   1628/  5202 | loss: 2.8760 | ds_loss: 3.5456 | lr: 8.0173e-07 | scale:    16.0000 | micro time: 0.307 | step time: 0.312\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1629/  5202 | global iter:   1629/  5202 | loss: 2.3954 | ds_loss: 3.2549 | lr: 8.0150e-07 | scale:    16.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1630/  5202 | global iter:   1630/  5202 | loss: 2.9654 | ds_loss: 3.5454 | lr: 8.0128e-07 | scale:    16.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1631/  5202 | global iter:   1631/  5202 | loss: 3.2079 | ds_loss: 3.9845 | lr: 8.0105e-07 | scale:    16.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1632/  5202 | global iter:   1632/  5202 | loss: 3.1016 | ds_loss: 3.9534 | lr: 8.0083e-07 | scale:    16.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1632/  5202 | global iter:   1632/  5202 | loss: 2.9176 | ds_loss: 3.6846 | lr: 8.0083e-07 | scale:    16.0000 | micro time: 0.311 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1633/  5202 | global iter:   1633/  5202 | loss: 2.8966 | ds_loss: 3.5470 | lr: 8.0060e-07 | scale:    16.0000 | micro time: 0.320 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1634/  5202 | global iter:   1634/  5202 | loss: 2.5723 | ds_loss: 3.2798 | lr: 8.0037e-07 | scale:    16.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1635/  5202 | global iter:   1635/  5202 | loss: 2.6116 | ds_loss: 3.4636 | lr: 8.0015e-07 | scale:    16.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1636/  5202 | global iter:   1636/  5202 | loss: 3.0138 | ds_loss: 3.6538 | lr: 7.9992e-07 | scale:    16.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1636/  5202 | global iter:   1636/  5202 | loss: 2.7736 | ds_loss: 3.4860 | lr: 7.9992e-07 | scale:    16.0000 | micro time: 0.307 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1637/  5202 | global iter:   1637/  5202 | loss: 3.0044 | ds_loss: 3.6135 | lr: 7.9970e-07 | scale:    16.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1638/  5202 | global iter:   1638/  5202 | loss: 2.7721 | ds_loss: 3.5392 | lr: 7.9947e-07 | scale:    16.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1639/  5202 | global iter:   1639/  5202 | loss: 1.1908 | ds_loss: 1.4132 | lr: 7.9924e-07 | scale:    16.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1640/  5202 | global iter:   1640/  5202 | loss: 3.0755 | ds_loss: 3.8242 | lr: 7.9902e-07 | scale:    16.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1640/  5202 | global iter:   1640/  5202 | loss: 2.5107 | ds_loss: 3.0975 | lr: 7.9902e-07 | scale:    16.0000 | micro time: 0.311 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1641/  5202 | global iter:   1641/  5202 | loss: 0.8687 | ds_loss: 1.3158 | lr: 7.9879e-07 | scale:    16.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1642/  5202 | global iter:   1642/  5202 | loss: 2.3451 | ds_loss: 3.4885 | lr: 7.9856e-07 | scale:    16.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1643/  5202 | global iter:   1643/  5202 | loss: 2.5389 | ds_loss: 3.2067 | lr: 7.9834e-07 | scale:    16.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1644/  5202 | global iter:   1644/  5202 | loss: 2.9013 | ds_loss: 3.8200 | lr: 7.9811e-07 | scale:    16.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1644/  5202 | global iter:   1644/  5202 | loss: 2.1635 | ds_loss: 2.9578 | lr: 7.9811e-07 | scale:    16.0000 | micro time: 0.307 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1645/  5202 | global iter:   1645/  5202 | loss: 3.1035 | ds_loss: 3.9742 | lr: 7.9788e-07 | scale:    16.0000 | micro time: 0.308 | step time: 0.000\n",
            "[2025-04-16 21:10:57,628] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16, reducing to 8\n",
            "train | epoch   0 | Iter:   1646/  5202 | global iter:   1646/  5202 | loss: 2.5000 | ds_loss: 3.4372 | lr: 7.9788e-07 | scale:     8.0000 | micro time: 0.234 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1647/  5202 | global iter:   1647/  5202 | loss: 1.3193 | ds_loss: 1.8941 | lr: 7.9766e-07 | scale:     8.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1648/  5202 | global iter:   1648/  5202 | loss: 3.1579 | ds_loss: 3.9645 | lr: 7.9743e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1648/  5202 | global iter:   1648/  5202 | loss: 2.5201 | ds_loss: 3.3175 | lr: 7.9743e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.288\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1649/  5202 | global iter:   1649/  5202 | loss: 1.3215 | ds_loss: 2.1544 | lr: 7.9720e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1650/  5202 | global iter:   1650/  5202 | loss: 1.4100 | ds_loss: 2.0489 | lr: 7.9698e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1651/  5202 | global iter:   1651/  5202 | loss: 2.0935 | ds_loss: 2.6389 | lr: 7.9675e-07 | scale:     8.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1652/  5202 | global iter:   1652/  5202 | loss: 2.6427 | ds_loss: 3.3942 | lr: 7.9652e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1652/  5202 | global iter:   1652/  5202 | loss: 1.8669 | ds_loss: 2.5591 | lr: 7.9652e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1653/  5202 | global iter:   1653/  5202 | loss: 1.9742 | ds_loss: 3.2800 | lr: 7.9629e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1654/  5202 | global iter:   1654/  5202 | loss: 1.9511 | ds_loss: 2.7503 | lr: 7.9607e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1655/  5202 | global iter:   1655/  5202 | loss: 2.0698 | ds_loss: 3.0357 | lr: 7.9584e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1656/  5202 | global iter:   1656/  5202 | loss: 2.7338 | ds_loss: 3.7866 | lr: 7.9561e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1656/  5202 | global iter:   1656/  5202 | loss: 2.1822 | ds_loss: 3.2131 | lr: 7.9561e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1657/  5202 | global iter:   1657/  5202 | loss: 2.5140 | ds_loss: 2.9973 | lr: 7.9538e-07 | scale:     8.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1658/  5202 | global iter:   1658/  5202 | loss: 1.9510 | ds_loss: 2.5082 | lr: 7.9516e-07 | scale:     8.0000 | micro time: 0.316 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1659/  5202 | global iter:   1659/  5202 | loss: 1.8224 | ds_loss: 2.7768 | lr: 7.9493e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1660/  5202 | global iter:   1660/  5202 | loss: 2.7519 | ds_loss: 3.3929 | lr: 7.9470e-07 | scale:     8.0000 | micro time: 0.316 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1660/  5202 | global iter:   1660/  5202 | loss: 2.2598 | ds_loss: 2.9188 | lr: 7.9470e-07 | scale:     8.0000 | micro time: 0.316 | step time: 0.314\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1661/  5202 | global iter:   1661/  5202 | loss: 2.6300 | ds_loss: 3.1353 | lr: 7.9447e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1662/  5202 | global iter:   1662/  5202 | loss: 2.7401 | ds_loss: 3.4476 | lr: 7.9424e-07 | scale:     8.0000 | micro time: 0.318 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1663/  5202 | global iter:   1663/  5202 | loss: 2.6720 | ds_loss: 3.4437 | lr: 7.9402e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1664/  5202 | global iter:   1664/  5202 | loss: 2.8974 | ds_loss: 3.5498 | lr: 7.9379e-07 | scale:     8.0000 | micro time: 0.314 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1664/  5202 | global iter:   1664/  5202 | loss: 2.7349 | ds_loss: 3.3941 | lr: 7.9379e-07 | scale:     8.0000 | micro time: 0.314 | step time: 0.313\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1665/  5202 | global iter:   1665/  5202 | loss: 3.0200 | ds_loss: 3.7137 | lr: 7.9356e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1666/  5202 | global iter:   1666/  5202 | loss: 2.6414 | ds_loss: 3.9382 | lr: 7.9333e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1667/  5202 | global iter:   1667/  5202 | loss: 2.8175 | ds_loss: 3.7674 | lr: 7.9310e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1668/  5202 | global iter:   1668/  5202 | loss: 1.5677 | ds_loss: 2.4036 | lr: 7.9287e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1668/  5202 | global iter:   1668/  5202 | loss: 2.5117 | ds_loss: 3.4557 | lr: 7.9287e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1669/  5202 | global iter:   1669/  5202 | loss: 1.7749 | ds_loss: 2.5537 | lr: 7.9264e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1670/  5202 | global iter:   1670/  5202 | loss: 2.7461 | ds_loss: 3.5911 | lr: 7.9241e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1671/  5202 | global iter:   1671/  5202 | loss: 2.9858 | ds_loss: 3.6550 | lr: 7.9219e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1672/  5202 | global iter:   1672/  5202 | loss: 1.2108 | ds_loss: 1.6931 | lr: 7.9196e-07 | scale:     8.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1672/  5202 | global iter:   1672/  5202 | loss: 2.1794 | ds_loss: 2.8732 | lr: 7.9196e-07 | scale:     8.0000 | micro time: 0.303 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1673/  5202 | global iter:   1673/  5202 | loss: 3.2478 | ds_loss: 4.0252 | lr: 7.9173e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1674/  5202 | global iter:   1674/  5202 | loss: 2.8454 | ds_loss: 4.0204 | lr: 7.9150e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1675/  5202 | global iter:   1675/  5202 | loss: 2.9242 | ds_loss: 3.5900 | lr: 7.9127e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1676/  5202 | global iter:   1676/  5202 | loss: 0.8303 | ds_loss: 1.3169 | lr: 7.9104e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1676/  5202 | global iter:   1676/  5202 | loss: 2.4619 | ds_loss: 3.2381 | lr: 7.9104e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1677/  5202 | global iter:   1677/  5202 | loss: 2.4945 | ds_loss: 3.2448 | lr: 7.9081e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1678/  5202 | global iter:   1678/  5202 | loss: 2.4580 | ds_loss: 3.4817 | lr: 7.9058e-07 | scale:     8.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1679/  5202 | global iter:   1679/  5202 | loss: 2.2660 | ds_loss: 2.9728 | lr: 7.9035e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1680/  5202 | global iter:   1680/  5202 | loss: 1.8959 | ds_loss: 2.3797 | lr: 7.9012e-07 | scale:     8.0000 | micro time: 0.320 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1680/  5202 | global iter:   1680/  5202 | loss: 2.2786 | ds_loss: 3.0198 | lr: 7.9012e-07 | scale:     8.0000 | micro time: 0.320 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1681/  5202 | global iter:   1681/  5202 | loss: 1.4569 | ds_loss: 1.9304 | lr: 7.8989e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1682/  5202 | global iter:   1682/  5202 | loss: 2.8476 | ds_loss: 3.5940 | lr: 7.8966e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1683/  5202 | global iter:   1683/  5202 | loss: 2.5954 | ds_loss: 3.7573 | lr: 7.8943e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1684/  5202 | global iter:   1684/  5202 | loss: 2.0259 | ds_loss: 2.8649 | lr: 7.8920e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1684/  5202 | global iter:   1684/  5202 | loss: 2.2314 | ds_loss: 3.0366 | lr: 7.8920e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1685/  5202 | global iter:   1685/  5202 | loss: 2.9270 | ds_loss: 3.8636 | lr: 7.8897e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1686/  5202 | global iter:   1686/  5202 | loss: 2.9876 | ds_loss: 3.5855 | lr: 7.8874e-07 | scale:     8.0000 | micro time: 0.316 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1687/  5202 | global iter:   1687/  5202 | loss: 1.5060 | ds_loss: 2.2742 | lr: 7.8851e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1688/  5202 | global iter:   1688/  5202 | loss: 2.4063 | ds_loss: 3.3394 | lr: 7.8828e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1688/  5202 | global iter:   1688/  5202 | loss: 2.4567 | ds_loss: 3.2656 | lr: 7.8828e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1689/  5202 | global iter:   1689/  5202 | loss: 2.7072 | ds_loss: 3.5274 | lr: 7.8805e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1690/  5202 | global iter:   1690/  5202 | loss: 2.8393 | ds_loss: 3.9997 | lr: 7.8782e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1691/  5202 | global iter:   1691/  5202 | loss: 2.5234 | ds_loss: 3.5286 | lr: 7.8759e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1692/  5202 | global iter:   1692/  5202 | loss: 1.2895 | ds_loss: 2.0192 | lr: 7.8736e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1692/  5202 | global iter:   1692/  5202 | loss: 2.3398 | ds_loss: 3.2687 | lr: 7.8736e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1693/  5202 | global iter:   1693/  5202 | loss: 2.6374 | ds_loss: 3.3721 | lr: 7.8713e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1694/  5202 | global iter:   1694/  5202 | loss: 2.8161 | ds_loss: 3.5781 | lr: 7.8689e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1695/  5202 | global iter:   1695/  5202 | loss: 2.6041 | ds_loss: 3.3371 | lr: 7.8666e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1696/  5202 | global iter:   1696/  5202 | loss: 2.3785 | ds_loss: 3.0501 | lr: 7.8643e-07 | scale:     8.0000 | micro time: 0.314 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1696/  5202 | global iter:   1696/  5202 | loss: 2.6090 | ds_loss: 3.3344 | lr: 7.8643e-07 | scale:     8.0000 | micro time: 0.314 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1697/  5202 | global iter:   1697/  5202 | loss: 2.7203 | ds_loss: 3.4500 | lr: 7.8620e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1698/  5202 | global iter:   1698/  5202 | loss: 2.7515 | ds_loss: 3.4652 | lr: 7.8597e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1699/  5202 | global iter:   1699/  5202 | loss: 2.8887 | ds_loss: 3.5311 | lr: 7.8574e-07 | scale:     8.0000 | micro time: 0.317 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1700/  5202 | global iter:   1700/  5202 | loss: 2.5599 | ds_loss: 3.4911 | lr: 7.8551e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1700/  5202 | global iter:   1700/  5202 | loss: 2.7301 | ds_loss: 3.4844 | lr: 7.8551e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1701/  5202 | global iter:   1701/  5202 | loss: 2.9127 | ds_loss: 3.7372 | lr: 7.8527e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1702/  5202 | global iter:   1702/  5202 | loss: 2.6587 | ds_loss: 3.3972 | lr: 7.8504e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1703/  5202 | global iter:   1703/  5202 | loss: 1.8506 | ds_loss: 2.5842 | lr: 7.8481e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1704/  5202 | global iter:   1704/  5202 | loss: 2.6258 | ds_loss: 3.3875 | lr: 7.8458e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1704/  5202 | global iter:   1704/  5202 | loss: 2.5120 | ds_loss: 3.2765 | lr: 7.8458e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1705/  5202 | global iter:   1705/  5202 | loss: 2.3822 | ds_loss: 3.4315 | lr: 7.8435e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1706/  5202 | global iter:   1706/  5202 | loss: 2.8043 | ds_loss: 3.4345 | lr: 7.8412e-07 | scale:     8.0000 | micro time: 0.325 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1707/  5202 | global iter:   1707/  5202 | loss: 2.8292 | ds_loss: 3.4558 | lr: 7.8388e-07 | scale:     8.0000 | micro time: 0.317 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1708/  5202 | global iter:   1708/  5202 | loss: 3.0369 | ds_loss: 3.6880 | lr: 7.8365e-07 | scale:     8.0000 | micro time: 0.317 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1708/  5202 | global iter:   1708/  5202 | loss: 2.7631 | ds_loss: 3.5025 | lr: 7.8365e-07 | scale:     8.0000 | micro time: 0.317 | step time: 0.316\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1709/  5202 | global iter:   1709/  5202 | loss: 2.9094 | ds_loss: 3.6627 | lr: 7.8342e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1710/  5202 | global iter:   1710/  5202 | loss: 2.1720 | ds_loss: 3.1190 | lr: 7.8319e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1711/  5202 | global iter:   1711/  5202 | loss: 2.7697 | ds_loss: 3.6606 | lr: 7.8295e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1712/  5202 | global iter:   1712/  5202 | loss: 3.0774 | ds_loss: 4.0152 | lr: 7.8272e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1712/  5202 | global iter:   1712/  5202 | loss: 2.7321 | ds_loss: 3.6144 | lr: 7.8272e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1713/  5202 | global iter:   1713/  5202 | loss: 2.7068 | ds_loss: 3.7306 | lr: 7.8249e-07 | scale:     8.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1714/  5202 | global iter:   1714/  5202 | loss: 1.9796 | ds_loss: 3.1656 | lr: 7.8226e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1715/  5202 | global iter:   1715/  5202 | loss: 2.6265 | ds_loss: 3.6938 | lr: 7.8202e-07 | scale:     8.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1716/  5202 | global iter:   1716/  5202 | loss: 1.9070 | ds_loss: 2.7831 | lr: 7.8179e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1716/  5202 | global iter:   1716/  5202 | loss: 2.3050 | ds_loss: 3.3433 | lr: 7.8179e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.304\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1717/  5202 | global iter:   1717/  5202 | loss: 3.1303 | ds_loss: 3.7573 | lr: 7.8156e-07 | scale:     8.0000 | micro time: 0.318 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1718/  5202 | global iter:   1718/  5202 | loss: 2.9739 | ds_loss: 3.7054 | lr: 7.8132e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1719/  5202 | global iter:   1719/  5202 | loss: 2.7524 | ds_loss: 3.7120 | lr: 7.8109e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1720/  5202 | global iter:   1720/  5202 | loss: 3.0103 | ds_loss: 3.6269 | lr: 7.8086e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1720/  5202 | global iter:   1720/  5202 | loss: 2.9667 | ds_loss: 3.7004 | lr: 7.8086e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.312\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1721/  5202 | global iter:   1721/  5202 | loss: 2.8735 | ds_loss: 3.9372 | lr: 7.8062e-07 | scale:     8.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1722/  5202 | global iter:   1722/  5202 | loss: 1.9116 | ds_loss: 2.4020 | lr: 7.8039e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1723/  5202 | global iter:   1723/  5202 | loss: 2.0276 | ds_loss: 3.0390 | lr: 7.8016e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1724/  5202 | global iter:   1724/  5202 | loss: 3.3324 | ds_loss: 3.9978 | lr: 7.7992e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1724/  5202 | global iter:   1724/  5202 | loss: 2.5363 | ds_loss: 3.3440 | lr: 7.7992e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1725/  5202 | global iter:   1725/  5202 | loss: 1.1119 | ds_loss: 1.6527 | lr: 7.7969e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1726/  5202 | global iter:   1726/  5202 | loss: 3.0865 | ds_loss: 4.0923 | lr: 7.7946e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1727/  5202 | global iter:   1727/  5202 | loss: 2.8797 | ds_loss: 3.8847 | lr: 7.7922e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1728/  5202 | global iter:   1728/  5202 | loss: 2.6739 | ds_loss: 3.3718 | lr: 7.7899e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1728/  5202 | global iter:   1728/  5202 | loss: 2.4380 | ds_loss: 3.2504 | lr: 7.7899e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1729/  5202 | global iter:   1729/  5202 | loss: 3.0223 | ds_loss: 3.7659 | lr: 7.7876e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1730/  5202 | global iter:   1730/  5202 | loss: 2.6775 | ds_loss: 3.2362 | lr: 7.7852e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1731/  5202 | global iter:   1731/  5202 | loss: 2.9953 | ds_loss: 3.7058 | lr: 7.7829e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1732/  5202 | global iter:   1732/  5202 | loss: 2.8198 | ds_loss: 3.7498 | lr: 7.7805e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1732/  5202 | global iter:   1732/  5202 | loss: 2.8787 | ds_loss: 3.6144 | lr: 7.7805e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1733/  5202 | global iter:   1733/  5202 | loss: 1.1061 | ds_loss: 1.6316 | lr: 7.7782e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1734/  5202 | global iter:   1734/  5202 | loss: 2.7781 | ds_loss: 3.7043 | lr: 7.7758e-07 | scale:     8.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1735/  5202 | global iter:   1735/  5202 | loss: 1.8967 | ds_loss: 2.6298 | lr: 7.7735e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1736/  5202 | global iter:   1736/  5202 | loss: 2.9541 | ds_loss: 3.7384 | lr: 7.7711e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1736/  5202 | global iter:   1736/  5202 | loss: 2.1838 | ds_loss: 2.9260 | lr: 7.7711e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1737/  5202 | global iter:   1737/  5202 | loss: 2.1296 | ds_loss: 3.1453 | lr: 7.7688e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1738/  5202 | global iter:   1738/  5202 | loss: 2.4493 | ds_loss: 3.3791 | lr: 7.7665e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1739/  5202 | global iter:   1739/  5202 | loss: 1.5250 | ds_loss: 2.1100 | lr: 7.7641e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1740/  5202 | global iter:   1740/  5202 | loss: 2.6464 | ds_loss: 3.5659 | lr: 7.7618e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1740/  5202 | global iter:   1740/  5202 | loss: 2.1876 | ds_loss: 3.0501 | lr: 7.7618e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1741/  5202 | global iter:   1741/  5202 | loss: 2.2176 | ds_loss: 2.9277 | lr: 7.7594e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1742/  5202 | global iter:   1742/  5202 | loss: 0.9692 | ds_loss: 1.4520 | lr: 7.7571e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1743/  5202 | global iter:   1743/  5202 | loss: 2.2638 | ds_loss: 3.2128 | lr: 7.7547e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1744/  5202 | global iter:   1744/  5202 | loss: 1.5850 | ds_loss: 2.3616 | lr: 7.7524e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1744/  5202 | global iter:   1744/  5202 | loss: 1.7589 | ds_loss: 2.4885 | lr: 7.7524e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1745/  5202 | global iter:   1745/  5202 | loss: 3.0343 | ds_loss: 3.8160 | lr: 7.7500e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1746/  5202 | global iter:   1746/  5202 | loss: 2.9391 | ds_loss: 3.6250 | lr: 7.7476e-07 | scale:     8.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1747/  5202 | global iter:   1747/  5202 | loss: 1.2559 | ds_loss: 1.8042 | lr: 7.7453e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1748/  5202 | global iter:   1748/  5202 | loss: 2.1471 | ds_loss: 2.8552 | lr: 7.7429e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1748/  5202 | global iter:   1748/  5202 | loss: 2.3441 | ds_loss: 3.0251 | lr: 7.7429e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1749/  5202 | global iter:   1749/  5202 | loss: 3.0019 | ds_loss: 4.1518 | lr: 7.7406e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1750/  5202 | global iter:   1750/  5202 | loss: 2.5992 | ds_loss: 3.8182 | lr: 7.7382e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1751/  5202 | global iter:   1751/  5202 | loss: 2.2834 | ds_loss: 2.9282 | lr: 7.7359e-07 | scale:     8.0000 | micro time: 0.320 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1752/  5202 | global iter:   1752/  5202 | loss: 3.0604 | ds_loss: 4.1620 | lr: 7.7335e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1752/  5202 | global iter:   1752/  5202 | loss: 2.7362 | ds_loss: 3.7650 | lr: 7.7335e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1753/  5202 | global iter:   1753/  5202 | loss: 1.0196 | ds_loss: 1.3345 | lr: 7.7311e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1754/  5202 | global iter:   1754/  5202 | loss: 1.3103 | ds_loss: 1.6679 | lr: 7.7288e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1755/  5202 | global iter:   1755/  5202 | loss: 2.0206 | ds_loss: 2.5717 | lr: 7.7264e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1756/  5202 | global iter:   1756/  5202 | loss: 2.6129 | ds_loss: 3.3763 | lr: 7.7241e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1756/  5202 | global iter:   1756/  5202 | loss: 1.7408 | ds_loss: 2.2376 | lr: 7.7241e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1757/  5202 | global iter:   1757/  5202 | loss: 2.4883 | ds_loss: 3.7638 | lr: 7.7217e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1758/  5202 | global iter:   1758/  5202 | loss: 0.8692 | ds_loss: 1.3970 | lr: 7.7193e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1759/  5202 | global iter:   1759/  5202 | loss: 0.6294 | ds_loss: 1.0026 | lr: 7.7170e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1760/  5202 | global iter:   1760/  5202 | loss: 2.8152 | ds_loss: 3.4696 | lr: 7.7146e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1760/  5202 | global iter:   1760/  5202 | loss: 1.7005 | ds_loss: 2.4082 | lr: 7.7146e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1761/  5202 | global iter:   1761/  5202 | loss: 2.7890 | ds_loss: 3.3721 | lr: 7.7122e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1762/  5202 | global iter:   1762/  5202 | loss: 1.3843 | ds_loss: 1.9402 | lr: 7.7099e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1763/  5202 | global iter:   1763/  5202 | loss: 2.9070 | ds_loss: 3.7320 | lr: 7.7075e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1764/  5202 | global iter:   1764/  5202 | loss: 2.7824 | ds_loss: 3.4610 | lr: 7.7051e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1764/  5202 | global iter:   1764/  5202 | loss: 2.4657 | ds_loss: 3.1263 | lr: 7.7051e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1765/  5202 | global iter:   1765/  5202 | loss: 2.6506 | ds_loss: 3.6105 | lr: 7.7028e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1766/  5202 | global iter:   1766/  5202 | loss: 2.1371 | ds_loss: 2.8777 | lr: 7.7004e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1767/  5202 | global iter:   1767/  5202 | loss: 2.6089 | ds_loss: 3.5790 | lr: 7.6980e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1768/  5202 | global iter:   1768/  5202 | loss: 2.7621 | ds_loss: 4.0244 | lr: 7.6957e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1768/  5202 | global iter:   1768/  5202 | loss: 2.5396 | ds_loss: 3.5229 | lr: 7.6957e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1769/  5202 | global iter:   1769/  5202 | loss: 2.2048 | ds_loss: 3.2915 | lr: 7.6933e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1770/  5202 | global iter:   1770/  5202 | loss: 2.1117 | ds_loss: 2.9349 | lr: 7.6909e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1771/  5202 | global iter:   1771/  5202 | loss: 2.7858 | ds_loss: 3.4801 | lr: 7.6885e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1772/  5202 | global iter:   1772/  5202 | loss: 1.8731 | ds_loss: 2.8580 | lr: 7.6862e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1772/  5202 | global iter:   1772/  5202 | loss: 2.2438 | ds_loss: 3.1411 | lr: 7.6862e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1773/  5202 | global iter:   1773/  5202 | loss: 2.7489 | ds_loss: 3.7722 | lr: 7.6838e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1774/  5202 | global iter:   1774/  5202 | loss: 2.4457 | ds_loss: 3.6602 | lr: 7.6814e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1775/  5202 | global iter:   1775/  5202 | loss: 2.6997 | ds_loss: 3.5041 | lr: 7.6790e-07 | scale:     8.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1776/  5202 | global iter:   1776/  5202 | loss: 1.8685 | ds_loss: 2.3635 | lr: 7.6767e-07 | scale:     8.0000 | micro time: 0.318 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1776/  5202 | global iter:   1776/  5202 | loss: 2.4407 | ds_loss: 3.3250 | lr: 7.6767e-07 | scale:     8.0000 | micro time: 0.318 | step time: 0.313\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1777/  5202 | global iter:   1777/  5202 | loss: 2.8106 | ds_loss: 3.7294 | lr: 7.6743e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1778/  5202 | global iter:   1778/  5202 | loss: 3.0008 | ds_loss: 4.0337 | lr: 7.6719e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1779/  5202 | global iter:   1779/  5202 | loss: 2.9733 | ds_loss: 3.7896 | lr: 7.6695e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1780/  5202 | global iter:   1780/  5202 | loss: 2.4133 | ds_loss: 3.4582 | lr: 7.6671e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1780/  5202 | global iter:   1780/  5202 | loss: 2.7995 | ds_loss: 3.7527 | lr: 7.6671e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1781/  5202 | global iter:   1781/  5202 | loss: 1.9660 | ds_loss: 2.9416 | lr: 7.6647e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1782/  5202 | global iter:   1782/  5202 | loss: 1.0934 | ds_loss: 1.7204 | lr: 7.6624e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1783/  5202 | global iter:   1783/  5202 | loss: 2.8902 | ds_loss: 3.7724 | lr: 7.6600e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1784/  5202 | global iter:   1784/  5202 | loss: 2.8930 | ds_loss: 3.6199 | lr: 7.6576e-07 | scale:     8.0000 | micro time: 0.321 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1784/  5202 | global iter:   1784/  5202 | loss: 2.2106 | ds_loss: 3.0136 | lr: 7.6576e-07 | scale:     8.0000 | micro time: 0.321 | step time: 0.312\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1785/  5202 | global iter:   1785/  5202 | loss: 2.7744 | ds_loss: 3.5540 | lr: 7.6552e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1786/  5202 | global iter:   1786/  5202 | loss: 2.8403 | ds_loss: 3.5389 | lr: 7.6528e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1787/  5202 | global iter:   1787/  5202 | loss: 2.7419 | ds_loss: 3.8417 | lr: 7.6504e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1788/  5202 | global iter:   1788/  5202 | loss: 2.5105 | ds_loss: 3.2264 | lr: 7.6481e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1788/  5202 | global iter:   1788/  5202 | loss: 2.7168 | ds_loss: 3.5402 | lr: 7.6481e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1789/  5202 | global iter:   1789/  5202 | loss: 3.2630 | ds_loss: 3.9610 | lr: 7.6457e-07 | scale:     8.0000 | micro time: 0.316 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1790/  5202 | global iter:   1790/  5202 | loss: 1.7093 | ds_loss: 2.4915 | lr: 7.6433e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1791/  5202 | global iter:   1791/  5202 | loss: 2.7248 | ds_loss: 3.7138 | lr: 7.6409e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1792/  5202 | global iter:   1792/  5202 | loss: 2.7455 | ds_loss: 3.0685 | lr: 7.6385e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1792/  5202 | global iter:   1792/  5202 | loss: 2.6107 | ds_loss: 3.3087 | lr: 7.6385e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1793/  5202 | global iter:   1793/  5202 | loss: 3.1049 | ds_loss: 3.8224 | lr: 7.6361e-07 | scale:     8.0000 | micro time: 0.317 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1794/  5202 | global iter:   1794/  5202 | loss: 2.6984 | ds_loss: 3.4324 | lr: 7.6337e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1795/  5202 | global iter:   1795/  5202 | loss: 2.7700 | ds_loss: 3.7121 | lr: 7.6313e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1796/  5202 | global iter:   1796/  5202 | loss: 2.4280 | ds_loss: 4.2441 | lr: 7.6289e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1796/  5202 | global iter:   1796/  5202 | loss: 2.7503 | ds_loss: 3.8028 | lr: 7.6289e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1797/  5202 | global iter:   1797/  5202 | loss: 2.5657 | ds_loss: 3.2471 | lr: 7.6265e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1798/  5202 | global iter:   1798/  5202 | loss: 3.0553 | ds_loss: 3.7436 | lr: 7.6241e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1799/  5202 | global iter:   1799/  5202 | loss: 3.0485 | ds_loss: 3.6774 | lr: 7.6217e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1800/  5202 | global iter:   1800/  5202 | loss: 1.7005 | ds_loss: 2.1340 | lr: 7.6193e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1800/  5202 | global iter:   1800/  5202 | loss: 2.5925 | ds_loss: 3.2005 | lr: 7.6193e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1801/  5202 | global iter:   1801/  5202 | loss: 2.9077 | ds_loss: 3.7328 | lr: 7.6169e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1802/  5202 | global iter:   1802/  5202 | loss: 2.8082 | ds_loss: 3.6635 | lr: 7.6145e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1803/  5202 | global iter:   1803/  5202 | loss: 3.1331 | ds_loss: 3.8387 | lr: 7.6121e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1804/  5202 | global iter:   1804/  5202 | loss: 2.5870 | ds_loss: 3.3775 | lr: 7.6097e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1804/  5202 | global iter:   1804/  5202 | loss: 2.8590 | ds_loss: 3.6531 | lr: 7.6097e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1805/  5202 | global iter:   1805/  5202 | loss: 2.3793 | ds_loss: 3.3311 | lr: 7.6073e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1806/  5202 | global iter:   1806/  5202 | loss: 2.9095 | ds_loss: 3.8040 | lr: 7.6049e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1807/  5202 | global iter:   1807/  5202 | loss: 2.7539 | ds_loss: 3.4949 | lr: 7.6025e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1808/  5202 | global iter:   1808/  5202 | loss: 2.3577 | ds_loss: 3.1519 | lr: 7.6001e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1808/  5202 | global iter:   1808/  5202 | loss: 2.6001 | ds_loss: 3.4455 | lr: 7.6001e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1809/  5202 | global iter:   1809/  5202 | loss: 1.8271 | ds_loss: 2.6793 | lr: 7.5977e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1810/  5202 | global iter:   1810/  5202 | loss: 1.9099 | ds_loss: 2.9056 | lr: 7.5953e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1811/  5202 | global iter:   1811/  5202 | loss: 2.7016 | ds_loss: 3.5883 | lr: 7.5929e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1812/  5202 | global iter:   1812/  5202 | loss: 2.9105 | ds_loss: 3.7004 | lr: 7.5905e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1812/  5202 | global iter:   1812/  5202 | loss: 2.3373 | ds_loss: 3.2184 | lr: 7.5905e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1813/  5202 | global iter:   1813/  5202 | loss: 1.2908 | ds_loss: 1.8058 | lr: 7.5881e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1814/  5202 | global iter:   1814/  5202 | loss: 2.9222 | ds_loss: 3.5710 | lr: 7.5857e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1815/  5202 | global iter:   1815/  5202 | loss: 2.5370 | ds_loss: 3.2279 | lr: 7.5833e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1816/  5202 | global iter:   1816/  5202 | loss: 2.4933 | ds_loss: 3.2605 | lr: 7.5809e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1816/  5202 | global iter:   1816/  5202 | loss: 2.3108 | ds_loss: 2.9663 | lr: 7.5809e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1817/  5202 | global iter:   1817/  5202 | loss: 2.9450 | ds_loss: 3.5333 | lr: 7.5785e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1818/  5202 | global iter:   1818/  5202 | loss: 2.6193 | ds_loss: 3.7129 | lr: 7.5761e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1819/  5202 | global iter:   1819/  5202 | loss: 2.8966 | ds_loss: 3.7881 | lr: 7.5736e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1820/  5202 | global iter:   1820/  5202 | loss: 2.6910 | ds_loss: 3.4409 | lr: 7.5712e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1820/  5202 | global iter:   1820/  5202 | loss: 2.7880 | ds_loss: 3.6188 | lr: 7.5712e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1821/  5202 | global iter:   1821/  5202 | loss: 2.4120 | ds_loss: 3.0751 | lr: 7.5688e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1822/  5202 | global iter:   1822/  5202 | loss: 2.8775 | ds_loss: 3.6215 | lr: 7.5664e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1823/  5202 | global iter:   1823/  5202 | loss: 2.7465 | ds_loss: 3.6573 | lr: 7.5640e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1824/  5202 | global iter:   1824/  5202 | loss: 2.7486 | ds_loss: 3.7844 | lr: 7.5616e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1824/  5202 | global iter:   1824/  5202 | loss: 2.6961 | ds_loss: 3.5346 | lr: 7.5616e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1825/  5202 | global iter:   1825/  5202 | loss: 1.9296 | ds_loss: 3.2225 | lr: 7.5592e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1826/  5202 | global iter:   1826/  5202 | loss: 2.5704 | ds_loss: 3.3775 | lr: 7.5567e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1827/  5202 | global iter:   1827/  5202 | loss: 2.9308 | ds_loss: 4.0036 | lr: 7.5543e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1828/  5202 | global iter:   1828/  5202 | loss: 2.6809 | ds_loss: 3.4235 | lr: 7.5519e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1828/  5202 | global iter:   1828/  5202 | loss: 2.5279 | ds_loss: 3.5068 | lr: 7.5519e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1829/  5202 | global iter:   1829/  5202 | loss: 2.3243 | ds_loss: 3.3747 | lr: 7.5495e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1830/  5202 | global iter:   1830/  5202 | loss: 1.9291 | ds_loss: 2.3359 | lr: 7.5471e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1831/  5202 | global iter:   1831/  5202 | loss: 2.8775 | ds_loss: 3.5916 | lr: 7.5447e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1832/  5202 | global iter:   1832/  5202 | loss: 1.8181 | ds_loss: 2.7939 | lr: 7.5422e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1832/  5202 | global iter:   1832/  5202 | loss: 2.2372 | ds_loss: 3.0240 | lr: 7.5422e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1833/  5202 | global iter:   1833/  5202 | loss: 1.4131 | ds_loss: 2.1362 | lr: 7.5398e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1834/  5202 | global iter:   1834/  5202 | loss: 2.5897 | ds_loss: 3.5221 | lr: 7.5374e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1835/  5202 | global iter:   1835/  5202 | loss: 1.6236 | ds_loss: 2.3028 | lr: 7.5350e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1836/  5202 | global iter:   1836/  5202 | loss: 1.7120 | ds_loss: 2.8706 | lr: 7.5325e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1836/  5202 | global iter:   1836/  5202 | loss: 1.8346 | ds_loss: 2.7079 | lr: 7.5325e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1837/  5202 | global iter:   1837/  5202 | loss: 2.8774 | ds_loss: 3.6682 | lr: 7.5301e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1838/  5202 | global iter:   1838/  5202 | loss: 2.7725 | ds_loss: 3.4553 | lr: 7.5277e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1839/  5202 | global iter:   1839/  5202 | loss: 2.6019 | ds_loss: 3.4768 | lr: 7.5253e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1840/  5202 | global iter:   1840/  5202 | loss: 2.9324 | ds_loss: 3.7604 | lr: 7.5228e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1840/  5202 | global iter:   1840/  5202 | loss: 2.7960 | ds_loss: 3.5901 | lr: 7.5228e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1841/  5202 | global iter:   1841/  5202 | loss: 2.6297 | ds_loss: 3.4641 | lr: 7.5204e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1842/  5202 | global iter:   1842/  5202 | loss: 2.9377 | ds_loss: 3.7954 | lr: 7.5180e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1843/  5202 | global iter:   1843/  5202 | loss: 2.9890 | ds_loss: 3.5915 | lr: 7.5155e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1844/  5202 | global iter:   1844/  5202 | loss: 3.0057 | ds_loss: 3.7226 | lr: 7.5131e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1844/  5202 | global iter:   1844/  5202 | loss: 2.8905 | ds_loss: 3.6434 | lr: 7.5131e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1845/  5202 | global iter:   1845/  5202 | loss: 2.2469 | ds_loss: 2.9628 | lr: 7.5107e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1846/  5202 | global iter:   1846/  5202 | loss: 2.9347 | ds_loss: 3.6008 | lr: 7.5083e-07 | scale:     8.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1847/  5202 | global iter:   1847/  5202 | loss: 2.9054 | ds_loss: 3.8621 | lr: 7.5058e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1848/  5202 | global iter:   1848/  5202 | loss: 2.4778 | ds_loss: 3.2002 | lr: 7.5034e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1848/  5202 | global iter:   1848/  5202 | loss: 2.6412 | ds_loss: 3.4065 | lr: 7.5034e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1849/  5202 | global iter:   1849/  5202 | loss: 1.5672 | ds_loss: 2.1304 | lr: 7.5010e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1850/  5202 | global iter:   1850/  5202 | loss: 2.7105 | ds_loss: 3.3727 | lr: 7.4985e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1851/  5202 | global iter:   1851/  5202 | loss: 2.6462 | ds_loss: 3.3349 | lr: 7.4961e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1852/  5202 | global iter:   1852/  5202 | loss: 2.7425 | ds_loss: 3.5611 | lr: 7.4936e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1852/  5202 | global iter:   1852/  5202 | loss: 2.4166 | ds_loss: 3.0998 | lr: 7.4936e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1853/  5202 | global iter:   1853/  5202 | loss: 2.1519 | ds_loss: 3.3485 | lr: 7.4912e-07 | scale:     8.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1854/  5202 | global iter:   1854/  5202 | loss: 2.4685 | ds_loss: 3.4423 | lr: 7.4888e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1855/  5202 | global iter:   1855/  5202 | loss: 2.6540 | ds_loss: 3.2238 | lr: 7.4863e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1856/  5202 | global iter:   1856/  5202 | loss: 1.2126 | ds_loss: 2.0396 | lr: 7.4839e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1856/  5202 | global iter:   1856/  5202 | loss: 2.1218 | ds_loss: 3.0135 | lr: 7.4839e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1857/  5202 | global iter:   1857/  5202 | loss: 2.9126 | ds_loss: 3.8250 | lr: 7.4815e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1858/  5202 | global iter:   1858/  5202 | loss: 2.2780 | ds_loss: 2.7954 | lr: 7.4790e-07 | scale:     8.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1859/  5202 | global iter:   1859/  5202 | loss: 2.9057 | ds_loss: 4.1133 | lr: 7.4766e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1860/  5202 | global iter:   1860/  5202 | loss: 1.6612 | ds_loss: 2.3581 | lr: 7.4741e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1860/  5202 | global iter:   1860/  5202 | loss: 2.4394 | ds_loss: 3.2729 | lr: 7.4741e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1861/  5202 | global iter:   1861/  5202 | loss: 1.5453 | ds_loss: 2.1507 | lr: 7.4717e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1862/  5202 | global iter:   1862/  5202 | loss: 1.1266 | ds_loss: 1.6363 | lr: 7.4692e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1863/  5202 | global iter:   1863/  5202 | loss: 3.0172 | ds_loss: 3.7086 | lr: 7.4668e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1864/  5202 | global iter:   1864/  5202 | loss: 1.4704 | ds_loss: 2.2767 | lr: 7.4644e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1864/  5202 | global iter:   1864/  5202 | loss: 1.7899 | ds_loss: 2.4431 | lr: 7.4644e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1865/  5202 | global iter:   1865/  5202 | loss: 2.9895 | ds_loss: 3.6290 | lr: 7.4619e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1866/  5202 | global iter:   1866/  5202 | loss: 2.7973 | ds_loss: 3.5927 | lr: 7.4595e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1867/  5202 | global iter:   1867/  5202 | loss: 2.4231 | ds_loss: 3.5336 | lr: 7.4570e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1868/  5202 | global iter:   1868/  5202 | loss: 2.4233 | ds_loss: 3.2648 | lr: 7.4546e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1868/  5202 | global iter:   1868/  5202 | loss: 2.6583 | ds_loss: 3.5050 | lr: 7.4546e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1869/  5202 | global iter:   1869/  5202 | loss: 2.3543 | ds_loss: 3.4910 | lr: 7.4521e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1870/  5202 | global iter:   1870/  5202 | loss: 2.1595 | ds_loss: 3.3167 | lr: 7.4497e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1871/  5202 | global iter:   1871/  5202 | loss: 2.4021 | ds_loss: 2.9589 | lr: 7.4472e-07 | scale:     8.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1872/  5202 | global iter:   1872/  5202 | loss: 3.0343 | ds_loss: 3.6681 | lr: 7.4448e-07 | scale:     8.0000 | micro time: 0.315 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1872/  5202 | global iter:   1872/  5202 | loss: 2.4876 | ds_loss: 3.3586 | lr: 7.4448e-07 | scale:     8.0000 | micro time: 0.315 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1873/  5202 | global iter:   1873/  5202 | loss: 3.2702 | ds_loss: 3.8757 | lr: 7.4423e-07 | scale:     8.0000 | micro time: 0.316 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1874/  5202 | global iter:   1874/  5202 | loss: 2.7875 | ds_loss: 3.5191 | lr: 7.4399e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1875/  5202 | global iter:   1875/  5202 | loss: 2.7656 | ds_loss: 3.6851 | lr: 7.4374e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1876/  5202 | global iter:   1876/  5202 | loss: 2.7931 | ds_loss: 3.4527 | lr: 7.4350e-07 | scale:     8.0000 | micro time: 0.315 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1876/  5202 | global iter:   1876/  5202 | loss: 2.9041 | ds_loss: 3.6332 | lr: 7.4350e-07 | scale:     8.0000 | micro time: 0.315 | step time: 0.312\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1877/  5202 | global iter:   1877/  5202 | loss: 2.7302 | ds_loss: 3.3499 | lr: 7.4325e-07 | scale:     8.0000 | micro time: 0.323 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1878/  5202 | global iter:   1878/  5202 | loss: 3.0438 | ds_loss: 4.3126 | lr: 7.4301e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1879/  5202 | global iter:   1879/  5202 | loss: 2.7487 | ds_loss: 3.2292 | lr: 7.4276e-07 | scale:     8.0000 | micro time: 0.319 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1880/  5202 | global iter:   1880/  5202 | loss: 2.9625 | ds_loss: 3.7915 | lr: 7.4251e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1880/  5202 | global iter:   1880/  5202 | loss: 2.8713 | ds_loss: 3.6708 | lr: 7.4251e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.315\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1881/  5202 | global iter:   1881/  5202 | loss: 0.7726 | ds_loss: 1.0990 | lr: 7.4227e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1882/  5202 | global iter:   1882/  5202 | loss: 3.4764 | ds_loss: 4.5026 | lr: 7.4202e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1883/  5202 | global iter:   1883/  5202 | loss: 2.9905 | ds_loss: 4.3921 | lr: 7.4178e-07 | scale:     8.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1884/  5202 | global iter:   1884/  5202 | loss: 3.0919 | ds_loss: 3.7142 | lr: 7.4153e-07 | scale:     8.0000 | micro time: 0.318 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1884/  5202 | global iter:   1884/  5202 | loss: 2.5829 | ds_loss: 3.4269 | lr: 7.4153e-07 | scale:     8.0000 | micro time: 0.318 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1885/  5202 | global iter:   1885/  5202 | loss: 1.7480 | ds_loss: 2.5916 | lr: 7.4129e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1886/  5202 | global iter:   1886/  5202 | loss: 2.5243 | ds_loss: 3.2075 | lr: 7.4104e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1887/  5202 | global iter:   1887/  5202 | loss: 2.5851 | ds_loss: 3.5798 | lr: 7.4079e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1888/  5202 | global iter:   1888/  5202 | loss: 2.3748 | ds_loss: 2.9410 | lr: 7.4055e-07 | scale:     8.0000 | micro time: 0.315 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1888/  5202 | global iter:   1888/  5202 | loss: 2.3081 | ds_loss: 3.0800 | lr: 7.4055e-07 | scale:     8.0000 | micro time: 0.315 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1889/  5202 | global iter:   1889/  5202 | loss: 2.6501 | ds_loss: 4.2257 | lr: 7.4030e-07 | scale:     8.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1890/  5202 | global iter:   1890/  5202 | loss: 1.4608 | ds_loss: 1.9310 | lr: 7.4006e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1891/  5202 | global iter:   1891/  5202 | loss: 2.4431 | ds_loss: 3.1676 | lr: 7.3981e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1892/  5202 | global iter:   1892/  5202 | loss: 1.4996 | ds_loss: 2.3036 | lr: 7.3956e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1892/  5202 | global iter:   1892/  5202 | loss: 2.0134 | ds_loss: 2.9070 | lr: 7.3956e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1893/  5202 | global iter:   1893/  5202 | loss: 2.0822 | ds_loss: 3.1704 | lr: 7.3932e-07 | scale:     8.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1894/  5202 | global iter:   1894/  5202 | loss: 2.4465 | ds_loss: 3.0239 | lr: 7.3907e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1895/  5202 | global iter:   1895/  5202 | loss: 3.0560 | ds_loss: 3.7861 | lr: 7.3882e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1896/  5202 | global iter:   1896/  5202 | loss: 1.1009 | ds_loss: 1.5791 | lr: 7.3858e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1896/  5202 | global iter:   1896/  5202 | loss: 2.1714 | ds_loss: 2.8899 | lr: 7.3858e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1897/  5202 | global iter:   1897/  5202 | loss: 3.0915 | ds_loss: 3.8000 | lr: 7.3833e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1898/  5202 | global iter:   1898/  5202 | loss: 2.7376 | ds_loss: 4.1246 | lr: 7.3808e-07 | scale:     8.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1899/  5202 | global iter:   1899/  5202 | loss: 2.5196 | ds_loss: 3.6139 | lr: 7.3784e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1900/  5202 | global iter:   1900/  5202 | loss: 2.5590 | ds_loss: 3.2275 | lr: 7.3759e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1900/  5202 | global iter:   1900/  5202 | loss: 2.7269 | ds_loss: 3.6915 | lr: 7.3759e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1901/  5202 | global iter:   1901/  5202 | loss: 2.8783 | ds_loss: 3.9581 | lr: 7.3734e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1902/  5202 | global iter:   1902/  5202 | loss: 1.4420 | ds_loss: 2.0540 | lr: 7.3709e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1903/  5202 | global iter:   1903/  5202 | loss: 2.5387 | ds_loss: 3.2552 | lr: 7.3685e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1904/  5202 | global iter:   1904/  5202 | loss: 2.4362 | ds_loss: 3.2121 | lr: 7.3660e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1904/  5202 | global iter:   1904/  5202 | loss: 2.3238 | ds_loss: 3.1199 | lr: 7.3660e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1905/  5202 | global iter:   1905/  5202 | loss: 2.7806 | ds_loss: 3.4684 | lr: 7.3635e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1906/  5202 | global iter:   1906/  5202 | loss: 2.8733 | ds_loss: 3.5569 | lr: 7.3610e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1907/  5202 | global iter:   1907/  5202 | loss: 2.5666 | ds_loss: 3.3768 | lr: 7.3586e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1908/  5202 | global iter:   1908/  5202 | loss: 2.3851 | ds_loss: 3.1550 | lr: 7.3561e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1908/  5202 | global iter:   1908/  5202 | loss: 2.6514 | ds_loss: 3.3893 | lr: 7.3561e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1909/  5202 | global iter:   1909/  5202 | loss: 3.1114 | ds_loss: 3.9153 | lr: 7.3536e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1910/  5202 | global iter:   1910/  5202 | loss: 2.7588 | ds_loss: 3.5007 | lr: 7.3511e-07 | scale:     8.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1911/  5202 | global iter:   1911/  5202 | loss: 2.8661 | ds_loss: 3.5956 | lr: 7.3487e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1912/  5202 | global iter:   1912/  5202 | loss: 2.6091 | ds_loss: 3.2646 | lr: 7.3462e-07 | scale:     8.0000 | micro time: 0.315 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1912/  5202 | global iter:   1912/  5202 | loss: 2.8363 | ds_loss: 3.5690 | lr: 7.3462e-07 | scale:     8.0000 | micro time: 0.315 | step time: 0.312\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1913/  5202 | global iter:   1913/  5202 | loss: 2.0555 | ds_loss: 2.9899 | lr: 7.3437e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1914/  5202 | global iter:   1914/  5202 | loss: 2.9074 | ds_loss: 3.5795 | lr: 7.3412e-07 | scale:     8.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1915/  5202 | global iter:   1915/  5202 | loss: 3.0012 | ds_loss: 3.6609 | lr: 7.3388e-07 | scale:     8.0000 | micro time: 0.318 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1916/  5202 | global iter:   1916/  5202 | loss: 1.9718 | ds_loss: 2.6275 | lr: 7.3363e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1916/  5202 | global iter:   1916/  5202 | loss: 2.4840 | ds_loss: 3.2144 | lr: 7.3363e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.312\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1917/  5202 | global iter:   1917/  5202 | loss: 2.0232 | ds_loss: 2.7862 | lr: 7.3338e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1918/  5202 | global iter:   1918/  5202 | loss: 2.6935 | ds_loss: 3.5941 | lr: 7.3313e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1919/  5202 | global iter:   1919/  5202 | loss: 2.9094 | ds_loss: 3.8246 | lr: 7.3288e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1920/  5202 | global iter:   1920/  5202 | loss: 0.9652 | ds_loss: 1.4420 | lr: 7.3263e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1920/  5202 | global iter:   1920/  5202 | loss: 2.1478 | ds_loss: 2.9117 | lr: 7.3263e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1921/  5202 | global iter:   1921/  5202 | loss: 2.7621 | ds_loss: 3.8041 | lr: 7.3239e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1922/  5202 | global iter:   1922/  5202 | loss: 3.0319 | ds_loss: 3.7183 | lr: 7.3214e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1923/  5202 | global iter:   1923/  5202 | loss: 2.8782 | ds_loss: 3.6362 | lr: 7.3189e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1924/  5202 | global iter:   1924/  5202 | loss: 2.9117 | ds_loss: 3.6570 | lr: 7.3164e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1924/  5202 | global iter:   1924/  5202 | loss: 2.8960 | ds_loss: 3.7039 | lr: 7.3164e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1925/  5202 | global iter:   1925/  5202 | loss: 2.9911 | ds_loss: 3.7605 | lr: 7.3139e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1926/  5202 | global iter:   1926/  5202 | loss: 3.0382 | ds_loss: 3.6084 | lr: 7.3114e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1927/  5202 | global iter:   1927/  5202 | loss: 2.6916 | ds_loss: 3.5234 | lr: 7.3089e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1928/  5202 | global iter:   1928/  5202 | loss: 2.2400 | ds_loss: 3.0963 | lr: 7.3065e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1928/  5202 | global iter:   1928/  5202 | loss: 2.7402 | ds_loss: 3.4972 | lr: 7.3065e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1929/  5202 | global iter:   1929/  5202 | loss: 2.7172 | ds_loss: 3.6336 | lr: 7.3040e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1930/  5202 | global iter:   1930/  5202 | loss: 2.7505 | ds_loss: 3.4614 | lr: 7.3015e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1931/  5202 | global iter:   1931/  5202 | loss: 2.6575 | ds_loss: 3.2762 | lr: 7.2990e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1932/  5202 | global iter:   1932/  5202 | loss: 1.3938 | ds_loss: 2.1432 | lr: 7.2965e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1932/  5202 | global iter:   1932/  5202 | loss: 2.3797 | ds_loss: 3.1286 | lr: 7.2965e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1933/  5202 | global iter:   1933/  5202 | loss: 3.0655 | ds_loss: 3.7685 | lr: 7.2940e-07 | scale:     8.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1934/  5202 | global iter:   1934/  5202 | loss: 3.2234 | ds_loss: 3.7994 | lr: 7.2915e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1935/  5202 | global iter:   1935/  5202 | loss: 1.9812 | ds_loss: 3.0253 | lr: 7.2890e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1936/  5202 | global iter:   1936/  5202 | loss: 2.2014 | ds_loss: 3.1390 | lr: 7.2865e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1936/  5202 | global iter:   1936/  5202 | loss: 2.6179 | ds_loss: 3.4331 | lr: 7.2865e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1937/  5202 | global iter:   1937/  5202 | loss: 2.8014 | ds_loss: 3.6733 | lr: 7.2840e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1938/  5202 | global iter:   1938/  5202 | loss: 3.0318 | ds_loss: 3.5895 | lr: 7.2815e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1939/  5202 | global iter:   1939/  5202 | loss: 2.3406 | ds_loss: 3.4215 | lr: 7.2790e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1940/  5202 | global iter:   1940/  5202 | loss: 2.9962 | ds_loss: 3.6450 | lr: 7.2765e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1940/  5202 | global iter:   1940/  5202 | loss: 2.7925 | ds_loss: 3.5823 | lr: 7.2765e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1941/  5202 | global iter:   1941/  5202 | loss: 1.4848 | ds_loss: 2.1895 | lr: 7.2740e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1942/  5202 | global iter:   1942/  5202 | loss: 2.8140 | ds_loss: 3.9306 | lr: 7.2715e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1943/  5202 | global iter:   1943/  5202 | loss: 2.9128 | ds_loss: 3.6860 | lr: 7.2690e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1944/  5202 | global iter:   1944/  5202 | loss: 2.9265 | ds_loss: 4.0326 | lr: 7.2665e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1944/  5202 | global iter:   1944/  5202 | loss: 2.5345 | ds_loss: 3.4597 | lr: 7.2665e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1945/  5202 | global iter:   1945/  5202 | loss: 3.2022 | ds_loss: 3.9451 | lr: 7.2640e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1946/  5202 | global iter:   1946/  5202 | loss: 1.4662 | ds_loss: 2.2640 | lr: 7.2615e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1947/  5202 | global iter:   1947/  5202 | loss: 3.0208 | ds_loss: 3.9285 | lr: 7.2590e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1948/  5202 | global iter:   1948/  5202 | loss: 2.7761 | ds_loss: 3.6758 | lr: 7.2565e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1948/  5202 | global iter:   1948/  5202 | loss: 2.6163 | ds_loss: 3.4533 | lr: 7.2565e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1949/  5202 | global iter:   1949/  5202 | loss: 2.1596 | ds_loss: 3.3969 | lr: 7.2540e-07 | scale:     8.0000 | micro time: 0.301 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1950/  5202 | global iter:   1950/  5202 | loss: 2.6838 | ds_loss: 3.7060 | lr: 7.2515e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1951/  5202 | global iter:   1951/  5202 | loss: 1.7105 | ds_loss: 2.2937 | lr: 7.2490e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1952/  5202 | global iter:   1952/  5202 | loss: 2.6925 | ds_loss: 3.3975 | lr: 7.2465e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1952/  5202 | global iter:   1952/  5202 | loss: 2.3116 | ds_loss: 3.1985 | lr: 7.2465e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1953/  5202 | global iter:   1953/  5202 | loss: 2.4124 | ds_loss: 3.0974 | lr: 7.2440e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1954/  5202 | global iter:   1954/  5202 | loss: 2.8244 | ds_loss: 3.5338 | lr: 7.2415e-07 | scale:     8.0000 | micro time: 0.319 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1955/  5202 | global iter:   1955/  5202 | loss: 2.8902 | ds_loss: 3.8247 | lr: 7.2390e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1956/  5202 | global iter:   1956/  5202 | loss: 2.6193 | ds_loss: 3.4668 | lr: 7.2365e-07 | scale:     8.0000 | micro time: 0.315 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1956/  5202 | global iter:   1956/  5202 | loss: 2.6866 | ds_loss: 3.4807 | lr: 7.2365e-07 | scale:     8.0000 | micro time: 0.315 | step time: 0.313\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1957/  5202 | global iter:   1957/  5202 | loss: 1.8864 | ds_loss: 2.5368 | lr: 7.2340e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1958/  5202 | global iter:   1958/  5202 | loss: 2.8304 | ds_loss: 3.6855 | lr: 7.2315e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1959/  5202 | global iter:   1959/  5202 | loss: 2.0771 | ds_loss: 2.8677 | lr: 7.2290e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1960/  5202 | global iter:   1960/  5202 | loss: 2.7144 | ds_loss: 3.4857 | lr: 7.2265e-07 | scale:     8.0000 | micro time: 0.315 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1960/  5202 | global iter:   1960/  5202 | loss: 2.3771 | ds_loss: 3.1439 | lr: 7.2265e-07 | scale:     8.0000 | micro time: 0.315 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1961/  5202 | global iter:   1961/  5202 | loss: 2.7294 | ds_loss: 3.4580 | lr: 7.2240e-07 | scale:     8.0000 | micro time: 0.316 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1962/  5202 | global iter:   1962/  5202 | loss: 2.4116 | ds_loss: 3.4066 | lr: 7.2214e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1963/  5202 | global iter:   1963/  5202 | loss: 2.4968 | ds_loss: 3.8040 | lr: 7.2189e-07 | scale:     8.0000 | micro time: 0.316 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1964/  5202 | global iter:   1964/  5202 | loss: 1.1759 | ds_loss: 1.7801 | lr: 7.2164e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1964/  5202 | global iter:   1964/  5202 | loss: 2.2034 | ds_loss: 3.1122 | lr: 7.2164e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1965/  5202 | global iter:   1965/  5202 | loss: 2.2516 | ds_loss: 3.1354 | lr: 7.2139e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1966/  5202 | global iter:   1966/  5202 | loss: 2.3891 | ds_loss: 3.0703 | lr: 7.2114e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1967/  5202 | global iter:   1967/  5202 | loss: 2.6281 | ds_loss: 3.4686 | lr: 7.2089e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1968/  5202 | global iter:   1968/  5202 | loss: 3.0083 | ds_loss: 4.6430 | lr: 7.2064e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1968/  5202 | global iter:   1968/  5202 | loss: 2.5693 | ds_loss: 3.5793 | lr: 7.2064e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1969/  5202 | global iter:   1969/  5202 | loss: 2.8869 | ds_loss: 3.5173 | lr: 7.2039e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1970/  5202 | global iter:   1970/  5202 | loss: 2.0335 | ds_loss: 2.9317 | lr: 7.2013e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1971/  5202 | global iter:   1971/  5202 | loss: 2.7867 | ds_loss: 3.7610 | lr: 7.1988e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1972/  5202 | global iter:   1972/  5202 | loss: 1.2574 | ds_loss: 1.9367 | lr: 7.1963e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1972/  5202 | global iter:   1972/  5202 | loss: 2.2411 | ds_loss: 3.0367 | lr: 7.1963e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1973/  5202 | global iter:   1973/  5202 | loss: 2.2532 | ds_loss: 3.1583 | lr: 7.1938e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1974/  5202 | global iter:   1974/  5202 | loss: 2.4930 | ds_loss: 3.5703 | lr: 7.1913e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1975/  5202 | global iter:   1975/  5202 | loss: 2.0501 | ds_loss: 3.3678 | lr: 7.1888e-07 | scale:     8.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1976/  5202 | global iter:   1976/  5202 | loss: 2.8694 | ds_loss: 3.7830 | lr: 7.1862e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1976/  5202 | global iter:   1976/  5202 | loss: 2.4164 | ds_loss: 3.4698 | lr: 7.1862e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1977/  5202 | global iter:   1977/  5202 | loss: 2.0568 | ds_loss: 3.0439 | lr: 7.1837e-07 | scale:     8.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1978/  5202 | global iter:   1978/  5202 | loss: 2.5907 | ds_loss: 3.3350 | lr: 7.1812e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1979/  5202 | global iter:   1979/  5202 | loss: 3.1068 | ds_loss: 3.8189 | lr: 7.1787e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1980/  5202 | global iter:   1980/  5202 | loss: 1.7701 | ds_loss: 2.4474 | lr: 7.1762e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1980/  5202 | global iter:   1980/  5202 | loss: 2.3811 | ds_loss: 3.1613 | lr: 7.1762e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1981/  5202 | global iter:   1981/  5202 | loss: 3.0193 | ds_loss: 3.6442 | lr: 7.1736e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1982/  5202 | global iter:   1982/  5202 | loss: 2.0024 | ds_loss: 2.8731 | lr: 7.1711e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1983/  5202 | global iter:   1983/  5202 | loss: 2.9966 | ds_loss: 3.8659 | lr: 7.1686e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1984/  5202 | global iter:   1984/  5202 | loss: 2.7860 | ds_loss: 3.4771 | lr: 7.1661e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1984/  5202 | global iter:   1984/  5202 | loss: 2.7011 | ds_loss: 3.4651 | lr: 7.1661e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1985/  5202 | global iter:   1985/  5202 | loss: 3.1463 | ds_loss: 3.8335 | lr: 7.1635e-07 | scale:     8.0000 | micro time: 0.318 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1986/  5202 | global iter:   1986/  5202 | loss: 2.8489 | ds_loss: 3.7613 | lr: 7.1610e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1987/  5202 | global iter:   1987/  5202 | loss: 1.5126 | ds_loss: 2.1670 | lr: 7.1585e-07 | scale:     8.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1988/  5202 | global iter:   1988/  5202 | loss: 2.1991 | ds_loss: 2.8613 | lr: 7.1560e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1988/  5202 | global iter:   1988/  5202 | loss: 2.4267 | ds_loss: 3.1558 | lr: 7.1560e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1989/  5202 | global iter:   1989/  5202 | loss: 2.1481 | ds_loss: 2.9627 | lr: 7.1534e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1990/  5202 | global iter:   1990/  5202 | loss: 2.9495 | ds_loss: 4.1433 | lr: 7.1509e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1991/  5202 | global iter:   1991/  5202 | loss: 2.4863 | ds_loss: 3.0285 | lr: 7.1484e-07 | scale:     8.0000 | micro time: 0.321 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1992/  5202 | global iter:   1992/  5202 | loss: 1.9809 | ds_loss: 2.6372 | lr: 7.1458e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1992/  5202 | global iter:   1992/  5202 | loss: 2.3912 | ds_loss: 3.1929 | lr: 7.1458e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.314\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1993/  5202 | global iter:   1993/  5202 | loss: 2.7803 | ds_loss: 3.5174 | lr: 7.1433e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1994/  5202 | global iter:   1994/  5202 | loss: 2.7694 | ds_loss: 3.3933 | lr: 7.1408e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1995/  5202 | global iter:   1995/  5202 | loss: 2.6716 | ds_loss: 3.4848 | lr: 7.1383e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1996/  5202 | global iter:   1996/  5202 | loss: 2.3246 | ds_loss: 3.1803 | lr: 7.1357e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1996/  5202 | global iter:   1996/  5202 | loss: 2.6365 | ds_loss: 3.3940 | lr: 7.1357e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.312\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   1997/  5202 | global iter:   1997/  5202 | loss: 3.0330 | ds_loss: 3.7198 | lr: 7.1332e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1998/  5202 | global iter:   1998/  5202 | loss: 2.5988 | ds_loss: 3.8632 | lr: 7.1307e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   1999/  5202 | global iter:   1999/  5202 | loss: 3.3005 | ds_loss: 4.1951 | lr: 7.1281e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2000/  5202 | global iter:   2000/  5202 | loss: 2.8327 | ds_loss: 3.5779 | lr: 7.1256e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2000/  5202 | global iter:   2000/  5202 | loss: 2.9413 | ds_loss: 3.8390 | lr: 7.1256e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2001/  5202 | global iter:   2001/  5202 | loss: 1.4256 | ds_loss: 2.2026 | lr: 7.1231e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2002/  5202 | global iter:   2002/  5202 | loss: 1.1886 | ds_loss: 1.5768 | lr: 7.1205e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2003/  5202 | global iter:   2003/  5202 | loss: 2.7685 | ds_loss: 3.5055 | lr: 7.1180e-07 | scale:     8.0000 | micro time: 0.317 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2004/  5202 | global iter:   2004/  5202 | loss: 2.6688 | ds_loss: 3.6894 | lr: 7.1154e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2004/  5202 | global iter:   2004/  5202 | loss: 2.0129 | ds_loss: 2.7436 | lr: 7.1154e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.313\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2005/  5202 | global iter:   2005/  5202 | loss: 1.9444 | ds_loss: 3.3916 | lr: 7.1129e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2006/  5202 | global iter:   2006/  5202 | loss: 2.4954 | ds_loss: 3.3244 | lr: 7.1104e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2007/  5202 | global iter:   2007/  5202 | loss: 2.8369 | ds_loss: 3.4240 | lr: 7.1078e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2008/  5202 | global iter:   2008/  5202 | loss: 2.1174 | ds_loss: 2.9073 | lr: 7.1053e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2008/  5202 | global iter:   2008/  5202 | loss: 2.3485 | ds_loss: 3.2618 | lr: 7.1053e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2009/  5202 | global iter:   2009/  5202 | loss: 2.7951 | ds_loss: 4.0556 | lr: 7.1028e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2010/  5202 | global iter:   2010/  5202 | loss: 2.4573 | ds_loss: 3.2048 | lr: 7.1002e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2011/  5202 | global iter:   2011/  5202 | loss: 2.8510 | ds_loss: 3.7942 | lr: 7.0977e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2012/  5202 | global iter:   2012/  5202 | loss: 2.6186 | ds_loss: 3.8662 | lr: 7.0951e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2012/  5202 | global iter:   2012/  5202 | loss: 2.6805 | ds_loss: 3.7302 | lr: 7.0951e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2013/  5202 | global iter:   2013/  5202 | loss: 2.5799 | ds_loss: 3.3410 | lr: 7.0926e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2014/  5202 | global iter:   2014/  5202 | loss: 2.9899 | ds_loss: 3.7268 | lr: 7.0901e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2015/  5202 | global iter:   2015/  5202 | loss: 1.6993 | ds_loss: 2.4183 | lr: 7.0875e-07 | scale:     8.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2016/  5202 | global iter:   2016/  5202 | loss: 2.6476 | ds_loss: 3.2404 | lr: 7.0850e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2016/  5202 | global iter:   2016/  5202 | loss: 2.4792 | ds_loss: 3.1816 | lr: 7.0850e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2017/  5202 | global iter:   2017/  5202 | loss: 2.7351 | ds_loss: 3.4185 | lr: 7.0824e-07 | scale:     8.0000 | micro time: 0.320 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2018/  5202 | global iter:   2018/  5202 | loss: 2.0105 | ds_loss: 2.5623 | lr: 7.0799e-07 | scale:     8.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2019/  5202 | global iter:   2019/  5202 | loss: 2.5878 | ds_loss: 3.2630 | lr: 7.0773e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2020/  5202 | global iter:   2020/  5202 | loss: 2.0997 | ds_loss: 2.8986 | lr: 7.0748e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2020/  5202 | global iter:   2020/  5202 | loss: 2.3583 | ds_loss: 3.0356 | lr: 7.0748e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2021/  5202 | global iter:   2021/  5202 | loss: 1.7984 | ds_loss: 2.5927 | lr: 7.0722e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2022/  5202 | global iter:   2022/  5202 | loss: 2.5105 | ds_loss: 3.4251 | lr: 7.0697e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2023/  5202 | global iter:   2023/  5202 | loss: 2.2027 | ds_loss: 2.8289 | lr: 7.0671e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2024/  5202 | global iter:   2024/  5202 | loss: 2.8942 | ds_loss: 3.6849 | lr: 7.0646e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2024/  5202 | global iter:   2024/  5202 | loss: 2.3514 | ds_loss: 3.1329 | lr: 7.0646e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2025/  5202 | global iter:   2025/  5202 | loss: 2.6977 | ds_loss: 3.4983 | lr: 7.0621e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2026/  5202 | global iter:   2026/  5202 | loss: 2.9939 | ds_loss: 3.7166 | lr: 7.0595e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2027/  5202 | global iter:   2027/  5202 | loss: 2.0800 | ds_loss: 2.5927 | lr: 7.0570e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2028/  5202 | global iter:   2028/  5202 | loss: 2.5710 | ds_loss: 3.3725 | lr: 7.0544e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2028/  5202 | global iter:   2028/  5202 | loss: 2.5857 | ds_loss: 3.2950 | lr: 7.0544e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2029/  5202 | global iter:   2029/  5202 | loss: 2.9971 | ds_loss: 4.0363 | lr: 7.0519e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2030/  5202 | global iter:   2030/  5202 | loss: 2.4381 | ds_loss: 3.3662 | lr: 7.0493e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2031/  5202 | global iter:   2031/  5202 | loss: 2.5739 | ds_loss: 3.3708 | lr: 7.0467e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2032/  5202 | global iter:   2032/  5202 | loss: 2.4194 | ds_loss: 3.3023 | lr: 7.0442e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2032/  5202 | global iter:   2032/  5202 | loss: 2.6071 | ds_loss: 3.5189 | lr: 7.0442e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2033/  5202 | global iter:   2033/  5202 | loss: 2.5982 | ds_loss: 3.2420 | lr: 7.0416e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2034/  5202 | global iter:   2034/  5202 | loss: 2.6944 | ds_loss: 3.4787 | lr: 7.0391e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2035/  5202 | global iter:   2035/  5202 | loss: 2.3030 | ds_loss: 3.2399 | lr: 7.0365e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2036/  5202 | global iter:   2036/  5202 | loss: 2.6235 | ds_loss: 3.2554 | lr: 7.0340e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2036/  5202 | global iter:   2036/  5202 | loss: 2.5548 | ds_loss: 3.3040 | lr: 7.0340e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2037/  5202 | global iter:   2037/  5202 | loss: 1.9706 | ds_loss: 2.9823 | lr: 7.0314e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2038/  5202 | global iter:   2038/  5202 | loss: 2.8540 | ds_loss: 3.3020 | lr: 7.0289e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2039/  5202 | global iter:   2039/  5202 | loss: 2.8714 | ds_loss: 3.4336 | lr: 7.0263e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2040/  5202 | global iter:   2040/  5202 | loss: 1.1686 | ds_loss: 1.6639 | lr: 7.0238e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2040/  5202 | global iter:   2040/  5202 | loss: 2.2162 | ds_loss: 2.8455 | lr: 7.0238e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2041/  5202 | global iter:   2041/  5202 | loss: 2.9217 | ds_loss: 3.4794 | lr: 7.0212e-07 | scale:     8.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2042/  5202 | global iter:   2042/  5202 | loss: 1.8417 | ds_loss: 2.4804 | lr: 7.0186e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2043/  5202 | global iter:   2043/  5202 | loss: 2.9128 | ds_loss: 3.5522 | lr: 7.0161e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2044/  5202 | global iter:   2044/  5202 | loss: 2.8008 | ds_loss: 3.5578 | lr: 7.0135e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2044/  5202 | global iter:   2044/  5202 | loss: 2.6193 | ds_loss: 3.2674 | lr: 7.0135e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2045/  5202 | global iter:   2045/  5202 | loss: 2.4600 | ds_loss: 3.3359 | lr: 7.0110e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2046/  5202 | global iter:   2046/  5202 | loss: 2.9575 | ds_loss: 3.7914 | lr: 7.0084e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2047/  5202 | global iter:   2047/  5202 | loss: 2.8360 | ds_loss: 3.7677 | lr: 7.0058e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2048/  5202 | global iter:   2048/  5202 | loss: 2.8809 | ds_loss: 3.8716 | lr: 7.0033e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2048/  5202 | global iter:   2048/  5202 | loss: 2.7836 | ds_loss: 3.6916 | lr: 7.0033e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.312\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2049/  5202 | global iter:   2049/  5202 | loss: 2.3033 | ds_loss: 3.5573 | lr: 7.0007e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2050/  5202 | global iter:   2050/  5202 | loss: 0.2197 | ds_loss: 0.3691 | lr: 6.9982e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2051/  5202 | global iter:   2051/  5202 | loss: 2.9952 | ds_loss: 3.5681 | lr: 6.9956e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2052/  5202 | global iter:   2052/  5202 | loss: 2.9160 | ds_loss: 3.7716 | lr: 6.9930e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2052/  5202 | global iter:   2052/  5202 | loss: 2.1086 | ds_loss: 2.8165 | lr: 6.9930e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2053/  5202 | global iter:   2053/  5202 | loss: 3.1166 | ds_loss: 3.7820 | lr: 6.9905e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2054/  5202 | global iter:   2054/  5202 | loss: 1.4661 | ds_loss: 2.2517 | lr: 6.9879e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2055/  5202 | global iter:   2055/  5202 | loss: 3.1774 | ds_loss: 3.8692 | lr: 6.9853e-07 | scale:     8.0000 | micro time: 0.316 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2056/  5202 | global iter:   2056/  5202 | loss: 2.7633 | ds_loss: 3.7683 | lr: 6.9828e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2056/  5202 | global iter:   2056/  5202 | loss: 2.6308 | ds_loss: 3.4178 | lr: 6.9828e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2057/  5202 | global iter:   2057/  5202 | loss: 3.2224 | ds_loss: 3.8313 | lr: 6.9802e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2058/  5202 | global iter:   2058/  5202 | loss: 3.1932 | ds_loss: 4.2839 | lr: 6.9776e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2059/  5202 | global iter:   2059/  5202 | loss: 1.7897 | ds_loss: 2.8547 | lr: 6.9751e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2060/  5202 | global iter:   2060/  5202 | loss: 3.3091 | ds_loss: 4.0541 | lr: 6.9725e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2060/  5202 | global iter:   2060/  5202 | loss: 2.8786 | ds_loss: 3.7560 | lr: 6.9725e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2061/  5202 | global iter:   2061/  5202 | loss: 2.9647 | ds_loss: 3.8362 | lr: 6.9699e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2062/  5202 | global iter:   2062/  5202 | loss: 2.7519 | ds_loss: 3.6126 | lr: 6.9674e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2063/  5202 | global iter:   2063/  5202 | loss: 3.4105 | ds_loss: 4.7209 | lr: 6.9648e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2064/  5202 | global iter:   2064/  5202 | loss: 2.4744 | ds_loss: 3.3554 | lr: 6.9622e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2064/  5202 | global iter:   2064/  5202 | loss: 2.9004 | ds_loss: 3.8813 | lr: 6.9622e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2065/  5202 | global iter:   2065/  5202 | loss: 2.0395 | ds_loss: 2.9643 | lr: 6.9597e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2066/  5202 | global iter:   2066/  5202 | loss: 2.3031 | ds_loss: 3.1917 | lr: 6.9571e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2067/  5202 | global iter:   2067/  5202 | loss: 1.9714 | ds_loss: 2.7225 | lr: 6.9545e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2068/  5202 | global iter:   2068/  5202 | loss: 2.8261 | ds_loss: 3.6135 | lr: 6.9519e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2068/  5202 | global iter:   2068/  5202 | loss: 2.2850 | ds_loss: 3.1230 | lr: 6.9519e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2069/  5202 | global iter:   2069/  5202 | loss: 2.8466 | ds_loss: 3.6901 | lr: 6.9494e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2070/  5202 | global iter:   2070/  5202 | loss: 2.8550 | ds_loss: 3.6281 | lr: 6.9468e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2071/  5202 | global iter:   2071/  5202 | loss: 3.2196 | ds_loss: 3.8851 | lr: 6.9442e-07 | scale:     8.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2072/  5202 | global iter:   2072/  5202 | loss: 2.7870 | ds_loss: 3.3941 | lr: 6.9417e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2072/  5202 | global iter:   2072/  5202 | loss: 2.9270 | ds_loss: 3.6493 | lr: 6.9417e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2073/  5202 | global iter:   2073/  5202 | loss: 2.3318 | ds_loss: 3.0599 | lr: 6.9391e-07 | scale:     8.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2074/  5202 | global iter:   2074/  5202 | loss: 2.3303 | ds_loss: 3.1241 | lr: 6.9365e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2075/  5202 | global iter:   2075/  5202 | loss: 2.4924 | ds_loss: 3.0942 | lr: 6.9339e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2076/  5202 | global iter:   2076/  5202 | loss: 2.1516 | ds_loss: 2.7867 | lr: 6.9313e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2076/  5202 | global iter:   2076/  5202 | loss: 2.3265 | ds_loss: 3.0162 | lr: 6.9313e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2077/  5202 | global iter:   2077/  5202 | loss: 2.9286 | ds_loss: 3.6910 | lr: 6.9288e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2078/  5202 | global iter:   2078/  5202 | loss: 3.1611 | ds_loss: 3.9071 | lr: 6.9262e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2079/  5202 | global iter:   2079/  5202 | loss: 2.4382 | ds_loss: 3.1509 | lr: 6.9236e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2080/  5202 | global iter:   2080/  5202 | loss: 2.9958 | ds_loss: 4.0219 | lr: 6.9210e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2080/  5202 | global iter:   2080/  5202 | loss: 2.8809 | ds_loss: 3.6928 | lr: 6.9210e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2081/  5202 | global iter:   2081/  5202 | loss: 1.0235 | ds_loss: 1.4197 | lr: 6.9185e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2082/  5202 | global iter:   2082/  5202 | loss: 3.1015 | ds_loss: 3.6458 | lr: 6.9159e-07 | scale:     8.0000 | micro time: 0.317 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2083/  5202 | global iter:   2083/  5202 | loss: 2.1454 | ds_loss: 2.6438 | lr: 6.9133e-07 | scale:     8.0000 | micro time: 0.327 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2084/  5202 | global iter:   2084/  5202 | loss: 2.8042 | ds_loss: 3.6569 | lr: 6.9107e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2084/  5202 | global iter:   2084/  5202 | loss: 2.2686 | ds_loss: 2.8415 | lr: 6.9107e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.317\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2085/  5202 | global iter:   2085/  5202 | loss: 3.0768 | ds_loss: 3.6706 | lr: 6.9081e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2086/  5202 | global iter:   2086/  5202 | loss: 2.5801 | ds_loss: 3.4410 | lr: 6.9056e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2087/  5202 | global iter:   2087/  5202 | loss: 2.9708 | ds_loss: 3.6476 | lr: 6.9030e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2088/  5202 | global iter:   2088/  5202 | loss: 2.9894 | ds_loss: 3.6087 | lr: 6.9004e-07 | scale:     8.0000 | micro time: 0.323 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2088/  5202 | global iter:   2088/  5202 | loss: 2.9043 | ds_loss: 3.5920 | lr: 6.9004e-07 | scale:     8.0000 | micro time: 0.323 | step time: 0.312\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2089/  5202 | global iter:   2089/  5202 | loss: 2.9048 | ds_loss: 3.6380 | lr: 6.8978e-07 | scale:     8.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2090/  5202 | global iter:   2090/  5202 | loss: 2.3187 | ds_loss: 3.3155 | lr: 6.8952e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2091/  5202 | global iter:   2091/  5202 | loss: 2.7186 | ds_loss: 3.5482 | lr: 6.8926e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2092/  5202 | global iter:   2092/  5202 | loss: 4.1112 | ds_loss: 4.8692 | lr: 6.8901e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2092/  5202 | global iter:   2092/  5202 | loss: 3.0133 | ds_loss: 3.8427 | lr: 6.8901e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.313\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2093/  5202 | global iter:   2093/  5202 | loss: 2.4468 | ds_loss: 3.2371 | lr: 6.8875e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2094/  5202 | global iter:   2094/  5202 | loss: 2.6771 | ds_loss: 3.5876 | lr: 6.8849e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2095/  5202 | global iter:   2095/  5202 | loss: 2.9460 | ds_loss: 3.8645 | lr: 6.8823e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2096/  5202 | global iter:   2096/  5202 | loss: 2.9877 | ds_loss: 3.8858 | lr: 6.8797e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2096/  5202 | global iter:   2096/  5202 | loss: 2.7644 | ds_loss: 3.6438 | lr: 6.8797e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2097/  5202 | global iter:   2097/  5202 | loss: 2.2694 | ds_loss: 3.1739 | lr: 6.8771e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2098/  5202 | global iter:   2098/  5202 | loss: 3.1251 | ds_loss: 4.0382 | lr: 6.8745e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2099/  5202 | global iter:   2099/  5202 | loss: 3.1522 | ds_loss: 3.7841 | lr: 6.8720e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2100/  5202 | global iter:   2100/  5202 | loss: 2.7240 | ds_loss: 3.3483 | lr: 6.8694e-07 | scale:     8.0000 | micro time: 0.316 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2100/  5202 | global iter:   2100/  5202 | loss: 2.8177 | ds_loss: 3.5861 | lr: 6.8694e-07 | scale:     8.0000 | micro time: 0.316 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2101/  5202 | global iter:   2101/  5202 | loss: 2.7608 | ds_loss: 3.4314 | lr: 6.8668e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2102/  5202 | global iter:   2102/  5202 | loss: 1.8384 | ds_loss: 2.4486 | lr: 6.8642e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2103/  5202 | global iter:   2103/  5202 | loss: 1.9577 | ds_loss: 2.5547 | lr: 6.8616e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2104/  5202 | global iter:   2104/  5202 | loss: 2.4505 | ds_loss: 3.2418 | lr: 6.8590e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2104/  5202 | global iter:   2104/  5202 | loss: 2.2519 | ds_loss: 2.9191 | lr: 6.8590e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2105/  5202 | global iter:   2105/  5202 | loss: 2.9237 | ds_loss: 3.6248 | lr: 6.8564e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2106/  5202 | global iter:   2106/  5202 | loss: 3.0670 | ds_loss: 3.7259 | lr: 6.8538e-07 | scale:     8.0000 | micro time: 0.318 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2107/  5202 | global iter:   2107/  5202 | loss: 2.7389 | ds_loss: 3.4860 | lr: 6.8512e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2108/  5202 | global iter:   2108/  5202 | loss: 2.9545 | ds_loss: 3.5284 | lr: 6.8486e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2108/  5202 | global iter:   2108/  5202 | loss: 2.9210 | ds_loss: 3.5913 | lr: 6.8486e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2109/  5202 | global iter:   2109/  5202 | loss: 2.5655 | ds_loss: 3.3777 | lr: 6.8460e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2110/  5202 | global iter:   2110/  5202 | loss: 2.7837 | ds_loss: 3.3510 | lr: 6.8435e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2111/  5202 | global iter:   2111/  5202 | loss: 2.2530 | ds_loss: 2.7593 | lr: 6.8409e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2112/  5202 | global iter:   2112/  5202 | loss: 2.7053 | ds_loss: 3.4006 | lr: 6.8383e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2112/  5202 | global iter:   2112/  5202 | loss: 2.5769 | ds_loss: 3.2221 | lr: 6.8383e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2113/  5202 | global iter:   2113/  5202 | loss: 2.9623 | ds_loss: 3.7381 | lr: 6.8357e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2114/  5202 | global iter:   2114/  5202 | loss: 2.7980 | ds_loss: 3.8812 | lr: 6.8331e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2115/  5202 | global iter:   2115/  5202 | loss: 2.0718 | ds_loss: 2.7177 | lr: 6.8305e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2116/  5202 | global iter:   2116/  5202 | loss: 2.3700 | ds_loss: 3.4597 | lr: 6.8279e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2116/  5202 | global iter:   2116/  5202 | loss: 2.5505 | ds_loss: 3.4491 | lr: 6.8279e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2117/  5202 | global iter:   2117/  5202 | loss: 2.7500 | ds_loss: 3.4331 | lr: 6.8253e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2118/  5202 | global iter:   2118/  5202 | loss: 2.7267 | ds_loss: 3.3968 | lr: 6.8227e-07 | scale:     8.0000 | micro time: 0.316 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2119/  5202 | global iter:   2119/  5202 | loss: 3.0669 | ds_loss: 4.0645 | lr: 6.8201e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2120/  5202 | global iter:   2120/  5202 | loss: 2.1742 | ds_loss: 2.9848 | lr: 6.8175e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2120/  5202 | global iter:   2120/  5202 | loss: 2.6794 | ds_loss: 3.4698 | lr: 6.8175e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2121/  5202 | global iter:   2121/  5202 | loss: 2.2750 | ds_loss: 2.9316 | lr: 6.8149e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2122/  5202 | global iter:   2122/  5202 | loss: 2.3773 | ds_loss: 3.0465 | lr: 6.8123e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2123/  5202 | global iter:   2123/  5202 | loss: 3.1449 | ds_loss: 3.8770 | lr: 6.8097e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2124/  5202 | global iter:   2124/  5202 | loss: 2.7305 | ds_loss: 3.7359 | lr: 6.8071e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2124/  5202 | global iter:   2124/  5202 | loss: 2.6319 | ds_loss: 3.3978 | lr: 6.8071e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2125/  5202 | global iter:   2125/  5202 | loss: 2.7786 | ds_loss: 3.4063 | lr: 6.8045e-07 | scale:     8.0000 | micro time: 0.317 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2126/  5202 | global iter:   2126/  5202 | loss: 2.2018 | ds_loss: 2.9787 | lr: 6.8019e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2127/  5202 | global iter:   2127/  5202 | loss: 3.0555 | ds_loss: 3.8249 | lr: 6.7993e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2128/  5202 | global iter:   2128/  5202 | loss: 1.5752 | ds_loss: 2.3214 | lr: 6.7967e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2128/  5202 | global iter:   2128/  5202 | loss: 2.4028 | ds_loss: 3.1328 | lr: 6.7967e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.313\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2129/  5202 | global iter:   2129/  5202 | loss: 2.4353 | ds_loss: 3.0950 | lr: 6.7941e-07 | scale:     8.0000 | micro time: 0.319 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2130/  5202 | global iter:   2130/  5202 | loss: 2.2983 | ds_loss: 3.0300 | lr: 6.7915e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2131/  5202 | global iter:   2131/  5202 | loss: 2.4875 | ds_loss: 3.0168 | lr: 6.7889e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2132/  5202 | global iter:   2132/  5202 | loss: 2.5407 | ds_loss: 3.3207 | lr: 6.7863e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2132/  5202 | global iter:   2132/  5202 | loss: 2.4405 | ds_loss: 3.1156 | lr: 6.7863e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.312\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2133/  5202 | global iter:   2133/  5202 | loss: 4.0544 | ds_loss: 5.4374 | lr: 6.7837e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2134/  5202 | global iter:   2134/  5202 | loss: 2.8611 | ds_loss: 3.8469 | lr: 6.7811e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2135/  5202 | global iter:   2135/  5202 | loss: 3.0143 | ds_loss: 3.7618 | lr: 6.7785e-07 | scale:     8.0000 | micro time: 0.319 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2136/  5202 | global iter:   2136/  5202 | loss: 2.7372 | ds_loss: 3.6058 | lr: 6.7759e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2136/  5202 | global iter:   2136/  5202 | loss: 3.1668 | ds_loss: 4.1630 | lr: 6.7759e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2137/  5202 | global iter:   2137/  5202 | loss: 1.8664 | ds_loss: 2.5239 | lr: 6.7732e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2138/  5202 | global iter:   2138/  5202 | loss: 2.1978 | ds_loss: 2.7518 | lr: 6.7706e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2139/  5202 | global iter:   2139/  5202 | loss: 2.7563 | ds_loss: 3.4381 | lr: 6.7680e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2140/  5202 | global iter:   2140/  5202 | loss: 2.2723 | ds_loss: 3.0701 | lr: 6.7654e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2140/  5202 | global iter:   2140/  5202 | loss: 2.2732 | ds_loss: 2.9460 | lr: 6.7654e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2141/  5202 | global iter:   2141/  5202 | loss: 2.8955 | ds_loss: 3.5015 | lr: 6.7628e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2142/  5202 | global iter:   2142/  5202 | loss: 2.1762 | ds_loss: 2.8643 | lr: 6.7602e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2143/  5202 | global iter:   2143/  5202 | loss: 2.9077 | ds_loss: 3.8522 | lr: 6.7576e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2144/  5202 | global iter:   2144/  5202 | loss: 2.4907 | ds_loss: 3.1489 | lr: 6.7550e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2144/  5202 | global iter:   2144/  5202 | loss: 2.6175 | ds_loss: 3.3417 | lr: 6.7550e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2145/  5202 | global iter:   2145/  5202 | loss: 2.0060 | ds_loss: 2.5574 | lr: 6.7524e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2146/  5202 | global iter:   2146/  5202 | loss: 3.0260 | ds_loss: 4.1654 | lr: 6.7498e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2147/  5202 | global iter:   2147/  5202 | loss: 2.1386 | ds_loss: 2.6028 | lr: 6.7472e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2148/  5202 | global iter:   2148/  5202 | loss: 2.6348 | ds_loss: 3.5690 | lr: 6.7445e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2148/  5202 | global iter:   2148/  5202 | loss: 2.4514 | ds_loss: 3.2236 | lr: 6.7445e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2149/  5202 | global iter:   2149/  5202 | loss: 3.1567 | ds_loss: 3.8466 | lr: 6.7419e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2150/  5202 | global iter:   2150/  5202 | loss: 2.0172 | ds_loss: 2.8760 | lr: 6.7393e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2151/  5202 | global iter:   2151/  5202 | loss: 2.8440 | ds_loss: 3.6138 | lr: 6.7367e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2152/  5202 | global iter:   2152/  5202 | loss: 1.5396 | ds_loss: 2.1169 | lr: 6.7341e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2152/  5202 | global iter:   2152/  5202 | loss: 2.3894 | ds_loss: 3.1133 | lr: 6.7341e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2153/  5202 | global iter:   2153/  5202 | loss: 1.8491 | ds_loss: 2.4862 | lr: 6.7315e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2154/  5202 | global iter:   2154/  5202 | loss: 2.8082 | ds_loss: 3.5212 | lr: 6.7289e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2155/  5202 | global iter:   2155/  5202 | loss: 2.6852 | ds_loss: 3.5941 | lr: 6.7263e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2156/  5202 | global iter:   2156/  5202 | loss: 2.1289 | ds_loss: 3.0378 | lr: 6.7236e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2156/  5202 | global iter:   2156/  5202 | loss: 2.3678 | ds_loss: 3.1598 | lr: 6.7236e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2157/  5202 | global iter:   2157/  5202 | loss: 2.1880 | ds_loss: 2.9089 | lr: 6.7210e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2158/  5202 | global iter:   2158/  5202 | loss: 3.0717 | ds_loss: 3.9280 | lr: 6.7184e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2159/  5202 | global iter:   2159/  5202 | loss: 2.7170 | ds_loss: 3.4960 | lr: 6.7158e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2160/  5202 | global iter:   2160/  5202 | loss: 2.6228 | ds_loss: 3.3433 | lr: 6.7132e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2160/  5202 | global iter:   2160/  5202 | loss: 2.6499 | ds_loss: 3.4190 | lr: 6.7132e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2161/  5202 | global iter:   2161/  5202 | loss: 2.4475 | ds_loss: 3.3825 | lr: 6.7106e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2162/  5202 | global iter:   2162/  5202 | loss: 2.3721 | ds_loss: 3.2425 | lr: 6.7079e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2163/  5202 | global iter:   2163/  5202 | loss: 1.8355 | ds_loss: 2.4493 | lr: 6.7053e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2164/  5202 | global iter:   2164/  5202 | loss: 1.6271 | ds_loss: 2.8379 | lr: 6.7027e-07 | scale:     8.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2164/  5202 | global iter:   2164/  5202 | loss: 2.0706 | ds_loss: 2.9781 | lr: 6.7027e-07 | scale:     8.0000 | micro time: 0.303 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2165/  5202 | global iter:   2165/  5202 | loss: 1.7542 | ds_loss: 2.4350 | lr: 6.7001e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2166/  5202 | global iter:   2166/  5202 | loss: 3.0042 | ds_loss: 3.9511 | lr: 6.6975e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2167/  5202 | global iter:   2167/  5202 | loss: 2.4642 | ds_loss: 3.1968 | lr: 6.6948e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2168/  5202 | global iter:   2168/  5202 | loss: 2.6307 | ds_loss: 3.3596 | lr: 6.6922e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2168/  5202 | global iter:   2168/  5202 | loss: 2.4633 | ds_loss: 3.2356 | lr: 6.6922e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2169/  5202 | global iter:   2169/  5202 | loss: 3.6961 | ds_loss: 4.6590 | lr: 6.6896e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2170/  5202 | global iter:   2170/  5202 | loss: 2.2917 | ds_loss: 3.3312 | lr: 6.6870e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2171/  5202 | global iter:   2171/  5202 | loss: 2.8633 | ds_loss: 3.6168 | lr: 6.6844e-07 | scale:     8.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2172/  5202 | global iter:   2172/  5202 | loss: 2.5303 | ds_loss: 3.4633 | lr: 6.6817e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2172/  5202 | global iter:   2172/  5202 | loss: 2.8453 | ds_loss: 3.7676 | lr: 6.6817e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2173/  5202 | global iter:   2173/  5202 | loss: 1.6735 | ds_loss: 2.4329 | lr: 6.6791e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2174/  5202 | global iter:   2174/  5202 | loss: 2.4239 | ds_loss: 3.3145 | lr: 6.6765e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2175/  5202 | global iter:   2175/  5202 | loss: 2.9135 | ds_loss: 3.9950 | lr: 6.6739e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2176/  5202 | global iter:   2176/  5202 | loss: 3.2128 | ds_loss: 3.8340 | lr: 6.6712e-07 | scale:     8.0000 | micro time: 0.328 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2176/  5202 | global iter:   2176/  5202 | loss: 2.5560 | ds_loss: 3.3941 | lr: 6.6712e-07 | scale:     8.0000 | micro time: 0.328 | step time: 0.312\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2177/  5202 | global iter:   2177/  5202 | loss: 1.8411 | ds_loss: 2.6276 | lr: 6.6686e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2178/  5202 | global iter:   2178/  5202 | loss: 3.2299 | ds_loss: 3.8040 | lr: 6.6660e-07 | scale:     8.0000 | micro time: 0.316 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2179/  5202 | global iter:   2179/  5202 | loss: 3.2688 | ds_loss: 4.1905 | lr: 6.6634e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2180/  5202 | global iter:   2180/  5202 | loss: 1.7375 | ds_loss: 2.1789 | lr: 6.6607e-07 | scale:     8.0000 | micro time: 0.315 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2180/  5202 | global iter:   2180/  5202 | loss: 2.5193 | ds_loss: 3.2002 | lr: 6.6607e-07 | scale:     8.0000 | micro time: 0.315 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2181/  5202 | global iter:   2181/  5202 | loss: 0.6298 | ds_loss: 1.0011 | lr: 6.6581e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2182/  5202 | global iter:   2182/  5202 | loss: 3.8027 | ds_loss: 5.0537 | lr: 6.6555e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2183/  5202 | global iter:   2183/  5202 | loss: 2.7554 | ds_loss: 3.7747 | lr: 6.6529e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2184/  5202 | global iter:   2184/  5202 | loss: 2.4454 | ds_loss: 3.8445 | lr: 6.6502e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2184/  5202 | global iter:   2184/  5202 | loss: 2.4083 | ds_loss: 3.4185 | lr: 6.6502e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2185/  5202 | global iter:   2185/  5202 | loss: 2.8943 | ds_loss: 3.4646 | lr: 6.6476e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2186/  5202 | global iter:   2186/  5202 | loss: 2.6996 | ds_loss: 3.7148 | lr: 6.6450e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2187/  5202 | global iter:   2187/  5202 | loss: 1.8804 | ds_loss: 2.8967 | lr: 6.6424e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2188/  5202 | global iter:   2188/  5202 | loss: 0.5190 | ds_loss: 0.7779 | lr: 6.6397e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2188/  5202 | global iter:   2188/  5202 | loss: 1.9983 | ds_loss: 2.7135 | lr: 6.6397e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2189/  5202 | global iter:   2189/  5202 | loss: 2.4098 | ds_loss: 3.7377 | lr: 6.6371e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2190/  5202 | global iter:   2190/  5202 | loss: 1.6127 | ds_loss: 2.3043 | lr: 6.6345e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2191/  5202 | global iter:   2191/  5202 | loss: 2.4126 | ds_loss: 3.3611 | lr: 6.6318e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2192/  5202 | global iter:   2192/  5202 | loss: 2.3677 | ds_loss: 2.9807 | lr: 6.6292e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2192/  5202 | global iter:   2192/  5202 | loss: 2.2007 | ds_loss: 3.0959 | lr: 6.6292e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2193/  5202 | global iter:   2193/  5202 | loss: 2.6960 | ds_loss: 3.3680 | lr: 6.6266e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2194/  5202 | global iter:   2194/  5202 | loss: 2.8850 | ds_loss: 3.5781 | lr: 6.6239e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2195/  5202 | global iter:   2195/  5202 | loss: 1.5264 | ds_loss: 2.5918 | lr: 6.6213e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2196/  5202 | global iter:   2196/  5202 | loss: 2.9465 | ds_loss: 3.7979 | lr: 6.6187e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2196/  5202 | global iter:   2196/  5202 | loss: 2.5135 | ds_loss: 3.3340 | lr: 6.6187e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2197/  5202 | global iter:   2197/  5202 | loss: 3.2703 | ds_loss: 3.9469 | lr: 6.6161e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2198/  5202 | global iter:   2198/  5202 | loss: 3.4693 | ds_loss: 4.1075 | lr: 6.6134e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2199/  5202 | global iter:   2199/  5202 | loss: 3.0699 | ds_loss: 3.7382 | lr: 6.6108e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2200/  5202 | global iter:   2200/  5202 | loss: 2.8751 | ds_loss: 3.4692 | lr: 6.6082e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2200/  5202 | global iter:   2200/  5202 | loss: 3.1712 | ds_loss: 3.8155 | lr: 6.6082e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.312\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2201/  5202 | global iter:   2201/  5202 | loss: 1.2693 | ds_loss: 2.0985 | lr: 6.6055e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2202/  5202 | global iter:   2202/  5202 | loss: 1.3171 | ds_loss: 1.8774 | lr: 6.6029e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2203/  5202 | global iter:   2203/  5202 | loss: 2.8902 | ds_loss: 3.9339 | lr: 6.6002e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2204/  5202 | global iter:   2204/  5202 | loss: 2.9280 | ds_loss: 3.5199 | lr: 6.5976e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2204/  5202 | global iter:   2204/  5202 | loss: 2.1011 | ds_loss: 2.8574 | lr: 6.5976e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2205/  5202 | global iter:   2205/  5202 | loss: 2.7333 | ds_loss: 3.5689 | lr: 6.5950e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2206/  5202 | global iter:   2206/  5202 | loss: 2.4480 | ds_loss: 3.0050 | lr: 6.5923e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2207/  5202 | global iter:   2207/  5202 | loss: 2.7522 | ds_loss: 3.5413 | lr: 6.5897e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2208/  5202 | global iter:   2208/  5202 | loss: 3.0397 | ds_loss: 3.7941 | lr: 6.5871e-07 | scale:     8.0000 | micro time: 0.318 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2208/  5202 | global iter:   2208/  5202 | loss: 2.7433 | ds_loss: 3.4773 | lr: 6.5871e-07 | scale:     8.0000 | micro time: 0.318 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2209/  5202 | global iter:   2209/  5202 | loss: 2.9446 | ds_loss: 3.6762 | lr: 6.5844e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2210/  5202 | global iter:   2210/  5202 | loss: 2.4550 | ds_loss: 3.1523 | lr: 6.5818e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2211/  5202 | global iter:   2211/  5202 | loss: 2.4112 | ds_loss: 3.3225 | lr: 6.5792e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2212/  5202 | global iter:   2212/  5202 | loss: 1.9022 | ds_loss: 2.6791 | lr: 6.5765e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2212/  5202 | global iter:   2212/  5202 | loss: 2.4282 | ds_loss: 3.2075 | lr: 6.5765e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2213/  5202 | global iter:   2213/  5202 | loss: 2.5455 | ds_loss: 3.2518 | lr: 6.5739e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2214/  5202 | global iter:   2214/  5202 | loss: 2.9977 | ds_loss: 3.8045 | lr: 6.5712e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2215/  5202 | global iter:   2215/  5202 | loss: 2.8595 | ds_loss: 4.4225 | lr: 6.5686e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2216/  5202 | global iter:   2216/  5202 | loss: 2.7926 | ds_loss: 3.4039 | lr: 6.5660e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2216/  5202 | global iter:   2216/  5202 | loss: 2.7988 | ds_loss: 3.7207 | lr: 6.5660e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2217/  5202 | global iter:   2217/  5202 | loss: 2.9649 | ds_loss: 3.4464 | lr: 6.5633e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2218/  5202 | global iter:   2218/  5202 | loss: 2.6073 | ds_loss: 3.3980 | lr: 6.5607e-07 | scale:     8.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2219/  5202 | global iter:   2219/  5202 | loss: 2.9441 | ds_loss: 3.6240 | lr: 6.5580e-07 | scale:     8.0000 | micro time: 0.316 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2220/  5202 | global iter:   2220/  5202 | loss: 1.5889 | ds_loss: 2.0487 | lr: 6.5554e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2220/  5202 | global iter:   2220/  5202 | loss: 2.5263 | ds_loss: 3.1293 | lr: 6.5554e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.313\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2221/  5202 | global iter:   2221/  5202 | loss: 2.7766 | ds_loss: 3.6396 | lr: 6.5528e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2222/  5202 | global iter:   2222/  5202 | loss: 2.7758 | ds_loss: 3.4548 | lr: 6.5501e-07 | scale:     8.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2223/  5202 | global iter:   2223/  5202 | loss: 3.1179 | ds_loss: 3.7751 | lr: 6.5475e-07 | scale:     8.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2224/  5202 | global iter:   2224/  5202 | loss: 3.4051 | ds_loss: 4.2479 | lr: 6.5448e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2224/  5202 | global iter:   2224/  5202 | loss: 3.0189 | ds_loss: 3.7793 | lr: 6.5448e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.313\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2225/  5202 | global iter:   2225/  5202 | loss: 1.9555 | ds_loss: 2.8954 | lr: 6.5422e-07 | scale:     8.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2226/  5202 | global iter:   2226/  5202 | loss: 2.5188 | ds_loss: 3.2666 | lr: 6.5395e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2227/  5202 | global iter:   2227/  5202 | loss: 2.3057 | ds_loss: 2.8324 | lr: 6.5369e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2228/  5202 | global iter:   2228/  5202 | loss: 1.9380 | ds_loss: 2.7498 | lr: 6.5342e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2228/  5202 | global iter:   2228/  5202 | loss: 2.1795 | ds_loss: 2.9361 | lr: 6.5342e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2229/  5202 | global iter:   2229/  5202 | loss: 2.6875 | ds_loss: 3.6444 | lr: 6.5316e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2230/  5202 | global iter:   2230/  5202 | loss: 2.5278 | ds_loss: 3.2923 | lr: 6.5290e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2231/  5202 | global iter:   2231/  5202 | loss: 2.8034 | ds_loss: 3.7180 | lr: 6.5263e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2232/  5202 | global iter:   2232/  5202 | loss: 3.3729 | ds_loss: 4.0226 | lr: 6.5237e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2232/  5202 | global iter:   2232/  5202 | loss: 2.8479 | ds_loss: 3.6693 | lr: 6.5237e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2233/  5202 | global iter:   2233/  5202 | loss: 3.2799 | ds_loss: 3.9677 | lr: 6.5210e-07 | scale:     8.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2234/  5202 | global iter:   2234/  5202 | loss: 3.0687 | ds_loss: 3.8550 | lr: 6.5184e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2235/  5202 | global iter:   2235/  5202 | loss: 2.9387 | ds_loss: 3.8319 | lr: 6.5157e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2236/  5202 | global iter:   2236/  5202 | loss: 2.4980 | ds_loss: 3.2606 | lr: 6.5131e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2236/  5202 | global iter:   2236/  5202 | loss: 2.9464 | ds_loss: 3.7288 | lr: 6.5131e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2237/  5202 | global iter:   2237/  5202 | loss: 3.0020 | ds_loss: 3.7750 | lr: 6.5104e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2238/  5202 | global iter:   2238/  5202 | loss: 3.0551 | ds_loss: 3.9092 | lr: 6.5078e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2239/  5202 | global iter:   2239/  5202 | loss: 2.5537 | ds_loss: 3.1845 | lr: 6.5051e-07 | scale:     8.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2240/  5202 | global iter:   2240/  5202 | loss: 2.5837 | ds_loss: 3.4029 | lr: 6.5025e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2240/  5202 | global iter:   2240/  5202 | loss: 2.7986 | ds_loss: 3.5679 | lr: 6.5025e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2241/  5202 | global iter:   2241/  5202 | loss: 2.9717 | ds_loss: 4.2137 | lr: 6.4998e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2242/  5202 | global iter:   2242/  5202 | loss: 2.3134 | ds_loss: 3.2128 | lr: 6.4972e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2243/  5202 | global iter:   2243/  5202 | loss: 2.6367 | ds_loss: 3.3722 | lr: 6.4945e-07 | scale:     8.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2244/  5202 | global iter:   2244/  5202 | loss: 3.0442 | ds_loss: 3.8882 | lr: 6.4919e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2244/  5202 | global iter:   2244/  5202 | loss: 2.7415 | ds_loss: 3.6717 | lr: 6.4919e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2245/  5202 | global iter:   2245/  5202 | loss: 2.6795 | ds_loss: 3.3799 | lr: 6.4892e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2246/  5202 | global iter:   2246/  5202 | loss: 3.0951 | ds_loss: 4.0669 | lr: 6.4866e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2247/  5202 | global iter:   2247/  5202 | loss: 2.6469 | ds_loss: 3.3860 | lr: 6.4839e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2248/  5202 | global iter:   2248/  5202 | loss: 2.6580 | ds_loss: 3.3717 | lr: 6.4813e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2248/  5202 | global iter:   2248/  5202 | loss: 2.7699 | ds_loss: 3.5511 | lr: 6.4813e-07 | scale:     8.0000 | micro time: 0.304 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2249/  5202 | global iter:   2249/  5202 | loss: 2.9744 | ds_loss: 3.6514 | lr: 6.4786e-07 | scale:     8.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2250/  5202 | global iter:   2250/  5202 | loss: 2.9909 | ds_loss: 3.8673 | lr: 6.4760e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2251/  5202 | global iter:   2251/  5202 | loss: 2.9648 | ds_loss: 3.5875 | lr: 6.4733e-07 | scale:     8.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2252/  5202 | global iter:   2252/  5202 | loss: 2.6659 | ds_loss: 3.2300 | lr: 6.4707e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2252/  5202 | global iter:   2252/  5202 | loss: 2.8990 | ds_loss: 3.5841 | lr: 6.4707e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2253/  5202 | global iter:   2253/  5202 | loss: 2.8594 | ds_loss: 3.7247 | lr: 6.4680e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2254/  5202 | global iter:   2254/  5202 | loss: 2.7412 | ds_loss: 3.2888 | lr: 6.4654e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2255/  5202 | global iter:   2255/  5202 | loss: 2.8629 | ds_loss: 3.4089 | lr: 6.4627e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2256/  5202 | global iter:   2256/  5202 | loss: 2.8647 | ds_loss: 3.4666 | lr: 6.4600e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2256/  5202 | global iter:   2256/  5202 | loss: 2.8321 | ds_loss: 3.4723 | lr: 6.4600e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2257/  5202 | global iter:   2257/  5202 | loss: 2.8559 | ds_loss: 3.5494 | lr: 6.4574e-07 | scale:     8.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2258/  5202 | global iter:   2258/  5202 | loss: 2.6430 | ds_loss: 3.5544 | lr: 6.4547e-07 | scale:     8.0000 | micro time: 0.316 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2259/  5202 | global iter:   2259/  5202 | loss: 2.8470 | ds_loss: 3.5663 | lr: 6.4521e-07 | scale:     8.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2260/  5202 | global iter:   2260/  5202 | loss: 2.8999 | ds_loss: 3.4387 | lr: 6.4494e-07 | scale:     8.0000 | micro time: 0.315 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2260/  5202 | global iter:   2260/  5202 | loss: 2.8115 | ds_loss: 3.5272 | lr: 6.4494e-07 | scale:     8.0000 | micro time: 0.315 | step time: 0.315\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2261/  5202 | global iter:   2261/  5202 | loss: 2.7414 | ds_loss: 4.0176 | lr: 6.4468e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2262/  5202 | global iter:   2262/  5202 | loss: 3.0297 | ds_loss: 3.8039 | lr: 6.4441e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2263/  5202 | global iter:   2263/  5202 | loss: 2.8777 | ds_loss: 3.6722 | lr: 6.4414e-07 | scale:     8.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2264/  5202 | global iter:   2264/  5202 | loss: 2.8453 | ds_loss: 3.7176 | lr: 6.4388e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2264/  5202 | global iter:   2264/  5202 | loss: 2.8735 | ds_loss: 3.8028 | lr: 6.4388e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2265/  5202 | global iter:   2265/  5202 | loss: 2.8762 | ds_loss: 3.6418 | lr: 6.4361e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2266/  5202 | global iter:   2266/  5202 | loss: 1.6173 | ds_loss: 2.2769 | lr: 6.4335e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2267/  5202 | global iter:   2267/  5202 | loss: 2.2767 | ds_loss: 3.4649 | lr: 6.4308e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2268/  5202 | global iter:   2268/  5202 | loss: 2.4459 | ds_loss: 3.5690 | lr: 6.4282e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2268/  5202 | global iter:   2268/  5202 | loss: 2.3040 | ds_loss: 3.2382 | lr: 6.4282e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2269/  5202 | global iter:   2269/  5202 | loss: 2.0981 | ds_loss: 2.8474 | lr: 6.4255e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2270/  5202 | global iter:   2270/  5202 | loss: 2.7185 | ds_loss: 3.4550 | lr: 6.4228e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2271/  5202 | global iter:   2271/  5202 | loss: 2.2734 | ds_loss: 3.2014 | lr: 6.4202e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2272/  5202 | global iter:   2272/  5202 | loss: 2.9394 | ds_loss: 3.6262 | lr: 6.4175e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2272/  5202 | global iter:   2272/  5202 | loss: 2.5074 | ds_loss: 3.2825 | lr: 6.4175e-07 | scale:     8.0000 | micro time: 0.305 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2273/  5202 | global iter:   2273/  5202 | loss: 2.8129 | ds_loss: 3.8122 | lr: 6.4149e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2274/  5202 | global iter:   2274/  5202 | loss: 2.8888 | ds_loss: 3.9532 | lr: 6.4122e-07 | scale:     8.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2275/  5202 | global iter:   2275/  5202 | loss: 1.5597 | ds_loss: 2.0591 | lr: 6.4095e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2276/  5202 | global iter:   2276/  5202 | loss: 2.9734 | ds_loss: 3.9353 | lr: 6.4069e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2276/  5202 | global iter:   2276/  5202 | loss: 2.5587 | ds_loss: 3.4400 | lr: 6.4069e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2277/  5202 | global iter:   2277/  5202 | loss: 2.2104 | ds_loss: 2.8644 | lr: 6.4042e-07 | scale:     8.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2278/  5202 | global iter:   2278/  5202 | loss: 2.2347 | ds_loss: 2.6997 | lr: 6.4015e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2279/  5202 | global iter:   2279/  5202 | loss: 3.2799 | ds_loss: 4.0590 | lr: 6.3989e-07 | scale:     8.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2280/  5202 | global iter:   2280/  5202 | loss: 3.1375 | ds_loss: 4.4264 | lr: 6.3962e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2280/  5202 | global iter:   2280/  5202 | loss: 2.7156 | ds_loss: 3.5124 | lr: 6.3962e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2281/  5202 | global iter:   2281/  5202 | loss: 1.4335 | ds_loss: 1.9301 | lr: 6.3936e-07 | scale:     8.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2282/  5202 | global iter:   2282/  5202 | loss: 2.6364 | ds_loss: 3.3847 | lr: 6.3909e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2283/  5202 | global iter:   2283/  5202 | loss: 3.3590 | ds_loss: 4.0007 | lr: 6.3882e-07 | scale:     8.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2284/  5202 | global iter:   2284/  5202 | loss: 2.6040 | ds_loss: 3.3133 | lr: 6.3856e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2284/  5202 | global iter:   2284/  5202 | loss: 2.5082 | ds_loss: 3.1572 | lr: 6.3856e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2285/  5202 | global iter:   2285/  5202 | loss: 2.3153 | ds_loss: 3.3863 | lr: 6.3829e-07 | scale:     8.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2286/  5202 | global iter:   2286/  5202 | loss: 2.7729 | ds_loss: 3.4879 | lr: 6.3802e-07 | scale:     8.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2287/  5202 | global iter:   2287/  5202 | loss: 0.4621 | ds_loss: 0.7140 | lr: 6.3776e-07 | scale:     8.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2288/  5202 | global iter:   2288/  5202 | loss: 3.1675 | ds_loss: 3.7990 | lr: 6.3749e-07 | scale:     8.0000 | micro time: 0.314 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2288/  5202 | global iter:   2288/  5202 | loss: 2.1795 | ds_loss: 2.8468 | lr: 6.3749e-07 | scale:     8.0000 | micro time: 0.314 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2289/  5202 | global iter:   2289/  5202 | loss: 1.6603 | ds_loss: 2.4456 | lr: 6.3722e-07 | scale:     8.0000 | micro time: 0.303 | step time: 0.000\n",
            "[2025-04-16 21:14:20,049] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8, reducing to 4\n",
            "train | epoch   0 | Iter:   2290/  5202 | global iter:   2290/  5202 | loss: 3.1543 | ds_loss: 4.1810 | lr: 6.3722e-07 | scale:     4.0000 | micro time: 0.237 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2291/  5202 | global iter:   2291/  5202 | loss: 2.8199 | ds_loss: 3.5183 | lr: 6.3696e-07 | scale:     4.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2292/  5202 | global iter:   2292/  5202 | loss: 1.9749 | ds_loss: 2.8054 | lr: 6.3669e-07 | scale:     4.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2292/  5202 | global iter:   2292/  5202 | loss: 2.4023 | ds_loss: 3.2376 | lr: 6.3669e-07 | scale:     4.0000 | micro time: 0.306 | step time: 0.289\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2293/  5202 | global iter:   2293/  5202 | loss: 2.9121 | ds_loss: 3.6436 | lr: 6.3642e-07 | scale:     4.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2294/  5202 | global iter:   2294/  5202 | loss: 2.8054 | ds_loss: 3.8386 | lr: 6.3616e-07 | scale:     4.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2295/  5202 | global iter:   2295/  5202 | loss: 2.4182 | ds_loss: 3.2547 | lr: 6.3589e-07 | scale:     4.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2296/  5202 | global iter:   2296/  5202 | loss: 3.1610 | ds_loss: 3.9203 | lr: 6.3562e-07 | scale:     4.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2296/  5202 | global iter:   2296/  5202 | loss: 2.8242 | ds_loss: 3.6643 | lr: 6.3562e-07 | scale:     4.0000 | micro time: 0.309 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2297/  5202 | global iter:   2297/  5202 | loss: 2.9561 | ds_loss: 3.5946 | lr: 6.3536e-07 | scale:     4.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2298/  5202 | global iter:   2298/  5202 | loss: 2.8021 | ds_loss: 3.4106 | lr: 6.3509e-07 | scale:     4.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2299/  5202 | global iter:   2299/  5202 | loss: 2.1981 | ds_loss: 2.9690 | lr: 6.3482e-07 | scale:     4.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2300/  5202 | global iter:   2300/  5202 | loss: 2.9067 | ds_loss: 3.6887 | lr: 6.3456e-07 | scale:     4.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2300/  5202 | global iter:   2300/  5202 | loss: 2.7157 | ds_loss: 3.4157 | lr: 6.3456e-07 | scale:     4.0000 | micro time: 0.307 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2301/  5202 | global iter:   2301/  5202 | loss: 2.9438 | ds_loss: 3.5841 | lr: 6.3429e-07 | scale:     4.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2302/  5202 | global iter:   2302/  5202 | loss: 3.1950 | ds_loss: 4.0669 | lr: 6.3402e-07 | scale:     4.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2303/  5202 | global iter:   2303/  5202 | loss: 2.9079 | ds_loss: 3.5245 | lr: 6.3376e-07 | scale:     4.0000 | micro time: 0.323 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2304/  5202 | global iter:   2304/  5202 | loss: 1.9205 | ds_loss: 2.9216 | lr: 6.3349e-07 | scale:     4.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2304/  5202 | global iter:   2304/  5202 | loss: 2.7418 | ds_loss: 3.5243 | lr: 6.3349e-07 | scale:     4.0000 | micro time: 0.309 | step time: 0.312\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2305/  5202 | global iter:   2305/  5202 | loss: 2.4636 | ds_loss: 3.6115 | lr: 6.3322e-07 | scale:     4.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2306/  5202 | global iter:   2306/  5202 | loss: 1.3593 | ds_loss: 2.1833 | lr: 6.3295e-07 | scale:     4.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2307/  5202 | global iter:   2307/  5202 | loss: 2.7054 | ds_loss: 3.5036 | lr: 6.3269e-07 | scale:     4.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2308/  5202 | global iter:   2308/  5202 | loss: 2.7399 | ds_loss: 3.3536 | lr: 6.3242e-07 | scale:     4.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2308/  5202 | global iter:   2308/  5202 | loss: 2.3170 | ds_loss: 3.1630 | lr: 6.3242e-07 | scale:     4.0000 | micro time: 0.311 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2309/  5202 | global iter:   2309/  5202 | loss: 2.9693 | ds_loss: 3.8192 | lr: 6.3215e-07 | scale:     4.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2310/  5202 | global iter:   2310/  5202 | loss: 2.7574 | ds_loss: 3.3069 | lr: 6.3189e-07 | scale:     4.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2311/  5202 | global iter:   2311/  5202 | loss: 2.6537 | ds_loss: 3.4144 | lr: 6.3162e-07 | scale:     4.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2312/  5202 | global iter:   2312/  5202 | loss: 2.6563 | ds_loss: 3.2824 | lr: 6.3135e-07 | scale:     4.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2312/  5202 | global iter:   2312/  5202 | loss: 2.7592 | ds_loss: 3.4557 | lr: 6.3135e-07 | scale:     4.0000 | micro time: 0.308 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2313/  5202 | global iter:   2313/  5202 | loss: 2.5002 | ds_loss: 3.4430 | lr: 6.3108e-07 | scale:     4.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2314/  5202 | global iter:   2314/  5202 | loss: 2.9791 | ds_loss: 3.9715 | lr: 6.3082e-07 | scale:     4.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2315/  5202 | global iter:   2315/  5202 | loss: 2.3199 | ds_loss: 3.3026 | lr: 6.3055e-07 | scale:     4.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2316/  5202 | global iter:   2316/  5202 | loss: 1.9337 | ds_loss: 2.8161 | lr: 6.3028e-07 | scale:     4.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2316/  5202 | global iter:   2316/  5202 | loss: 2.4332 | ds_loss: 3.3833 | lr: 6.3028e-07 | scale:     4.0000 | micro time: 0.304 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2317/  5202 | global iter:   2317/  5202 | loss: 2.9643 | ds_loss: 4.2701 | lr: 6.3001e-07 | scale:     4.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2318/  5202 | global iter:   2318/  5202 | loss: 3.0758 | ds_loss: 3.8973 | lr: 6.2975e-07 | scale:     4.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2319/  5202 | global iter:   2319/  5202 | loss: 2.8809 | ds_loss: 4.0159 | lr: 6.2948e-07 | scale:     4.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2320/  5202 | global iter:   2320/  5202 | loss: 2.9554 | ds_loss: 3.6455 | lr: 6.2921e-07 | scale:     4.0000 | micro time: 0.315 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2320/  5202 | global iter:   2320/  5202 | loss: 2.9691 | ds_loss: 3.9572 | lr: 6.2921e-07 | scale:     4.0000 | micro time: 0.315 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2321/  5202 | global iter:   2321/  5202 | loss: 2.6903 | ds_loss: 3.4609 | lr: 6.2894e-07 | scale:     4.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2322/  5202 | global iter:   2322/  5202 | loss: 3.1312 | ds_loss: 3.9613 | lr: 6.2868e-07 | scale:     4.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2323/  5202 | global iter:   2323/  5202 | loss: 2.8386 | ds_loss: 3.5537 | lr: 6.2841e-07 | scale:     4.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2324/  5202 | global iter:   2324/  5202 | loss: 2.9771 | ds_loss: 3.7381 | lr: 6.2814e-07 | scale:     4.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2324/  5202 | global iter:   2324/  5202 | loss: 2.9093 | ds_loss: 3.6785 | lr: 6.2814e-07 | scale:     4.0000 | micro time: 0.307 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2325/  5202 | global iter:   2325/  5202 | loss: 2.7335 | ds_loss: 3.5277 | lr: 6.2787e-07 | scale:     4.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2326/  5202 | global iter:   2326/  5202 | loss: 2.0162 | ds_loss: 3.3243 | lr: 6.2761e-07 | scale:     4.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2327/  5202 | global iter:   2327/  5202 | loss: 2.1724 | ds_loss: 2.8411 | lr: 6.2734e-07 | scale:     4.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2328/  5202 | global iter:   2328/  5202 | loss: 2.9143 | ds_loss: 3.5182 | lr: 6.2707e-07 | scale:     4.0000 | micro time: 0.313 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2328/  5202 | global iter:   2328/  5202 | loss: 2.4591 | ds_loss: 3.3028 | lr: 6.2707e-07 | scale:     4.0000 | micro time: 0.313 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "[2025-04-16 21:14:32,246] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4, reducing to 2\n",
            "train | epoch   0 | Iter:   2329/  5202 | global iter:   2329/  5202 | loss: 2.1468 | ds_loss: 2.9337 | lr: 6.2707e-07 | scale:     2.0000 | micro time: 0.237 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2330/  5202 | global iter:   2330/  5202 | loss: 2.4446 | ds_loss: 3.3597 | lr: 6.2680e-07 | scale:     2.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2331/  5202 | global iter:   2331/  5202 | loss: 2.7543 | ds_loss: 3.5923 | lr: 6.2654e-07 | scale:     2.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2332/  5202 | global iter:   2332/  5202 | loss: 2.7434 | ds_loss: 3.3936 | lr: 6.2627e-07 | scale:     2.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2332/  5202 | global iter:   2332/  5202 | loss: 2.5222 | ds_loss: 3.3198 | lr: 6.2627e-07 | scale:     2.0000 | micro time: 0.303 | step time: 0.287\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2333/  5202 | global iter:   2333/  5202 | loss: 2.6977 | ds_loss: 3.5086 | lr: 6.2600e-07 | scale:     2.0000 | micro time: 0.316 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2334/  5202 | global iter:   2334/  5202 | loss: 3.0383 | ds_loss: 3.8257 | lr: 6.2573e-07 | scale:     2.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2335/  5202 | global iter:   2335/  5202 | loss: 0.7446 | ds_loss: 1.0701 | lr: 6.2546e-07 | scale:     2.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2336/  5202 | global iter:   2336/  5202 | loss: 3.1679 | ds_loss: 3.8386 | lr: 6.2520e-07 | scale:     2.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2336/  5202 | global iter:   2336/  5202 | loss: 2.4121 | ds_loss: 3.0608 | lr: 6.2520e-07 | scale:     2.0000 | micro time: 0.311 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2337/  5202 | global iter:   2337/  5202 | loss: 2.6734 | ds_loss: 3.4934 | lr: 6.2493e-07 | scale:     2.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2338/  5202 | global iter:   2338/  5202 | loss: 2.8755 | ds_loss: 3.8000 | lr: 6.2466e-07 | scale:     2.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2339/  5202 | global iter:   2339/  5202 | loss: 2.9355 | ds_loss: 3.7145 | lr: 6.2439e-07 | scale:     2.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2340/  5202 | global iter:   2340/  5202 | loss: 2.2926 | ds_loss: 3.0493 | lr: 6.2412e-07 | scale:     2.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2340/  5202 | global iter:   2340/  5202 | loss: 2.6943 | ds_loss: 3.5143 | lr: 6.2412e-07 | scale:     2.0000 | micro time: 0.308 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2341/  5202 | global iter:   2341/  5202 | loss: 1.5299 | ds_loss: 2.1063 | lr: 6.2386e-07 | scale:     2.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2342/  5202 | global iter:   2342/  5202 | loss: 2.7149 | ds_loss: 3.6036 | lr: 6.2359e-07 | scale:     2.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2343/  5202 | global iter:   2343/  5202 | loss: 3.0681 | ds_loss: 4.3322 | lr: 6.2332e-07 | scale:     2.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2344/  5202 | global iter:   2344/  5202 | loss: 2.9294 | ds_loss: 3.7710 | lr: 6.2305e-07 | scale:     2.0000 | micro time: 0.312 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2344/  5202 | global iter:   2344/  5202 | loss: 2.5606 | ds_loss: 3.4533 | lr: 6.2305e-07 | scale:     2.0000 | micro time: 0.312 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2345/  5202 | global iter:   2345/  5202 | loss: 2.6564 | ds_loss: 3.1551 | lr: 6.2278e-07 | scale:     2.0000 | micro time: 0.321 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2346/  5202 | global iter:   2346/  5202 | loss: 2.6720 | ds_loss: 3.2954 | lr: 6.2252e-07 | scale:     2.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2347/  5202 | global iter:   2347/  5202 | loss: 2.7388 | ds_loss: 3.4361 | lr: 6.2225e-07 | scale:     2.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2348/  5202 | global iter:   2348/  5202 | loss: 3.4517 | ds_loss: 4.6728 | lr: 6.2198e-07 | scale:     2.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2348/  5202 | global iter:   2348/  5202 | loss: 2.8797 | ds_loss: 3.6399 | lr: 6.2198e-07 | scale:     2.0000 | micro time: 0.305 | step time: 0.312\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2349/  5202 | global iter:   2349/  5202 | loss: 2.9312 | ds_loss: 3.8649 | lr: 6.2171e-07 | scale:     2.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2350/  5202 | global iter:   2350/  5202 | loss: 2.0035 | ds_loss: 2.6360 | lr: 6.2144e-07 | scale:     2.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2351/  5202 | global iter:   2351/  5202 | loss: 2.1323 | ds_loss: 2.6269 | lr: 6.2117e-07 | scale:     2.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2352/  5202 | global iter:   2352/  5202 | loss: 2.8389 | ds_loss: 3.8036 | lr: 6.2091e-07 | scale:     2.0000 | micro time: 0.312 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2352/  5202 | global iter:   2352/  5202 | loss: 2.4765 | ds_loss: 3.2328 | lr: 6.2091e-07 | scale:     2.0000 | micro time: 0.312 | step time: 0.312\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2353/  5202 | global iter:   2353/  5202 | loss: 2.8233 | ds_loss: 3.5497 | lr: 6.2064e-07 | scale:     2.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2354/  5202 | global iter:   2354/  5202 | loss: 2.8256 | ds_loss: 3.7113 | lr: 6.2037e-07 | scale:     2.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2355/  5202 | global iter:   2355/  5202 | loss: 3.2155 | ds_loss: 3.8138 | lr: 6.2010e-07 | scale:     2.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2356/  5202 | global iter:   2356/  5202 | loss: 2.7019 | ds_loss: 3.4777 | lr: 6.1983e-07 | scale:     2.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2356/  5202 | global iter:   2356/  5202 | loss: 2.8916 | ds_loss: 3.6381 | lr: 6.1983e-07 | scale:     2.0000 | micro time: 0.307 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2357/  5202 | global iter:   2357/  5202 | loss: 2.9731 | ds_loss: 3.5837 | lr: 6.1956e-07 | scale:     2.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2358/  5202 | global iter:   2358/  5202 | loss: 0.5847 | ds_loss: 0.9474 | lr: 6.1929e-07 | scale:     2.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2359/  5202 | global iter:   2359/  5202 | loss: 3.0074 | ds_loss: 4.2546 | lr: 6.1903e-07 | scale:     2.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2360/  5202 | global iter:   2360/  5202 | loss: 3.1071 | ds_loss: 3.9115 | lr: 6.1876e-07 | scale:     2.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2360/  5202 | global iter:   2360/  5202 | loss: 2.4181 | ds_loss: 3.1743 | lr: 6.1876e-07 | scale:     2.0000 | micro time: 0.306 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2361/  5202 | global iter:   2361/  5202 | loss: 2.4602 | ds_loss: 3.2052 | lr: 6.1849e-07 | scale:     2.0000 | micro time: 0.310 | step time: 0.000\n",
            "[2025-04-16 21:14:42,555] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2, reducing to 1\n",
            "train | epoch   0 | Iter:   2362/  5202 | global iter:   2362/  5202 | loss: 3.1891 | ds_loss: 3.8201 | lr: 6.1849e-07 | scale:     1.0000 | micro time: 0.242 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2363/  5202 | global iter:   2363/  5202 | loss: 2.6046 | ds_loss: 3.6391 | lr: 6.1822e-07 | scale:     1.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2364/  5202 | global iter:   2364/  5202 | loss: 2.7116 | ds_loss: 3.4595 | lr: 6.1795e-07 | scale:     1.0000 | micro time: 0.303 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2364/  5202 | global iter:   2364/  5202 | loss: 2.7414 | ds_loss: 3.5310 | lr: 6.1795e-07 | scale:     1.0000 | micro time: 0.303 | step time: 0.289\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2365/  5202 | global iter:   2365/  5202 | loss: 2.9913 | ds_loss: 3.8012 | lr: 6.1768e-07 | scale:     1.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2366/  5202 | global iter:   2366/  5202 | loss: 2.0438 | ds_loss: 2.6832 | lr: 6.1741e-07 | scale:     1.0000 | micro time: 0.320 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2367/  5202 | global iter:   2367/  5202 | loss: 3.0953 | ds_loss: 3.6767 | lr: 6.1715e-07 | scale:     1.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2368/  5202 | global iter:   2368/  5202 | loss: 2.8254 | ds_loss: 3.8443 | lr: 6.1688e-07 | scale:     1.0000 | micro time: 0.308 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2368/  5202 | global iter:   2368/  5202 | loss: 2.7390 | ds_loss: 3.5014 | lr: 6.1688e-07 | scale:     1.0000 | micro time: 0.308 | step time: 0.312\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2369/  5202 | global iter:   2369/  5202 | loss: 2.6012 | ds_loss: 3.3514 | lr: 6.1661e-07 | scale:     1.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2370/  5202 | global iter:   2370/  5202 | loss: 2.9395 | ds_loss: 3.8053 | lr: 6.1634e-07 | scale:     1.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2371/  5202 | global iter:   2371/  5202 | loss: 2.7386 | ds_loss: 3.8554 | lr: 6.1607e-07 | scale:     1.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2372/  5202 | global iter:   2372/  5202 | loss: 2.9725 | ds_loss: 3.9501 | lr: 6.1580e-07 | scale:     1.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2372/  5202 | global iter:   2372/  5202 | loss: 2.8129 | ds_loss: 3.7406 | lr: 6.1580e-07 | scale:     1.0000 | micro time: 0.305 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2373/  5202 | global iter:   2373/  5202 | loss: 3.0328 | ds_loss: 3.6993 | lr: 6.1553e-07 | scale:     1.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2374/  5202 | global iter:   2374/  5202 | loss: 2.4857 | ds_loss: 3.2970 | lr: 6.1526e-07 | scale:     1.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2375/  5202 | global iter:   2375/  5202 | loss: 3.0462 | ds_loss: 4.0213 | lr: 6.1500e-07 | scale:     1.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2376/  5202 | global iter:   2376/  5202 | loss: 3.2256 | ds_loss: 3.8678 | lr: 6.1473e-07 | scale:     1.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2376/  5202 | global iter:   2376/  5202 | loss: 2.9476 | ds_loss: 3.7214 | lr: 6.1473e-07 | scale:     1.0000 | micro time: 0.310 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2377/  5202 | global iter:   2377/  5202 | loss: 2.9695 | ds_loss: 3.7579 | lr: 6.1446e-07 | scale:     1.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2378/  5202 | global iter:   2378/  5202 | loss: 1.8928 | ds_loss: 2.7960 | lr: 6.1419e-07 | scale:     1.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2379/  5202 | global iter:   2379/  5202 | loss: 1.9880 | ds_loss: 2.7258 | lr: 6.1392e-07 | scale:     1.0000 | micro time: 0.302 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2380/  5202 | global iter:   2380/  5202 | loss: 2.9878 | ds_loss: 3.8581 | lr: 6.1365e-07 | scale:     1.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2380/  5202 | global iter:   2380/  5202 | loss: 2.4595 | ds_loss: 3.2844 | lr: 6.1365e-07 | scale:     1.0000 | micro time: 0.305 | step time: 0.305\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2381/  5202 | global iter:   2381/  5202 | loss: 2.6436 | ds_loss: 3.2959 | lr: 6.1338e-07 | scale:     1.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2382/  5202 | global iter:   2382/  5202 | loss: 0.9537 | ds_loss: 1.2465 | lr: 6.1311e-07 | scale:     1.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2383/  5202 | global iter:   2383/  5202 | loss: 2.5951 | ds_loss: 3.2576 | lr: 6.1284e-07 | scale:     1.0000 | micro time: 0.315 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2384/  5202 | global iter:   2384/  5202 | loss: 2.9958 | ds_loss: 3.8541 | lr: 6.1257e-07 | scale:     1.0000 | micro time: 0.310 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2384/  5202 | global iter:   2384/  5202 | loss: 2.2970 | ds_loss: 2.9135 | lr: 6.1257e-07 | scale:     1.0000 | micro time: 0.310 | step time: 0.313\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2385/  5202 | global iter:   2385/  5202 | loss: 2.9627 | ds_loss: 3.5948 | lr: 6.1230e-07 | scale:     1.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2386/  5202 | global iter:   2386/  5202 | loss: 1.2899 | ds_loss: 1.8737 | lr: 6.1204e-07 | scale:     1.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2387/  5202 | global iter:   2387/  5202 | loss: 2.9115 | ds_loss: 4.0945 | lr: 6.1177e-07 | scale:     1.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2388/  5202 | global iter:   2388/  5202 | loss: 2.6529 | ds_loss: 3.1687 | lr: 6.1150e-07 | scale:     1.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2388/  5202 | global iter:   2388/  5202 | loss: 2.4542 | ds_loss: 3.1829 | lr: 6.1150e-07 | scale:     1.0000 | micro time: 0.311 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2389/  5202 | global iter:   2389/  5202 | loss: 2.6880 | ds_loss: 3.5288 | lr: 6.1123e-07 | scale:     1.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2390/  5202 | global iter:   2390/  5202 | loss: 3.0066 | ds_loss: 3.6572 | lr: 6.1096e-07 | scale:     1.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2391/  5202 | global iter:   2391/  5202 | loss: 2.2546 | ds_loss: 3.5511 | lr: 6.1069e-07 | scale:     1.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2392/  5202 | global iter:   2392/  5202 | loss: 2.5991 | ds_loss: 3.1814 | lr: 6.1042e-07 | scale:     1.0000 | micro time: 0.317 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2392/  5202 | global iter:   2392/  5202 | loss: 2.6371 | ds_loss: 3.4796 | lr: 6.1042e-07 | scale:     1.0000 | micro time: 0.317 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2393/  5202 | global iter:   2393/  5202 | loss: 2.8693 | ds_loss: 3.4952 | lr: 6.1015e-07 | scale:     1.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2394/  5202 | global iter:   2394/  5202 | loss: 1.6114 | ds_loss: 2.2193 | lr: 6.0988e-07 | scale:     1.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2395/  5202 | global iter:   2395/  5202 | loss: 2.8295 | ds_loss: 3.5924 | lr: 6.0961e-07 | scale:     1.0000 | micro time: 0.316 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2396/  5202 | global iter:   2396/  5202 | loss: 2.0295 | ds_loss: 2.7806 | lr: 6.0934e-07 | scale:     1.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2396/  5202 | global iter:   2396/  5202 | loss: 2.3349 | ds_loss: 3.0219 | lr: 6.0934e-07 | scale:     1.0000 | micro time: 0.309 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2397/  5202 | global iter:   2397/  5202 | loss: 2.5750 | ds_loss: 3.1544 | lr: 6.0907e-07 | scale:     1.0000 | micro time: 0.316 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2398/  5202 | global iter:   2398/  5202 | loss: 2.9698 | ds_loss: 3.6165 | lr: 6.0880e-07 | scale:     1.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2399/  5202 | global iter:   2399/  5202 | loss: 2.5971 | ds_loss: 3.7060 | lr: 6.0853e-07 | scale:     1.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2400/  5202 | global iter:   2400/  5202 | loss: 2.8693 | ds_loss: 3.6937 | lr: 6.0827e-07 | scale:     1.0000 | micro time: 0.311 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2400/  5202 | global iter:   2400/  5202 | loss: 2.7528 | ds_loss: 3.5426 | lr: 6.0827e-07 | scale:     1.0000 | micro time: 0.311 | step time: 0.311\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2401/  5202 | global iter:   2401/  5202 | loss: 3.0408 | ds_loss: 3.8814 | lr: 6.0800e-07 | scale:     1.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2402/  5202 | global iter:   2402/  5202 | loss: 2.3951 | ds_loss: 3.3913 | lr: 6.0773e-07 | scale:     1.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2403/  5202 | global iter:   2403/  5202 | loss: 2.9557 | ds_loss: 3.7429 | lr: 6.0746e-07 | scale:     1.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2404/  5202 | global iter:   2404/  5202 | loss: 2.0628 | ds_loss: 2.6262 | lr: 6.0719e-07 | scale:     1.0000 | micro time: 0.307 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2404/  5202 | global iter:   2404/  5202 | loss: 2.6136 | ds_loss: 3.4104 | lr: 6.0719e-07 | scale:     1.0000 | micro time: 0.307 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2405/  5202 | global iter:   2405/  5202 | loss: 1.8155 | ds_loss: 2.7043 | lr: 6.0692e-07 | scale:     1.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2406/  5202 | global iter:   2406/  5202 | loss: 3.2241 | ds_loss: 4.0946 | lr: 6.0665e-07 | scale:     1.0000 | micro time: 0.316 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2407/  5202 | global iter:   2407/  5202 | loss: 2.8101 | ds_loss: 3.5592 | lr: 6.0638e-07 | scale:     1.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2408/  5202 | global iter:   2408/  5202 | loss: 2.6588 | ds_loss: 3.3155 | lr: 6.0611e-07 | scale:     1.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2408/  5202 | global iter:   2408/  5202 | loss: 2.6271 | ds_loss: 3.4184 | lr: 6.0611e-07 | scale:     1.0000 | micro time: 0.306 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2409/  5202 | global iter:   2409/  5202 | loss: 3.0834 | ds_loss: 3.6693 | lr: 6.0584e-07 | scale:     1.0000 | micro time: 0.316 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2410/  5202 | global iter:   2410/  5202 | loss: 2.2279 | ds_loss: 2.7431 | lr: 6.0557e-07 | scale:     1.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2411/  5202 | global iter:   2411/  5202 | loss: 3.6202 | ds_loss: 4.6950 | lr: 6.0530e-07 | scale:     1.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2412/  5202 | global iter:   2412/  5202 | loss: 1.7111 | ds_loss: 1.9867 | lr: 6.0503e-07 | scale:     1.0000 | micro time: 0.306 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2412/  5202 | global iter:   2412/  5202 | loss: 2.6607 | ds_loss: 3.2735 | lr: 6.0503e-07 | scale:     1.0000 | micro time: 0.306 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2413/  5202 | global iter:   2413/  5202 | loss: 2.9828 | ds_loss: 3.6264 | lr: 6.0476e-07 | scale:     1.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2414/  5202 | global iter:   2414/  5202 | loss: 3.1670 | ds_loss: 3.8613 | lr: 6.0449e-07 | scale:     1.0000 | micro time: 0.313 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2415/  5202 | global iter:   2415/  5202 | loss: 2.9165 | ds_loss: 3.7432 | lr: 6.0422e-07 | scale:     1.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2416/  5202 | global iter:   2416/  5202 | loss: 1.8309 | ds_loss: 2.6168 | lr: 6.0395e-07 | scale:     1.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2416/  5202 | global iter:   2416/  5202 | loss: 2.7243 | ds_loss: 3.4619 | lr: 6.0395e-07 | scale:     1.0000 | micro time: 0.309 | step time: 0.310\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2417/  5202 | global iter:   2417/  5202 | loss: 2.9359 | ds_loss: 3.8006 | lr: 6.0368e-07 | scale:     1.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2418/  5202 | global iter:   2418/  5202 | loss: 3.1807 | ds_loss: 4.1979 | lr: 6.0341e-07 | scale:     1.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2419/  5202 | global iter:   2419/  5202 | loss: 2.8750 | ds_loss: 3.8351 | lr: 6.0314e-07 | scale:     1.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2420/  5202 | global iter:   2420/  5202 | loss: 2.9208 | ds_loss: 3.7706 | lr: 6.0287e-07 | scale:     1.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2420/  5202 | global iter:   2420/  5202 | loss: 2.9781 | ds_loss: 3.9011 | lr: 6.0287e-07 | scale:     1.0000 | micro time: 0.304 | step time: 0.306\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2421/  5202 | global iter:   2421/  5202 | loss: 2.6445 | ds_loss: 3.5510 | lr: 6.0260e-07 | scale:     1.0000 | micro time: 0.311 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2422/  5202 | global iter:   2422/  5202 | loss: 2.6578 | ds_loss: 3.3778 | lr: 6.0233e-07 | scale:     1.0000 | micro time: 0.303 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2423/  5202 | global iter:   2423/  5202 | loss: 2.9389 | ds_loss: 3.5574 | lr: 6.0206e-07 | scale:     1.0000 | micro time: 0.309 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2424/  5202 | global iter:   2424/  5202 | loss: 2.9441 | ds_loss: 4.1147 | lr: 6.0179e-07 | scale:     1.0000 | micro time: 0.304 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2424/  5202 | global iter:   2424/  5202 | loss: 2.7963 | ds_loss: 3.6502 | lr: 6.0179e-07 | scale:     1.0000 | micro time: 0.304 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2425/  5202 | global iter:   2425/  5202 | loss: 0.5880 | ds_loss: 0.7296 | lr: 6.0152e-07 | scale:     1.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2426/  5202 | global iter:   2426/  5202 | loss: 3.0750 | ds_loss: 3.8696 | lr: 6.0125e-07 | scale:     1.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2427/  5202 | global iter:   2427/  5202 | loss: 2.5606 | ds_loss: 3.7106 | lr: 6.0098e-07 | scale:     1.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2428/  5202 | global iter:   2428/  5202 | loss: 3.0997 | ds_loss: 3.7714 | lr: 6.0071e-07 | scale:     1.0000 | micro time: 0.313 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2428/  5202 | global iter:   2428/  5202 | loss: 2.3308 | ds_loss: 3.0203 | lr: 6.0071e-07 | scale:     1.0000 | micro time: 0.313 | step time: 0.308\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2429/  5202 | global iter:   2429/  5202 | loss: 3.0481 | ds_loss: 3.7937 | lr: 6.0044e-07 | scale:     1.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2430/  5202 | global iter:   2430/  5202 | loss: 3.0458 | ds_loss: 4.0755 | lr: 6.0017e-07 | scale:     1.0000 | micro time: 0.307 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2431/  5202 | global iter:   2431/  5202 | loss: 2.7204 | ds_loss: 3.3544 | lr: 5.9990e-07 | scale:     1.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2432/  5202 | global iter:   2432/  5202 | loss: 3.5636 | ds_loss: 4.8719 | lr: 5.9963e-07 | scale:     1.0000 | micro time: 0.305 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2432/  5202 | global iter:   2432/  5202 | loss: 3.0945 | ds_loss: 4.0239 | lr: 5.9963e-07 | scale:     1.0000 | micro time: 0.305 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2433/  5202 | global iter:   2433/  5202 | loss: 3.2027 | ds_loss: 5.1541 | lr: 5.9936e-07 | scale:     1.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2434/  5202 | global iter:   2434/  5202 | loss: 3.1267 | ds_loss: 3.9728 | lr: 5.9909e-07 | scale:     1.0000 | micro time: 0.314 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2435/  5202 | global iter:   2435/  5202 | loss: 2.8638 | ds_loss: 3.7290 | lr: 5.9882e-07 | scale:     1.0000 | micro time: 0.308 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2436/  5202 | global iter:   2436/  5202 | loss: 2.8043 | ds_loss: 3.5135 | lr: 5.9855e-07 | scale:     1.0000 | micro time: 0.309 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2436/  5202 | global iter:   2436/  5202 | loss: 2.9994 | ds_loss: 4.0923 | lr: 5.9855e-07 | scale:     1.0000 | micro time: 0.309 | step time: 0.309\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2437/  5202 | global iter:   2437/  5202 | loss: 2.7935 | ds_loss: 3.4375 | lr: 5.9828e-07 | scale:     1.0000 | micro time: 0.318 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2438/  5202 | global iter:   2438/  5202 | loss: 2.7233 | ds_loss: 3.3688 | lr: 5.9801e-07 | scale:     1.0000 | micro time: 0.312 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2439/  5202 | global iter:   2439/  5202 | loss: 2.3605 | ds_loss: 3.1516 | lr: 5.9774e-07 | scale:     1.0000 | micro time: 0.310 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2440/  5202 | global iter:   2440/  5202 | loss: 3.1277 | ds_loss: 4.0789 | lr: 5.9747e-07 | scale:     1.0000 | micro time: 0.314 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2440/  5202 | global iter:   2440/  5202 | loss: 2.7513 | ds_loss: 3.5092 | lr: 5.9747e-07 | scale:     1.0000 | micro time: 0.314 | step time: 0.314\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2441/  5202 | global iter:   2441/  5202 | loss: 2.4789 | ds_loss: 3.0451 | lr: 5.9720e-07 | scale:     1.0000 | micro time: 0.305 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2442/  5202 | global iter:   2442/  5202 | loss: 2.7773 | ds_loss: 3.4721 | lr: 5.9693e-07 | scale:     1.0000 | micro time: 0.306 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2443/  5202 | global iter:   2443/  5202 | loss: 1.8558 | ds_loss: 2.6237 | lr: 5.9666e-07 | scale:     1.0000 | micro time: 0.304 | step time: 0.000\n",
            "train | epoch   0 | Iter:   2444/  5202 | global iter:   2444/  5202 | loss: 2.9153 | ds_loss: 3.5635 | lr: 5.9639e-07 | scale:     1.0000 | micro time: 0.313 | step time: 0.000\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2444/  5202 | global iter:   2444/  5202 | loss: 2.5068 | ds_loss: 3.1761 | lr: 5.9639e-07 | scale:     1.0000 | micro time: 0.313 | step time: 0.307\n",
            "/content/LMOps/minillm/results/gpt2/train/kd/e1-bs2-lr1e-06-G1-N1-NN1-kd0.5\n",
            "****************************************************************************************************\n",
            "train | epoch   0 | Iter:   2445/  5202 | global iter:   2445/  5202 | loss: 2.6409 | ds_loss: 3.7239 | lr: 5.9612e-07 | scale:     1.0000 | micro time: 0.305 | step time: 0.000\n",
            "[rank0]: Traceback (most recent call last):\n",
            "[rank0]:   File \"/content/LMOps/minillm/finetune.py\", line 555, in <module>\n",
            "[rank0]:     main()\n",
            "[rank0]:   File \"/content/LMOps/minillm/finetune.py\", line 548, in main\n",
            "[rank0]:     model = finetune(args, tokenizer, model, optimizer, lr_scheduler, dataset, device, teacher_model=teacher_model)\n",
            "[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/content/LMOps/minillm/finetune.py\", line 277, in finetune\n",
            "[rank0]:     model.step()\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 2378, in step\n",
            "[rank0]:     self._take_model_step(lr_kwargs)\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\", line 2281, in _take_model_step\n",
            "[rank0]:     self.optimizer.step()\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 1848, in step\n",
            "[rank0]:     self._update_scale(self.overflow)\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 2089, in _update_scale\n",
            "[rank0]:     self.loss_scaler.update_scale(has_overflow)\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/fp16/loss_scaler.py\", line 175, in update_scale\n",
            "[rank0]:     raise Exception(\n",
            "[rank0]: Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.\n",
            "[rank0]:[W416 21:15:10.320072633 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "E0416 21:15:12.187000 60805 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 60830) of binary: /usr/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/torchrun\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py\", line 918, in main\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py\", line 909, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "/content/LMOps/minillm/finetune.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2025-04-16_21:15:12\n",
            "  host      : ddb1166cb5eb\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : 1 (pid: 60830)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "#Run sft\n",
        "!bash scripts/gpt2/sft/sft_large.sh /content/LMOps/minillm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy the process data into drive\n",
        "source = '/content/LMOps/minillm/results/gpt2/train/sft'\n",
        "destination = '/content/drive/MyDrive/MIDS/minillm/gpt2/train/sft'\n",
        "shutil.copytree(source, destination, dirs_exist_ok=True)"
      ],
      "metadata": {
        "id": "a79pRBhQCRmV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}