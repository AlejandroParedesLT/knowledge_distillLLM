compsci-cluster-fitz-12
Sun Apr  6 08:20:52 PM EDT 2025
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2012 ./finetune.py --base-path . --model-path gpt2-xl --ckpt-name gpt2-xlarge --n-gpu 4 --data-dir ./processed_data/bugnet_python/full/gpt2/ --num-workers 0 --dev-num 1000 --lr 0.00005 --batch-size 2 --eval-batch-size 4 --gradient-accumulation-steps 1 --warmup-iters 0 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --epochs 10 --max-length 512 --max-prompt-length 256 --do-train --do-valid --eval-gen --save-interval 5000 --eval-interval 5000 --log-interval 4 --mid-log-num 10 --save ./results/gpt2/train/sft --seed 10 --seed-order 10 --deepspeed --deepspeed_config ./configs/deepspeed/ds_config_zero1_fp16.json --type lm --do-sample --top-k 0 --top-p 1.0 --temperature 1.0
PYTHONPATH=.
W0406 20:20:57.136000 3209798 torch/distributed/run.py:792] 
W0406 20:20:57.136000 3209798 torch/distributed/run.py:792] *****************************************
W0406 20:20:57.136000 3209798 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0406 20:20:57.136000 3209798 torch/distributed/run.py:792] *****************************************
[2025-04-06 20:21:02,833] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-06 20:21:02,835] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-06 20:21:02,837] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-06 20:21:02,839] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/users/ap794/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.Warning: The cache directory for DeepSpeed Triton autotune, /home/users/ap794/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.

Warning: The cache directory for DeepSpeed Triton autotune, /home/users/ap794/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/users/ap794/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
using world size: 4
[2025-04-06 20:21:11,342] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-06 20:21:11,348] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
arguments:
  model_path ................... gpt2-xl
  ckpt_name .................... gpt2-xlarge
  model_type ................... gpt2
  teacher_model_type ........... None
  n_gpu ........................ 4
  n_nodes ...................... 1
  teacher_model_path ........... None
  teacher_ckpt_name ............ None
  teacher_model_fp16 ........... False
  model_parallel ............... False
  model_parallel_size .......... None
  no_value ..................... False
  dropout_path_rate ............ None
  dtype ........................ torch.float16
  type ......................... lm
  do_train ..................... True
  do_valid ..................... True
  do_eval ...................... False
  base_path .................... .
  load ......................... None
  save ......................... ./results/gpt2/train/sft/e10-bs2-lr5e-05-G1-N4-NN1
  log_interval ................. 4
  mid_log_num .................. 10
  save_interval ................ 5000
  eval_interval ................ 5000
  local_rank ................... 0
  save_additional_suffix ....... 
  save_rollout ................. False
  eb_sample_times .............. 3
  data_dir ..................... ./processed_data/bugnet_python/full/gpt2/
  processed_data_dir ........... None
  force_process ................ False
  force_process_demo ........... False
  data_process_workers ......... -1
  train_num .................... -1
  train_ratio .................. 1
  dev_num ...................... 1000
  dev_ratio .................... 1
  gen_num ...................... -1
  data_names ................... None
  prompt_type .................. None
  num_workers .................. 0
  max_prompt_length ............ 256
  min_prompt_length ............ 128
  json_data .................... False
  bin_data ..................... False
  txt_data ..................... False
  prompt_data_dir .............. None
  lm_data_dir .................. None
  eval_ppl ..................... False
  eval_rw ...................... False
  eval_gen ..................... True
  only_prompt .................. False
  batch_size ................... 2
  eval_batch_size .............. 4
  clip_grad .................... 1.0
  total_iters .................. None
  train_iters_per_epoch ........ -1
  max_length ................... 512
  seed ......................... 10
  seed_order ................... 10
  seed_data .................... 42
  seed_ppo ..................... 42
  seed_lm ...................... 7
  epochs ....................... 10
  training_epochs .............. 10000
  gradient_accumulation_steps .. 1
  gradient_checkpointing ....... False
  attn_dtype ................... None
  lr ........................... 5e-05
  lr_min ....................... 1e-07
  weight_decay ................. 0.01
  loss_scale ................... 65536
  kd_ratio ..................... None
  warmup_iters ................. 0
  lr_decay_iters ............... None
  lr_decay_style ............... cosine
  scheduler_name ............... constant_trm
  reward_scaling ............... None
  cliprange_reward ............. 1
  ppo_epochs ................... None
  num_rollouts ................. 256
  num_rollouts_per_device ...... None
  cliprange .................... 0.2
  chunk_size ................... None
  gamma ........................ 0.95
  length_norm .................. False
  single_step_reg .............. False
  teacher_mixed_alpha .......... None
  lm_coef ...................... 1
  top_k ........................ 0
  top_p ........................ 1.0
  do_sample .................... True
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  num_beams .................... 1
  temperature .................. 1.0
  peft ......................... None
  peft_lora_r .................. 8
  peft_lora_alpha .............. 32
  peft_lora_dropout ............ 0.1
  peft_name .................... None
  peft_path .................... None
  teacher_peft_name ............ None
  teacher_peft_path ............ None
  deepspeed .................... True
  deepspeed_config ............. ./configs/deepspeed/ds_config_zero1_fp16.json
  deepscale .................... False
  deepscale_config ............. None
  rank ......................... 0
  world_size ................... 4
[2025-04-06 20:21:11,589] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-06 20:21:11,591] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-06 20:21:11,596] [INFO] [comm.py:658:init_distributed] cdb=None
Probing Dataset
Probing end. Max data state 1, total length 1693
1693
Num LM instances: 1693
train num 1693
Probing Dataset
Probing end. Max data state 1, total length 596
596
Num LM instances: 596
Train iters per epoch 211
total_iters 2110
 > number of parameters: 1557611200
Model load time: 76.24203658103943s
Optimizer = AdamW
[2025-04-06 20:22:30,075] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.4, git-hash=unknown, git-branch=unknown
[2025-04-06 20:22:30,076] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 4
[2025-04-06 20:22:30,158] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 4
[2025-04-06 20:22:30,210] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 4
[2025-04-06 20:22:30,220] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 4
[2025-04-06 20:22:31,237] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-04-06 20:22:31,240] [INFO] [logging.py:128:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-04-06 20:22:31,241] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-06 20:22:31,377] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2025-04-06 20:22:31,378] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2025-04-06 20:22:31,378] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 1 optimizer
[2025-04-06 20:22:31,379] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-06 20:22:31,379] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-06 20:22:31,379] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-06 20:22:31,379] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
Sun Apr  6 20:23:03 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                  Off |
| 30%   23C    P8             18W /  230W |     413MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                  Off |
| 30%   25C    P8             19W /  230W |     413MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                  Off |
| 30%   29C    P8             16W /  230W |     413MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                  Off |
| 30%   30C    P2             60W /  230W |    9341MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   3209801      C   ...project_distillLLM/venv/bin/python3        406MiB |
|    1   N/A  N/A   3209802      C   ...project_distillLLM/venv/bin/python3        406MiB |
|    2   N/A  N/A   3209803      C   ...project_distillLLM/venv/bin/python3        406MiB |
|    3   N/A  N/A   3209804      C   ...project_distillLLM/venv/bin/python3       9334MiB |
+-----------------------------------------------------------------------------------------+

Sun Apr  6 20:23:24 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                  Off |
| 30%   25C    P2             59W /  230W |    7855MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                  Off |
| 30%   26C    P2             61W /  230W |    9341MiB /  24564MiB |      1%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                  Off |
| 30%   30C    P2             59W /  230W |    9341MiB /  24564MiB |      1%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                  Off |
| 30%   38C    P2            149W /  230W |   10851MiB /  24564MiB |     55%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   3209801      C   ...project_distillLLM/venv/bin/python3       7848MiB |
|    1   N/A  N/A   3209802      C   ...project_distillLLM/venv/bin/python3       9334MiB |
|    2   N/A  N/A   3209803      C   ...project_distillLLM/venv/bin/python3       9334MiB |
|    3   N/A  N/A   3209804      C   ...project_distillLLM/venv/bin/python3      10844MiB |
+-----------------------------------------------------------------------------------------+

Sun Apr  6 20:23:24 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                  Off |
| 30%   25C    P2             59W /  230W |    7855MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                  Off |
| 30%   26C    P2             61W /  230W |    9341MiB /  24564MiB |      1%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                  Off |
| 30%   30C    P2             59W /  230W |    9341MiB /  24564MiB |      1%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                  Off |
| 30%   38C    P2            149W /  230W |   10851MiB /  24564MiB |     55%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   3209801      C   ...project_distillLLM/venv/bin/python3       7848MiB |
|    1   N/A  N/A   3209802      C   ...project_distillLLM/venv/bin/python3       9334MiB |
|    2   N/A  N/A   3209803      C   ...project_distillLLM/venv/bin/python3       9334MiB |
|    3   N/A  N/A   3209804      C   ...project_distillLLM/venv/bin/python3      10844MiB |
+-----------------------------------------------------------------------------------------+

[2025-04-06 20:23:26,656] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-06 20:23:26,659] [INFO] [utils.py:782:see_memory_usage] MA 7.3 GB         Max_MA 7.3 GB         CA 7.32 GB         Max_CA 7 GB 
[2025-04-06 20:23:26,659] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 36.18 GB, percent = 4.8%
[2025-04-06 20:23:26,859] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-06 20:23:26,860] [INFO] [utils.py:782:see_memory_usage] MA 7.3 GB         Max_MA 8.75 GB         CA 8.77 GB         Max_CA 9 GB 
[2025-04-06 20:23:26,860] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 36.26 GB, percent = 4.8%
[2025-04-06 20:23:26,861] [INFO] [stage_1_and_2.py:550:__init__] optimizer state initialized
[2025-04-06 20:23:27,021] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-06 20:23:27,022] [INFO] [utils.py:782:see_memory_usage] MA 7.3 GB         Max_MA 7.3 GB         CA 8.77 GB         Max_CA 9 GB 
[2025-04-06 20:23:27,023] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 36.3 GB, percent = 4.8%
[2025-04-06 20:23:27,030] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-06 20:23:27,031] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-04-06 20:23:27,031] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f0b4fe5a140>
[2025-04-06 20:23:27,031] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05, 5e-05], mom=[(0.9, 0.999), (0.9, 0.999)]
[2025-04-06 20:23:27,033] [INFO] [config.py:1001:print] DeepSpeedEngine configuration:
[2025-04-06 20:23:27,034] [INFO] [config.py:1005:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-06 20:23:27,034] [INFO] [config.py:1005:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-06 20:23:27,034] [INFO] [config.py:1005:print]   amp_enabled .................. False
[2025-04-06 20:23:27,034] [INFO] [config.py:1005:print]   amp_params ................... False
[2025-04-06 20:23:27,035] [INFO] [config.py:1005:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-06 20:23:27,035] [INFO] [config.py:1005:print]   bfloat16_enabled ............. False
[2025-04-06 20:23:27,035] [INFO] [config.py:1005:print]   bfloat16_immediate_grad_update  False
[2025-04-06 20:23:27,035] [INFO] [config.py:1005:print]   checkpoint_parallel_write_pipeline  False
[2025-04-06 20:23:27,035] [INFO] [config.py:1005:print]   checkpoint_tag_validation_enabled  True
[2025-04-06 20:23:27,035] [INFO] [config.py:1005:print]   checkpoint_tag_validation_fail  False
[2025-04-06 20:23:27,036] [INFO] [config.py:1005:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f0b615b1b70>
[2025-04-06 20:23:27,036] [INFO] [config.py:1005:print]   communication_data_type ...... None
[2025-04-06 20:23:27,036] [INFO] [config.py:1005:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-06 20:23:27,036] [INFO] [config.py:1005:print]   curriculum_enabled_legacy .... False
[2025-04-06 20:23:27,036] [INFO] [config.py:1005:print]   curriculum_params_legacy ..... False
[2025-04-06 20:23:27,036] [INFO] [config.py:1005:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-06 20:23:27,036] [INFO] [config.py:1005:print]   data_efficiency_enabled ...... False
[2025-04-06 20:23:27,036] [INFO] [config.py:1005:print]   dataloader_drop_last ......... False
[2025-04-06 20:23:27,036] [INFO] [config.py:1005:print]   disable_allgather ............ False
[2025-04-06 20:23:27,037] [INFO] [config.py:1005:print]   dump_state ................... False
[2025-04-06 20:23:27,037] [INFO] [config.py:1005:print]   dynamic_loss_scale_args ...... None
[2025-04-06 20:23:27,037] [INFO] [config.py:1005:print]   eigenvalue_enabled ........... False
[2025-04-06 20:23:27,037] [INFO] [config.py:1005:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-06 20:23:27,037] [INFO] [config.py:1005:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-06 20:23:27,037] [INFO] [config.py:1005:print]   eigenvalue_layer_num ......... 0
[2025-04-06 20:23:27,037] [INFO] [config.py:1005:print]   eigenvalue_max_iter .......... 100
[2025-04-06 20:23:27,037] [INFO] [config.py:1005:print]   eigenvalue_stability ......... 1e-06
[2025-04-06 20:23:27,037] [INFO] [config.py:1005:print]   eigenvalue_tol ............... 0.01
[2025-04-06 20:23:27,037] [INFO] [config.py:1005:print]   eigenvalue_verbose ........... False
[2025-04-06 20:23:27,037] [INFO] [config.py:1005:print]   elasticity_enabled ........... False
[2025-04-06 20:23:27,038] [INFO] [config.py:1005:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-06 20:23:27,038] [INFO] [config.py:1005:print]   fp16_auto_cast ............... None
[2025-04-06 20:23:27,038] [INFO] [config.py:1005:print]   fp16_enabled ................. False
[2025-04-06 20:23:27,038] [INFO] [config.py:1005:print]   fp16_master_weights_and_gradients  False
[2025-04-06 20:23:27,038] [INFO] [config.py:1005:print]   global_rank .................. 0
[2025-04-06 20:23:27,038] [INFO] [config.py:1005:print]   grad_accum_dtype ............. None
[2025-04-06 20:23:27,038] [INFO] [config.py:1005:print]   gradient_accumulation_steps .. 1
[2025-04-06 20:23:27,038] [INFO] [config.py:1005:print]   gradient_clipping ............ 1.0
[2025-04-06 20:23:27,038] [INFO] [config.py:1005:print]   gradient_predivide_factor .... 1.0
[2025-04-06 20:23:27,039] [INFO] [config.py:1005:print]   graph_harvesting ............. False
[2025-04-06 20:23:27,039] [INFO] [config.py:1005:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-06 20:23:27,039] [INFO] [config.py:1005:print]   initial_dynamic_scale ........ 65536
[2025-04-06 20:23:27,039] [INFO] [config.py:1005:print]   load_universal_checkpoint .... False
[2025-04-06 20:23:27,039] [INFO] [config.py:1005:print]   loss_scale ................... 0
[2025-04-06 20:23:27,039] [INFO] [config.py:1005:print]   memory_breakdown ............. False
[2025-04-06 20:23:27,039] [INFO] [config.py:1005:print]   mics_hierarchial_params_gather  False
[2025-04-06 20:23:27,039] [INFO] [config.py:1005:print]   mics_shard_size .............. -1
[2025-04-06 20:23:27,039] [INFO] [config.py:1005:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-06 20:23:27,040] [INFO] [config.py:1005:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-06 20:23:27,040] [INFO] [config.py:1005:print]   optimizer_legacy_fusion ...... False
[2025-04-06 20:23:27,040] [INFO] [config.py:1005:print]   optimizer_name ............... None
[2025-04-06 20:23:27,040] [INFO] [config.py:1005:print]   optimizer_params ............. None
[2025-04-06 20:23:27,040] [INFO] [config.py:1005:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-06 20:23:27,040] [INFO] [config.py:1005:print]   pld_enabled .................. False
[2025-04-06 20:23:27,040] [INFO] [config.py:1005:print]   pld_params ................... False
[2025-04-06 20:23:27,040] [INFO] [config.py:1005:print]   prescale_gradients ........... False
[2025-04-06 20:23:27,040] [INFO] [config.py:1005:print]   scheduler_name ............... None
[2025-04-06 20:23:27,041] [INFO] [config.py:1005:print]   scheduler_params ............. None
[2025-04-06 20:23:27,041] [INFO] [config.py:1005:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-06 20:23:27,041] [INFO] [config.py:1005:print]   sparse_attention ............. None
[2025-04-06 20:23:27,041] [INFO] [config.py:1005:print]   sparse_gradients_enabled ..... False
[2025-04-06 20:23:27,041] [INFO] [config.py:1005:print]   steps_per_print .............. 10000000
[2025-04-06 20:23:27,041] [INFO] [config.py:1005:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-06 20:23:27,041] [INFO] [config.py:1005:print]   timers_config ................ enabled=True synchronized=True
[2025-04-06 20:23:27,041] [INFO] [config.py:1005:print]   train_batch_size ............. 8
[2025-04-06 20:23:27,041] [INFO] [config.py:1005:print]   train_micro_batch_size_per_gpu  2
[2025-04-06 20:23:27,042] [INFO] [config.py:1005:print]   use_data_before_expert_parallel_  False
[2025-04-06 20:23:27,042] [INFO] [config.py:1005:print]   use_node_local_storage ....... False
[2025-04-06 20:23:27,042] [INFO] [config.py:1005:print]   wall_clock_breakdown ......... False
[2025-04-06 20:23:27,042] [INFO] [config.py:1005:print]   weight_quantization_config ... None
[2025-04-06 20:23:27,042] [INFO] [config.py:1005:print]   world_size ................... 4
[2025-04-06 20:23:27,042] [INFO] [config.py:1005:print]   zero_allow_untested_optimizer  True
[2025-04-06 20:23:27,042] [INFO] [config.py:1005:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-06 20:23:27,042] [INFO] [config.py:1005:print]   zero_enabled ................. True
[2025-04-06 20:23:27,042] [INFO] [config.py:1005:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-06 20:23:27,043] [INFO] [config.py:1005:print]   zero_optimization_stage ...... 1
[2025-04-06 20:23:27,043] [INFO] [config.py:991:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 2, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 1
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false, 
    "gradient_clipping": 1.0, 
    "steps_per_print": 1.000000e+07
}
Model mem
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   7475 MiB |   7475 MiB |  15037 MiB |   7562 MiB |
|       from large pool |   7426 MiB |   7426 MiB |  14983 MiB |   7557 MiB |
|       from small pool |     48 MiB |     48 MiB |     54 MiB |      5 MiB |
|---------------------------------------------------------------------------|
| Active memory         |   7475 MiB |   7475 MiB |  15037 MiB |   7562 MiB |
|       from large pool |   7426 MiB |   7426 MiB |  14983 MiB |   7557 MiB |
|       from small pool |     48 MiB |     48 MiB |     54 MiB |      5 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |   7475 MiB |   7475 MiB |  14902 MiB |   7427 MiB |
|       from large pool |   7426 MiB |   7426 MiB |  14848 MiB |   7422 MiB |
|       from small pool |     48 MiB |     48 MiB |     53 MiB |      4 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   8982 MiB |   8982 MiB |  15072 MiB |   6090 MiB |
|       from large pool |   8930 MiB |   8930 MiB |  15018 MiB |   6088 MiB |
|       from small pool |     52 MiB |     52 MiB |     54 MiB |      2 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  21198 KiB |  21198 KiB | 563427 KiB | 542229 KiB |
|       from large pool |  18104 KiB |  18104 KiB | 527487 KiB | 509382 KiB |
|       from small pool |   3093 KiB |   3093 KiB |  35940 KiB |  32847 KiB |
|---------------------------------------------------------------------------|
| Allocations           |     100    |     100    |    1262    |    1162    |
|       from large pool |       3    |       3    |     198    |     195    |
|       from small pool |      97    |      97    |    1064    |     967    |
|---------------------------------------------------------------------------|
| Active allocs         |     100    |     100    |    1262    |    1162    |
|       from large pool |       3    |       3    |     198    |     195    |
|       from small pool |      97    |      97    |    1064    |     967    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      30    |      30    |     201    |     171    |
|       from large pool |       4    |       4    |     174    |     170    |
|       from small pool |      26    |      26    |      27    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       5    |       5    |     231    |     226    |
|       from large pool |       2    |       2    |      53    |      51    |
|       from small pool |       3    |       3    |     178    |     175    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

Start Fine-tuning
Sun Apr  6 20:23:27 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                  Off |
| 30%   25C    P2             58W /  230W |    9341MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                  Off |
| 30%   30C    P2             76W /  230W |   10547MiB /  24564MiB |      2%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                  Off |
| 30%   30C    P2             58W /  230W |    9353MiB /  24564MiB |      1%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                  Off |
| 30%   35C    P2             73W /  230W |   10851MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   3209801      C   ...project_distillLLM/venv/bin/python3       9334MiB |
|    1   N/A  N/A   3209802      C   ...project_distillLLM/venv/bin/python3      10540MiB |
|    2   N/A  N/A   3209803      C   ...project_distillLLM/venv/bin/python3       9346MiB |
|    3   N/A  N/A   3209804      C   ...project_distillLLM/venv/bin/python3      10844MiB |
+-----------------------------------------------------------------------------------------+

dp size 4
0/63
1/63
2/63
3/63
4/63
5/63
6/63
7/63
8/63
9/63
10/63
11/63
12/63
13/63
14/63
15/63
Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]Evaluating:   2%|▏         | 1/63 [00:17<18:11, 17.60s/it]Evaluating:   3%|▎         | 2/63 [00:39<20:14, 19.90s/it]Evaluating:   5%|▍         | 3/63 [01:00<20:38, 20.64s/it]Evaluating:   6%|▋         | 4/63 [01:22<20:35, 20.94s/it]Evaluating:   8%|▊         | 5/63 [01:43<20:25, 21.12s/it]Evaluating:  10%|▉         | 6/63 [02:04<20:10, 21.23s/it]Evaluating:  11%|█         | 7/63 [02:25<19:44, 21.16s/it]Evaluating:  13%|█▎        | 8/63 [02:45<18:56, 20.67s/it]Evaluating:  14%|█▍        | 9/63 [03:04<18:09, 20.19s/it]Evaluating:  16%|█▌        | 10/63 [03:23<17:29, 19.81s/it]Evaluating:  17%|█▋        | 11/63 [03:42<16:56, 19.54s/it]Evaluating:  19%|█▉        | 12/63 [04:01<16:28, 19.38s/it]Evaluating:  21%|██        | 13/63 [04:21<16:15, 19.51s/it]Evaluating:  22%|██▏       | 14/63 [04:42<16:24, 20.09s/it]Evaluating:  24%|██▍       | 15/63 [05:04<16:27, 20.57s/it]Evaluating:  25%|██▌     16/63
17/63
18/63
19/63
20/63
21/63
22/63
23/63
24/63
25/63
26/63
27/63
28/63
29/63
30/63
  | 16/63 [05:26<16:21, 20.87s/it]Evaluating:  27%|██▋       | 17/63 [05:47<16:09, 21.08s/it]Evaluating:  29%|██▊       | 18/63 [06:09<15:55, 21.24s/it]Evaluating:  30%|███       | 19/63 [06:29<15:25, 21.03s/it]Evaluating:  32%|███▏      | 20/63 [06:51<15:11, 21.19s/it]Evaluating:  33%|███▎      | 21/63 [07:13<14:55, 21.33s/it]Evaluating:  35%|███▍      | 22/63 [07:34<14:37, 21.40s/it]Evaluating:  37%|███▋      | 23/63 [07:56<14:18, 21.47s/it]Evaluating:  38%|███▊      | 24/63 [08:17<13:58, 21.51s/it]Evaluating:  40%|███▉      | 25/63 [08:39<13:38, 21.54s/it]Evaluating:  41%|████▏     | 26/63 [09:00<13:15, 21.50s/it]Evaluating:  43%|████▎     | 27/63 [09:21<12:45, 21.25s/it]Evaluating:  44%|████▍     | 28/63 [09:40<12:01, 20.61s/it]Evaluating:  46%|████▌     | 29/63 [09:59<11:24, 20.15s/it]Evaluating:  48%|████▊     | 30/63 [10:18<10:53, 19.80s/it]Evaluating:  49%|████▉ 31/63
32/63
33/63
34/63
35/63
36/63
Distributed index stop interation. Idx: 596 Total_length: 596
    | 31/63 [10:37<10:26, 19.57s/it]Evaluating:  51%|█████     | 32/63 [10:57<10:04, 19.50s/it]Evaluating:  52%|█████▏    | 33/63 [11:18<10:01, 20.04s/it]Evaluating:  54%|█████▍    | 34/63 [11:39<09:52, 20.45s/it]Evaluating:  56%|█████▌    | 35/63 [12:01<09:39, 20.70s/it]Evaluating:  57%|█████▋    | 36/63 [12:22<09:23, 20.88s/it]Evaluating:  59%|█████▊    | 37/63 [12:43<09:06, 21.03s/it]Evaluating:  59%|█████▊    | 37/63 [12:43<08:56, 20.64s/it]
Distributed index stop interation. Idx: 597 Total_length: 596
Distributed index stop interation. Idx: 599 Total_length: 596
Distributed index stop interation. Idx: 598 Total_length: 596
Sun Apr  6 20:36:11 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                  Off |
| 30%   34C    P2             94W /  230W |   12665MiB /  24564MiB |      7%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                  Off |
| 30%   36C    P2             93W /  230W |   12665MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                  Off |
| 30%   41C    P2             97W /  230W |   12665MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                  Off |
| 30%   39C    P2             95W /  230W |   12665MiB /  24564MiB |     26%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   3209801      C   ...project_distillLLM/venv/bin/python3      12658MiB |
|    1   N/A  N/A   3209802      C   ...project_distillLLM/venv/bin/python3      12658MiB |
|    2   N/A  N/A   3209803      C   ...project_distillLLM/venv/bin/python3      12658MiB |
|    3   N/A  N/A   3209804      C   ...project_distillLLM/venv/bin/python3      12658MiB |
+-----------------------------------------------------------------------------------------+

Sun Apr  6 20:36:11 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                  Off |
| 30%   34C    P2             89W /  230W |   12665MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                  Off |
| 30%   36C    P2             90W /  230W |   12665MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                  Off |
| 30%   41C    P2             88W /  230W |   12665MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                  Off |
| 30%   39C    P2             89W /  230W |   12665MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   3209801      C   ...project_distillLLM/venv/bin/python3      12658MiB |
|    1   N/A  N/A   3209802      C   ...project_distillLLM/venv/bin/python3      12658MiB |
|    2   N/A  N/A   3209803      C   ...project_distillLLM/venv/bin/python3      12658MiB |
|    3   N/A  N/A   3209804      C   ...project_distillLLM/venv/bin/python3      12658MiB |
+-----------------------------------------------------------------------------------------+

Sun Apr  6 20:36:12 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                  Off |
| 30%   34C    P2             78W /  230W |   12665MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                  Off |
| 30%   36C    P2             85W /  230W |   12665MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                  Off |
| 30%   40C    P2             68W /  230W |   12665MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                  Off |
| 30%   40C    P2             79W /  230W |   12665MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   3209801      C   ...project_distillLLM/venv/bin/python3      12658MiB |
|    1   N/A  N/A   3209802      C   ...project_distillLLM/venv/bin/python3      12658MiB |
|    2   N/A  N/A   3209803      C   ...project_distillLLM/venv/bin/python3      12658MiB |
|    3   N/A  N/A   3209804      C   ...project_distillLLM/venv/bin/python3      12658MiB |
+-----------------------------------------------------------------------------------------+

./results/gpt2/train/sft/e10-bs2-lr5e-05-G1-N4-NN1/eval/0
dev | avg_loss: 2.0389120449890963 | {'exact_match': 0.0, 'rougeL': 5.9842}
Sun Apr  6 20:36:14 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                  Off |
| 30%   33C    P2             72W /  230W |   12665MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                  Off |
| 30%   37C    P2            177W /  230W |   24123MiB /  24564MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                  Off |
| 30%   43C    P2            160W /  230W |   24123MiB /  24564MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                  Off |
| 30%   41C    P2            151W /  230W |   24123MiB /  24564MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   3209801      C   ...project_distillLLM/venv/bin/python3      12658MiB |
|    1   N/A  N/A   3209802      C   ...project_distillLLM/venv/bin/python3      24116MiB |
|    2   N/A  N/A   3209803      C   ...project_distillLLM/venv/bin/python3      24116MiB |
|    3   N/A  N/A   3209804      C   ...project_distillLLM/venv/bin/python3      24116MiB |
+-----------------------------------------------------------------------------------------+

train | epoch   0 | Iter:      1/  2110 | global iter:      1/  2110 | loss: 2.0153 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale:     1.0000 | micro time: 3.473 | step time: 0.000
train | epoch   0 | Iter:      2/  2110 | global iter:      2/  2110 | loss: 1.9044 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale:     1.0000 | micro time: 3.343 | step time: 0.000
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/users/ap794/final_project_distillLLM/minillm/./finetune.py", line 558, in <module>
[rank2]:     main()
[rank2]:   File "/home/users/ap794/final_project_distillLLM/minillm/./finetune.py", line 551, in main
[rank2]:     model = finetune(args, tokenizer, model, optimizer, lr_scheduler, dataset, device, teacher_model=teacher_model)
[rank2]:   File "/home/users/ap794/final_project_distillLLM/minillm/./finetune.py", line 279, in finetune
[rank2]:     model.backward(loss)
[rank2]:   File "/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank2]:     ret_val = func(*args, **kwargs)
[rank2]:   File "/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2152, in backward
[rank2]:     self.allreduce_gradients()
[rank2]:   File "/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank2]:     ret_val = func(*args, **kwargs)
[rank2]:   File "/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2059, in allreduce_gradients
[rank2]:     self.optimizer.reduce_gradients(pipeline_parallel=self.pipeline_parallelism)
[rank2]:   File "/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 724, in reduce_gradients
[rank2]:     self.reduce_ready_partitions_and_remove_grads(param, i)
[rank2]:   File "/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1426, in reduce_ready_partitions_and_remove_grads
[rank2]:     self.reduce_independent_p_g_buckets_and_remove_grads(param, i)
[rank2]:   File "/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 944, in reduce_independent_p_g_buckets_and_remove_grads
[rank2]:     self.reduce_ipg_grads()
[rank2]:   File "/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1376, in reduce_ipg_grads
[rank2]:     self.average_tensor(self.ipg_buffer[self.ipg_index].narrow(0, 0, self.elements_in_ipg_bucket))
[rank2]:   File "/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1138, in average_tensor
[rank2]:     self.allreduce_and_scatter(buckets[bucket_key],
[rank2]:   File "/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1045, in allreduce_and_scatter
[rank2]:     self.allreduce_and_copy_with_multiple_ranks(small_bucket,
[rank2]:   File "/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1018, in allreduce_and_copy_with_multiple_ranks
[rank2]:     allreduced = self.allreduce_bucket(small_bucket, log=log, divide=divide, process_group=process_group)
[rank2]:   File "/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1491, in allreduce_bucket
[rank2]:     tensor = self.flatten(bucket)
[rank2]:   File "/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/torch/_utils.py", line 537, in _flatten_dense_tensors
[rank2]:     return torch._C._nn.flatten_dense_tensors(tensors)
[rank2]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.83 GiB. GPU 2 has a total capacity of 23.67 GiB of which 1.74 GiB is free. Including non-PyTorch memory, this process has 21.93 GiB memory in use. Of the allocated memory 16.25 GiB is allocated by PyTorch, and 5.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/users/ap794/final_project_distillLLM/minillm/./finetune.py", line 558, in <module>
[rank3]:     main()
[rank3]:   File "/home/users/ap794/final_project_distillLLM/minillm/./finetune.py", line 551, in main
[rank3]:     model = finetune(args, tokenizer, model, optimizer, lr_scheduler, dataset, device, teacher_model=teacher_model)
[rank3]:   File "/home/users/ap794/final_project_distillLLM/minillm/./finetune.py", line 279, in finetune
[rank3]:     model.backward(loss)
[rank3]:   File "/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank3]:     ret_val = func(*args, **kwargs)
[rank3]:   File "/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2152, in backward
[rank3]:     self.allreduce_gradients()
[rank3]:   File "/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank3]:     ret_val = func(*args, **kwargs)
[rank3]:   File "/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2059, in allreduce_gradients
[rank3]:     self.optimizer.reduce_gradients(pipeline_parallel=self.pipeline_parallelism)
[rank3]:   File "/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 724, in reduce_gradients
[rank3]:     self.reduce_ready_partitions_and_remove_grads(param, i)
[rank3]:   File "/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1426, in reduce_ready_partitions_and_remove_grads
[rank3]:     self.reduce_independent_p_g_buckets_and_remove_grads(param, i)
[rank3]:   File "/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 944, in reduce_independent_p_g_buckets_and_remove_grads
[rank3]:     self.reduce_ipg_grads()
[rank3]:   File "/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1376, in reduce_ipg_grads
[rank3]:     self.average_tensor(self.ipg_buffer[self.ipg_index].narrow(0, 0, self.elements_in_ipg_bucket))
[rank3]:   File "/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1138, in average_tensor
[rank3]:     self.allreduce_and_scatter(buckets[bucket_key],
[rank3]:   File "/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1045, in allreduce_and_scatter
[rank3]:     self.allreduce_and_copy_with_multiple_ranks(small_bucket,
[rank3]:   File "/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1018, in allreduce_and_copy_with_multiple_ranks
[rank3]:     allreduced = self.allreduce_bucket(small_bucket, log=log, divide=divide, process_group=process_group)
[rank3]:   File "/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1491, in allreduce_bucket
[rank3]:     tensor = self.flatten(bucket)
[rank3]:   File "/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/torch/_utils.py", line 537, in _flatten_dense_tensors
[rank3]:     return torch._C._nn.flatten_dense_tensors(tensors)
[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.83 GiB. GPU 3 has a total capacity of 23.67 GiB of which 1.77 GiB is free. Including non-PyTorch memory, this process has 21.90 GiB memory in use. Of the allocated memory 16.24 GiB is allocated by PyTorch, and 5.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
W0406 20:36:28.407000 3209798 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3209801 closing signal SIGTERM
W0406 20:36:28.441000 3209798 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3209802 closing signal SIGTERM
W0406 20:36:28.442000 3209798 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3209804 closing signal SIGTERM
E0406 20:36:29.181000 3209798 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 2 (pid: 3209803) of binary: /home/users/ap794/final_project_distillLLM/venv/bin/python3
Traceback (most recent call last):
  File "/home/users/ap794/final_project_distillLLM/venv/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/torch/distributed/run.py", line 918, in main
    run(args)
  File "/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-04-06_20:36:28
  host      : compsci-cluster-fitz-12.cs.duke.edu.
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 3209803)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: compsci-cluster-fitz-12: task 0: Exited with exit code 1
