compsci-cluster-fitz-34
Sat Apr 19 10:52:15 AM EDT 2025
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
The token `llama3` has been saved to /dev/shm/hf-home/stored_tokens
Your token has been saved to /dev/shm/hf-home/token
Login successful.
The current active token is: `llama3`
torchrun --nproc_per_node 4 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 2012 ./finetune.py --base-path . --model-path gpt2-xl --ckpt-name gpt2-xlarge --n-gpu 4 --data-dir ./processed_data/dolly_spanish/full/gpt2/ --num-workers 0 --dev-num 1000 --lr 0.00005 --batch-size 2 --eval-batch-size 4 --gradient-accumulation-steps 1 --warmup-iters 0 --lr-decay-style cosine --weight-decay 1e-2 --clip-grad 1.0 --epochs 10 --max-length 512 --max-prompt-length 256 --do-train --do-valid --eval-gen --save-interval 4000 --eval-interval 4000 --log-interval 10 --mid-log-num 100 --save ./results/gpt2/train/sft/gpt2-xlarge-spanish/ --seed 10 --seed-order 10 --deepspeed --deepspeed_config ./configs/deepspeed/ds_config_zero1_fp16.json --type lm --do-sample --top-k 0 --top-p 1.0 --temperature 1.0
PYTHONPATH=.
W0419 10:52:20.673000 4164985 torch/distributed/run.py:792] 
W0419 10:52:20.673000 4164985 torch/distributed/run.py:792] *****************************************
W0419 10:52:20.673000 4164985 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0419 10:52:20.673000 4164985 torch/distributed/run.py:792] *****************************************
[2025-04-19 10:52:26,028] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-19 10:52:26,030] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-19 10:52:26,031] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-19 10:52:26,033] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/users/ap794/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/users/ap794/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/users/ap794/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/users/ap794/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/users/ap794/final_project_distillLLM/venv/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
using world size: 4
[2025-04-19 10:52:34,120] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-19 10:52:34,120] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
arguments:
  model_path ................... gpt2-xl
  ckpt_name .................... gpt2-xlarge
  model_type ................... gpt2
  teacher_model_type ........... None
  n_gpu ........................ 4
  n_nodes ...................... 1
  teacher_model_path ........... None
  teacher_ckpt_name ............ None
  teacher_model_fp16 ........... False
  model_parallel ............... False
  model_parallel_size .......... None
  no_value ..................... False
  dropout_path_rate ............ None
  dtype ........................ torch.float16
  type ......................... lm
  do_train ..................... True
  do_valid ..................... True
  do_eval ...................... False
  base_path .................... .
  load ......................... None
  save ......................... ./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
  log_interval ................. 10
  mid_log_num .................. 100
  save_interval ................ 4000
  eval_interval ................ 4000
  local_rank ................... 0
  save_additional_suffix ....... 
  save_rollout ................. False
  eb_sample_times .............. 3
  data_dir ..................... ./processed_data/dolly_spanish/full/gpt2/
  processed_data_dir ........... None
  force_process ................ False
  force_process_demo ........... False
  data_process_workers ......... -1
  train_num .................... -1
  train_ratio .................. 1
  dev_num ...................... 1000
  dev_ratio .................... 1
  gen_num ...................... -1
  data_names ................... None
  prompt_type .................. None
  num_workers .................. 0
  max_prompt_length ............ 256
  min_prompt_length ............ 128
  json_data .................... False
  bin_data ..................... False
  txt_data ..................... False
  prompt_data_dir .............. None
  lm_data_dir .................. None
  eval_ppl ..................... False
  eval_rw ...................... False
  eval_gen ..................... True
  only_prompt .................. False
  batch_size ................... 2
  eval_batch_size .............. 4
  clip_grad .................... 1.0
  total_iters .................. None
  train_iters_per_epoch ........ -1
  max_length ................... 512
  seed ......................... 10
  seed_order ................... 10
  seed_data .................... 42
  seed_ppo ..................... 42
  seed_lm ...................... 7
  epochs ....................... 10
  training_epochs .............. 10000
  gradient_accumulation_steps .. 1
  gradient_checkpointing ....... False
  attn_dtype ................... None
  lr ........................... 5e-05
  lr_min ....................... 1e-07
  weight_decay ................. 0.01
  loss_scale ................... 65536
  kd_ratio ..................... None
  warmup_iters ................. 0
  lr_decay_iters ............... None
  lr_decay_style ............... cosine
  scheduler_name ............... constant_trm
  reward_scaling ............... None
  cliprange_reward ............. 1
  ppo_epochs ................... None
  num_rollouts ................. 256
  num_rollouts_per_device ...... None
  cliprange .................... 0.2
  chunk_size ................... None
  gamma ........................ 0.95
  length_norm .................. False
  single_step_reg .............. False
  teacher_mixed_alpha .......... None
  lm_coef ...................... 1
  top_k ........................ 0
  top_p ........................ 1.0
  do_sample .................... True
  no_repeat_ngram_size ......... 6
  repetition_penalty ........... None
  num_beams .................... 1
  temperature .................. 1.0
  peft ......................... None
  peft_lora_r .................. 8
  peft_lora_alpha .............. 32
  peft_lora_dropout ............ 0.1
  peft_name .................... None
  peft_path .................... None
  teacher_peft_name ............ None
  teacher_peft_path ............ None
  deepspeed .................... True
  deepspeed_config ............. ./configs/deepspeed/ds_config_zero1_fp16.json
  deepscale .................... False
  deepscale_config ............. None
  rank ......................... 0
  world_size ................... 4
[2025-04-19 10:52:34,320] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-19 10:52:34,711] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-19 10:52:34,716] [INFO] [comm.py:658:init_distributed] cdb=None
Sat Apr 19 10:52:34 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   29C    P8             12W /  230W |       4MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   22C    P2             23W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   23C    P2             18W /  230W |      59MiB /  23028MiB |      2%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   19C    P2             21W /  230W |      27MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3        202MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3         52MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3         20MiB |
+-----------------------------------------------------------------------------------------+

Sat Apr 19 10:52:34 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   29C    P8             13W /  230W |       4MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   23C    P2             33W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   24C    P2             27W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   20C    P2             31W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3        202MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3        202MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3        202MiB |
+-----------------------------------------------------------------------------------------+

Sat Apr 19 10:52:35 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   29C    P8             16W /  230W |       4MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   23C    P2             59W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   24C    P2             48W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   20C    P2             53W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3        202MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3        202MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3        202MiB |
+-----------------------------------------------------------------------------------------+

Sat Apr 19 10:52:35 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   29C    P8             16W /  230W |       4MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   23C    P2             59W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   24C    P2             48W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   20C    P2             53W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3        202MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3        202MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3        202MiB |
+-----------------------------------------------------------------------------------------+

Sat Apr 19 10:52:35 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   29C    P8             16W /  230W |       4MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   23C    P2             59W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   24C    P2             48W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   20C    P2             62W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3        202MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3        202MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3        202MiB |
+-----------------------------------------------------------------------------------------+

Sat Apr 19 10:52:35 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   29C    P8             16W /  230W |       4MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   23C    P2             64W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   24C    P2             58W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   20C    P2             63W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3        202MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3        202MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3        202MiB |
+-----------------------------------------------------------------------------------------+

Sat Apr 19 10:52:36 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   29C    P8             13W /  230W |       4MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   23C    P2             65W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   24C    P2             58W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   20C    P2             63W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3        202MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3        202MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3        202MiB |
+-----------------------------------------------------------------------------------------+

Sat Apr 19 10:52:36 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   29C    P8             13W /  230W |       4MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   23C    P2             65W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   24C    P2             58W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   20C    P2             62W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3        202MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3        202MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3        202MiB |
+-----------------------------------------------------------------------------------------+

Sat Apr 19 10:52:37 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   29C    P8             13W /  230W |       4MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   23C    P2             65W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   24C    P2             58W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   20C    P2             62W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3        202MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3        202MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3        202MiB |
+-----------------------------------------------------------------------------------------+

Probing Dataset
Probing end. Max data state 1, total length 10405
Sat Apr 19 10:52:37 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   29C    P8             13W /  230W |       4MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   23C    P2             65W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   24C    P2             58W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   21C    P2             62W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3        202MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3        202MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3        202MiB |
+-----------------------------------------------------------------------------------------+

10405
Num LM instances: 10405
train num 10405
Probing Dataset
Probing end. Max data state 1, total length 777
777
Num LM instances: 777
Train iters per epoch 1300
total_iters 13000
Sat Apr 19 10:52:37 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   29C    P8             12W /  230W |       4MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   23C    P2             65W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   25C    P2             58W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   21C    P2             62W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3        202MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3        202MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3        202MiB |
+-----------------------------------------------------------------------------------------+

Sat Apr 19 10:52:38 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   29C    P8             12W /  230W |       4MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   24C    P2             65W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   25C    P2             58W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   21C    P2             62W /  230W |     209MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3        202MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3        202MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3        202MiB |
+-----------------------------------------------------------------------------------------+

 > number of parameters: 1557611200
Model load time: 54.88371562957764s
Optimizer = AdamW
[2025-04-19 10:53:33,505] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.4, git-hash=unknown, git-branch=unknown
[2025-04-19 10:53:33,505] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 4
[2025-04-19 10:53:33,521] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 4
[2025-04-19 10:53:33,549] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 4
[2025-04-19 10:53:33,714] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 4
[2025-04-19 10:53:34,567] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-04-19 10:53:34,576] [INFO] [logging.py:128:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-04-19 10:53:34,592] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-04-19 10:53:34,684] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2025-04-19 10:53:34,700] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2025-04-19 10:53:34,700] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 1 optimizer
[2025-04-19 10:53:34,700] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-04-19 10:53:34,700] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-04-19 10:53:34,701] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-04-19 10:53:34,701] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-04-19 10:53:41,724] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-04-19 10:53:41,725] [INFO] [utils.py:782:see_memory_usage] MA 4.4 GB         Max_MA 4.4 GB         CA 4.42 GB         Max_CA 4 GB 
[2025-04-19 10:53:41,725] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 34.9 GB, percent = 3.5%
Sat Apr 19 10:53:41 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   26C    P2             54W /  230W |    4887MiB /  23028MiB |     14%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   26C    P2             66W /  230W |    6373MiB /  23028MiB |     15%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   25C    P2             58W /  230W |    6373MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   23C    P2             64W /  230W |    6373MiB /  23028MiB |     15%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3       4880MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3       6366MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3       6366MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3       6366MiB |
+-----------------------------------------------------------------------------------------+

[2025-04-19 10:53:42,280] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-04-19 10:53:42,281] [INFO] [utils.py:782:see_memory_usage] MA 4.4 GB         Max_MA 5.85 GB         CA 5.87 GB         Max_CA 6 GB 
[2025-04-19 10:53:42,282] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 34.89 GB, percent = 3.5%
[2025-04-19 10:53:42,282] [INFO] [stage_1_and_2.py:550:__init__] optimizer state initialized
Sat Apr 19 10:53:41 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   26C    P2             54W /  230W |    4887MiB /  23028MiB |     14%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   26C    P2             66W /  230W |    6373MiB /  23028MiB |     15%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   25C    P2             58W /  230W |    6373MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   23C    P2             64W /  230W |    6373MiB /  23028MiB |     15%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3       4880MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3       6366MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3       6366MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3       6366MiB |
+-----------------------------------------------------------------------------------------+

[2025-04-19 10:53:42,404] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-04-19 10:53:42,405] [INFO] [utils.py:782:see_memory_usage] MA 4.4 GB         Max_MA 4.4 GB         CA 5.87 GB         Max_CA 6 GB 
[2025-04-19 10:53:42,405] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 34.89 GB, percent = 3.5%
[2025-04-19 10:53:42,409] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-04-19 10:53:42,409] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-04-19 10:53:42,409] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x7f24765ab940>
[2025-04-19 10:53:42,410] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05, 5e-05], mom=[(0.9, 0.999), (0.9, 0.999)]
[2025-04-19 10:53:42,411] [INFO] [config.py:1001:print] DeepSpeedEngine configuration:
[2025-04-19 10:53:42,412] [INFO] [config.py:1005:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-04-19 10:53:42,412] [INFO] [config.py:1005:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-04-19 10:53:42,412] [INFO] [config.py:1005:print]   amp_enabled .................. False
[2025-04-19 10:53:42,412] [INFO] [config.py:1005:print]   amp_params ................... False
[2025-04-19 10:53:42,412] [INFO] [config.py:1005:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-04-19 10:53:42,413] [INFO] [config.py:1005:print]   bfloat16_enabled ............. False
[2025-04-19 10:53:42,413] [INFO] [config.py:1005:print]   bfloat16_immediate_grad_update  False
[2025-04-19 10:53:42,413] [INFO] [config.py:1005:print]   checkpoint_parallel_write_pipeline  False
[2025-04-19 10:53:42,413] [INFO] [config.py:1005:print]   checkpoint_tag_validation_enabled  True
[2025-04-19 10:53:42,413] [INFO] [config.py:1005:print]   checkpoint_tag_validation_fail  False
[2025-04-19 10:53:42,413] [INFO] [config.py:1005:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f24766d8100>
[2025-04-19 10:53:42,413] [INFO] [config.py:1005:print]   communication_data_type ...... None
[2025-04-19 10:53:42,413] [INFO] [config.py:1005:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-04-19 10:53:42,413] [INFO] [config.py:1005:print]   curriculum_enabled_legacy .... False
[2025-04-19 10:53:42,414] [INFO] [config.py:1005:print]   curriculum_params_legacy ..... False
[2025-04-19 10:53:42,414] [INFO] [config.py:1005:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-04-19 10:53:42,414] [INFO] [config.py:1005:print]   data_efficiency_enabled ...... False
[2025-04-19 10:53:42,414] [INFO] [config.py:1005:print]   dataloader_drop_last ......... False
[2025-04-19 10:53:42,414] [INFO] [config.py:1005:print]   disable_allgather ............ False
[2025-04-19 10:53:42,414] [INFO] [config.py:1005:print]   dump_state ................... False
[2025-04-19 10:53:42,414] [INFO] [config.py:1005:print]   dynamic_loss_scale_args ...... None
[2025-04-19 10:53:42,414] [INFO] [config.py:1005:print]   eigenvalue_enabled ........... False
[2025-04-19 10:53:42,414] [INFO] [config.py:1005:print]   eigenvalue_gas_boundary_resolution  1
[2025-04-19 10:53:42,414] [INFO] [config.py:1005:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-04-19 10:53:42,414] [INFO] [config.py:1005:print]   eigenvalue_layer_num ......... 0
[2025-04-19 10:53:42,415] [INFO] [config.py:1005:print]   eigenvalue_max_iter .......... 100
[2025-04-19 10:53:42,415] [INFO] [config.py:1005:print]   eigenvalue_stability ......... 1e-06
[2025-04-19 10:53:42,415] [INFO] [config.py:1005:print]   eigenvalue_tol ............... 0.01
[2025-04-19 10:53:42,415] [INFO] [config.py:1005:print]   eigenvalue_verbose ........... False
[2025-04-19 10:53:42,415] [INFO] [config.py:1005:print]   elasticity_enabled ........... False
[2025-04-19 10:53:42,415] [INFO] [config.py:1005:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-04-19 10:53:42,415] [INFO] [config.py:1005:print]   fp16_auto_cast ............... None
[2025-04-19 10:53:42,415] [INFO] [config.py:1005:print]   fp16_enabled ................. False
[2025-04-19 10:53:42,415] [INFO] [config.py:1005:print]   fp16_master_weights_and_gradients  False
[2025-04-19 10:53:42,415] [INFO] [config.py:1005:print]   global_rank .................. 0
[2025-04-19 10:53:42,416] [INFO] [config.py:1005:print]   grad_accum_dtype ............. None
[2025-04-19 10:53:42,416] [INFO] [config.py:1005:print]   gradient_accumulation_steps .. 1
[2025-04-19 10:53:42,416] [INFO] [config.py:1005:print]   gradient_clipping ............ 1.0
[2025-04-19 10:53:42,416] [INFO] [config.py:1005:print]   gradient_predivide_factor .... 1.0
[2025-04-19 10:53:42,416] [INFO] [config.py:1005:print]   graph_harvesting ............. False
[2025-04-19 10:53:42,416] [INFO] [config.py:1005:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-04-19 10:53:42,416] [INFO] [config.py:1005:print]   initial_dynamic_scale ........ 65536
[2025-04-19 10:53:42,416] [INFO] [config.py:1005:print]   load_universal_checkpoint .... False
[2025-04-19 10:53:42,416] [INFO] [config.py:1005:print]   loss_scale ................... 0
[2025-04-19 10:53:42,416] [INFO] [config.py:1005:print]   memory_breakdown ............. False
[2025-04-19 10:53:42,417] [INFO] [config.py:1005:print]   mics_hierarchial_params_gather  False
[2025-04-19 10:53:42,417] [INFO] [config.py:1005:print]   mics_shard_size .............. -1
[2025-04-19 10:53:42,417] [INFO] [config.py:1005:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-04-19 10:53:42,417] [INFO] [config.py:1005:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-04-19 10:53:42,417] [INFO] [config.py:1005:print]   optimizer_legacy_fusion ...... False
[2025-04-19 10:53:42,417] [INFO] [config.py:1005:print]   optimizer_name ............... None
[2025-04-19 10:53:42,417] [INFO] [config.py:1005:print]   optimizer_params ............. None
[2025-04-19 10:53:42,417] [INFO] [config.py:1005:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-04-19 10:53:42,417] [INFO] [config.py:1005:print]   pld_enabled .................. False
[2025-04-19 10:53:42,418] [INFO] [config.py:1005:print]   pld_params ................... False
[2025-04-19 10:53:42,418] [INFO] [config.py:1005:print]   prescale_gradients ........... False
[2025-04-19 10:53:42,418] [INFO] [config.py:1005:print]   scheduler_name ............... None
[2025-04-19 10:53:42,418] [INFO] [config.py:1005:print]   scheduler_params ............. None
[2025-04-19 10:53:42,418] [INFO] [config.py:1005:print]   seq_parallel_communication_data_type  torch.float32
[2025-04-19 10:53:42,418] [INFO] [config.py:1005:print]   sparse_attention ............. None
[2025-04-19 10:53:42,418] [INFO] [config.py:1005:print]   sparse_gradients_enabled ..... False
[2025-04-19 10:53:42,418] [INFO] [config.py:1005:print]   steps_per_print .............. 10000000
[2025-04-19 10:53:42,418] [INFO] [config.py:1005:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-04-19 10:53:42,418] [INFO] [config.py:1005:print]   timers_config ................ enabled=True synchronized=True
[2025-04-19 10:53:42,419] [INFO] [config.py:1005:print]   train_batch_size ............. 8
[2025-04-19 10:53:42,419] [INFO] [config.py:1005:print]   train_micro_batch_size_per_gpu  2
[2025-04-19 10:53:42,419] [INFO] [config.py:1005:print]   use_data_before_expert_parallel_  False
[2025-04-19 10:53:42,419] [INFO] [config.py:1005:print]   use_node_local_storage ....... False
[2025-04-19 10:53:42,419] [INFO] [config.py:1005:print]   wall_clock_breakdown ......... False
[2025-04-19 10:53:42,419] [INFO] [config.py:1005:print]   weight_quantization_config ... None
[2025-04-19 10:53:42,419] [INFO] [config.py:1005:print]   world_size ................... 4
[2025-04-19 10:53:42,419] [INFO] [config.py:1005:print]   zero_allow_untested_optimizer  True
[2025-04-19 10:53:42,419] [INFO] [config.py:1005:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-04-19 10:53:42,419] [INFO] [config.py:1005:print]   zero_enabled ................. True
[2025-04-19 10:53:42,420] [INFO] [config.py:1005:print]   zero_force_ds_cpu_optimizer .. True
[2025-04-19 10:53:42,420] [INFO] [config.py:1005:print]   zero_optimization_stage ...... 1
[2025-04-19 10:53:42,420] [INFO] [config.py:991:print_user_config]   json = {
    "train_micro_batch_size_per_gpu": 2, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 1
    }, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "initial_scale_power": 11, 
        "loss_scale_window": 2.000000e+03, 
        "hysteresis": 4
    }, 
    "wall_clock_breakdown": false, 
    "gradient_clipping": 1.0, 
    "steps_per_print": 1.000000e+07
}
Model mem
 |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   4504 MiB |   4504 MiB |   9771 MiB |   5267 MiB |
|       from large pool |   4455 MiB |   4455 MiB |   9718 MiB |   5263 MiB |
|       from small pool |     48 MiB |     48 MiB |     52 MiB |      3 MiB |
|---------------------------------------------------------------------------|
| Active memory         |   4504 MiB |   4504 MiB |   9771 MiB |   5267 MiB |
|       from large pool |   4455 MiB |   4455 MiB |   9718 MiB |   5263 MiB |
|       from small pool |     48 MiB |     48 MiB |     52 MiB |      3 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |   4504 MiB |   4504 MiB |   9703 MiB |   5199 MiB |
|       from large pool |   4455 MiB |   4455 MiB |   9651 MiB |   5195 MiB |
|       from small pool |     48 MiB |     48 MiB |     52 MiB |      3 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   6014 MiB |   6014 MiB |   9808 MiB |   3794 MiB |
|       from large pool |   5962 MiB |   5962 MiB |   9756 MiB |   3794 MiB |
|       from small pool |     52 MiB |     52 MiB |     52 MiB |      0 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  22132 KiB |  22132 KiB |   1056 MiB |   1034 MiB |
|       from large pool |  21087 KiB |  21087 KiB |   1024 MiB |   1003 MiB |
|       from small pool |   1045 KiB |   1045 KiB |     31 MiB |     30 MiB |
|---------------------------------------------------------------------------|
| Allocations           |     100    |     100    |    1264    |    1164    |
|       from large pool |       3    |       3    |     199    |     196    |
|       from small pool |      97    |      97    |    1065    |     968    |
|---------------------------------------------------------------------------|
| Active allocs         |     100    |     100    |    1264    |    1164    |
|       from large pool |       3    |       3    |     199    |     196    |
|       from small pool |      97    |      97    |    1065    |     968    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      30    |      30    |     177    |     147    |
|       from large pool |       4    |       4    |     151    |     147    |
|       from small pool |      26    |      26    |      26    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       6    |     281    |     275    |
|       from large pool |       3    |       3    |     101    |      98    |
|       from small pool |       3    |       3    |     180    |     177    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

Start Fine-tuning
Sat Apr 19 10:53:42 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   26C    P2             54W /  230W |    6373MiB /  23028MiB |      1%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   26C    P2             66W /  230W |    6373MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   25C    P2             59W /  230W |    6373MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   23C    P2             64W /  230W |    6373MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3       6366MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3       6366MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3       6366MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3       6366MiB |
+-----------------------------------------------------------------------------------------+

Sat Apr 19 10:53:42 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   26C    P2             54W /  230W |    6373MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   26C    P2             67W /  230W |    6373MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   25C    P2             59W /  230W |    6373MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   23C    P2             64W /  230W |    6373MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3       6366MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3       6366MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3       6366MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3       6366MiB |
+-----------------------------------------------------------------------------------------+

Sat Apr 19 10:53:42 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   26C    P2             53W /  230W |    6373MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   26C    P2             67W /  230W |    6373MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   25C    P2             59W /  230W |    6373MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   23C    P2             64W /  230W |    6373MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3       6366MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3       6366MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3       6366MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3       6366MiB |
+-----------------------------------------------------------------------------------------+
Sat Apr 19 10:53:42 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   26C    P2             53W /  230W |    6373MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   26C    P2             67W /  230W |    6373MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   25C    P2             59W /  230W |    6373MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   23C    P2             64W /  230W |    6373MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3       6366MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3       6366MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3       6366MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3       6366MiB |
+-----------------------------------------------------------------------------------------+


Start Fine-tuning
Sat Apr 19 10:53:43 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   26C    P2             54W /  230W |    6373MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   26C    P2             66W /  230W |    6373MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   25C    P2             58W /  230W |    6373MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   23C    P2             63W /  230W |    6373MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3       6366MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3       6366MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3       6366MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3       6366MiB |
+-----------------------------------------------------------------------------------------+

Sat Apr 19 10:53:44 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   26C    P2             54W /  230W |    6373MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   26C    P2             66W /  230W |    6383MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   25C    P2             58W /  230W |    6373MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   23C    P2             63W /  230W |    6383MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3       6366MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3       6376MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3       6366MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3       6376MiB |
+-----------------------------------------------------------------------------------------+

dp size 4
0/63
1/63
2/63
3/63
4/63
5/63
6/63
7/63
8/63
9/63
10/63
11/63
12/63
13/63
14/63
15/63
Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]Evaluating:   2%|         | 1/63 [00:19<19:50, 19.20s/it]Evaluating:   3%|         | 2/63 [00:37<18:56, 18.63s/it]Evaluating:   5%|         | 3/63 [00:55<18:32, 18.55s/it]Evaluating:   6%|         | 4/63 [01:14<18:12, 18.52s/it]Evaluating:   8%|         | 5/63 [01:32<17:53, 18.51s/it]Evaluating:  10%|         | 6/63 [01:51<17:35, 18.51s/it]Evaluating:  11%|         | 7/63 [02:09<17:15, 18.48s/it]Evaluating:  13%|        | 8/63 [02:28<16:56, 18.49s/it]Evaluating:  14%|        | 9/63 [02:46<16:38, 18.48s/it]Evaluating:  16%|        | 10/63 [03:05<16:19, 18.48s/it]Evaluating:  17%|        | 11/63 [03:23<16:00, 18.47s/it]Evaluating:  19%|        | 12/63 [03:42<15:41, 18.45s/it]Evaluating:  21%|        | 13/63 [04:00<15:22, 18.46s/it]Evaluating:  22%|       | 14/63 [04:19<15:05, 18.47s/it]Evaluating:  24%|       | 15/63 [04:37<14:47, 18.48s/it]Evaluating:  25%|     16/63
17/63
18/63
19/63
20/63
21/63
22/63
23/63
24/63
25/63
26/63
27/63
28/63
29/63
30/63
  | 16/63 [04:56<14:28, 18.48s/it]Evaluating:  27%|       | 17/63 [05:14<14:10, 18.50s/it]Evaluating:  29%|       | 18/63 [05:33<13:52, 18.50s/it]Evaluating:  30%|       | 19/63 [05:51<13:33, 18.49s/it]Evaluating:  32%|      | 20/63 [06:10<13:14, 18.48s/it]Evaluating:  33%|      | 21/63 [06:28<12:55, 18.46s/it]Evaluating:  35%|      | 22/63 [06:46<12:37, 18.48s/it]Evaluating:  37%|      | 23/63 [07:05<12:19, 18.49s/it]Evaluating:  38%|      | 24/63 [07:23<12:01, 18.49s/it]Evaluating:  40%|      | 25/63 [07:42<11:43, 18.50s/it]Evaluating:  41%|     | 26/63 [08:00<11:23, 18.48s/it]Evaluating:  43%|     | 27/63 [08:19<11:05, 18.48s/it]Evaluating:  44%|     | 28/63 [08:37<10:46, 18.48s/it]Evaluating:  46%|     | 29/63 [08:56<10:28, 18.47s/it]Evaluating:  48%|     | 30/63 [09:14<10:09, 18.48s/it]Evaluating:  49%| 31/63
32/63
33/63
34/63
35/63
36/63
37/63
38/63
39/63
40/63
41/63
42/63
43/63
44/63
    | 31/63 [09:33<09:51, 18.49s/it]Evaluating:  51%|     | 32/63 [09:51<09:33, 18.49s/it]Evaluating:  52%|    | 33/63 [10:10<09:14, 18.48s/it]Evaluating:  54%|    | 34/63 [10:28<08:55, 18.46s/it]Evaluating:  56%|    | 35/63 [10:47<08:37, 18.47s/it]Evaluating:  57%|    | 36/63 [11:05<08:18, 18.47s/it]Evaluating:  59%|    | 37/63 [11:24<08:00, 18.46s/it]Evaluating:  60%|    | 38/63 [11:42<07:41, 18.46s/it]Evaluating:  62%|   | 39/63 [12:01<07:23, 18.46s/it]Evaluating:  63%|   | 40/63 [12:19<07:04, 18.46s/it]Evaluating:  65%|   | 41/63 [12:37<06:46, 18.47s/it]Evaluating:  67%|   | 42/63 [12:56<06:27, 18.46s/it]Evaluating:  68%|   | 43/63 [13:14<06:09, 18.48s/it]Evaluating:  70%|   | 44/63 [13:33<05:51, 18.48s/it]Evaluating:  71%|45/63
46/63
47/63
Distributed index stop interation. Idx: 777 Total_length: 777Distributed index stop interation. Idx: 779 Total_length: 777

Distributed index stop interation. Idx: 778 Total_length: 777
Distributed index stop interation. Idx: 780 Total_length: 777
  | 45/63 [13:51<05:32, 18.49s/it]Evaluating:  73%|  | 46/63 [14:10<05:14, 18.49s/it]Evaluating:  75%|  | 47/63 [14:28<04:55, 18.49s/it]Evaluating:  76%|  | 48/63 [14:46<04:33, 18.24s/it]Evaluating:  76%|  | 48/63 [14:46<04:37, 18.47s/it]
Sat Apr 19 11:08:31 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   33C    P2             81W /  230W |    7703MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   36C    P2             94W /  230W |    7703MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   34C    P2             84W /  230W |    7703MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   32C    P2             91W /  230W |    7703MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3       7696MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3       7696MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3       7696MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3       7696MiB |
+-----------------------------------------------------------------------------------------+

Sat Apr 19 11:08:31 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   33C    P2             81W /  230W |    7703MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   36C    P2             94W /  230W |    7703MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   34C    P2             84W /  230W |    7703MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   32C    P2             91W /  230W |    7703MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3       7696MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3       7696MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3       7696MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3       7696MiB |
+-----------------------------------------------------------------------------------------+

Sat Apr 19 11:08:31 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   32C    P2             56W /  230W |    7703MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   36C    P2             87W /  230W |    7703MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   33C    P2             78W /  230W |    7703MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   32C    P2             85W /  230W |    7703MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3       7696MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3       7696MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3       7696MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3       7696MiB |
+-----------------------------------------------------------------------------------------+

./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1/eval/0
dev | avg_loss: 3.0522867838541665 | {'exact_match': 0.0, 'rougeL': 3.5571}
Sat Apr 19 11:08:39 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   31C    P2             54W /  230W |    7703MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   37C    P2             96W /  230W |   20931MiB /  23028MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   34C    P2             87W /  230W |   20931MiB /  23028MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   33C    P2             94W /  230W |   21869MiB /  23028MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3       7696MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3      20924MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3      20924MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3      21862MiB |
+-----------------------------------------------------------------------------------------+

[2025-04-19 11:08:41,297] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648
train | epoch   0 | Iter:      1/ 13000 | global iter:      1/ 13000 | loss: 3.4312 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 2147483648.0000 | micro time: 1.386 | step time: 0.000
[2025-04-19 11:08:42,684] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648, reducing to 1073741824
train | epoch   0 | Iter:      2/ 13000 | global iter:      2/ 13000 | loss: 2.9954 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 1073741824.0000 | micro time: 1.372 | step time: 0.000
[2025-04-19 11:08:43,816] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1073741824, reducing to 536870912
train | epoch   0 | Iter:      3/ 13000 | global iter:      3/ 13000 | loss: 3.0629 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 536870912.0000 | micro time: 1.137 | step time: 0.000
[2025-04-19 11:08:44,936] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 536870912, reducing to 268435456
train | epoch   0 | Iter:      4/ 13000 | global iter:      4/ 13000 | loss: 3.4471 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 268435456.0000 | micro time: 1.103 | step time: 0.000
[2025-04-19 11:08:46,084] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 268435456, reducing to 134217728
train | epoch   0 | Iter:      5/ 13000 | global iter:      5/ 13000 | loss: 3.6495 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 134217728.0000 | micro time: 1.144 | step time: 0.000
[2025-04-19 11:08:47,212] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 134217728, reducing to 67108864
train | epoch   0 | Iter:      6/ 13000 | global iter:      6/ 13000 | loss: 2.8885 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 67108864.0000 | micro time: 1.120 | step time: 0.000
[2025-04-19 11:08:48,328] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 67108864, reducing to 33554432
train | epoch   0 | Iter:      7/ 13000 | global iter:      7/ 13000 | loss: 2.7877 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 33554432.0000 | micro time: 1.146 | step time: 0.000
[2025-04-19 11:08:49,472] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 33554432, reducing to 16777216
train | epoch   0 | Iter:      8/ 13000 | global iter:      8/ 13000 | loss: 4.0029 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 16777216.0000 | micro time: 1.112 | step time: 0.000
[2025-04-19 11:08:50,607] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16777216, reducing to 8388608
train | epoch   0 | Iter:      9/ 13000 | global iter:      9/ 13000 | loss: 3.4400 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 8388608.0000 | micro time: 1.134 | step time: 0.000
[2025-04-19 11:08:51,740] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8388608, reducing to 4194304
train | epoch   0 | Iter:     10/ 13000 | global iter:     10/ 13000 | loss: 3.1614 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 4194304.0000 | micro time: 1.132 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:     10/ 13000 | global iter:     10/ 13000 | loss: 3.2867 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 4194304.0000 | micro time: 1.132 | step time: 1.178
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
[2025-04-19 11:08:52,848] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4194304, reducing to 2097152
train | epoch   0 | Iter:     11/ 13000 | global iter:     11/ 13000 | loss: 3.3135 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 2097152.0000 | micro time: 1.111 | step time: 0.000
[2025-04-19 11:08:53,964] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2097152, reducing to 1048576
train | epoch   0 | Iter:     12/ 13000 | global iter:     12/ 13000 | loss: 3.5059 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 1048576.0000 | micro time: 1.088 | step time: 0.000
[2025-04-19 11:08:55,092] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1048576, reducing to 524288
train | epoch   0 | Iter:     13/ 13000 | global iter:     13/ 13000 | loss: 3.3438 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 524288.0000 | micro time: 1.127 | step time: 0.000
[2025-04-19 11:08:56,186] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 524288, reducing to 262144
train | epoch   0 | Iter:     14/ 13000 | global iter:     14/ 13000 | loss: 3.6938 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 262144.0000 | micro time: 1.126 | step time: 0.000
train | epoch   0 | Iter:     15/ 13000 | global iter:     15/ 13000 | loss: 3.2134 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 262144.0000 | micro time: 1.512 | step time: 0.000
train | epoch   0 | Iter:     16/ 13000 | global iter:     16/ 13000 | loss: 3.0395 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 262144.0000 | micro time: 1.778 | step time: 0.000
[2025-04-19 11:09:00,914] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072
train | epoch   0 | Iter:     17/ 13000 | global iter:     17/ 13000 | loss: 2.9012 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 131072.0000 | micro time: 1.412 | step time: 0.000
train | epoch   0 | Iter:     18/ 13000 | global iter:     18/ 13000 | loss: 3.4053 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 131072.0000 | micro time: 1.700 | step time: 0.000
train | epoch   0 | Iter:     19/ 13000 | global iter:     19/ 13000 | loss: 2.6118 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 131072.0000 | micro time: 1.798 | step time: 0.000
train | epoch   0 | Iter:     20/ 13000 | global iter:     20/ 13000 | loss: 3.5026 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 131072.0000 | micro time: 1.766 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:     20/ 13000 | global iter:     20/ 13000 | loss: 3.2531 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 131072.0000 | micro time: 1.766 | step time: 1.442
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:     21/ 13000 | global iter:     21/ 13000 | loss: 2.4344 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 131072.0000 | micro time: 1.814 | step time: 0.000
train | epoch   0 | Iter:     22/ 13000 | global iter:     22/ 13000 | loss: 3.1093 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 131072.0000 | micro time: 1.808 | step time: 0.000
train | epoch   0 | Iter:     23/ 13000 | global iter:     23/ 13000 | loss: 3.2454 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 131072.0000 | micro time: 1.842 | step time: 0.000
train | epoch   0 | Iter:     24/ 13000 | global iter:     24/ 13000 | loss: 2.9053 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 131072.0000 | micro time: 1.810 | step time: 0.000
train | epoch   0 | Iter:     25/ 13000 | global iter:     25/ 13000 | loss: 2.9723 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 131072.0000 | micro time: 1.831 | step time: 0.000
train | epoch   0 | Iter:     26/ 13000 | global iter:     26/ 13000 | loss: 3.2001 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 131072.0000 | micro time: 1.825 | step time: 0.000
train | epoch   0 | Iter:     27/ 13000 | global iter:     27/ 13000 | loss: 2.7147 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 131072.0000 | micro time: 1.802 | step time: 0.000
train | epoch   0 | Iter:     28/ 13000 | global iter:     28/ 13000 | loss: 3.4634 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 131072.0000 | micro time: 1.791 | step time: 0.000
train | epoch   0 | Iter:     29/ 13000 | global iter:     29/ 13000 | loss: 2.9130 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 131072.0000 | micro time: 1.809 | step time: 0.000
train | epoch   0 | Iter:     30/ 13000 | global iter:     30/ 13000 | loss: 3.1692 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 131072.0000 | micro time: 1.831 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:     30/ 13000 | global iter:     30/ 13000 | loss: 3.0127 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 131072.0000 | micro time: 1.831 | step time: 1.816
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:     31/ 13000 | global iter:     31/ 13000 | loss: 3.3186 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 131072.0000 | micro time: 1.786 | step time: 0.000
train | epoch   0 | Iter:     32/ 13000 | global iter:     32/ 13000 | loss: 2.9632 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 131072.0000 | micro time: 1.739 | step time: 0.000
train | epoch   0 | Iter:     33/ 13000 | global iter:     33/ 13000 | loss: 3.1088 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 131072.0000 | micro time: 1.874 | step time: 0.000
train | epoch   0 | Iter:     34/ 13000 | global iter:     34/ 13000 | loss: 2.9592 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 131072.0000 | micro time: 1.803 | step time: 0.000
train | epoch   0 | Iter:     35/ 13000 | global iter:     35/ 13000 | loss: 2.9782 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 131072.0000 | micro time: 1.731 | step time: 0.000
train | epoch   0 | Iter:     36/ 13000 | global iter:     36/ 13000 | loss: 2.9496 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 131072.0000 | micro time: 1.787 | step time: 0.000
train | epoch   0 | Iter:     37/ 13000 | global iter:     37/ 13000 | loss: 2.6419 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 131072.0000 | micro time: 1.831 | step time: 0.000
train | epoch   0 | Iter:     38/ 13000 | global iter:     38/ 13000 | loss: 2.7606 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 131072.0000 | micro time: 1.812 | step time: 0.000
train | epoch   0 | Iter:     39/ 13000 | global iter:     39/ 13000 | loss: 2.6477 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 131072.0000 | micro time: 1.790 | step time: 0.000
train | epoch   0 | Iter:     40/ 13000 | global iter:     40/ 13000 | loss: 2.1095 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 131072.0000 | micro time: 1.807 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:     40/ 13000 | global iter:     40/ 13000 | loss: 2.8437 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 131072.0000 | micro time: 1.807 | step time: 1.796
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:     41/ 13000 | global iter:     41/ 13000 | loss: 2.3772 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 131072.0000 | micro time: 1.765 | step time: 0.000
[2025-04-19 11:09:45,672] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536
train | epoch   0 | Iter:     42/ 13000 | global iter:     42/ 13000 | loss: 3.1295 | ds_loss: 0.0000 | lr: 5.0000e-05 | scale: 65536.0000 | micro time: 1.527 | step time: 0.000
train | epoch   0 | Iter:     43/ 13000 | global iter:     43/ 13000 | loss: 3.1414 | ds_loss: 0.0000 | lr: 4.9999e-05 | scale: 65536.0000 | micro time: 1.870 | step time: 0.000
train | epoch   0 | Iter:     44/ 13000 | global iter:     44/ 13000 | loss: 2.4072 | ds_loss: 0.0000 | lr: 4.9999e-05 | scale: 65536.0000 | micro time: 1.860 | step time: 0.000
train | epoch   0 | Iter:     45/ 13000 | global iter:     45/ 13000 | loss: 2.9202 | ds_loss: 0.0000 | lr: 4.9999e-05 | scale: 65536.0000 | micro time: 1.854 | step time: 0.000
train | epoch   0 | Iter:     46/ 13000 | global iter:     46/ 13000 | loss: 2.8763 | ds_loss: 0.0000 | lr: 4.9999e-05 | scale: 65536.0000 | micro time: 1.799 | step time: 0.000
train | epoch   0 | Iter:     47/ 13000 | global iter:     47/ 13000 | loss: 3.2288 | ds_loss: 0.0000 | lr: 4.9999e-05 | scale: 65536.0000 | micro time: 1.719 | step time: 0.000
train | epoch   0 | Iter:     48/ 13000 | global iter:     48/ 13000 | loss: 3.0053 | ds_loss: 0.0000 | lr: 4.9999e-05 | scale: 65536.0000 | micro time: 1.806 | step time: 0.000
train | epoch   0 | Iter:     49/ 13000 | global iter:     49/ 13000 | loss: 3.0968 | ds_loss: 0.0000 | lr: 4.9999e-05 | scale: 65536.0000 | micro time: 1.747 | step time: 0.000
train | epoch   0 | Iter:     50/ 13000 | global iter:     50/ 13000 | loss: 2.2776 | ds_loss: 0.0000 | lr: 4.9999e-05 | scale: 65536.0000 | micro time: 1.807 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:     50/ 13000 | global iter:     50/ 13000 | loss: 2.8460 | ds_loss: 0.0000 | lr: 4.9999e-05 | scale: 65536.0000 | micro time: 1.807 | step time: 1.775
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:     51/ 13000 | global iter:     51/ 13000 | loss: 3.3276 | ds_loss: 0.0000 | lr: 4.9999e-05 | scale: 65536.0000 | micro time: 1.774 | step time: 0.000
train | epoch   0 | Iter:     52/ 13000 | global iter:     52/ 13000 | loss: 2.9059 | ds_loss: 0.0000 | lr: 4.9999e-05 | scale: 65536.0000 | micro time: 1.756 | step time: 0.000
train | epoch   0 | Iter:     53/ 13000 | global iter:     53/ 13000 | loss: 2.8615 | ds_loss: 0.0000 | lr: 4.9999e-05 | scale: 65536.0000 | micro time: 1.763 | step time: 0.000
train | epoch   0 | Iter:     54/ 13000 | global iter:     54/ 13000 | loss: 2.6569 | ds_loss: 0.0000 | lr: 4.9999e-05 | scale: 65536.0000 | micro time: 1.737 | step time: 0.000
train | epoch   0 | Iter:     55/ 13000 | global iter:     55/ 13000 | loss: 3.2382 | ds_loss: 0.0000 | lr: 4.9999e-05 | scale: 65536.0000 | micro time: 1.780 | step time: 0.000
train | epoch   0 | Iter:     56/ 13000 | global iter:     56/ 13000 | loss: 3.3376 | ds_loss: 0.0000 | lr: 4.9999e-05 | scale: 65536.0000 | micro time: 1.659 | step time: 0.000
train | epoch   0 | Iter:     57/ 13000 | global iter:     57/ 13000 | loss: 2.4429 | ds_loss: 0.0000 | lr: 4.9999e-05 | scale: 65536.0000 | micro time: 1.797 | step time: 0.000
train | epoch   0 | Iter:     58/ 13000 | global iter:     58/ 13000 | loss: 3.0875 | ds_loss: 0.0000 | lr: 4.9999e-05 | scale: 65536.0000 | micro time: 1.754 | step time: 0.000
train | epoch   0 | Iter:     59/ 13000 | global iter:     59/ 13000 | loss: 2.6320 | ds_loss: 0.0000 | lr: 4.9999e-05 | scale: 65536.0000 | micro time: 1.870 | step time: 0.000
train | epoch   0 | Iter:     60/ 13000 | global iter:     60/ 13000 | loss: 2.9187 | ds_loss: 0.0000 | lr: 4.9999e-05 | scale: 65536.0000 | micro time: 1.792 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:     60/ 13000 | global iter:     60/ 13000 | loss: 2.9409 | ds_loss: 0.0000 | lr: 4.9999e-05 | scale: 65536.0000 | micro time: 1.792 | step time: 1.768
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:     61/ 13000 | global iter:     61/ 13000 | loss: 2.8938 | ds_loss: 0.0000 | lr: 4.9999e-05 | scale: 65536.0000 | micro time: 1.811 | step time: 0.000
train | epoch   0 | Iter:     62/ 13000 | global iter:     62/ 13000 | loss: 2.8701 | ds_loss: 0.0000 | lr: 4.9998e-05 | scale: 65536.0000 | micro time: 1.781 | step time: 0.000
train | epoch   0 | Iter:     63/ 13000 | global iter:     63/ 13000 | loss: 3.0688 | ds_loss: 0.0000 | lr: 4.9998e-05 | scale: 65536.0000 | micro time: 1.883 | step time: 0.000
train | epoch   0 | Iter:     64/ 13000 | global iter:     64/ 13000 | loss: 2.7532 | ds_loss: 0.0000 | lr: 4.9998e-05 | scale: 65536.0000 | micro time: 1.743 | step time: 0.000
train | epoch   0 | Iter:     65/ 13000 | global iter:     65/ 13000 | loss: 2.4426 | ds_loss: 0.0000 | lr: 4.9998e-05 | scale: 65536.0000 | micro time: 1.825 | step time: 0.000
train | epoch   0 | Iter:     66/ 13000 | global iter:     66/ 13000 | loss: 2.9939 | ds_loss: 0.0000 | lr: 4.9998e-05 | scale: 65536.0000 | micro time: 1.825 | step time: 0.000
train | epoch   0 | Iter:     67/ 13000 | global iter:     67/ 13000 | loss: 3.2223 | ds_loss: 0.0000 | lr: 4.9998e-05 | scale: 65536.0000 | micro time: 1.742 | step time: 0.000
train | epoch   0 | Iter:     68/ 13000 | global iter:     68/ 13000 | loss: 2.9495 | ds_loss: 0.0000 | lr: 4.9998e-05 | scale: 65536.0000 | micro time: 1.736 | step time: 0.000
train | epoch   0 | Iter:     69/ 13000 | global iter:     69/ 13000 | loss: 2.4838 | ds_loss: 0.0000 | lr: 4.9998e-05 | scale: 65536.0000 | micro time: 1.831 | step time: 0.000
train | epoch   0 | Iter:     70/ 13000 | global iter:     70/ 13000 | loss: 3.3485 | ds_loss: 0.0000 | lr: 4.9998e-05 | scale: 65536.0000 | micro time: 1.807 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:     70/ 13000 | global iter:     70/ 13000 | loss: 2.9027 | ds_loss: 0.0000 | lr: 4.9998e-05 | scale: 65536.0000 | micro time: 1.807 | step time: 1.798
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:     71/ 13000 | global iter:     71/ 13000 | loss: 3.0092 | ds_loss: 0.0000 | lr: 4.9998e-05 | scale: 65536.0000 | micro time: 1.810 | step time: 0.000
train | epoch   0 | Iter:     72/ 13000 | global iter:     72/ 13000 | loss: 3.5642 | ds_loss: 0.0000 | lr: 4.9998e-05 | scale: 65536.0000 | micro time: 1.747 | step time: 0.000
train | epoch   0 | Iter:     73/ 13000 | global iter:     73/ 13000 | loss: 2.9657 | ds_loss: 0.0000 | lr: 4.9998e-05 | scale: 65536.0000 | micro time: 1.755 | step time: 0.000
train | epoch   0 | Iter:     74/ 13000 | global iter:     74/ 13000 | loss: 3.4154 | ds_loss: 0.0000 | lr: 4.9998e-05 | scale: 65536.0000 | micro time: 1.879 | step time: 0.000
train | epoch   0 | Iter:     75/ 13000 | global iter:     75/ 13000 | loss: 2.1910 | ds_loss: 0.0000 | lr: 4.9997e-05 | scale: 65536.0000 | micro time: 1.771 | step time: 0.000
train | epoch   0 | Iter:     76/ 13000 | global iter:     76/ 13000 | loss: 2.9265 | ds_loss: 0.0000 | lr: 4.9997e-05 | scale: 65536.0000 | micro time: 1.819 | step time: 0.000
train | epoch   0 | Iter:     77/ 13000 | global iter:     77/ 13000 | loss: 3.1028 | ds_loss: 0.0000 | lr: 4.9997e-05 | scale: 65536.0000 | micro time: 1.802 | step time: 0.000
train | epoch   0 | Iter:     78/ 13000 | global iter:     78/ 13000 | loss: 2.9877 | ds_loss: 0.0000 | lr: 4.9997e-05 | scale: 65536.0000 | micro time: 1.892 | step time: 0.000
train | epoch   0 | Iter:     79/ 13000 | global iter:     79/ 13000 | loss: 2.5738 | ds_loss: 0.0000 | lr: 4.9997e-05 | scale: 65536.0000 | micro time: 1.818 | step time: 0.000
train | epoch   0 | Iter:     80/ 13000 | global iter:     80/ 13000 | loss: 2.2877 | ds_loss: 0.0000 | lr: 4.9997e-05 | scale: 65536.0000 | micro time: 1.716 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:     80/ 13000 | global iter:     80/ 13000 | loss: 2.9024 | ds_loss: 0.0000 | lr: 4.9997e-05 | scale: 65536.0000 | micro time: 1.716 | step time: 1.801
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:     81/ 13000 | global iter:     81/ 13000 | loss: 3.3121 | ds_loss: 0.0000 | lr: 4.9997e-05 | scale: 65536.0000 | micro time: 1.737 | step time: 0.000
train | epoch   0 | Iter:     82/ 13000 | global iter:     82/ 13000 | loss: 3.1780 | ds_loss: 0.0000 | lr: 4.9997e-05 | scale: 65536.0000 | micro time: 1.896 | step time: 0.000
train | epoch   0 | Iter:     83/ 13000 | global iter:     83/ 13000 | loss: 3.2219 | ds_loss: 0.0000 | lr: 4.9997e-05 | scale: 65536.0000 | micro time: 1.767 | step time: 0.000
train | epoch   0 | Iter:     84/ 13000 | global iter:     84/ 13000 | loss: 3.2193 | ds_loss: 0.0000 | lr: 4.9997e-05 | scale: 65536.0000 | micro time: 1.904 | step time: 0.000
train | epoch   0 | Iter:     85/ 13000 | global iter:     85/ 13000 | loss: 2.8720 | ds_loss: 0.0000 | lr: 4.9997e-05 | scale: 65536.0000 | micro time: 1.760 | step time: 0.000
train | epoch   0 | Iter:     86/ 13000 | global iter:     86/ 13000 | loss: 2.9135 | ds_loss: 0.0000 | lr: 4.9996e-05 | scale: 65536.0000 | micro time: 1.789 | step time: 0.000
train | epoch   0 | Iter:     87/ 13000 | global iter:     87/ 13000 | loss: 3.2248 | ds_loss: 0.0000 | lr: 4.9996e-05 | scale: 65536.0000 | micro time: 1.743 | step time: 0.000
train | epoch   0 | Iter:     88/ 13000 | global iter:     88/ 13000 | loss: 2.5778 | ds_loss: 0.0000 | lr: 4.9996e-05 | scale: 65536.0000 | micro time: 1.846 | step time: 0.000
train | epoch   0 | Iter:     89/ 13000 | global iter:     89/ 13000 | loss: 2.3301 | ds_loss: 0.0000 | lr: 4.9996e-05 | scale: 65536.0000 | micro time: 1.887 | step time: 0.000
train | epoch   0 | Iter:     90/ 13000 | global iter:     90/ 13000 | loss: 2.4856 | ds_loss: 0.0000 | lr: 4.9996e-05 | scale: 65536.0000 | micro time: 1.855 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:     90/ 13000 | global iter:     90/ 13000 | loss: 2.9335 | ds_loss: 0.0000 | lr: 4.9996e-05 | scale: 65536.0000 | micro time: 1.855 | step time: 1.819
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:     91/ 13000 | global iter:     91/ 13000 | loss: 2.7881 | ds_loss: 0.0000 | lr: 4.9996e-05 | scale: 65536.0000 | micro time: 1.870 | step time: 0.000
train | epoch   0 | Iter:     92/ 13000 | global iter:     92/ 13000 | loss: 2.7207 | ds_loss: 0.0000 | lr: 4.9996e-05 | scale: 65536.0000 | micro time: 1.912 | step time: 0.000
train | epoch   0 | Iter:     93/ 13000 | global iter:     93/ 13000 | loss: 2.4071 | ds_loss: 0.0000 | lr: 4.9996e-05 | scale: 65536.0000 | micro time: 1.706 | step time: 0.000
train | epoch   0 | Iter:     94/ 13000 | global iter:     94/ 13000 | loss: 2.9850 | ds_loss: 0.0000 | lr: 4.9996e-05 | scale: 65536.0000 | micro time: 1.767 | step time: 0.000
train | epoch   0 | Iter:     95/ 13000 | global iter:     95/ 13000 | loss: 2.9832 | ds_loss: 0.0000 | lr: 4.9995e-05 | scale: 65536.0000 | micro time: 1.848 | step time: 0.000
train | epoch   0 | Iter:     96/ 13000 | global iter:     96/ 13000 | loss: 2.4375 | ds_loss: 0.0000 | lr: 4.9995e-05 | scale: 65536.0000 | micro time: 1.787 | step time: 0.000
train | epoch   0 | Iter:     97/ 13000 | global iter:     97/ 13000 | loss: 2.9391 | ds_loss: 0.0000 | lr: 4.9995e-05 | scale: 65536.0000 | micro time: 1.728 | step time: 0.000
train | epoch   0 | Iter:     98/ 13000 | global iter:     98/ 13000 | loss: 2.1588 | ds_loss: 0.0000 | lr: 4.9995e-05 | scale: 65536.0000 | micro time: 1.849 | step time: 0.000
train | epoch   0 | Iter:     99/ 13000 | global iter:     99/ 13000 | loss: 3.2862 | ds_loss: 0.0000 | lr: 4.9995e-05 | scale: 65536.0000 | micro time: 1.783 | step time: 0.000
train | epoch   0 | Iter:    100/ 13000 | global iter:    100/ 13000 | loss: 3.1292 | ds_loss: 0.0000 | lr: 4.9995e-05 | scale: 65536.0000 | micro time: 1.871 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    100/ 13000 | global iter:    100/ 13000 | loss: 2.7835 | ds_loss: 0.0000 | lr: 4.9995e-05 | scale: 65536.0000 | micro time: 1.871 | step time: 1.812
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    101/ 13000 | global iter:    101/ 13000 | loss: 3.2799 | ds_loss: 0.0000 | lr: 4.9995e-05 | scale: 65536.0000 | micro time: 1.781 | step time: 0.000
train | epoch   0 | Iter:    102/ 13000 | global iter:    102/ 13000 | loss: 2.7089 | ds_loss: 0.0000 | lr: 4.9995e-05 | scale: 65536.0000 | micro time: 1.809 | step time: 0.000
train | epoch   0 | Iter:    103/ 13000 | global iter:    103/ 13000 | loss: 2.2647 | ds_loss: 0.0000 | lr: 4.9994e-05 | scale: 65536.0000 | micro time: 1.729 | step time: 0.000
train | epoch   0 | Iter:    104/ 13000 | global iter:    104/ 13000 | loss: 3.3267 | ds_loss: 0.0000 | lr: 4.9994e-05 | scale: 65536.0000 | micro time: 1.745 | step time: 0.000
train | epoch   0 | Iter:    105/ 13000 | global iter:    105/ 13000 | loss: 2.9234 | ds_loss: 0.0000 | lr: 4.9994e-05 | scale: 65536.0000 | micro time: 1.865 | step time: 0.000
train | epoch   0 | Iter:    106/ 13000 | global iter:    106/ 13000 | loss: 2.6700 | ds_loss: 0.0000 | lr: 4.9994e-05 | scale: 65536.0000 | micro time: 1.787 | step time: 0.000
train | epoch   0 | Iter:    107/ 13000 | global iter:    107/ 13000 | loss: 2.9180 | ds_loss: 0.0000 | lr: 4.9994e-05 | scale: 65536.0000 | micro time: 1.815 | step time: 0.000
train | epoch   0 | Iter:    108/ 13000 | global iter:    108/ 13000 | loss: 3.1834 | ds_loss: 0.0000 | lr: 4.9994e-05 | scale: 65536.0000 | micro time: 1.855 | step time: 0.000
train | epoch   0 | Iter:    109/ 13000 | global iter:    109/ 13000 | loss: 3.0112 | ds_loss: 0.0000 | lr: 4.9994e-05 | scale: 65536.0000 | micro time: 1.774 | step time: 0.000
train | epoch   0 | Iter:    110/ 13000 | global iter:    110/ 13000 | loss: 2.9362 | ds_loss: 0.0000 | lr: 4.9994e-05 | scale: 65536.0000 | micro time: 1.772 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    110/ 13000 | global iter:    110/ 13000 | loss: 2.9222 | ds_loss: 0.0000 | lr: 4.9994e-05 | scale: 65536.0000 | micro time: 1.772 | step time: 1.793
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    111/ 13000 | global iter:    111/ 13000 | loss: 2.3050 | ds_loss: 0.0000 | lr: 4.9993e-05 | scale: 65536.0000 | micro time: 1.830 | step time: 0.000
train | epoch   0 | Iter:    112/ 13000 | global iter:    112/ 13000 | loss: 2.1695 | ds_loss: 0.0000 | lr: 4.9993e-05 | scale: 65536.0000 | micro time: 1.835 | step time: 0.000
train | epoch   0 | Iter:    113/ 13000 | global iter:    113/ 13000 | loss: 2.2143 | ds_loss: 0.0000 | lr: 4.9993e-05 | scale: 65536.0000 | micro time: 1.813 | step time: 0.000
train | epoch   0 | Iter:    114/ 13000 | global iter:    114/ 13000 | loss: 2.8548 | ds_loss: 0.0000 | lr: 4.9993e-05 | scale: 65536.0000 | micro time: 1.793 | step time: 0.000
train | epoch   0 | Iter:    115/ 13000 | global iter:    115/ 13000 | loss: 3.2688 | ds_loss: 0.0000 | lr: 4.9993e-05 | scale: 65536.0000 | micro time: 1.707 | step time: 0.000
train | epoch   0 | Iter:    116/ 13000 | global iter:    116/ 13000 | loss: 2.7141 | ds_loss: 0.0000 | lr: 4.9993e-05 | scale: 65536.0000 | micro time: 1.833 | step time: 0.000
train | epoch   0 | Iter:    117/ 13000 | global iter:    117/ 13000 | loss: 2.2531 | ds_loss: 0.0000 | lr: 4.9993e-05 | scale: 65536.0000 | micro time: 1.774 | step time: 0.000
train | epoch   0 | Iter:    118/ 13000 | global iter:    118/ 13000 | loss: 3.1241 | ds_loss: 0.0000 | lr: 4.9992e-05 | scale: 65536.0000 | micro time: 1.744 | step time: 0.000
train | epoch   0 | Iter:    119/ 13000 | global iter:    119/ 13000 | loss: 2.8342 | ds_loss: 0.0000 | lr: 4.9992e-05 | scale: 65536.0000 | micro time: 1.819 | step time: 0.000
train | epoch   0 | Iter:    120/ 13000 | global iter:    120/ 13000 | loss: 2.8179 | ds_loss: 0.0000 | lr: 4.9992e-05 | scale: 65536.0000 | micro time: 1.707 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    120/ 13000 | global iter:    120/ 13000 | loss: 2.6556 | ds_loss: 0.0000 | lr: 4.9992e-05 | scale: 65536.0000 | micro time: 1.707 | step time: 1.785
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    121/ 13000 | global iter:    121/ 13000 | loss: 3.3404 | ds_loss: 0.0000 | lr: 4.9992e-05 | scale: 65536.0000 | micro time: 1.757 | step time: 0.000
train | epoch   0 | Iter:    122/ 13000 | global iter:    122/ 13000 | loss: 2.9952 | ds_loss: 0.0000 | lr: 4.9992e-05 | scale: 65536.0000 | micro time: 1.743 | step time: 0.000
train | epoch   0 | Iter:    123/ 13000 | global iter:    123/ 13000 | loss: 2.5094 | ds_loss: 0.0000 | lr: 4.9992e-05 | scale: 65536.0000 | micro time: 1.791 | step time: 0.000
train | epoch   0 | Iter:    124/ 13000 | global iter:    124/ 13000 | loss: 2.8360 | ds_loss: 0.0000 | lr: 4.9992e-05 | scale: 65536.0000 | micro time: 1.751 | step time: 0.000
train | epoch   0 | Iter:    125/ 13000 | global iter:    125/ 13000 | loss: 3.4049 | ds_loss: 0.0000 | lr: 4.9991e-05 | scale: 65536.0000 | micro time: 1.816 | step time: 0.000
train | epoch   0 | Iter:    126/ 13000 | global iter:    126/ 13000 | loss: 2.1917 | ds_loss: 0.0000 | lr: 4.9991e-05 | scale: 65536.0000 | micro time: 1.786 | step time: 0.000
train | epoch   0 | Iter:    127/ 13000 | global iter:    127/ 13000 | loss: 3.3482 | ds_loss: 0.0000 | lr: 4.9991e-05 | scale: 65536.0000 | micro time: 1.683 | step time: 0.000
train | epoch   0 | Iter:    128/ 13000 | global iter:    128/ 13000 | loss: 2.2551 | ds_loss: 0.0000 | lr: 4.9991e-05 | scale: 65536.0000 | micro time: 1.831 | step time: 0.000
train | epoch   0 | Iter:    129/ 13000 | global iter:    129/ 13000 | loss: 2.2899 | ds_loss: 0.0000 | lr: 4.9991e-05 | scale: 65536.0000 | micro time: 1.676 | step time: 0.000
train | epoch   0 | Iter:    130/ 13000 | global iter:    130/ 13000 | loss: 2.2486 | ds_loss: 0.0000 | lr: 4.9991e-05 | scale: 65536.0000 | micro time: 1.798 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    130/ 13000 | global iter:    130/ 13000 | loss: 2.7420 | ds_loss: 0.0000 | lr: 4.9991e-05 | scale: 65536.0000 | micro time: 1.798 | step time: 1.763
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    131/ 13000 | global iter:    131/ 13000 | loss: 2.8007 | ds_loss: 0.0000 | lr: 4.9990e-05 | scale: 65536.0000 | micro time: 1.851 | step time: 0.000
train | epoch   0 | Iter:    132/ 13000 | global iter:    132/ 13000 | loss: 3.0009 | ds_loss: 0.0000 | lr: 4.9990e-05 | scale: 65536.0000 | micro time: 1.799 | step time: 0.000
train | epoch   0 | Iter:    133/ 13000 | global iter:    133/ 13000 | loss: 3.3647 | ds_loss: 0.0000 | lr: 4.9990e-05 | scale: 65536.0000 | micro time: 1.798 | step time: 0.000
train | epoch   0 | Iter:    134/ 13000 | global iter:    134/ 13000 | loss: 2.5264 | ds_loss: 0.0000 | lr: 4.9990e-05 | scale: 65536.0000 | micro time: 1.852 | step time: 0.000
train | epoch   0 | Iter:    135/ 13000 | global iter:    135/ 13000 | loss: 3.1455 | ds_loss: 0.0000 | lr: 4.9990e-05 | scale: 65536.0000 | micro time: 1.820 | step time: 0.000
train | epoch   0 | Iter:    136/ 13000 | global iter:    136/ 13000 | loss: 3.0806 | ds_loss: 0.0000 | lr: 4.9990e-05 | scale: 65536.0000 | micro time: 1.742 | step time: 0.000
train | epoch   0 | Iter:    137/ 13000 | global iter:    137/ 13000 | loss: 2.8502 | ds_loss: 0.0000 | lr: 4.9989e-05 | scale: 65536.0000 | micro time: 1.753 | step time: 0.000
train | epoch   0 | Iter:    138/ 13000 | global iter:    138/ 13000 | loss: 2.6290 | ds_loss: 0.0000 | lr: 4.9989e-05 | scale: 65536.0000 | micro time: 1.713 | step time: 0.000
train | epoch   0 | Iter:    139/ 13000 | global iter:    139/ 13000 | loss: 3.0363 | ds_loss: 0.0000 | lr: 4.9989e-05 | scale: 65536.0000 | micro time: 1.819 | step time: 0.000
train | epoch   0 | Iter:    140/ 13000 | global iter:    140/ 13000 | loss: 2.7699 | ds_loss: 0.0000 | lr: 4.9989e-05 | scale: 65536.0000 | micro time: 1.813 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    140/ 13000 | global iter:    140/ 13000 | loss: 2.9204 | ds_loss: 0.0000 | lr: 4.9989e-05 | scale: 65536.0000 | micro time: 1.813 | step time: 1.796
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    141/ 13000 | global iter:    141/ 13000 | loss: 2.6390 | ds_loss: 0.0000 | lr: 4.9989e-05 | scale: 65536.0000 | micro time: 1.855 | step time: 0.000
train | epoch   0 | Iter:    142/ 13000 | global iter:    142/ 13000 | loss: 2.7962 | ds_loss: 0.0000 | lr: 4.9988e-05 | scale: 65536.0000 | micro time: 1.827 | step time: 0.000
train | epoch   0 | Iter:    143/ 13000 | global iter:    143/ 13000 | loss: 2.7491 | ds_loss: 0.0000 | lr: 4.9988e-05 | scale: 65536.0000 | micro time: 1.786 | step time: 0.000
train | epoch   0 | Iter:    144/ 13000 | global iter:    144/ 13000 | loss: 2.3672 | ds_loss: 0.0000 | lr: 4.9988e-05 | scale: 65536.0000 | micro time: 1.867 | step time: 0.000
train | epoch   0 | Iter:    145/ 13000 | global iter:    145/ 13000 | loss: 2.8658 | ds_loss: 0.0000 | lr: 4.9988e-05 | scale: 65536.0000 | micro time: 1.791 | step time: 0.000
train | epoch   0 | Iter:    146/ 13000 | global iter:    146/ 13000 | loss: 3.0666 | ds_loss: 0.0000 | lr: 4.9988e-05 | scale: 65536.0000 | micro time: 1.794 | step time: 0.000
train | epoch   0 | Iter:    147/ 13000 | global iter:    147/ 13000 | loss: 2.6105 | ds_loss: 0.0000 | lr: 4.9987e-05 | scale: 65536.0000 | micro time: 1.755 | step time: 0.000
train | epoch   0 | Iter:    148/ 13000 | global iter:    148/ 13000 | loss: 3.0037 | ds_loss: 0.0000 | lr: 4.9987e-05 | scale: 65536.0000 | micro time: 1.795 | step time: 0.000
train | epoch   0 | Iter:    149/ 13000 | global iter:    149/ 13000 | loss: 2.8739 | ds_loss: 0.0000 | lr: 4.9987e-05 | scale: 65536.0000 | micro time: 1.831 | step time: 0.000
train | epoch   0 | Iter:    150/ 13000 | global iter:    150/ 13000 | loss: 2.8358 | ds_loss: 0.0000 | lr: 4.9987e-05 | scale: 65536.0000 | micro time: 1.735 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    150/ 13000 | global iter:    150/ 13000 | loss: 2.7808 | ds_loss: 0.0000 | lr: 4.9987e-05 | scale: 65536.0000 | micro time: 1.735 | step time: 1.804
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    151/ 13000 | global iter:    151/ 13000 | loss: 2.6668 | ds_loss: 0.0000 | lr: 4.9987e-05 | scale: 65536.0000 | micro time: 1.794 | step time: 0.000
train | epoch   0 | Iter:    152/ 13000 | global iter:    152/ 13000 | loss: 3.1364 | ds_loss: 0.0000 | lr: 4.9987e-05 | scale: 65536.0000 | micro time: 1.803 | step time: 0.000
train | epoch   0 | Iter:    153/ 13000 | global iter:    153/ 13000 | loss: 2.6781 | ds_loss: 0.0000 | lr: 4.9986e-05 | scale: 65536.0000 | micro time: 1.859 | step time: 0.000
train | epoch   0 | Iter:    154/ 13000 | global iter:    154/ 13000 | loss: 3.2989 | ds_loss: 0.0000 | lr: 4.9986e-05 | scale: 65536.0000 | micro time: 1.831 | step time: 0.000
train | epoch   0 | Iter:    155/ 13000 | global iter:    155/ 13000 | loss: 2.6461 | ds_loss: 0.0000 | lr: 4.9986e-05 | scale: 65536.0000 | micro time: 1.736 | step time: 0.000
train | epoch   0 | Iter:    156/ 13000 | global iter:    156/ 13000 | loss: 2.9406 | ds_loss: 0.0000 | lr: 4.9986e-05 | scale: 65536.0000 | micro time: 1.777 | step time: 0.000
train | epoch   0 | Iter:    157/ 13000 | global iter:    157/ 13000 | loss: 2.5555 | ds_loss: 0.0000 | lr: 4.9986e-05 | scale: 65536.0000 | micro time: 1.796 | step time: 0.000
train | epoch   0 | Iter:    158/ 13000 | global iter:    158/ 13000 | loss: 3.0459 | ds_loss: 0.0000 | lr: 4.9985e-05 | scale: 65536.0000 | micro time: 1.790 | step time: 0.000
train | epoch   0 | Iter:    159/ 13000 | global iter:    159/ 13000 | loss: 3.4446 | ds_loss: 0.0000 | lr: 4.9985e-05 | scale: 65536.0000 | micro time: 1.816 | step time: 0.000
train | epoch   0 | Iter:    160/ 13000 | global iter:    160/ 13000 | loss: 2.3940 | ds_loss: 0.0000 | lr: 4.9985e-05 | scale: 65536.0000 | micro time: 1.869 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    160/ 13000 | global iter:    160/ 13000 | loss: 2.8807 | ds_loss: 0.0000 | lr: 4.9985e-05 | scale: 65536.0000 | micro time: 1.869 | step time: 1.807
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    161/ 13000 | global iter:    161/ 13000 | loss: 2.9789 | ds_loss: 0.0000 | lr: 4.9985e-05 | scale: 65536.0000 | micro time: 1.854 | step time: 0.000
train | epoch   0 | Iter:    162/ 13000 | global iter:    162/ 13000 | loss: 3.3379 | ds_loss: 0.0000 | lr: 4.9984e-05 | scale: 65536.0000 | micro time: 1.838 | step time: 0.000
train | epoch   0 | Iter:    163/ 13000 | global iter:    163/ 13000 | loss: 2.6677 | ds_loss: 0.0000 | lr: 4.9984e-05 | scale: 65536.0000 | micro time: 1.737 | step time: 0.000
train | epoch   0 | Iter:    164/ 13000 | global iter:    164/ 13000 | loss: 2.6490 | ds_loss: 0.0000 | lr: 4.9984e-05 | scale: 65536.0000 | micro time: 1.813 | step time: 0.000
train | epoch   0 | Iter:    165/ 13000 | global iter:    165/ 13000 | loss: 2.9154 | ds_loss: 0.0000 | lr: 4.9984e-05 | scale: 65536.0000 | micro time: 1.828 | step time: 0.000
train | epoch   0 | Iter:    166/ 13000 | global iter:    166/ 13000 | loss: 3.0696 | ds_loss: 0.0000 | lr: 4.9984e-05 | scale: 65536.0000 | micro time: 1.817 | step time: 0.000
train | epoch   0 | Iter:    167/ 13000 | global iter:    167/ 13000 | loss: 2.9270 | ds_loss: 0.0000 | lr: 4.9983e-05 | scale: 65536.0000 | micro time: 1.879 | step time: 0.000
train | epoch   0 | Iter:    168/ 13000 | global iter:    168/ 13000 | loss: 2.7055 | ds_loss: 0.0000 | lr: 4.9983e-05 | scale: 65536.0000 | micro time: 1.831 | step time: 0.000
train | epoch   0 | Iter:    169/ 13000 | global iter:    169/ 13000 | loss: 2.7728 | ds_loss: 0.0000 | lr: 4.9983e-05 | scale: 65536.0000 | micro time: 1.895 | step time: 0.000
train | epoch   0 | Iter:    170/ 13000 | global iter:    170/ 13000 | loss: 2.3356 | ds_loss: 0.0000 | lr: 4.9983e-05 | scale: 65536.0000 | micro time: 1.774 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    170/ 13000 | global iter:    170/ 13000 | loss: 2.8359 | ds_loss: 0.0000 | lr: 4.9983e-05 | scale: 65536.0000 | micro time: 1.774 | step time: 1.827
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    171/ 13000 | global iter:    171/ 13000 | loss: 2.5576 | ds_loss: 0.0000 | lr: 4.9982e-05 | scale: 65536.0000 | micro time: 1.757 | step time: 0.000
train | epoch   0 | Iter:    172/ 13000 | global iter:    172/ 13000 | loss: 3.0609 | ds_loss: 0.0000 | lr: 4.9982e-05 | scale: 65536.0000 | micro time: 1.745 | step time: 0.000
train | epoch   0 | Iter:    173/ 13000 | global iter:    173/ 13000 | loss: 2.1387 | ds_loss: 0.0000 | lr: 4.9982e-05 | scale: 65536.0000 | micro time: 1.787 | step time: 0.000
train | epoch   0 | Iter:    174/ 13000 | global iter:    174/ 13000 | loss: 3.2060 | ds_loss: 0.0000 | lr: 4.9982e-05 | scale: 65536.0000 | micro time: 1.822 | step time: 0.000
train | epoch   0 | Iter:    175/ 13000 | global iter:    175/ 13000 | loss: 3.0342 | ds_loss: 0.0000 | lr: 4.9982e-05 | scale: 65536.0000 | micro time: 1.788 | step time: 0.000
train | epoch   0 | Iter:    176/ 13000 | global iter:    176/ 13000 | loss: 2.6664 | ds_loss: 0.0000 | lr: 4.9981e-05 | scale: 65536.0000 | micro time: 1.805 | step time: 0.000
train | epoch   0 | Iter:    177/ 13000 | global iter:    177/ 13000 | loss: 2.2153 | ds_loss: 0.0000 | lr: 4.9981e-05 | scale: 65536.0000 | micro time: 1.785 | step time: 0.000
train | epoch   0 | Iter:    178/ 13000 | global iter:    178/ 13000 | loss: 2.2819 | ds_loss: 0.0000 | lr: 4.9981e-05 | scale: 65536.0000 | micro time: 1.753 | step time: 0.000
train | epoch   0 | Iter:    179/ 13000 | global iter:    179/ 13000 | loss: 3.2791 | ds_loss: 0.0000 | lr: 4.9981e-05 | scale: 65536.0000 | micro time: 1.740 | step time: 0.000
train | epoch   0 | Iter:    180/ 13000 | global iter:    180/ 13000 | loss: 2.5699 | ds_loss: 0.0000 | lr: 4.9980e-05 | scale: 65536.0000 | micro time: 1.855 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    180/ 13000 | global iter:    180/ 13000 | loss: 2.7010 | ds_loss: 0.0000 | lr: 4.9980e-05 | scale: 65536.0000 | micro time: 1.855 | step time: 1.784
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    181/ 13000 | global iter:    181/ 13000 | loss: 2.4305 | ds_loss: 0.0000 | lr: 4.9980e-05 | scale: 65536.0000 | micro time: 1.822 | step time: 0.000
train | epoch   0 | Iter:    182/ 13000 | global iter:    182/ 13000 | loss: 2.5664 | ds_loss: 0.0000 | lr: 4.9980e-05 | scale: 65536.0000 | micro time: 1.759 | step time: 0.000
train | epoch   0 | Iter:    183/ 13000 | global iter:    183/ 13000 | loss: 2.7997 | ds_loss: 0.0000 | lr: 4.9980e-05 | scale: 65536.0000 | micro time: 1.768 | step time: 0.000
train | epoch   0 | Iter:    184/ 13000 | global iter:    184/ 13000 | loss: 2.7776 | ds_loss: 0.0000 | lr: 4.9979e-05 | scale: 65536.0000 | micro time: 1.822 | step time: 0.000
train | epoch   0 | Iter:    185/ 13000 | global iter:    185/ 13000 | loss: 3.2188 | ds_loss: 0.0000 | lr: 4.9979e-05 | scale: 65536.0000 | micro time: 1.817 | step time: 0.000
train | epoch   0 | Iter:    186/ 13000 | global iter:    186/ 13000 | loss: 3.0664 | ds_loss: 0.0000 | lr: 4.9979e-05 | scale: 65536.0000 | micro time: 1.752 | step time: 0.000
train | epoch   0 | Iter:    187/ 13000 | global iter:    187/ 13000 | loss: 2.8321 | ds_loss: 0.0000 | lr: 4.9979e-05 | scale: 65536.0000 | micro time: 1.639 | step time: 0.000
train | epoch   0 | Iter:    188/ 13000 | global iter:    188/ 13000 | loss: 3.1250 | ds_loss: 0.0000 | lr: 4.9978e-05 | scale: 65536.0000 | micro time: 1.842 | step time: 0.000
train | epoch   0 | Iter:    189/ 13000 | global iter:    189/ 13000 | loss: 3.2145 | ds_loss: 0.0000 | lr: 4.9978e-05 | scale: 65536.0000 | micro time: 1.843 | step time: 0.000
train | epoch   0 | Iter:    190/ 13000 | global iter:    190/ 13000 | loss: 3.3245 | ds_loss: 0.0000 | lr: 4.9978e-05 | scale: 65536.0000 | micro time: 1.834 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    190/ 13000 | global iter:    190/ 13000 | loss: 2.9356 | ds_loss: 0.0000 | lr: 4.9978e-05 | scale: 65536.0000 | micro time: 1.834 | step time: 1.790
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    191/ 13000 | global iter:    191/ 13000 | loss: 2.6357 | ds_loss: 0.0000 | lr: 4.9978e-05 | scale: 65536.0000 | micro time: 1.854 | step time: 0.000
train | epoch   0 | Iter:    192/ 13000 | global iter:    192/ 13000 | loss: 2.6146 | ds_loss: 0.0000 | lr: 4.9977e-05 | scale: 65536.0000 | micro time: 1.765 | step time: 0.000
train | epoch   0 | Iter:    193/ 13000 | global iter:    193/ 13000 | loss: 2.5966 | ds_loss: 0.0000 | lr: 4.9977e-05 | scale: 65536.0000 | micro time: 1.681 | step time: 0.000
train | epoch   0 | Iter:    194/ 13000 | global iter:    194/ 13000 | loss: 2.9809 | ds_loss: 0.0000 | lr: 4.9977e-05 | scale: 65536.0000 | micro time: 1.847 | step time: 0.000
train | epoch   0 | Iter:    195/ 13000 | global iter:    195/ 13000 | loss: 2.8166 | ds_loss: 0.0000 | lr: 4.9977e-05 | scale: 65536.0000 | micro time: 1.851 | step time: 0.000
train | epoch   0 | Iter:    196/ 13000 | global iter:    196/ 13000 | loss: 3.3907 | ds_loss: 0.0000 | lr: 4.9976e-05 | scale: 65536.0000 | micro time: 1.843 | step time: 0.000
train | epoch   0 | Iter:    197/ 13000 | global iter:    197/ 13000 | loss: 1.9076 | ds_loss: 0.0000 | lr: 4.9976e-05 | scale: 65536.0000 | micro time: 1.828 | step time: 0.000
train | epoch   0 | Iter:    198/ 13000 | global iter:    198/ 13000 | loss: 2.4586 | ds_loss: 0.0000 | lr: 4.9976e-05 | scale: 65536.0000 | micro time: 1.887 | step time: 0.000
train | epoch   0 | Iter:    199/ 13000 | global iter:    199/ 13000 | loss: 2.8532 | ds_loss: 0.0000 | lr: 4.9976e-05 | scale: 65536.0000 | micro time: 1.767 | step time: 0.000
train | epoch   0 | Iter:    200/ 13000 | global iter:    200/ 13000 | loss: 3.0101 | ds_loss: 0.0000 | lr: 4.9975e-05 | scale: 65536.0000 | micro time: 1.843 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    200/ 13000 | global iter:    200/ 13000 | loss: 2.7264 | ds_loss: 0.0000 | lr: 4.9975e-05 | scale: 65536.0000 | micro time: 1.843 | step time: 1.816
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    201/ 13000 | global iter:    201/ 13000 | loss: 2.1722 | ds_loss: 0.0000 | lr: 4.9975e-05 | scale: 65536.0000 | micro time: 1.824 | step time: 0.000
train | epoch   0 | Iter:    202/ 13000 | global iter:    202/ 13000 | loss: 2.4687 | ds_loss: 0.0000 | lr: 4.9975e-05 | scale: 65536.0000 | micro time: 1.737 | step time: 0.000
train | epoch   0 | Iter:    203/ 13000 | global iter:    203/ 13000 | loss: 2.2279 | ds_loss: 0.0000 | lr: 4.9975e-05 | scale: 65536.0000 | micro time: 1.812 | step time: 0.000
train | epoch   0 | Iter:    204/ 13000 | global iter:    204/ 13000 | loss: 2.6359 | ds_loss: 0.0000 | lr: 4.9974e-05 | scale: 65536.0000 | micro time: 1.776 | step time: 0.000
train | epoch   0 | Iter:    205/ 13000 | global iter:    205/ 13000 | loss: 2.7988 | ds_loss: 0.0000 | lr: 4.9974e-05 | scale: 65536.0000 | micro time: 1.836 | step time: 0.000
train | epoch   0 | Iter:    206/ 13000 | global iter:    206/ 13000 | loss: 2.7187 | ds_loss: 0.0000 | lr: 4.9974e-05 | scale: 65536.0000 | micro time: 1.766 | step time: 0.000
train | epoch   0 | Iter:    207/ 13000 | global iter:    207/ 13000 | loss: 2.8083 | ds_loss: 0.0000 | lr: 4.9973e-05 | scale: 65536.0000 | micro time: 1.852 | step time: 0.000
train | epoch   0 | Iter:    208/ 13000 | global iter:    208/ 13000 | loss: 3.1940 | ds_loss: 0.0000 | lr: 4.9973e-05 | scale: 65536.0000 | micro time: 1.782 | step time: 0.000
train | epoch   0 | Iter:    209/ 13000 | global iter:    209/ 13000 | loss: 2.6575 | ds_loss: 0.0000 | lr: 4.9973e-05 | scale: 65536.0000 | micro time: 1.755 | step time: 0.000
train | epoch   0 | Iter:    210/ 13000 | global iter:    210/ 13000 | loss: 3.3915 | ds_loss: 0.0000 | lr: 4.9973e-05 | scale: 65536.0000 | micro time: 1.791 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    210/ 13000 | global iter:    210/ 13000 | loss: 2.7074 | ds_loss: 0.0000 | lr: 4.9973e-05 | scale: 65536.0000 | micro time: 1.791 | step time: 1.793
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    211/ 13000 | global iter:    211/ 13000 | loss: 2.9564 | ds_loss: 0.0000 | lr: 4.9972e-05 | scale: 65536.0000 | micro time: 1.809 | step time: 0.000
train | epoch   0 | Iter:    212/ 13000 | global iter:    212/ 13000 | loss: 3.5061 | ds_loss: 0.0000 | lr: 4.9972e-05 | scale: 65536.0000 | micro time: 1.767 | step time: 0.000
train | epoch   0 | Iter:    213/ 13000 | global iter:    213/ 13000 | loss: 3.2021 | ds_loss: 0.0000 | lr: 4.9972e-05 | scale: 65536.0000 | micro time: 1.695 | step time: 0.000
train | epoch   0 | Iter:    214/ 13000 | global iter:    214/ 13000 | loss: 3.2690 | ds_loss: 0.0000 | lr: 4.9971e-05 | scale: 65536.0000 | micro time: 1.751 | step time: 0.000
train | epoch   0 | Iter:    215/ 13000 | global iter:    215/ 13000 | loss: 2.8169 | ds_loss: 0.0000 | lr: 4.9971e-05 | scale: 65536.0000 | micro time: 1.755 | step time: 0.000
train | epoch   0 | Iter:    216/ 13000 | global iter:    216/ 13000 | loss: 2.9481 | ds_loss: 0.0000 | lr: 4.9971e-05 | scale: 65536.0000 | micro time: 1.860 | step time: 0.000
train | epoch   0 | Iter:    217/ 13000 | global iter:    217/ 13000 | loss: 3.0243 | ds_loss: 0.0000 | lr: 4.9971e-05 | scale: 65536.0000 | micro time: 1.779 | step time: 0.000
train | epoch   0 | Iter:    218/ 13000 | global iter:    218/ 13000 | loss: 3.0040 | ds_loss: 0.0000 | lr: 4.9970e-05 | scale: 65536.0000 | micro time: 1.803 | step time: 0.000
train | epoch   0 | Iter:    219/ 13000 | global iter:    219/ 13000 | loss: 2.8337 | ds_loss: 0.0000 | lr: 4.9970e-05 | scale: 65536.0000 | micro time: 1.818 | step time: 0.000
train | epoch   0 | Iter:    220/ 13000 | global iter:    220/ 13000 | loss: 2.2228 | ds_loss: 0.0000 | lr: 4.9970e-05 | scale: 65536.0000 | micro time: 1.778 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    220/ 13000 | global iter:    220/ 13000 | loss: 2.9783 | ds_loss: 0.0000 | lr: 4.9970e-05 | scale: 65536.0000 | micro time: 1.778 | step time: 1.782
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    221/ 13000 | global iter:    221/ 13000 | loss: 2.6916 | ds_loss: 0.0000 | lr: 4.9969e-05 | scale: 65536.0000 | micro time: 1.797 | step time: 0.000
train | epoch   0 | Iter:    222/ 13000 | global iter:    222/ 13000 | loss: 2.5989 | ds_loss: 0.0000 | lr: 4.9969e-05 | scale: 65536.0000 | micro time: 1.814 | step time: 0.000
train | epoch   0 | Iter:    223/ 13000 | global iter:    223/ 13000 | loss: 2.8721 | ds_loss: 0.0000 | lr: 4.9969e-05 | scale: 65536.0000 | micro time: 1.778 | step time: 0.000
train | epoch   0 | Iter:    224/ 13000 | global iter:    224/ 13000 | loss: 2.2843 | ds_loss: 0.0000 | lr: 4.9968e-05 | scale: 65536.0000 | micro time: 1.699 | step time: 0.000
train | epoch   0 | Iter:    225/ 13000 | global iter:    225/ 13000 | loss: 2.7322 | ds_loss: 0.0000 | lr: 4.9968e-05 | scale: 65536.0000 | micro time: 1.803 | step time: 0.000
train | epoch   0 | Iter:    226/ 13000 | global iter:    226/ 13000 | loss: 2.8011 | ds_loss: 0.0000 | lr: 4.9968e-05 | scale: 65536.0000 | micro time: 1.855 | step time: 0.000
train | epoch   0 | Iter:    227/ 13000 | global iter:    227/ 13000 | loss: 2.3863 | ds_loss: 0.0000 | lr: 4.9968e-05 | scale: 65536.0000 | micro time: 1.843 | step time: 0.000
train | epoch   0 | Iter:    228/ 13000 | global iter:    228/ 13000 | loss: 1.8875 | ds_loss: 0.0000 | lr: 4.9967e-05 | scale: 65536.0000 | micro time: 1.839 | step time: 0.000
train | epoch   0 | Iter:    229/ 13000 | global iter:    229/ 13000 | loss: 2.8854 | ds_loss: 0.0000 | lr: 4.9967e-05 | scale: 65536.0000 | micro time: 1.843 | step time: 0.000
train | epoch   0 | Iter:    230/ 13000 | global iter:    230/ 13000 | loss: 2.5391 | ds_loss: 0.0000 | lr: 4.9967e-05 | scale: 65536.0000 | micro time: 1.763 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    230/ 13000 | global iter:    230/ 13000 | loss: 2.5678 | ds_loss: 0.0000 | lr: 4.9967e-05 | scale: 65536.0000 | micro time: 1.763 | step time: 1.803
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    231/ 13000 | global iter:    231/ 13000 | loss: 3.1659 | ds_loss: 0.0000 | lr: 4.9966e-05 | scale: 65536.0000 | micro time: 1.791 | step time: 0.000
train | epoch   0 | Iter:    232/ 13000 | global iter:    232/ 13000 | loss: 2.6847 | ds_loss: 0.0000 | lr: 4.9966e-05 | scale: 65536.0000 | micro time: 1.710 | step time: 0.000
train | epoch   0 | Iter:    233/ 13000 | global iter:    233/ 13000 | loss: 2.9793 | ds_loss: 0.0000 | lr: 4.9966e-05 | scale: 65536.0000 | micro time: 1.843 | step time: 0.000
train | epoch   0 | Iter:    234/ 13000 | global iter:    234/ 13000 | loss: 2.6150 | ds_loss: 0.0000 | lr: 4.9965e-05 | scale: 65536.0000 | micro time: 1.743 | step time: 0.000
train | epoch   0 | Iter:    235/ 13000 | global iter:    235/ 13000 | loss: 2.7423 | ds_loss: 0.0000 | lr: 4.9965e-05 | scale: 65536.0000 | micro time: 1.799 | step time: 0.000
train | epoch   0 | Iter:    236/ 13000 | global iter:    236/ 13000 | loss: 2.8393 | ds_loss: 0.0000 | lr: 4.9965e-05 | scale: 65536.0000 | micro time: 1.723 | step time: 0.000
train | epoch   0 | Iter:    237/ 13000 | global iter:    237/ 13000 | loss: 2.9544 | ds_loss: 0.0000 | lr: 4.9964e-05 | scale: 65536.0000 | micro time: 1.719 | step time: 0.000
train | epoch   0 | Iter:    238/ 13000 | global iter:    238/ 13000 | loss: 2.6515 | ds_loss: 0.0000 | lr: 4.9964e-05 | scale: 65536.0000 | micro time: 1.838 | step time: 0.000
train | epoch   0 | Iter:    239/ 13000 | global iter:    239/ 13000 | loss: 2.5513 | ds_loss: 0.0000 | lr: 4.9964e-05 | scale: 65536.0000 | micro time: 1.714 | step time: 0.000
train | epoch   0 | Iter:    240/ 13000 | global iter:    240/ 13000 | loss: 2.9092 | ds_loss: 0.0000 | lr: 4.9963e-05 | scale: 65536.0000 | micro time: 1.725 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    240/ 13000 | global iter:    240/ 13000 | loss: 2.8093 | ds_loss: 0.0000 | lr: 4.9963e-05 | scale: 65536.0000 | micro time: 1.725 | step time: 1.760
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    241/ 13000 | global iter:    241/ 13000 | loss: 2.4912 | ds_loss: 0.0000 | lr: 4.9963e-05 | scale: 65536.0000 | micro time: 1.757 | step time: 0.000
train | epoch   0 | Iter:    242/ 13000 | global iter:    242/ 13000 | loss: 2.9144 | ds_loss: 0.0000 | lr: 4.9963e-05 | scale: 65536.0000 | micro time: 1.815 | step time: 0.000
train | epoch   0 | Iter:    243/ 13000 | global iter:    243/ 13000 | loss: 2.4562 | ds_loss: 0.0000 | lr: 4.9962e-05 | scale: 65536.0000 | micro time: 1.819 | step time: 0.000
train | epoch   0 | Iter:    244/ 13000 | global iter:    244/ 13000 | loss: 2.8079 | ds_loss: 0.0000 | lr: 4.9962e-05 | scale: 65536.0000 | micro time: 1.855 | step time: 0.000
train | epoch   0 | Iter:    245/ 13000 | global iter:    245/ 13000 | loss: 2.9373 | ds_loss: 0.0000 | lr: 4.9962e-05 | scale: 65536.0000 | micro time: 1.759 | step time: 0.000
train | epoch   0 | Iter:    246/ 13000 | global iter:    246/ 13000 | loss: 2.7562 | ds_loss: 0.0000 | lr: 4.9961e-05 | scale: 65536.0000 | micro time: 1.837 | step time: 0.000
train | epoch   0 | Iter:    247/ 13000 | global iter:    247/ 13000 | loss: 2.6413 | ds_loss: 0.0000 | lr: 4.9961e-05 | scale: 65536.0000 | micro time: 1.783 | step time: 0.000
train | epoch   0 | Iter:    248/ 13000 | global iter:    248/ 13000 | loss: 2.1816 | ds_loss: 0.0000 | lr: 4.9961e-05 | scale: 65536.0000 | micro time: 1.827 | step time: 0.000
train | epoch   0 | Iter:    249/ 13000 | global iter:    249/ 13000 | loss: 2.8992 | ds_loss: 0.0000 | lr: 4.9960e-05 | scale: 65536.0000 | micro time: 1.764 | step time: 0.000
train | epoch   0 | Iter:    250/ 13000 | global iter:    250/ 13000 | loss: 1.9797 | ds_loss: 0.0000 | lr: 4.9960e-05 | scale: 65536.0000 | micro time: 1.750 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    250/ 13000 | global iter:    250/ 13000 | loss: 2.6065 | ds_loss: 0.0000 | lr: 4.9960e-05 | scale: 65536.0000 | micro time: 1.750 | step time: 1.796
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    251/ 13000 | global iter:    251/ 13000 | loss: 2.6122 | ds_loss: 0.0000 | lr: 4.9960e-05 | scale: 65536.0000 | micro time: 1.796 | step time: 0.000
train | epoch   0 | Iter:    252/ 13000 | global iter:    252/ 13000 | loss: 2.8004 | ds_loss: 0.0000 | lr: 4.9959e-05 | scale: 65536.0000 | micro time: 1.793 | step time: 0.000
train | epoch   0 | Iter:    253/ 13000 | global iter:    253/ 13000 | loss: 2.9715 | ds_loss: 0.0000 | lr: 4.9959e-05 | scale: 65536.0000 | micro time: 1.872 | step time: 0.000
train | epoch   0 | Iter:    254/ 13000 | global iter:    254/ 13000 | loss: 2.8320 | ds_loss: 0.0000 | lr: 4.9959e-05 | scale: 65536.0000 | micro time: 1.825 | step time: 0.000
train | epoch   0 | Iter:    255/ 13000 | global iter:    255/ 13000 | loss: 1.9926 | ds_loss: 0.0000 | lr: 4.9958e-05 | scale: 65536.0000 | micro time: 1.845 | step time: 0.000
train | epoch   0 | Iter:    256/ 13000 | global iter:    256/ 13000 | loss: 3.0890 | ds_loss: 0.0000 | lr: 4.9958e-05 | scale: 65536.0000 | micro time: 1.738 | step time: 0.000
train | epoch   0 | Iter:    257/ 13000 | global iter:    257/ 13000 | loss: 2.5421 | ds_loss: 0.0000 | lr: 4.9958e-05 | scale: 65536.0000 | micro time: 1.810 | step time: 0.000
train | epoch   0 | Iter:    258/ 13000 | global iter:    258/ 13000 | loss: 2.6704 | ds_loss: 0.0000 | lr: 4.9957e-05 | scale: 65536.0000 | micro time: 1.865 | step time: 0.000
train | epoch   0 | Iter:    259/ 13000 | global iter:    259/ 13000 | loss: 2.7935 | ds_loss: 0.0000 | lr: 4.9957e-05 | scale: 65536.0000 | micro time: 1.803 | step time: 0.000
train | epoch   0 | Iter:    260/ 13000 | global iter:    260/ 13000 | loss: 3.1174 | ds_loss: 0.0000 | lr: 4.9957e-05 | scale: 65536.0000 | micro time: 1.819 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    260/ 13000 | global iter:    260/ 13000 | loss: 2.7421 | ds_loss: 0.0000 | lr: 4.9957e-05 | scale: 65536.0000 | micro time: 1.819 | step time: 1.817
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    261/ 13000 | global iter:    261/ 13000 | loss: 2.5315 | ds_loss: 0.0000 | lr: 4.9956e-05 | scale: 65536.0000 | micro time: 1.790 | step time: 0.000
train | epoch   0 | Iter:    262/ 13000 | global iter:    262/ 13000 | loss: 2.9424 | ds_loss: 0.0000 | lr: 4.9956e-05 | scale: 65536.0000 | micro time: 1.867 | step time: 0.000
train | epoch   0 | Iter:    263/ 13000 | global iter:    263/ 13000 | loss: 2.8167 | ds_loss: 0.0000 | lr: 4.9956e-05 | scale: 65536.0000 | micro time: 1.657 | step time: 0.000
train | epoch   0 | Iter:    264/ 13000 | global iter:    264/ 13000 | loss: 2.3566 | ds_loss: 0.0000 | lr: 4.9955e-05 | scale: 65536.0000 | micro time: 1.793 | step time: 0.000
train | epoch   0 | Iter:    265/ 13000 | global iter:    265/ 13000 | loss: 3.3136 | ds_loss: 0.0000 | lr: 4.9955e-05 | scale: 65536.0000 | micro time: 1.859 | step time: 0.000
train | epoch   0 | Iter:    266/ 13000 | global iter:    266/ 13000 | loss: 3.1524 | ds_loss: 0.0000 | lr: 4.9954e-05 | scale: 65536.0000 | micro time: 1.783 | step time: 0.000
train | epoch   0 | Iter:    267/ 13000 | global iter:    267/ 13000 | loss: 2.7809 | ds_loss: 0.0000 | lr: 4.9954e-05 | scale: 65536.0000 | micro time: 1.703 | step time: 0.000
train | epoch   0 | Iter:    268/ 13000 | global iter:    268/ 13000 | loss: 2.9352 | ds_loss: 0.0000 | lr: 4.9954e-05 | scale: 65536.0000 | micro time: 1.820 | step time: 0.000
train | epoch   0 | Iter:    269/ 13000 | global iter:    269/ 13000 | loss: 2.8784 | ds_loss: 0.0000 | lr: 4.9953e-05 | scale: 65536.0000 | micro time: 1.832 | step time: 0.000
train | epoch   0 | Iter:    270/ 13000 | global iter:    270/ 13000 | loss: 1.9862 | ds_loss: 0.0000 | lr: 4.9953e-05 | scale: 65536.0000 | micro time: 1.730 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    270/ 13000 | global iter:    270/ 13000 | loss: 2.7694 | ds_loss: 0.0000 | lr: 4.9953e-05 | scale: 65536.0000 | micro time: 1.730 | step time: 1.783
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    271/ 13000 | global iter:    271/ 13000 | loss: 2.8170 | ds_loss: 0.0000 | lr: 4.9953e-05 | scale: 65536.0000 | micro time: 1.783 | step time: 0.000
train | epoch   0 | Iter:    272/ 13000 | global iter:    272/ 13000 | loss: 3.0076 | ds_loss: 0.0000 | lr: 4.9952e-05 | scale: 65536.0000 | micro time: 1.782 | step time: 0.000
train | epoch   0 | Iter:    273/ 13000 | global iter:    273/ 13000 | loss: 3.0892 | ds_loss: 0.0000 | lr: 4.9952e-05 | scale: 65536.0000 | micro time: 1.772 | step time: 0.000
train | epoch   0 | Iter:    274/ 13000 | global iter:    274/ 13000 | loss: 2.3428 | ds_loss: 0.0000 | lr: 4.9952e-05 | scale: 65536.0000 | micro time: 1.690 | step time: 0.000
train | epoch   0 | Iter:    275/ 13000 | global iter:    275/ 13000 | loss: 2.3283 | ds_loss: 0.0000 | lr: 4.9951e-05 | scale: 65536.0000 | micro time: 1.827 | step time: 0.000
train | epoch   0 | Iter:    276/ 13000 | global iter:    276/ 13000 | loss: 3.0303 | ds_loss: 0.0000 | lr: 4.9951e-05 | scale: 65536.0000 | micro time: 1.823 | step time: 0.000
train | epoch   0 | Iter:    277/ 13000 | global iter:    277/ 13000 | loss: 2.0611 | ds_loss: 0.0000 | lr: 4.9950e-05 | scale: 65536.0000 | micro time: 1.818 | step time: 0.000
train | epoch   0 | Iter:    278/ 13000 | global iter:    278/ 13000 | loss: 2.9650 | ds_loss: 0.0000 | lr: 4.9950e-05 | scale: 65536.0000 | micro time: 1.828 | step time: 0.000
train | epoch   0 | Iter:    279/ 13000 | global iter:    279/ 13000 | loss: 2.8788 | ds_loss: 0.0000 | lr: 4.9950e-05 | scale: 65536.0000 | micro time: 1.861 | step time: 0.000
train | epoch   0 | Iter:    280/ 13000 | global iter:    280/ 13000 | loss: 2.1411 | ds_loss: 0.0000 | lr: 4.9949e-05 | scale: 65536.0000 | micro time: 1.865 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    280/ 13000 | global iter:    280/ 13000 | loss: 2.6661 | ds_loss: 0.0000 | lr: 4.9949e-05 | scale: 65536.0000 | micro time: 1.865 | step time: 1.805
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    281/ 13000 | global iter:    281/ 13000 | loss: 3.1397 | ds_loss: 0.0000 | lr: 4.9949e-05 | scale: 65536.0000 | micro time: 1.794 | step time: 0.000
train | epoch   0 | Iter:    282/ 13000 | global iter:    282/ 13000 | loss: 2.9698 | ds_loss: 0.0000 | lr: 4.9948e-05 | scale: 65536.0000 | micro time: 1.843 | step time: 0.000
train | epoch   0 | Iter:    283/ 13000 | global iter:    283/ 13000 | loss: 2.5329 | ds_loss: 0.0000 | lr: 4.9948e-05 | scale: 65536.0000 | micro time: 1.839 | step time: 0.000
train | epoch   0 | Iter:    284/ 13000 | global iter:    284/ 13000 | loss: 2.4251 | ds_loss: 0.0000 | lr: 4.9948e-05 | scale: 65536.0000 | micro time: 1.747 | step time: 0.000
train | epoch   0 | Iter:    285/ 13000 | global iter:    285/ 13000 | loss: 2.0650 | ds_loss: 0.0000 | lr: 4.9947e-05 | scale: 65536.0000 | micro time: 1.767 | step time: 0.000
train | epoch   0 | Iter:    286/ 13000 | global iter:    286/ 13000 | loss: 2.9824 | ds_loss: 0.0000 | lr: 4.9947e-05 | scale: 65536.0000 | micro time: 1.795 | step time: 0.000
train | epoch   0 | Iter:    287/ 13000 | global iter:    287/ 13000 | loss: 2.3443 | ds_loss: 0.0000 | lr: 4.9947e-05 | scale: 65536.0000 | micro time: 1.863 | step time: 0.000
train | epoch   0 | Iter:    288/ 13000 | global iter:    288/ 13000 | loss: 1.8293 | ds_loss: 0.0000 | lr: 4.9946e-05 | scale: 65536.0000 | micro time: 1.799 | step time: 0.000
train | epoch   0 | Iter:    289/ 13000 | global iter:    289/ 13000 | loss: 2.8090 | ds_loss: 0.0000 | lr: 4.9946e-05 | scale: 65536.0000 | micro time: 1.839 | step time: 0.000
train | epoch   0 | Iter:    290/ 13000 | global iter:    290/ 13000 | loss: 2.3082 | ds_loss: 0.0000 | lr: 4.9945e-05 | scale: 65536.0000 | micro time: 1.851 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    290/ 13000 | global iter:    290/ 13000 | loss: 2.5406 | ds_loss: 0.0000 | lr: 4.9945e-05 | scale: 65536.0000 | micro time: 1.851 | step time: 1.814
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    291/ 13000 | global iter:    291/ 13000 | loss: 2.0476 | ds_loss: 0.0000 | lr: 4.9945e-05 | scale: 65536.0000 | micro time: 1.819 | step time: 0.000
train | epoch   0 | Iter:    292/ 13000 | global iter:    292/ 13000 | loss: 3.0638 | ds_loss: 0.0000 | lr: 4.9945e-05 | scale: 65536.0000 | micro time: 1.855 | step time: 0.000
train | epoch   0 | Iter:    293/ 13000 | global iter:    293/ 13000 | loss: 2.6420 | ds_loss: 0.0000 | lr: 4.9944e-05 | scale: 65536.0000 | micro time: 1.856 | step time: 0.000
train | epoch   0 | Iter:    294/ 13000 | global iter:    294/ 13000 | loss: 2.1179 | ds_loss: 0.0000 | lr: 4.9944e-05 | scale: 65536.0000 | micro time: 1.834 | step time: 0.000
train | epoch   0 | Iter:    295/ 13000 | global iter:    295/ 13000 | loss: 2.9117 | ds_loss: 0.0000 | lr: 4.9943e-05 | scale: 65536.0000 | micro time: 1.783 | step time: 0.000
train | epoch   0 | Iter:    296/ 13000 | global iter:    296/ 13000 | loss: 2.6822 | ds_loss: 0.0000 | lr: 4.9943e-05 | scale: 65536.0000 | micro time: 1.837 | step time: 0.000
train | epoch   0 | Iter:    297/ 13000 | global iter:    297/ 13000 | loss: 3.0111 | ds_loss: 0.0000 | lr: 4.9942e-05 | scale: 65536.0000 | micro time: 1.817 | step time: 0.000
train | epoch   0 | Iter:    298/ 13000 | global iter:    298/ 13000 | loss: 2.0313 | ds_loss: 0.0000 | lr: 4.9942e-05 | scale: 65536.0000 | micro time: 1.753 | step time: 0.000
train | epoch   0 | Iter:    299/ 13000 | global iter:    299/ 13000 | loss: 3.0509 | ds_loss: 0.0000 | lr: 4.9942e-05 | scale: 65536.0000 | micro time: 1.833 | step time: 0.000
train | epoch   0 | Iter:    300/ 13000 | global iter:    300/ 13000 | loss: 2.7983 | ds_loss: 0.0000 | lr: 4.9941e-05 | scale: 65536.0000 | micro time: 1.807 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    300/ 13000 | global iter:    300/ 13000 | loss: 2.6357 | ds_loss: 0.0000 | lr: 4.9941e-05 | scale: 65536.0000 | micro time: 1.807 | step time: 1.819
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    301/ 13000 | global iter:    301/ 13000 | loss: 2.3400 | ds_loss: 0.0000 | lr: 4.9941e-05 | scale: 65536.0000 | micro time: 1.870 | step time: 0.000
train | epoch   0 | Iter:    302/ 13000 | global iter:    302/ 13000 | loss: 1.7883 | ds_loss: 0.0000 | lr: 4.9940e-05 | scale: 65536.0000 | micro time: 1.750 | step time: 0.000
train | epoch   0 | Iter:    303/ 13000 | global iter:    303/ 13000 | loss: 2.8279 | ds_loss: 0.0000 | lr: 4.9940e-05 | scale: 65536.0000 | micro time: 1.781 | step time: 0.000
train | epoch   0 | Iter:    304/ 13000 | global iter:    304/ 13000 | loss: 2.5166 | ds_loss: 0.0000 | lr: 4.9940e-05 | scale: 65536.0000 | micro time: 1.897 | step time: 0.000
train | epoch   0 | Iter:    305/ 13000 | global iter:    305/ 13000 | loss: 2.9129 | ds_loss: 0.0000 | lr: 4.9939e-05 | scale: 65536.0000 | micro time: 1.863 | step time: 0.000
train | epoch   0 | Iter:    306/ 13000 | global iter:    306/ 13000 | loss: 3.0118 | ds_loss: 0.0000 | lr: 4.9939e-05 | scale: 65536.0000 | micro time: 1.771 | step time: 0.000
train | epoch   0 | Iter:    307/ 13000 | global iter:    307/ 13000 | loss: 1.5524 | ds_loss: 0.0000 | lr: 4.9938e-05 | scale: 65536.0000 | micro time: 1.867 | step time: 0.000
train | epoch   0 | Iter:    308/ 13000 | global iter:    308/ 13000 | loss: 2.3663 | ds_loss: 0.0000 | lr: 4.9938e-05 | scale: 65536.0000 | micro time: 1.778 | step time: 0.000
train | epoch   0 | Iter:    309/ 13000 | global iter:    309/ 13000 | loss: 2.8708 | ds_loss: 0.0000 | lr: 4.9937e-05 | scale: 65536.0000 | micro time: 1.731 | step time: 0.000
train | epoch   0 | Iter:    310/ 13000 | global iter:    310/ 13000 | loss: 2.3642 | ds_loss: 0.0000 | lr: 4.9937e-05 | scale: 65536.0000 | micro time: 1.867 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    310/ 13000 | global iter:    310/ 13000 | loss: 2.4551 | ds_loss: 0.0000 | lr: 4.9937e-05 | scale: 65536.0000 | micro time: 1.867 | step time: 1.818
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    311/ 13000 | global iter:    311/ 13000 | loss: 3.0412 | ds_loss: 0.0000 | lr: 4.9937e-05 | scale: 65536.0000 | micro time: 1.762 | step time: 0.000
[2025-04-19 11:17:51,584] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
train | epoch   0 | Iter:    312/ 13000 | global iter:    312/ 13000 | loss: 3.7408 | ds_loss: 0.0000 | lr: 4.9937e-05 | scale: 32768.0000 | micro time: 1.520 | step time: 0.000
train | epoch   0 | Iter:    313/ 13000 | global iter:    313/ 13000 | loss: 2.8019 | ds_loss: 0.0000 | lr: 4.9936e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   0 | Iter:    314/ 13000 | global iter:    314/ 13000 | loss: 2.1657 | ds_loss: 0.0000 | lr: 4.9936e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   0 | Iter:    315/ 13000 | global iter:    315/ 13000 | loss: 2.6216 | ds_loss: 0.0000 | lr: 4.9935e-05 | scale: 32768.0000 | micro time: 1.747 | step time: 0.000
train | epoch   0 | Iter:    316/ 13000 | global iter:    316/ 13000 | loss: 2.9276 | ds_loss: 0.0000 | lr: 4.9935e-05 | scale: 32768.0000 | micro time: 1.808 | step time: 0.000
train | epoch   0 | Iter:    317/ 13000 | global iter:    317/ 13000 | loss: 2.9873 | ds_loss: 0.0000 | lr: 4.9934e-05 | scale: 32768.0000 | micro time: 1.746 | step time: 0.000
train | epoch   0 | Iter:    318/ 13000 | global iter:    318/ 13000 | loss: 2.7200 | ds_loss: 0.0000 | lr: 4.9934e-05 | scale: 32768.0000 | micro time: 1.751 | step time: 0.000
train | epoch   0 | Iter:    319/ 13000 | global iter:    319/ 13000 | loss: 2.5546 | ds_loss: 0.0000 | lr: 4.9934e-05 | scale: 32768.0000 | micro time: 1.870 | step time: 0.000
train | epoch   0 | Iter:    320/ 13000 | global iter:    320/ 13000 | loss: 2.9098 | ds_loss: 0.0000 | lr: 4.9933e-05 | scale: 32768.0000 | micro time: 1.788 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    320/ 13000 | global iter:    320/ 13000 | loss: 2.8471 | ds_loss: 0.0000 | lr: 4.9933e-05 | scale: 32768.0000 | micro time: 1.788 | step time: 1.763
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    321/ 13000 | global iter:    321/ 13000 | loss: 2.7947 | ds_loss: 0.0000 | lr: 4.9933e-05 | scale: 32768.0000 | micro time: 1.663 | step time: 0.000
train | epoch   0 | Iter:    322/ 13000 | global iter:    322/ 13000 | loss: 2.4681 | ds_loss: 0.0000 | lr: 4.9932e-05 | scale: 32768.0000 | micro time: 1.780 | step time: 0.000
train | epoch   0 | Iter:    323/ 13000 | global iter:    323/ 13000 | loss: 2.8419 | ds_loss: 0.0000 | lr: 4.9932e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   0 | Iter:    324/ 13000 | global iter:    324/ 13000 | loss: 3.1731 | ds_loss: 0.0000 | lr: 4.9931e-05 | scale: 32768.0000 | micro time: 1.821 | step time: 0.000
train | epoch   0 | Iter:    325/ 13000 | global iter:    325/ 13000 | loss: 2.6575 | ds_loss: 0.0000 | lr: 4.9931e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   0 | Iter:    326/ 13000 | global iter:    326/ 13000 | loss: 2.5787 | ds_loss: 0.0000 | lr: 4.9930e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   0 | Iter:    327/ 13000 | global iter:    327/ 13000 | loss: 3.7159 | ds_loss: 0.0000 | lr: 4.9930e-05 | scale: 32768.0000 | micro time: 1.879 | step time: 0.000
train | epoch   0 | Iter:    328/ 13000 | global iter:    328/ 13000 | loss: 3.3189 | ds_loss: 0.0000 | lr: 4.9930e-05 | scale: 32768.0000 | micro time: 1.679 | step time: 0.000
train | epoch   0 | Iter:    329/ 13000 | global iter:    329/ 13000 | loss: 2.6194 | ds_loss: 0.0000 | lr: 4.9929e-05 | scale: 32768.0000 | micro time: 1.841 | step time: 0.000
train | epoch   0 | Iter:    330/ 13000 | global iter:    330/ 13000 | loss: 2.4918 | ds_loss: 0.0000 | lr: 4.9929e-05 | scale: 32768.0000 | micro time: 1.801 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    330/ 13000 | global iter:    330/ 13000 | loss: 2.8660 | ds_loss: 0.0000 | lr: 4.9929e-05 | scale: 32768.0000 | micro time: 1.801 | step time: 1.788
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    331/ 13000 | global iter:    331/ 13000 | loss: 3.2940 | ds_loss: 0.0000 | lr: 4.9928e-05 | scale: 32768.0000 | micro time: 1.722 | step time: 0.000
train | epoch   0 | Iter:    332/ 13000 | global iter:    332/ 13000 | loss: 2.4031 | ds_loss: 0.0000 | lr: 4.9928e-05 | scale: 32768.0000 | micro time: 1.736 | step time: 0.000
train | epoch   0 | Iter:    333/ 13000 | global iter:    333/ 13000 | loss: 2.3129 | ds_loss: 0.0000 | lr: 4.9927e-05 | scale: 32768.0000 | micro time: 1.766 | step time: 0.000
train | epoch   0 | Iter:    334/ 13000 | global iter:    334/ 13000 | loss: 2.7898 | ds_loss: 0.0000 | lr: 4.9927e-05 | scale: 32768.0000 | micro time: 1.731 | step time: 0.000
train | epoch   0 | Iter:    335/ 13000 | global iter:    335/ 13000 | loss: 2.6713 | ds_loss: 0.0000 | lr: 4.9926e-05 | scale: 32768.0000 | micro time: 1.739 | step time: 0.000
train | epoch   0 | Iter:    336/ 13000 | global iter:    336/ 13000 | loss: 2.8931 | ds_loss: 0.0000 | lr: 4.9926e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   0 | Iter:    337/ 13000 | global iter:    337/ 13000 | loss: 2.1160 | ds_loss: 0.0000 | lr: 4.9925e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   0 | Iter:    338/ 13000 | global iter:    338/ 13000 | loss: 2.5094 | ds_loss: 0.0000 | lr: 4.9925e-05 | scale: 32768.0000 | micro time: 1.838 | step time: 0.000
train | epoch   0 | Iter:    339/ 13000 | global iter:    339/ 13000 | loss: 2.8723 | ds_loss: 0.0000 | lr: 4.9925e-05 | scale: 32768.0000 | micro time: 1.868 | step time: 0.000
train | epoch   0 | Iter:    340/ 13000 | global iter:    340/ 13000 | loss: 2.7983 | ds_loss: 0.0000 | lr: 4.9924e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    340/ 13000 | global iter:    340/ 13000 | loss: 2.6660 | ds_loss: 0.0000 | lr: 4.9924e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 1.778
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    341/ 13000 | global iter:    341/ 13000 | loss: 1.6824 | ds_loss: 0.0000 | lr: 4.9924e-05 | scale: 32768.0000 | micro time: 1.786 | step time: 0.000
train | epoch   0 | Iter:    342/ 13000 | global iter:    342/ 13000 | loss: 2.9025 | ds_loss: 0.0000 | lr: 4.9923e-05 | scale: 32768.0000 | micro time: 1.870 | step time: 0.000
train | epoch   0 | Iter:    343/ 13000 | global iter:    343/ 13000 | loss: 2.5664 | ds_loss: 0.0000 | lr: 4.9923e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   0 | Iter:    344/ 13000 | global iter:    344/ 13000 | loss: 3.0780 | ds_loss: 0.0000 | lr: 4.9922e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   0 | Iter:    345/ 13000 | global iter:    345/ 13000 | loss: 2.7542 | ds_loss: 0.0000 | lr: 4.9922e-05 | scale: 32768.0000 | micro time: 1.894 | step time: 0.000
train | epoch   0 | Iter:    346/ 13000 | global iter:    346/ 13000 | loss: 3.3306 | ds_loss: 0.0000 | lr: 4.9921e-05 | scale: 32768.0000 | micro time: 1.747 | step time: 0.000
train | epoch   0 | Iter:    347/ 13000 | global iter:    347/ 13000 | loss: 2.7891 | ds_loss: 0.0000 | lr: 4.9921e-05 | scale: 32768.0000 | micro time: 1.782 | step time: 0.000
train | epoch   0 | Iter:    348/ 13000 | global iter:    348/ 13000 | loss: 2.9631 | ds_loss: 0.0000 | lr: 4.9920e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   0 | Iter:    349/ 13000 | global iter:    349/ 13000 | loss: 2.8290 | ds_loss: 0.0000 | lr: 4.9920e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   0 | Iter:    350/ 13000 | global iter:    350/ 13000 | loss: 2.8909 | ds_loss: 0.0000 | lr: 4.9919e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    350/ 13000 | global iter:    350/ 13000 | loss: 2.7786 | ds_loss: 0.0000 | lr: 4.9919e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 1.820
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    351/ 13000 | global iter:    351/ 13000 | loss: 3.2878 | ds_loss: 0.0000 | lr: 4.9919e-05 | scale: 32768.0000 | micro time: 1.810 | step time: 0.000
train | epoch   0 | Iter:    352/ 13000 | global iter:    352/ 13000 | loss: 3.1304 | ds_loss: 0.0000 | lr: 4.9918e-05 | scale: 32768.0000 | micro time: 1.676 | step time: 0.000
train | epoch   0 | Iter:    353/ 13000 | global iter:    353/ 13000 | loss: 2.3269 | ds_loss: 0.0000 | lr: 4.9918e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   0 | Iter:    354/ 13000 | global iter:    354/ 13000 | loss: 3.0386 | ds_loss: 0.0000 | lr: 4.9917e-05 | scale: 32768.0000 | micro time: 1.830 | step time: 0.000
train | epoch   0 | Iter:    355/ 13000 | global iter:    355/ 13000 | loss: 2.9136 | ds_loss: 0.0000 | lr: 4.9917e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 0.000
train | epoch   0 | Iter:    356/ 13000 | global iter:    356/ 13000 | loss: 2.9107 | ds_loss: 0.0000 | lr: 4.9916e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   0 | Iter:    357/ 13000 | global iter:    357/ 13000 | loss: 1.9187 | ds_loss: 0.0000 | lr: 4.9916e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   0 | Iter:    358/ 13000 | global iter:    358/ 13000 | loss: 2.9598 | ds_loss: 0.0000 | lr: 4.9915e-05 | scale: 32768.0000 | micro time: 1.879 | step time: 0.000
train | epoch   0 | Iter:    359/ 13000 | global iter:    359/ 13000 | loss: 2.6961 | ds_loss: 0.0000 | lr: 4.9915e-05 | scale: 32768.0000 | micro time: 1.862 | step time: 0.000
train | epoch   0 | Iter:    360/ 13000 | global iter:    360/ 13000 | loss: 2.2326 | ds_loss: 0.0000 | lr: 4.9914e-05 | scale: 32768.0000 | micro time: 1.888 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    360/ 13000 | global iter:    360/ 13000 | loss: 2.7415 | ds_loss: 0.0000 | lr: 4.9914e-05 | scale: 32768.0000 | micro time: 1.888 | step time: 1.826
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    361/ 13000 | global iter:    361/ 13000 | loss: 2.9532 | ds_loss: 0.0000 | lr: 4.9914e-05 | scale: 32768.0000 | micro time: 1.888 | step time: 0.000
train | epoch   0 | Iter:    362/ 13000 | global iter:    362/ 13000 | loss: 2.9025 | ds_loss: 0.0000 | lr: 4.9913e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   0 | Iter:    363/ 13000 | global iter:    363/ 13000 | loss: 2.2551 | ds_loss: 0.0000 | lr: 4.9913e-05 | scale: 32768.0000 | micro time: 1.805 | step time: 0.000
train | epoch   0 | Iter:    364/ 13000 | global iter:    364/ 13000 | loss: 2.6746 | ds_loss: 0.0000 | lr: 4.9912e-05 | scale: 32768.0000 | micro time: 1.753 | step time: 0.000
train | epoch   0 | Iter:    365/ 13000 | global iter:    365/ 13000 | loss: 2.7481 | ds_loss: 0.0000 | lr: 4.9912e-05 | scale: 32768.0000 | micro time: 1.814 | step time: 0.000
train | epoch   0 | Iter:    366/ 13000 | global iter:    366/ 13000 | loss: 2.8934 | ds_loss: 0.0000 | lr: 4.9911e-05 | scale: 32768.0000 | micro time: 1.773 | step time: 0.000
train | epoch   0 | Iter:    367/ 13000 | global iter:    367/ 13000 | loss: 2.0830 | ds_loss: 0.0000 | lr: 4.9911e-05 | scale: 32768.0000 | micro time: 1.817 | step time: 0.000
train | epoch   0 | Iter:    368/ 13000 | global iter:    368/ 13000 | loss: 2.3659 | ds_loss: 0.0000 | lr: 4.9910e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   0 | Iter:    369/ 13000 | global iter:    369/ 13000 | loss: 2.1398 | ds_loss: 0.0000 | lr: 4.9910e-05 | scale: 32768.0000 | micro time: 1.884 | step time: 0.000
train | epoch   0 | Iter:    370/ 13000 | global iter:    370/ 13000 | loss: 2.4618 | ds_loss: 0.0000 | lr: 4.9909e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    370/ 13000 | global iter:    370/ 13000 | loss: 2.5478 | ds_loss: 0.0000 | lr: 4.9909e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 1.813
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    371/ 13000 | global iter:    371/ 13000 | loss: 3.2764 | ds_loss: 0.0000 | lr: 4.9909e-05 | scale: 32768.0000 | micro time: 1.825 | step time: 0.000
train | epoch   0 | Iter:    372/ 13000 | global iter:    372/ 13000 | loss: 2.1713 | ds_loss: 0.0000 | lr: 4.9908e-05 | scale: 32768.0000 | micro time: 1.829 | step time: 0.000
train | epoch   0 | Iter:    373/ 13000 | global iter:    373/ 13000 | loss: 2.4255 | ds_loss: 0.0000 | lr: 4.9908e-05 | scale: 32768.0000 | micro time: 1.701 | step time: 0.000
train | epoch   0 | Iter:    374/ 13000 | global iter:    374/ 13000 | loss: 2.7301 | ds_loss: 0.0000 | lr: 4.9907e-05 | scale: 32768.0000 | micro time: 1.814 | step time: 0.000
train | epoch   0 | Iter:    375/ 13000 | global iter:    375/ 13000 | loss: 2.8634 | ds_loss: 0.0000 | lr: 4.9907e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   0 | Iter:    376/ 13000 | global iter:    376/ 13000 | loss: 2.5167 | ds_loss: 0.0000 | lr: 4.9906e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   0 | Iter:    377/ 13000 | global iter:    377/ 13000 | loss: 2.4208 | ds_loss: 0.0000 | lr: 4.9906e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 0.000
train | epoch   0 | Iter:    378/ 13000 | global iter:    378/ 13000 | loss: 2.5430 | ds_loss: 0.0000 | lr: 4.9905e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   0 | Iter:    379/ 13000 | global iter:    379/ 13000 | loss: 3.1254 | ds_loss: 0.0000 | lr: 4.9905e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   0 | Iter:    380/ 13000 | global iter:    380/ 13000 | loss: 2.2071 | ds_loss: 0.0000 | lr: 4.9904e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    380/ 13000 | global iter:    380/ 13000 | loss: 2.6280 | ds_loss: 0.0000 | lr: 4.9904e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 1.818
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    381/ 13000 | global iter:    381/ 13000 | loss: 2.6650 | ds_loss: 0.0000 | lr: 4.9904e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   0 | Iter:    382/ 13000 | global iter:    382/ 13000 | loss: 2.4757 | ds_loss: 0.0000 | lr: 4.9903e-05 | scale: 32768.0000 | micro time: 1.727 | step time: 0.000
train | epoch   0 | Iter:    383/ 13000 | global iter:    383/ 13000 | loss: 3.0722 | ds_loss: 0.0000 | lr: 4.9902e-05 | scale: 32768.0000 | micro time: 1.842 | step time: 0.000
train | epoch   0 | Iter:    384/ 13000 | global iter:    384/ 13000 | loss: 2.5011 | ds_loss: 0.0000 | lr: 4.9902e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   0 | Iter:    385/ 13000 | global iter:    385/ 13000 | loss: 2.9331 | ds_loss: 0.0000 | lr: 4.9901e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   0 | Iter:    386/ 13000 | global iter:    386/ 13000 | loss: 3.3190 | ds_loss: 0.0000 | lr: 4.9901e-05 | scale: 32768.0000 | micro time: 1.834 | step time: 0.000
train | epoch   0 | Iter:    387/ 13000 | global iter:    387/ 13000 | loss: 2.3453 | ds_loss: 0.0000 | lr: 4.9900e-05 | scale: 32768.0000 | micro time: 1.780 | step time: 0.000
train | epoch   0 | Iter:    388/ 13000 | global iter:    388/ 13000 | loss: 1.8012 | ds_loss: 0.0000 | lr: 4.9900e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   0 | Iter:    389/ 13000 | global iter:    389/ 13000 | loss: 3.1808 | ds_loss: 0.0000 | lr: 4.9899e-05 | scale: 32768.0000 | micro time: 1.898 | step time: 0.000
train | epoch   0 | Iter:    390/ 13000 | global iter:    390/ 13000 | loss: 2.9860 | ds_loss: 0.0000 | lr: 4.9899e-05 | scale: 32768.0000 | micro time: 1.703 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    390/ 13000 | global iter:    390/ 13000 | loss: 2.7279 | ds_loss: 0.0000 | lr: 4.9899e-05 | scale: 32768.0000 | micro time: 1.703 | step time: 1.808
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    391/ 13000 | global iter:    391/ 13000 | loss: 2.4674 | ds_loss: 0.0000 | lr: 4.9898e-05 | scale: 32768.0000 | micro time: 1.850 | step time: 0.000
train | epoch   0 | Iter:    392/ 13000 | global iter:    392/ 13000 | loss: 2.9481 | ds_loss: 0.0000 | lr: 4.9898e-05 | scale: 32768.0000 | micro time: 1.939 | step time: 0.000
train | epoch   0 | Iter:    393/ 13000 | global iter:    393/ 13000 | loss: 2.7685 | ds_loss: 0.0000 | lr: 4.9897e-05 | scale: 32768.0000 | micro time: 1.732 | step time: 0.000
train | epoch   0 | Iter:    394/ 13000 | global iter:    394/ 13000 | loss: 2.2940 | ds_loss: 0.0000 | lr: 4.9897e-05 | scale: 32768.0000 | micro time: 1.786 | step time: 0.000
train | epoch   0 | Iter:    395/ 13000 | global iter:    395/ 13000 | loss: 2.8133 | ds_loss: 0.0000 | lr: 4.9896e-05 | scale: 32768.0000 | micro time: 1.837 | step time: 0.000
train | epoch   0 | Iter:    396/ 13000 | global iter:    396/ 13000 | loss: 2.3123 | ds_loss: 0.0000 | lr: 4.9895e-05 | scale: 32768.0000 | micro time: 1.801 | step time: 0.000
train | epoch   0 | Iter:    397/ 13000 | global iter:    397/ 13000 | loss: 3.1019 | ds_loss: 0.0000 | lr: 4.9895e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   0 | Iter:    398/ 13000 | global iter:    398/ 13000 | loss: 3.5685 | ds_loss: 0.0000 | lr: 4.9894e-05 | scale: 32768.0000 | micro time: 1.747 | step time: 0.000
train | epoch   0 | Iter:    399/ 13000 | global iter:    399/ 13000 | loss: 2.2952 | ds_loss: 0.0000 | lr: 4.9894e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   0 | Iter:    400/ 13000 | global iter:    400/ 13000 | loss: 3.1416 | ds_loss: 0.0000 | lr: 4.9893e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    400/ 13000 | global iter:    400/ 13000 | loss: 2.7711 | ds_loss: 0.0000 | lr: 4.9893e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 1.821
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    401/ 13000 | global iter:    401/ 13000 | loss: 2.8047 | ds_loss: 0.0000 | lr: 4.9893e-05 | scale: 32768.0000 | micro time: 1.846 | step time: 0.000
train | epoch   0 | Iter:    402/ 13000 | global iter:    402/ 13000 | loss: 2.9943 | ds_loss: 0.0000 | lr: 4.9892e-05 | scale: 32768.0000 | micro time: 1.770 | step time: 0.000
train | epoch   0 | Iter:    403/ 13000 | global iter:    403/ 13000 | loss: 3.1956 | ds_loss: 0.0000 | lr: 4.9892e-05 | scale: 32768.0000 | micro time: 1.740 | step time: 0.000
train | epoch   0 | Iter:    404/ 13000 | global iter:    404/ 13000 | loss: 2.1588 | ds_loss: 0.0000 | lr: 4.9891e-05 | scale: 32768.0000 | micro time: 1.723 | step time: 0.000
train | epoch   0 | Iter:    405/ 13000 | global iter:    405/ 13000 | loss: 2.6623 | ds_loss: 0.0000 | lr: 4.9890e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   0 | Iter:    406/ 13000 | global iter:    406/ 13000 | loss: 3.0632 | ds_loss: 0.0000 | lr: 4.9890e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   0 | Iter:    407/ 13000 | global iter:    407/ 13000 | loss: 3.2621 | ds_loss: 0.0000 | lr: 4.9889e-05 | scale: 32768.0000 | micro time: 1.834 | step time: 0.000
train | epoch   0 | Iter:    408/ 13000 | global iter:    408/ 13000 | loss: 2.6453 | ds_loss: 0.0000 | lr: 4.9889e-05 | scale: 32768.0000 | micro time: 1.711 | step time: 0.000
train | epoch   0 | Iter:    409/ 13000 | global iter:    409/ 13000 | loss: 2.9115 | ds_loss: 0.0000 | lr: 4.9888e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   0 | Iter:    410/ 13000 | global iter:    410/ 13000 | loss: 2.8666 | ds_loss: 0.0000 | lr: 4.9888e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    410/ 13000 | global iter:    410/ 13000 | loss: 2.8564 | ds_loss: 0.0000 | lr: 4.9888e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 1.794
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    411/ 13000 | global iter:    411/ 13000 | loss: 2.1347 | ds_loss: 0.0000 | lr: 4.9887e-05 | scale: 32768.0000 | micro time: 1.711 | step time: 0.000
train | epoch   0 | Iter:    412/ 13000 | global iter:    412/ 13000 | loss: 3.1740 | ds_loss: 0.0000 | lr: 4.9886e-05 | scale: 32768.0000 | micro time: 1.800 | step time: 0.000
train | epoch   0 | Iter:    413/ 13000 | global iter:    413/ 13000 | loss: 2.1833 | ds_loss: 0.0000 | lr: 4.9886e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   0 | Iter:    414/ 13000 | global iter:    414/ 13000 | loss: 2.2686 | ds_loss: 0.0000 | lr: 4.9885e-05 | scale: 32768.0000 | micro time: 1.739 | step time: 0.000
train | epoch   0 | Iter:    415/ 13000 | global iter:    415/ 13000 | loss: 2.2755 | ds_loss: 0.0000 | lr: 4.9885e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   0 | Iter:    416/ 13000 | global iter:    416/ 13000 | loss: 2.4767 | ds_loss: 0.0000 | lr: 4.9884e-05 | scale: 32768.0000 | micro time: 1.777 | step time: 0.000
train | epoch   0 | Iter:    417/ 13000 | global iter:    417/ 13000 | loss: 2.7369 | ds_loss: 0.0000 | lr: 4.9884e-05 | scale: 32768.0000 | micro time: 1.728 | step time: 0.000
train | epoch   0 | Iter:    418/ 13000 | global iter:    418/ 13000 | loss: 2.6402 | ds_loss: 0.0000 | lr: 4.9883e-05 | scale: 32768.0000 | micro time: 1.786 | step time: 0.000
train | epoch   0 | Iter:    419/ 13000 | global iter:    419/ 13000 | loss: 3.0474 | ds_loss: 0.0000 | lr: 4.9882e-05 | scale: 32768.0000 | micro time: 1.800 | step time: 0.000
train | epoch   0 | Iter:    420/ 13000 | global iter:    420/ 13000 | loss: 1.9384 | ds_loss: 0.0000 | lr: 4.9882e-05 | scale: 32768.0000 | micro time: 1.751 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    420/ 13000 | global iter:    420/ 13000 | loss: 2.4876 | ds_loss: 0.0000 | lr: 4.9882e-05 | scale: 32768.0000 | micro time: 1.751 | step time: 1.776
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    421/ 13000 | global iter:    421/ 13000 | loss: 2.3176 | ds_loss: 0.0000 | lr: 4.9881e-05 | scale: 32768.0000 | micro time: 1.813 | step time: 0.000
train | epoch   0 | Iter:    422/ 13000 | global iter:    422/ 13000 | loss: 2.9174 | ds_loss: 0.0000 | lr: 4.9881e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   0 | Iter:    423/ 13000 | global iter:    423/ 13000 | loss: 2.6379 | ds_loss: 0.0000 | lr: 4.9880e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   0 | Iter:    424/ 13000 | global iter:    424/ 13000 | loss: 2.3180 | ds_loss: 0.0000 | lr: 4.9879e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   0 | Iter:    425/ 13000 | global iter:    425/ 13000 | loss: 2.8849 | ds_loss: 0.0000 | lr: 4.9879e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   0 | Iter:    426/ 13000 | global iter:    426/ 13000 | loss: 2.6605 | ds_loss: 0.0000 | lr: 4.9878e-05 | scale: 32768.0000 | micro time: 1.747 | step time: 0.000
train | epoch   0 | Iter:    427/ 13000 | global iter:    427/ 13000 | loss: 2.9492 | ds_loss: 0.0000 | lr: 4.9878e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   0 | Iter:    428/ 13000 | global iter:    428/ 13000 | loss: 3.0274 | ds_loss: 0.0000 | lr: 4.9877e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   0 | Iter:    429/ 13000 | global iter:    429/ 13000 | loss: 2.0956 | ds_loss: 0.0000 | lr: 4.9876e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   0 | Iter:    430/ 13000 | global iter:    430/ 13000 | loss: 3.0956 | ds_loss: 0.0000 | lr: 4.9876e-05 | scale: 32768.0000 | micro time: 1.731 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    430/ 13000 | global iter:    430/ 13000 | loss: 2.6904 | ds_loss: 0.0000 | lr: 4.9876e-05 | scale: 32768.0000 | micro time: 1.731 | step time: 1.794
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    431/ 13000 | global iter:    431/ 13000 | loss: 2.5382 | ds_loss: 0.0000 | lr: 4.9875e-05 | scale: 32768.0000 | micro time: 1.866 | step time: 0.000
train | epoch   0 | Iter:    432/ 13000 | global iter:    432/ 13000 | loss: 2.8226 | ds_loss: 0.0000 | lr: 4.9875e-05 | scale: 32768.0000 | micro time: 1.778 | step time: 0.000
train | epoch   0 | Iter:    433/ 13000 | global iter:    433/ 13000 | loss: 2.7917 | ds_loss: 0.0000 | lr: 4.9874e-05 | scale: 32768.0000 | micro time: 1.812 | step time: 0.000
train | epoch   0 | Iter:    434/ 13000 | global iter:    434/ 13000 | loss: 2.6074 | ds_loss: 0.0000 | lr: 4.9873e-05 | scale: 32768.0000 | micro time: 1.719 | step time: 0.000
train | epoch   0 | Iter:    435/ 13000 | global iter:    435/ 13000 | loss: 2.2241 | ds_loss: 0.0000 | lr: 4.9873e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   0 | Iter:    436/ 13000 | global iter:    436/ 13000 | loss: 1.6313 | ds_loss: 0.0000 | lr: 4.9872e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   0 | Iter:    437/ 13000 | global iter:    437/ 13000 | loss: 2.7611 | ds_loss: 0.0000 | lr: 4.9872e-05 | scale: 32768.0000 | micro time: 1.700 | step time: 0.000
train | epoch   0 | Iter:    438/ 13000 | global iter:    438/ 13000 | loss: 2.5066 | ds_loss: 0.0000 | lr: 4.9871e-05 | scale: 32768.0000 | micro time: 1.825 | step time: 0.000
train | epoch   0 | Iter:    439/ 13000 | global iter:    439/ 13000 | loss: 1.5432 | ds_loss: 0.0000 | lr: 4.9870e-05 | scale: 32768.0000 | micro time: 1.795 | step time: 0.000
train | epoch   0 | Iter:    440/ 13000 | global iter:    440/ 13000 | loss: 2.1543 | ds_loss: 0.0000 | lr: 4.9870e-05 | scale: 32768.0000 | micro time: 1.669 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    440/ 13000 | global iter:    440/ 13000 | loss: 2.3581 | ds_loss: 0.0000 | lr: 4.9870e-05 | scale: 32768.0000 | micro time: 1.669 | step time: 1.777
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    441/ 13000 | global iter:    441/ 13000 | loss: 2.0363 | ds_loss: 0.0000 | lr: 4.9869e-05 | scale: 32768.0000 | micro time: 1.709 | step time: 0.000
train | epoch   0 | Iter:    442/ 13000 | global iter:    442/ 13000 | loss: 2.5501 | ds_loss: 0.0000 | lr: 4.9869e-05 | scale: 32768.0000 | micro time: 1.818 | step time: 0.000
train | epoch   0 | Iter:    443/ 13000 | global iter:    443/ 13000 | loss: 1.9464 | ds_loss: 0.0000 | lr: 4.9868e-05 | scale: 32768.0000 | micro time: 1.888 | step time: 0.000
train | epoch   0 | Iter:    444/ 13000 | global iter:    444/ 13000 | loss: 2.1589 | ds_loss: 0.0000 | lr: 4.9867e-05 | scale: 32768.0000 | micro time: 1.891 | step time: 0.000
train | epoch   0 | Iter:    445/ 13000 | global iter:    445/ 13000 | loss: 2.5199 | ds_loss: 0.0000 | lr: 4.9867e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   0 | Iter:    446/ 13000 | global iter:    446/ 13000 | loss: 2.5184 | ds_loss: 0.0000 | lr: 4.9866e-05 | scale: 32768.0000 | micro time: 1.679 | step time: 0.000
train | epoch   0 | Iter:    447/ 13000 | global iter:    447/ 13000 | loss: 2.7800 | ds_loss: 0.0000 | lr: 4.9865e-05 | scale: 32768.0000 | micro time: 1.666 | step time: 0.000
train | epoch   0 | Iter:    448/ 13000 | global iter:    448/ 13000 | loss: 2.9719 | ds_loss: 0.0000 | lr: 4.9865e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   0 | Iter:    449/ 13000 | global iter:    449/ 13000 | loss: 2.5013 | ds_loss: 0.0000 | lr: 4.9864e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
train | epoch   0 | Iter:    450/ 13000 | global iter:    450/ 13000 | loss: 2.9899 | ds_loss: 0.0000 | lr: 4.9864e-05 | scale: 32768.0000 | micro time: 1.784 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    450/ 13000 | global iter:    450/ 13000 | loss: 2.4973 | ds_loss: 0.0000 | lr: 4.9864e-05 | scale: 32768.0000 | micro time: 1.784 | step time: 1.785
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    451/ 13000 | global iter:    451/ 13000 | loss: 2.7617 | ds_loss: 0.0000 | lr: 4.9863e-05 | scale: 32768.0000 | micro time: 1.720 | step time: 0.000
[2025-04-19 11:22:03,512] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
train | epoch   0 | Iter:    452/ 13000 | global iter:    452/ 13000 | loss: 2.6790 | ds_loss: 0.0000 | lr: 4.9863e-05 | scale: 16384.0000 | micro time: 1.468 | step time: 0.000
train | epoch   0 | Iter:    453/ 13000 | global iter:    453/ 13000 | loss: 2.5184 | ds_loss: 0.0000 | lr: 4.9862e-05 | scale: 16384.0000 | micro time: 1.810 | step time: 0.000
train | epoch   0 | Iter:    454/ 13000 | global iter:    454/ 13000 | loss: 2.2208 | ds_loss: 0.0000 | lr: 4.9862e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   0 | Iter:    455/ 13000 | global iter:    455/ 13000 | loss: 3.2913 | ds_loss: 0.0000 | lr: 4.9861e-05 | scale: 16384.0000 | micro time: 1.786 | step time: 0.000
train | epoch   0 | Iter:    456/ 13000 | global iter:    456/ 13000 | loss: 2.3381 | ds_loss: 0.0000 | lr: 4.9860e-05 | scale: 16384.0000 | micro time: 1.687 | step time: 0.000
train | epoch   0 | Iter:    457/ 13000 | global iter:    457/ 13000 | loss: 3.1877 | ds_loss: 0.0000 | lr: 4.9860e-05 | scale: 16384.0000 | micro time: 1.729 | step time: 0.000
train | epoch   0 | Iter:    458/ 13000 | global iter:    458/ 13000 | loss: 2.4662 | ds_loss: 0.0000 | lr: 4.9859e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   0 | Iter:    459/ 13000 | global iter:    459/ 13000 | loss: 1.7705 | ds_loss: 0.0000 | lr: 4.9858e-05 | scale: 16384.0000 | micro time: 1.842 | step time: 0.000
train | epoch   0 | Iter:    460/ 13000 | global iter:    460/ 13000 | loss: 2.9286 | ds_loss: 0.0000 | lr: 4.9858e-05 | scale: 16384.0000 | micro time: 1.849 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    460/ 13000 | global iter:    460/ 13000 | loss: 2.6162 | ds_loss: 0.0000 | lr: 4.9858e-05 | scale: 16384.0000 | micro time: 1.849 | step time: 1.748
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    461/ 13000 | global iter:    461/ 13000 | loss: 3.0170 | ds_loss: 0.0000 | lr: 4.9857e-05 | scale: 16384.0000 | micro time: 1.842 | step time: 0.000
train | epoch   0 | Iter:    462/ 13000 | global iter:    462/ 13000 | loss: 2.2545 | ds_loss: 0.0000 | lr: 4.9857e-05 | scale: 16384.0000 | micro time: 1.766 | step time: 0.000
train | epoch   0 | Iter:    463/ 13000 | global iter:    463/ 13000 | loss: 2.4127 | ds_loss: 0.0000 | lr: 4.9856e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   0 | Iter:    464/ 13000 | global iter:    464/ 13000 | loss: 2.1080 | ds_loss: 0.0000 | lr: 4.9855e-05 | scale: 16384.0000 | micro time: 1.905 | step time: 0.000
train | epoch   0 | Iter:    465/ 13000 | global iter:    465/ 13000 | loss: 2.8192 | ds_loss: 0.0000 | lr: 4.9855e-05 | scale: 16384.0000 | micro time: 1.697 | step time: 0.000
train | epoch   0 | Iter:    466/ 13000 | global iter:    466/ 13000 | loss: 2.1583 | ds_loss: 0.0000 | lr: 4.9854e-05 | scale: 16384.0000 | micro time: 1.832 | step time: 0.000
train | epoch   0 | Iter:    467/ 13000 | global iter:    467/ 13000 | loss: 2.8866 | ds_loss: 0.0000 | lr: 4.9853e-05 | scale: 16384.0000 | micro time: 1.731 | step time: 0.000
train | epoch   0 | Iter:    468/ 13000 | global iter:    468/ 13000 | loss: 2.4982 | ds_loss: 0.0000 | lr: 4.9853e-05 | scale: 16384.0000 | micro time: 1.786 | step time: 0.000
train | epoch   0 | Iter:    469/ 13000 | global iter:    469/ 13000 | loss: 2.9009 | ds_loss: 0.0000 | lr: 4.9852e-05 | scale: 16384.0000 | micro time: 1.715 | step time: 0.000
train | epoch   0 | Iter:    470/ 13000 | global iter:    470/ 13000 | loss: 2.7395 | ds_loss: 0.0000 | lr: 4.9851e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    470/ 13000 | global iter:    470/ 13000 | loss: 2.5795 | ds_loss: 0.0000 | lr: 4.9851e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 1.794
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    471/ 13000 | global iter:    471/ 13000 | loss: 2.4657 | ds_loss: 0.0000 | lr: 4.9851e-05 | scale: 16384.0000 | micro time: 1.892 | step time: 0.000
train | epoch   0 | Iter:    472/ 13000 | global iter:    472/ 13000 | loss: 2.2066 | ds_loss: 0.0000 | lr: 4.9850e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   0 | Iter:    473/ 13000 | global iter:    473/ 13000 | loss: 1.7744 | ds_loss: 0.0000 | lr: 4.9849e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   0 | Iter:    474/ 13000 | global iter:    474/ 13000 | loss: 2.3807 | ds_loss: 0.0000 | lr: 4.9849e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   0 | Iter:    475/ 13000 | global iter:    475/ 13000 | loss: 3.2267 | ds_loss: 0.0000 | lr: 4.9848e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   0 | Iter:    476/ 13000 | global iter:    476/ 13000 | loss: 2.5657 | ds_loss: 0.0000 | lr: 4.9847e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   0 | Iter:    477/ 13000 | global iter:    477/ 13000 | loss: 2.7919 | ds_loss: 0.0000 | lr: 4.9847e-05 | scale: 16384.0000 | micro time: 1.792 | step time: 0.000
train | epoch   0 | Iter:    478/ 13000 | global iter:    478/ 13000 | loss: 2.8925 | ds_loss: 0.0000 | lr: 4.9846e-05 | scale: 16384.0000 | micro time: 1.738 | step time: 0.000
train | epoch   0 | Iter:    479/ 13000 | global iter:    479/ 13000 | loss: 2.6968 | ds_loss: 0.0000 | lr: 4.9845e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   0 | Iter:    480/ 13000 | global iter:    480/ 13000 | loss: 2.4769 | ds_loss: 0.0000 | lr: 4.9845e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    480/ 13000 | global iter:    480/ 13000 | loss: 2.5478 | ds_loss: 0.0000 | lr: 4.9845e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 1.805
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    481/ 13000 | global iter:    481/ 13000 | loss: 3.0659 | ds_loss: 0.0000 | lr: 4.9844e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   0 | Iter:    482/ 13000 | global iter:    482/ 13000 | loss: 3.0938 | ds_loss: 0.0000 | lr: 4.9843e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   0 | Iter:    483/ 13000 | global iter:    483/ 13000 | loss: 2.6149 | ds_loss: 0.0000 | lr: 4.9843e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   0 | Iter:    484/ 13000 | global iter:    484/ 13000 | loss: 2.5254 | ds_loss: 0.0000 | lr: 4.9842e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   0 | Iter:    485/ 13000 | global iter:    485/ 13000 | loss: 2.9794 | ds_loss: 0.0000 | lr: 4.9841e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   0 | Iter:    486/ 13000 | global iter:    486/ 13000 | loss: 3.0081 | ds_loss: 0.0000 | lr: 4.9841e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   0 | Iter:    487/ 13000 | global iter:    487/ 13000 | loss: 2.6260 | ds_loss: 0.0000 | lr: 4.9840e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   0 | Iter:    488/ 13000 | global iter:    488/ 13000 | loss: 2.3805 | ds_loss: 0.0000 | lr: 4.9839e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   0 | Iter:    489/ 13000 | global iter:    489/ 13000 | loss: 2.7809 | ds_loss: 0.0000 | lr: 4.9839e-05 | scale: 16384.0000 | micro time: 1.931 | step time: 0.000
train | epoch   0 | Iter:    490/ 13000 | global iter:    490/ 13000 | loss: 2.3049 | ds_loss: 0.0000 | lr: 4.9838e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    490/ 13000 | global iter:    490/ 13000 | loss: 2.7380 | ds_loss: 0.0000 | lr: 4.9838e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 1.821
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    491/ 13000 | global iter:    491/ 13000 | loss: 2.1261 | ds_loss: 0.0000 | lr: 4.9837e-05 | scale: 16384.0000 | micro time: 1.690 | step time: 0.000
train | epoch   0 | Iter:    492/ 13000 | global iter:    492/ 13000 | loss: 2.7726 | ds_loss: 0.0000 | lr: 4.9836e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   0 | Iter:    493/ 13000 | global iter:    493/ 13000 | loss: 2.0860 | ds_loss: 0.0000 | lr: 4.9836e-05 | scale: 16384.0000 | micro time: 1.759 | step time: 0.000
train | epoch   0 | Iter:    494/ 13000 | global iter:    494/ 13000 | loss: 2.4077 | ds_loss: 0.0000 | lr: 4.9835e-05 | scale: 16384.0000 | micro time: 1.891 | step time: 0.000
train | epoch   0 | Iter:    495/ 13000 | global iter:    495/ 13000 | loss: 2.8212 | ds_loss: 0.0000 | lr: 4.9834e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   0 | Iter:    496/ 13000 | global iter:    496/ 13000 | loss: 2.9435 | ds_loss: 0.0000 | lr: 4.9834e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   0 | Iter:    497/ 13000 | global iter:    497/ 13000 | loss: 2.9908 | ds_loss: 0.0000 | lr: 4.9833e-05 | scale: 16384.0000 | micro time: 1.782 | step time: 0.000
train | epoch   0 | Iter:    498/ 13000 | global iter:    498/ 13000 | loss: 2.3722 | ds_loss: 0.0000 | lr: 4.9832e-05 | scale: 16384.0000 | micro time: 1.786 | step time: 0.000
train | epoch   0 | Iter:    499/ 13000 | global iter:    499/ 13000 | loss: 2.4132 | ds_loss: 0.0000 | lr: 4.9832e-05 | scale: 16384.0000 | micro time: 1.862 | step time: 0.000
train | epoch   0 | Iter:    500/ 13000 | global iter:    500/ 13000 | loss: 2.9920 | ds_loss: 0.0000 | lr: 4.9831e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    500/ 13000 | global iter:    500/ 13000 | loss: 2.5925 | ds_loss: 0.0000 | lr: 4.9831e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 1.803
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    501/ 13000 | global iter:    501/ 13000 | loss: 2.0767 | ds_loss: 0.0000 | lr: 4.9830e-05 | scale: 16384.0000 | micro time: 1.829 | step time: 0.000
train | epoch   0 | Iter:    502/ 13000 | global iter:    502/ 13000 | loss: 2.3847 | ds_loss: 0.0000 | lr: 4.9830e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   0 | Iter:    503/ 13000 | global iter:    503/ 13000 | loss: 2.6785 | ds_loss: 0.0000 | lr: 4.9829e-05 | scale: 16384.0000 | micro time: 1.853 | step time: 0.000
train | epoch   0 | Iter:    504/ 13000 | global iter:    504/ 13000 | loss: 2.5520 | ds_loss: 0.0000 | lr: 4.9828e-05 | scale: 16384.0000 | micro time: 1.733 | step time: 0.000
train | epoch   0 | Iter:    505/ 13000 | global iter:    505/ 13000 | loss: 2.6938 | ds_loss: 0.0000 | lr: 4.9827e-05 | scale: 16384.0000 | micro time: 1.707 | step time: 0.000
train | epoch   0 | Iter:    506/ 13000 | global iter:    506/ 13000 | loss: 2.2815 | ds_loss: 0.0000 | lr: 4.9827e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   0 | Iter:    507/ 13000 | global iter:    507/ 13000 | loss: 2.7657 | ds_loss: 0.0000 | lr: 4.9826e-05 | scale: 16384.0000 | micro time: 1.743 | step time: 0.000
train | epoch   0 | Iter:    508/ 13000 | global iter:    508/ 13000 | loss: 2.5508 | ds_loss: 0.0000 | lr: 4.9825e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   0 | Iter:    509/ 13000 | global iter:    509/ 13000 | loss: 2.3188 | ds_loss: 0.0000 | lr: 4.9825e-05 | scale: 16384.0000 | micro time: 1.865 | step time: 0.000
train | epoch   0 | Iter:    510/ 13000 | global iter:    510/ 13000 | loss: 1.7886 | ds_loss: 0.0000 | lr: 4.9824e-05 | scale: 16384.0000 | micro time: 1.882 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    510/ 13000 | global iter:    510/ 13000 | loss: 2.4091 | ds_loss: 0.0000 | lr: 4.9824e-05 | scale: 16384.0000 | micro time: 1.882 | step time: 1.793
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    511/ 13000 | global iter:    511/ 13000 | loss: 2.4435 | ds_loss: 0.0000 | lr: 4.9823e-05 | scale: 16384.0000 | micro time: 1.727 | step time: 0.000
train | epoch   0 | Iter:    512/ 13000 | global iter:    512/ 13000 | loss: 2.6496 | ds_loss: 0.0000 | lr: 4.9822e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   0 | Iter:    513/ 13000 | global iter:    513/ 13000 | loss: 2.1010 | ds_loss: 0.0000 | lr: 4.9822e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   0 | Iter:    514/ 13000 | global iter:    514/ 13000 | loss: 2.6364 | ds_loss: 0.0000 | lr: 4.9821e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   0 | Iter:    515/ 13000 | global iter:    515/ 13000 | loss: 3.0887 | ds_loss: 0.0000 | lr: 4.9820e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   0 | Iter:    516/ 13000 | global iter:    516/ 13000 | loss: 2.9062 | ds_loss: 0.0000 | lr: 4.9820e-05 | scale: 16384.0000 | micro time: 1.769 | step time: 0.000
train | epoch   0 | Iter:    517/ 13000 | global iter:    517/ 13000 | loss: 2.1740 | ds_loss: 0.0000 | lr: 4.9819e-05 | scale: 16384.0000 | micro time: 1.765 | step time: 0.000
train | epoch   0 | Iter:    518/ 13000 | global iter:    518/ 13000 | loss: 3.0041 | ds_loss: 0.0000 | lr: 4.9818e-05 | scale: 16384.0000 | micro time: 1.756 | step time: 0.000
train | epoch   0 | Iter:    519/ 13000 | global iter:    519/ 13000 | loss: 2.2118 | ds_loss: 0.0000 | lr: 4.9817e-05 | scale: 16384.0000 | micro time: 1.718 | step time: 0.000
train | epoch   0 | Iter:    520/ 13000 | global iter:    520/ 13000 | loss: 2.9256 | ds_loss: 0.0000 | lr: 4.9817e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    520/ 13000 | global iter:    520/ 13000 | loss: 2.6141 | ds_loss: 0.0000 | lr: 4.9817e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 1.770
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    521/ 13000 | global iter:    521/ 13000 | loss: 2.6164 | ds_loss: 0.0000 | lr: 4.9816e-05 | scale: 16384.0000 | micro time: 1.789 | step time: 0.000
train | epoch   0 | Iter:    522/ 13000 | global iter:    522/ 13000 | loss: 2.9552 | ds_loss: 0.0000 | lr: 4.9815e-05 | scale: 16384.0000 | micro time: 1.899 | step time: 0.000
train | epoch   0 | Iter:    523/ 13000 | global iter:    523/ 13000 | loss: 2.8043 | ds_loss: 0.0000 | lr: 4.9814e-05 | scale: 16384.0000 | micro time: 1.766 | step time: 0.000
train | epoch   0 | Iter:    524/ 13000 | global iter:    524/ 13000 | loss: 3.0105 | ds_loss: 0.0000 | lr: 4.9814e-05 | scale: 16384.0000 | micro time: 1.864 | step time: 0.000
train | epoch   0 | Iter:    525/ 13000 | global iter:    525/ 13000 | loss: 2.5917 | ds_loss: 0.0000 | lr: 4.9813e-05 | scale: 16384.0000 | micro time: 1.887 | step time: 0.000
train | epoch   0 | Iter:    526/ 13000 | global iter:    526/ 13000 | loss: 2.0943 | ds_loss: 0.0000 | lr: 4.9812e-05 | scale: 16384.0000 | micro time: 1.907 | step time: 0.000
train | epoch   0 | Iter:    527/ 13000 | global iter:    527/ 13000 | loss: 3.0403 | ds_loss: 0.0000 | lr: 4.9811e-05 | scale: 16384.0000 | micro time: 1.728 | step time: 0.000
train | epoch   0 | Iter:    528/ 13000 | global iter:    528/ 13000 | loss: 2.1876 | ds_loss: 0.0000 | lr: 4.9811e-05 | scale: 16384.0000 | micro time: 1.826 | step time: 0.000
train | epoch   0 | Iter:    529/ 13000 | global iter:    529/ 13000 | loss: 2.9520 | ds_loss: 0.0000 | lr: 4.9810e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   0 | Iter:    530/ 13000 | global iter:    530/ 13000 | loss: 2.6327 | ds_loss: 0.0000 | lr: 4.9809e-05 | scale: 16384.0000 | micro time: 1.719 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    530/ 13000 | global iter:    530/ 13000 | loss: 2.6885 | ds_loss: 0.0000 | lr: 4.9809e-05 | scale: 16384.0000 | micro time: 1.719 | step time: 1.817
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    531/ 13000 | global iter:    531/ 13000 | loss: 2.6510 | ds_loss: 0.0000 | lr: 4.9809e-05 | scale: 16384.0000 | micro time: 1.774 | step time: 0.000
train | epoch   0 | Iter:    532/ 13000 | global iter:    532/ 13000 | loss: 2.4361 | ds_loss: 0.0000 | lr: 4.9808e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   0 | Iter:    533/ 13000 | global iter:    533/ 13000 | loss: 2.1012 | ds_loss: 0.0000 | lr: 4.9807e-05 | scale: 16384.0000 | micro time: 1.754 | step time: 0.000
train | epoch   0 | Iter:    534/ 13000 | global iter:    534/ 13000 | loss: 3.0355 | ds_loss: 0.0000 | lr: 4.9806e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   0 | Iter:    535/ 13000 | global iter:    535/ 13000 | loss: 2.2279 | ds_loss: 0.0000 | lr: 4.9806e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   0 | Iter:    536/ 13000 | global iter:    536/ 13000 | loss: 2.6869 | ds_loss: 0.0000 | lr: 4.9805e-05 | scale: 16384.0000 | micro time: 1.828 | step time: 0.000
train | epoch   0 | Iter:    537/ 13000 | global iter:    537/ 13000 | loss: 2.3160 | ds_loss: 0.0000 | lr: 4.9804e-05 | scale: 16384.0000 | micro time: 1.830 | step time: 0.000
train | epoch   0 | Iter:    538/ 13000 | global iter:    538/ 13000 | loss: 2.3796 | ds_loss: 0.0000 | lr: 4.9803e-05 | scale: 16384.0000 | micro time: 1.703 | step time: 0.000
train | epoch   0 | Iter:    539/ 13000 | global iter:    539/ 13000 | loss: 2.3972 | ds_loss: 0.0000 | lr: 4.9803e-05 | scale: 16384.0000 | micro time: 1.858 | step time: 0.000
train | epoch   0 | Iter:    540/ 13000 | global iter:    540/ 13000 | loss: 2.5216 | ds_loss: 0.0000 | lr: 4.9802e-05 | scale: 16384.0000 | micro time: 1.696 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    540/ 13000 | global iter:    540/ 13000 | loss: 2.4753 | ds_loss: 0.0000 | lr: 4.9802e-05 | scale: 16384.0000 | micro time: 1.696 | step time: 1.791
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    541/ 13000 | global iter:    541/ 13000 | loss: 1.5917 | ds_loss: 0.0000 | lr: 4.9801e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   0 | Iter:    542/ 13000 | global iter:    542/ 13000 | loss: 2.3508 | ds_loss: 0.0000 | lr: 4.9800e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 0.000
train | epoch   0 | Iter:    543/ 13000 | global iter:    543/ 13000 | loss: 2.9745 | ds_loss: 0.0000 | lr: 4.9799e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   0 | Iter:    544/ 13000 | global iter:    544/ 13000 | loss: 2.5052 | ds_loss: 0.0000 | lr: 4.9799e-05 | scale: 16384.0000 | micro time: 1.893 | step time: 0.000
train | epoch   0 | Iter:    545/ 13000 | global iter:    545/ 13000 | loss: 2.7649 | ds_loss: 0.0000 | lr: 4.9798e-05 | scale: 16384.0000 | micro time: 1.828 | step time: 0.000
train | epoch   0 | Iter:    546/ 13000 | global iter:    546/ 13000 | loss: 2.1207 | ds_loss: 0.0000 | lr: 4.9797e-05 | scale: 16384.0000 | micro time: 1.715 | step time: 0.000
train | epoch   0 | Iter:    547/ 13000 | global iter:    547/ 13000 | loss: 1.6888 | ds_loss: 0.0000 | lr: 4.9796e-05 | scale: 16384.0000 | micro time: 1.753 | step time: 0.000
train | epoch   0 | Iter:    548/ 13000 | global iter:    548/ 13000 | loss: 1.8490 | ds_loss: 0.0000 | lr: 4.9796e-05 | scale: 16384.0000 | micro time: 1.764 | step time: 0.000
train | epoch   0 | Iter:    549/ 13000 | global iter:    549/ 13000 | loss: 2.8095 | ds_loss: 0.0000 | lr: 4.9795e-05 | scale: 16384.0000 | micro time: 1.735 | step time: 0.000
train | epoch   0 | Iter:    550/ 13000 | global iter:    550/ 13000 | loss: 2.8117 | ds_loss: 0.0000 | lr: 4.9794e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    550/ 13000 | global iter:    550/ 13000 | loss: 2.3467 | ds_loss: 0.0000 | lr: 4.9794e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 1.784
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    551/ 13000 | global iter:    551/ 13000 | loss: 2.7279 | ds_loss: 0.0000 | lr: 4.9793e-05 | scale: 16384.0000 | micro time: 1.738 | step time: 0.000
train | epoch   0 | Iter:    552/ 13000 | global iter:    552/ 13000 | loss: 2.8019 | ds_loss: 0.0000 | lr: 4.9793e-05 | scale: 16384.0000 | micro time: 1.647 | step time: 0.000
train | epoch   0 | Iter:    553/ 13000 | global iter:    553/ 13000 | loss: 2.2804 | ds_loss: 0.0000 | lr: 4.9792e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   0 | Iter:    554/ 13000 | global iter:    554/ 13000 | loss: 2.5596 | ds_loss: 0.0000 | lr: 4.9791e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   0 | Iter:    555/ 13000 | global iter:    555/ 13000 | loss: 2.4424 | ds_loss: 0.0000 | lr: 4.9790e-05 | scale: 16384.0000 | micro time: 1.685 | step time: 0.000
train | epoch   0 | Iter:    556/ 13000 | global iter:    556/ 13000 | loss: 2.9158 | ds_loss: 0.0000 | lr: 4.9789e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   0 | Iter:    557/ 13000 | global iter:    557/ 13000 | loss: 2.5694 | ds_loss: 0.0000 | lr: 4.9789e-05 | scale: 16384.0000 | micro time: 1.813 | step time: 0.000
train | epoch   0 | Iter:    558/ 13000 | global iter:    558/ 13000 | loss: 2.7722 | ds_loss: 0.0000 | lr: 4.9788e-05 | scale: 16384.0000 | micro time: 1.849 | step time: 0.000
train | epoch   0 | Iter:    559/ 13000 | global iter:    559/ 13000 | loss: 2.8865 | ds_loss: 0.0000 | lr: 4.9787e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   0 | Iter:    560/ 13000 | global iter:    560/ 13000 | loss: 2.5533 | ds_loss: 0.0000 | lr: 4.9786e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    560/ 13000 | global iter:    560/ 13000 | loss: 2.6509 | ds_loss: 0.0000 | lr: 4.9786e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 1.776
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    561/ 13000 | global iter:    561/ 13000 | loss: 2.8655 | ds_loss: 0.0000 | lr: 4.9785e-05 | scale: 16384.0000 | micro time: 1.780 | step time: 0.000
train | epoch   0 | Iter:    562/ 13000 | global iter:    562/ 13000 | loss: 2.6965 | ds_loss: 0.0000 | lr: 4.9785e-05 | scale: 16384.0000 | micro time: 1.728 | step time: 0.000
train | epoch   0 | Iter:    563/ 13000 | global iter:    563/ 13000 | loss: 2.7975 | ds_loss: 0.0000 | lr: 4.9784e-05 | scale: 16384.0000 | micro time: 1.786 | step time: 0.000
train | epoch   0 | Iter:    564/ 13000 | global iter:    564/ 13000 | loss: 2.8263 | ds_loss: 0.0000 | lr: 4.9783e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   0 | Iter:    565/ 13000 | global iter:    565/ 13000 | loss: 2.8004 | ds_loss: 0.0000 | lr: 4.9782e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   0 | Iter:    566/ 13000 | global iter:    566/ 13000 | loss: 2.9068 | ds_loss: 0.0000 | lr: 4.9782e-05 | scale: 16384.0000 | micro time: 1.789 | step time: 0.000
train | epoch   0 | Iter:    567/ 13000 | global iter:    567/ 13000 | loss: 1.8021 | ds_loss: 0.0000 | lr: 4.9781e-05 | scale: 16384.0000 | micro time: 1.701 | step time: 0.000
train | epoch   0 | Iter:    568/ 13000 | global iter:    568/ 13000 | loss: 2.5187 | ds_loss: 0.0000 | lr: 4.9780e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   0 | Iter:    569/ 13000 | global iter:    569/ 13000 | loss: 2.4698 | ds_loss: 0.0000 | lr: 4.9779e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   0 | Iter:    570/ 13000 | global iter:    570/ 13000 | loss: 2.6617 | ds_loss: 0.0000 | lr: 4.9778e-05 | scale: 16384.0000 | micro time: 1.804 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    570/ 13000 | global iter:    570/ 13000 | loss: 2.6345 | ds_loss: 0.0000 | lr: 4.9778e-05 | scale: 16384.0000 | micro time: 1.804 | step time: 1.778
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    571/ 13000 | global iter:    571/ 13000 | loss: 2.4207 | ds_loss: 0.0000 | lr: 4.9778e-05 | scale: 16384.0000 | micro time: 1.782 | step time: 0.000
train | epoch   0 | Iter:    572/ 13000 | global iter:    572/ 13000 | loss: 2.2289 | ds_loss: 0.0000 | lr: 4.9777e-05 | scale: 16384.0000 | micro time: 1.715 | step time: 0.000
train | epoch   0 | Iter:    573/ 13000 | global iter:    573/ 13000 | loss: 2.8213 | ds_loss: 0.0000 | lr: 4.9776e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   0 | Iter:    574/ 13000 | global iter:    574/ 13000 | loss: 2.8697 | ds_loss: 0.0000 | lr: 4.9775e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   0 | Iter:    575/ 13000 | global iter:    575/ 13000 | loss: 2.6569 | ds_loss: 0.0000 | lr: 4.9774e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   0 | Iter:    576/ 13000 | global iter:    576/ 13000 | loss: 2.8327 | ds_loss: 0.0000 | lr: 4.9774e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   0 | Iter:    577/ 13000 | global iter:    577/ 13000 | loss: 1.8932 | ds_loss: 0.0000 | lr: 4.9773e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   0 | Iter:    578/ 13000 | global iter:    578/ 13000 | loss: 2.6022 | ds_loss: 0.0000 | lr: 4.9772e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   0 | Iter:    579/ 13000 | global iter:    579/ 13000 | loss: 2.4137 | ds_loss: 0.0000 | lr: 4.9771e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   0 | Iter:    580/ 13000 | global iter:    580/ 13000 | loss: 2.9029 | ds_loss: 0.0000 | lr: 4.9770e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    580/ 13000 | global iter:    580/ 13000 | loss: 2.5642 | ds_loss: 0.0000 | lr: 4.9770e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 1.790
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    581/ 13000 | global iter:    581/ 13000 | loss: 2.7944 | ds_loss: 0.0000 | lr: 4.9769e-05 | scale: 16384.0000 | micro time: 1.786 | step time: 0.000
train | epoch   0 | Iter:    582/ 13000 | global iter:    582/ 13000 | loss: 2.8791 | ds_loss: 0.0000 | lr: 4.9769e-05 | scale: 16384.0000 | micro time: 1.730 | step time: 0.000
train | epoch   0 | Iter:    583/ 13000 | global iter:    583/ 13000 | loss: 2.4091 | ds_loss: 0.0000 | lr: 4.9768e-05 | scale: 16384.0000 | micro time: 1.812 | step time: 0.000
train | epoch   0 | Iter:    584/ 13000 | global iter:    584/ 13000 | loss: 2.8920 | ds_loss: 0.0000 | lr: 4.9767e-05 | scale: 16384.0000 | micro time: 1.754 | step time: 0.000
train | epoch   0 | Iter:    585/ 13000 | global iter:    585/ 13000 | loss: 2.7741 | ds_loss: 0.0000 | lr: 4.9766e-05 | scale: 16384.0000 | micro time: 1.800 | step time: 0.000
train | epoch   0 | Iter:    586/ 13000 | global iter:    586/ 13000 | loss: 2.4497 | ds_loss: 0.0000 | lr: 4.9765e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   0 | Iter:    587/ 13000 | global iter:    587/ 13000 | loss: 2.9808 | ds_loss: 0.0000 | lr: 4.9764e-05 | scale: 16384.0000 | micro time: 1.777 | step time: 0.000
train | epoch   0 | Iter:    588/ 13000 | global iter:    588/ 13000 | loss: 2.1851 | ds_loss: 0.0000 | lr: 4.9764e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   0 | Iter:    589/ 13000 | global iter:    589/ 13000 | loss: 2.9692 | ds_loss: 0.0000 | lr: 4.9763e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   0 | Iter:    590/ 13000 | global iter:    590/ 13000 | loss: 2.2939 | ds_loss: 0.0000 | lr: 4.9762e-05 | scale: 16384.0000 | micro time: 1.699 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    590/ 13000 | global iter:    590/ 13000 | loss: 2.6627 | ds_loss: 0.0000 | lr: 4.9762e-05 | scale: 16384.0000 | micro time: 1.699 | step time: 1.769
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    591/ 13000 | global iter:    591/ 13000 | loss: 2.5142 | ds_loss: 0.0000 | lr: 4.9761e-05 | scale: 16384.0000 | micro time: 1.818 | step time: 0.000
train | epoch   0 | Iter:    592/ 13000 | global iter:    592/ 13000 | loss: 2.5201 | ds_loss: 0.0000 | lr: 4.9760e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   0 | Iter:    593/ 13000 | global iter:    593/ 13000 | loss: 2.6192 | ds_loss: 0.0000 | lr: 4.9760e-05 | scale: 16384.0000 | micro time: 1.864 | step time: 0.000
train | epoch   0 | Iter:    594/ 13000 | global iter:    594/ 13000 | loss: 3.0650 | ds_loss: 0.0000 | lr: 4.9759e-05 | scale: 16384.0000 | micro time: 1.753 | step time: 0.000
train | epoch   0 | Iter:    595/ 13000 | global iter:    595/ 13000 | loss: 2.6658 | ds_loss: 0.0000 | lr: 4.9758e-05 | scale: 16384.0000 | micro time: 1.813 | step time: 0.000
train | epoch   0 | Iter:    596/ 13000 | global iter:    596/ 13000 | loss: 2.2662 | ds_loss: 0.0000 | lr: 4.9757e-05 | scale: 16384.0000 | micro time: 1.725 | step time: 0.000
train | epoch   0 | Iter:    597/ 13000 | global iter:    597/ 13000 | loss: 2.3148 | ds_loss: 0.0000 | lr: 4.9756e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   0 | Iter:    598/ 13000 | global iter:    598/ 13000 | loss: 2.2259 | ds_loss: 0.0000 | lr: 4.9755e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   0 | Iter:    599/ 13000 | global iter:    599/ 13000 | loss: 2.4752 | ds_loss: 0.0000 | lr: 4.9754e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   0 | Iter:    600/ 13000 | global iter:    600/ 13000 | loss: 2.7114 | ds_loss: 0.0000 | lr: 4.9754e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    600/ 13000 | global iter:    600/ 13000 | loss: 2.5378 | ds_loss: 0.0000 | lr: 4.9754e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 1.800
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    601/ 13000 | global iter:    601/ 13000 | loss: 2.5462 | ds_loss: 0.0000 | lr: 4.9753e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 0.000
train | epoch   0 | Iter:    602/ 13000 | global iter:    602/ 13000 | loss: 2.7344 | ds_loss: 0.0000 | lr: 4.9752e-05 | scale: 16384.0000 | micro time: 1.707 | step time: 0.000
train | epoch   0 | Iter:    603/ 13000 | global iter:    603/ 13000 | loss: 2.6558 | ds_loss: 0.0000 | lr: 4.9751e-05 | scale: 16384.0000 | micro time: 1.727 | step time: 0.000
train | epoch   0 | Iter:    604/ 13000 | global iter:    604/ 13000 | loss: 2.9912 | ds_loss: 0.0000 | lr: 4.9750e-05 | scale: 16384.0000 | micro time: 1.850 | step time: 0.000
train | epoch   0 | Iter:    605/ 13000 | global iter:    605/ 13000 | loss: 3.1723 | ds_loss: 0.0000 | lr: 4.9749e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   0 | Iter:    606/ 13000 | global iter:    606/ 13000 | loss: 2.7869 | ds_loss: 0.0000 | lr: 4.9749e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   0 | Iter:    607/ 13000 | global iter:    607/ 13000 | loss: 2.4647 | ds_loss: 0.0000 | lr: 4.9748e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   0 | Iter:    608/ 13000 | global iter:    608/ 13000 | loss: 3.0109 | ds_loss: 0.0000 | lr: 4.9747e-05 | scale: 16384.0000 | micro time: 1.850 | step time: 0.000
train | epoch   0 | Iter:    609/ 13000 | global iter:    609/ 13000 | loss: 2.9161 | ds_loss: 0.0000 | lr: 4.9746e-05 | scale: 16384.0000 | micro time: 1.828 | step time: 0.000
train | epoch   0 | Iter:    610/ 13000 | global iter:    610/ 13000 | loss: 2.3758 | ds_loss: 0.0000 | lr: 4.9745e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    610/ 13000 | global iter:    610/ 13000 | loss: 2.7654 | ds_loss: 0.0000 | lr: 4.9745e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 1.801
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    611/ 13000 | global iter:    611/ 13000 | loss: 2.5091 | ds_loss: 0.0000 | lr: 4.9744e-05 | scale: 16384.0000 | micro time: 1.797 | step time: 0.000
train | epoch   0 | Iter:    612/ 13000 | global iter:    612/ 13000 | loss: 2.6915 | ds_loss: 0.0000 | lr: 4.9743e-05 | scale: 16384.0000 | micro time: 1.734 | step time: 0.000
train | epoch   0 | Iter:    613/ 13000 | global iter:    613/ 13000 | loss: 2.5965 | ds_loss: 0.0000 | lr: 4.9743e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   0 | Iter:    614/ 13000 | global iter:    614/ 13000 | loss: 2.7877 | ds_loss: 0.0000 | lr: 4.9742e-05 | scale: 16384.0000 | micro time: 1.801 | step time: 0.000
train | epoch   0 | Iter:    615/ 13000 | global iter:    615/ 13000 | loss: 2.5981 | ds_loss: 0.0000 | lr: 4.9741e-05 | scale: 16384.0000 | micro time: 1.728 | step time: 0.000
train | epoch   0 | Iter:    616/ 13000 | global iter:    616/ 13000 | loss: 2.6996 | ds_loss: 0.0000 | lr: 4.9740e-05 | scale: 16384.0000 | micro time: 1.817 | step time: 0.000
train | epoch   0 | Iter:    617/ 13000 | global iter:    617/ 13000 | loss: 2.3382 | ds_loss: 0.0000 | lr: 4.9739e-05 | scale: 16384.0000 | micro time: 1.745 | step time: 0.000
train | epoch   0 | Iter:    618/ 13000 | global iter:    618/ 13000 | loss: 2.7389 | ds_loss: 0.0000 | lr: 4.9738e-05 | scale: 16384.0000 | micro time: 1.865 | step time: 0.000
train | epoch   0 | Iter:    619/ 13000 | global iter:    619/ 13000 | loss: 2.4910 | ds_loss: 0.0000 | lr: 4.9737e-05 | scale: 16384.0000 | micro time: 1.709 | step time: 0.000
train | epoch   0 | Iter:    620/ 13000 | global iter:    620/ 13000 | loss: 2.2347 | ds_loss: 0.0000 | lr: 4.9736e-05 | scale: 16384.0000 | micro time: 1.733 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    620/ 13000 | global iter:    620/ 13000 | loss: 2.5685 | ds_loss: 0.0000 | lr: 4.9736e-05 | scale: 16384.0000 | micro time: 1.733 | step time: 1.768
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    621/ 13000 | global iter:    621/ 13000 | loss: 2.6952 | ds_loss: 0.0000 | lr: 4.9736e-05 | scale: 16384.0000 | micro time: 1.858 | step time: 0.000
train | epoch   0 | Iter:    622/ 13000 | global iter:    622/ 13000 | loss: 2.2515 | ds_loss: 0.0000 | lr: 4.9735e-05 | scale: 16384.0000 | micro time: 1.828 | step time: 0.000
train | epoch   0 | Iter:    623/ 13000 | global iter:    623/ 13000 | loss: 2.7904 | ds_loss: 0.0000 | lr: 4.9734e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   0 | Iter:    624/ 13000 | global iter:    624/ 13000 | loss: 2.6933 | ds_loss: 0.0000 | lr: 4.9733e-05 | scale: 16384.0000 | micro time: 1.757 | step time: 0.000
train | epoch   0 | Iter:    625/ 13000 | global iter:    625/ 13000 | loss: 2.3968 | ds_loss: 0.0000 | lr: 4.9732e-05 | scale: 16384.0000 | micro time: 1.797 | step time: 0.000
train | epoch   0 | Iter:    626/ 13000 | global iter:    626/ 13000 | loss: 2.3487 | ds_loss: 0.0000 | lr: 4.9731e-05 | scale: 16384.0000 | micro time: 1.946 | step time: 0.000
train | epoch   0 | Iter:    627/ 13000 | global iter:    627/ 13000 | loss: 3.1176 | ds_loss: 0.0000 | lr: 4.9730e-05 | scale: 16384.0000 | micro time: 1.816 | step time: 0.000
train | epoch   0 | Iter:    628/ 13000 | global iter:    628/ 13000 | loss: 2.8864 | ds_loss: 0.0000 | lr: 4.9729e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   0 | Iter:    629/ 13000 | global iter:    629/ 13000 | loss: 2.3499 | ds_loss: 0.0000 | lr: 4.9729e-05 | scale: 16384.0000 | micro time: 1.854 | step time: 0.000
train | epoch   0 | Iter:    630/ 13000 | global iter:    630/ 13000 | loss: 2.4817 | ds_loss: 0.0000 | lr: 4.9728e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    630/ 13000 | global iter:    630/ 13000 | loss: 2.6011 | ds_loss: 0.0000 | lr: 4.9728e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 1.835
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    631/ 13000 | global iter:    631/ 13000 | loss: 2.7616 | ds_loss: 0.0000 | lr: 4.9727e-05 | scale: 16384.0000 | micro time: 1.930 | step time: 0.000
train | epoch   0 | Iter:    632/ 13000 | global iter:    632/ 13000 | loss: 3.0830 | ds_loss: 0.0000 | lr: 4.9726e-05 | scale: 16384.0000 | micro time: 1.737 | step time: 0.000
train | epoch   0 | Iter:    633/ 13000 | global iter:    633/ 13000 | loss: 2.7757 | ds_loss: 0.0000 | lr: 4.9725e-05 | scale: 16384.0000 | micro time: 1.816 | step time: 0.000
train | epoch   0 | Iter:    634/ 13000 | global iter:    634/ 13000 | loss: 2.5676 | ds_loss: 0.0000 | lr: 4.9724e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   0 | Iter:    635/ 13000 | global iter:    635/ 13000 | loss: 2.4252 | ds_loss: 0.0000 | lr: 4.9723e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   0 | Iter:    636/ 13000 | global iter:    636/ 13000 | loss: 2.7478 | ds_loss: 0.0000 | lr: 4.9722e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   0 | Iter:    637/ 13000 | global iter:    637/ 13000 | loss: 2.4468 | ds_loss: 0.0000 | lr: 4.9721e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   0 | Iter:    638/ 13000 | global iter:    638/ 13000 | loss: 2.9435 | ds_loss: 0.0000 | lr: 4.9720e-05 | scale: 16384.0000 | micro time: 1.905 | step time: 0.000
train | epoch   0 | Iter:    639/ 13000 | global iter:    639/ 13000 | loss: 2.9693 | ds_loss: 0.0000 | lr: 4.9720e-05 | scale: 16384.0000 | micro time: 1.849 | step time: 0.000
train | epoch   0 | Iter:    640/ 13000 | global iter:    640/ 13000 | loss: 2.2912 | ds_loss: 0.0000 | lr: 4.9719e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    640/ 13000 | global iter:    640/ 13000 | loss: 2.7012 | ds_loss: 0.0000 | lr: 4.9719e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 1.830
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    641/ 13000 | global iter:    641/ 13000 | loss: 2.5627 | ds_loss: 0.0000 | lr: 4.9718e-05 | scale: 16384.0000 | micro time: 1.765 | step time: 0.000
train | epoch   0 | Iter:    642/ 13000 | global iter:    642/ 13000 | loss: 2.6299 | ds_loss: 0.0000 | lr: 4.9717e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   0 | Iter:    643/ 13000 | global iter:    643/ 13000 | loss: 2.2216 | ds_loss: 0.0000 | lr: 4.9716e-05 | scale: 16384.0000 | micro time: 1.778 | step time: 0.000
train | epoch   0 | Iter:    644/ 13000 | global iter:    644/ 13000 | loss: 2.7076 | ds_loss: 0.0000 | lr: 4.9715e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   0 | Iter:    645/ 13000 | global iter:    645/ 13000 | loss: 2.8051 | ds_loss: 0.0000 | lr: 4.9714e-05 | scale: 16384.0000 | micro time: 1.768 | step time: 0.000
train | epoch   0 | Iter:    646/ 13000 | global iter:    646/ 13000 | loss: 2.7107 | ds_loss: 0.0000 | lr: 4.9713e-05 | scale: 16384.0000 | micro time: 1.830 | step time: 0.000
train | epoch   0 | Iter:    647/ 13000 | global iter:    647/ 13000 | loss: 2.5208 | ds_loss: 0.0000 | lr: 4.9712e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   0 | Iter:    648/ 13000 | global iter:    648/ 13000 | loss: 2.3549 | ds_loss: 0.0000 | lr: 4.9711e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   0 | Iter:    649/ 13000 | global iter:    649/ 13000 | loss: 2.3511 | ds_loss: 0.0000 | lr: 4.9710e-05 | scale: 16384.0000 | micro time: 1.813 | step time: 0.000
train | epoch   0 | Iter:    650/ 13000 | global iter:    650/ 13000 | loss: 2.2381 | ds_loss: 0.0000 | lr: 4.9710e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    650/ 13000 | global iter:    650/ 13000 | loss: 2.5103 | ds_loss: 0.0000 | lr: 4.9710e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 1.802
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    651/ 13000 | global iter:    651/ 13000 | loss: 2.2817 | ds_loss: 0.0000 | lr: 4.9709e-05 | scale: 16384.0000 | micro time: 1.809 | step time: 0.000
train | epoch   0 | Iter:    652/ 13000 | global iter:    652/ 13000 | loss: 3.0482 | ds_loss: 0.0000 | lr: 4.9708e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   0 | Iter:    653/ 13000 | global iter:    653/ 13000 | loss: 1.8832 | ds_loss: 0.0000 | lr: 4.9707e-05 | scale: 16384.0000 | micro time: 1.723 | step time: 0.000
train | epoch   0 | Iter:    654/ 13000 | global iter:    654/ 13000 | loss: 2.1783 | ds_loss: 0.0000 | lr: 4.9706e-05 | scale: 16384.0000 | micro time: 1.679 | step time: 0.000
train | epoch   0 | Iter:    655/ 13000 | global iter:    655/ 13000 | loss: 2.2420 | ds_loss: 0.0000 | lr: 4.9705e-05 | scale: 16384.0000 | micro time: 1.730 | step time: 0.000
train | epoch   0 | Iter:    656/ 13000 | global iter:    656/ 13000 | loss: 2.7525 | ds_loss: 0.0000 | lr: 4.9704e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   0 | Iter:    657/ 13000 | global iter:    657/ 13000 | loss: 2.7754 | ds_loss: 0.0000 | lr: 4.9703e-05 | scale: 16384.0000 | micro time: 1.838 | step time: 0.000
train | epoch   0 | Iter:    658/ 13000 | global iter:    658/ 13000 | loss: 1.8214 | ds_loss: 0.0000 | lr: 4.9702e-05 | scale: 16384.0000 | micro time: 1.743 | step time: 0.000
train | epoch   0 | Iter:    659/ 13000 | global iter:    659/ 13000 | loss: 2.7190 | ds_loss: 0.0000 | lr: 4.9701e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   0 | Iter:    660/ 13000 | global iter:    660/ 13000 | loss: 2.8565 | ds_loss: 0.0000 | lr: 4.9700e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    660/ 13000 | global iter:    660/ 13000 | loss: 2.4558 | ds_loss: 0.0000 | lr: 4.9700e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 1.774
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    661/ 13000 | global iter:    661/ 13000 | loss: 2.3373 | ds_loss: 0.0000 | lr: 4.9699e-05 | scale: 16384.0000 | micro time: 1.842 | step time: 0.000
train | epoch   0 | Iter:    662/ 13000 | global iter:    662/ 13000 | loss: 2.2685 | ds_loss: 0.0000 | lr: 4.9698e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   0 | Iter:    663/ 13000 | global iter:    663/ 13000 | loss: 3.0784 | ds_loss: 0.0000 | lr: 4.9698e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 0.000
train | epoch   0 | Iter:    664/ 13000 | global iter:    664/ 13000 | loss: 2.2577 | ds_loss: 0.0000 | lr: 4.9697e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 0.000
train | epoch   0 | Iter:    665/ 13000 | global iter:    665/ 13000 | loss: 2.2587 | ds_loss: 0.0000 | lr: 4.9696e-05 | scale: 16384.0000 | micro time: 1.870 | step time: 0.000
train | epoch   0 | Iter:    666/ 13000 | global iter:    666/ 13000 | loss: 2.6199 | ds_loss: 0.0000 | lr: 4.9695e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   0 | Iter:    667/ 13000 | global iter:    667/ 13000 | loss: 2.9115 | ds_loss: 0.0000 | lr: 4.9694e-05 | scale: 16384.0000 | micro time: 1.778 | step time: 0.000
train | epoch   0 | Iter:    668/ 13000 | global iter:    668/ 13000 | loss: 2.2658 | ds_loss: 0.0000 | lr: 4.9693e-05 | scale: 16384.0000 | micro time: 1.736 | step time: 0.000
train | epoch   0 | Iter:    669/ 13000 | global iter:    669/ 13000 | loss: 2.5992 | ds_loss: 0.0000 | lr: 4.9692e-05 | scale: 16384.0000 | micro time: 1.735 | step time: 0.000
train | epoch   0 | Iter:    670/ 13000 | global iter:    670/ 13000 | loss: 1.9509 | ds_loss: 0.0000 | lr: 4.9691e-05 | scale: 16384.0000 | micro time: 1.891 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    670/ 13000 | global iter:    670/ 13000 | loss: 2.4548 | ds_loss: 0.0000 | lr: 4.9691e-05 | scale: 16384.0000 | micro time: 1.891 | step time: 1.814
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    671/ 13000 | global iter:    671/ 13000 | loss: 1.8593 | ds_loss: 0.0000 | lr: 4.9690e-05 | scale: 16384.0000 | micro time: 1.802 | step time: 0.000
train | epoch   0 | Iter:    672/ 13000 | global iter:    672/ 13000 | loss: 2.4142 | ds_loss: 0.0000 | lr: 4.9689e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   0 | Iter:    673/ 13000 | global iter:    673/ 13000 | loss: 3.0907 | ds_loss: 0.0000 | lr: 4.9688e-05 | scale: 16384.0000 | micro time: 1.890 | step time: 0.000
train | epoch   0 | Iter:    674/ 13000 | global iter:    674/ 13000 | loss: 2.8594 | ds_loss: 0.0000 | lr: 4.9687e-05 | scale: 16384.0000 | micro time: 1.802 | step time: 0.000
train | epoch   0 | Iter:    675/ 13000 | global iter:    675/ 13000 | loss: 2.0239 | ds_loss: 0.0000 | lr: 4.9686e-05 | scale: 16384.0000 | micro time: 1.829 | step time: 0.000
train | epoch   0 | Iter:    676/ 13000 | global iter:    676/ 13000 | loss: 2.7684 | ds_loss: 0.0000 | lr: 4.9685e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   0 | Iter:    677/ 13000 | global iter:    677/ 13000 | loss: 2.9776 | ds_loss: 0.0000 | lr: 4.9684e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   0 | Iter:    678/ 13000 | global iter:    678/ 13000 | loss: 2.5156 | ds_loss: 0.0000 | lr: 4.9683e-05 | scale: 16384.0000 | micro time: 1.899 | step time: 0.000
train | epoch   0 | Iter:    679/ 13000 | global iter:    679/ 13000 | loss: 1.8745 | ds_loss: 0.0000 | lr: 4.9682e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   0 | Iter:    680/ 13000 | global iter:    680/ 13000 | loss: 2.9065 | ds_loss: 0.0000 | lr: 4.9681e-05 | scale: 16384.0000 | micro time: 1.878 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    680/ 13000 | global iter:    680/ 13000 | loss: 2.5290 | ds_loss: 0.0000 | lr: 4.9681e-05 | scale: 16384.0000 | micro time: 1.878 | step time: 1.846
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    681/ 13000 | global iter:    681/ 13000 | loss: 2.3239 | ds_loss: 0.0000 | lr: 4.9680e-05 | scale: 16384.0000 | micro time: 1.857 | step time: 0.000
train | epoch   0 | Iter:    682/ 13000 | global iter:    682/ 13000 | loss: 2.1537 | ds_loss: 0.0000 | lr: 4.9679e-05 | scale: 16384.0000 | micro time: 1.768 | step time: 0.000
train | epoch   0 | Iter:    683/ 13000 | global iter:    683/ 13000 | loss: 2.7030 | ds_loss: 0.0000 | lr: 4.9679e-05 | scale: 16384.0000 | micro time: 1.737 | step time: 0.000
train | epoch   0 | Iter:    684/ 13000 | global iter:    684/ 13000 | loss: 2.2901 | ds_loss: 0.0000 | lr: 4.9678e-05 | scale: 16384.0000 | micro time: 1.832 | step time: 0.000
train | epoch   0 | Iter:    685/ 13000 | global iter:    685/ 13000 | loss: 2.6038 | ds_loss: 0.0000 | lr: 4.9677e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   0 | Iter:    686/ 13000 | global iter:    686/ 13000 | loss: 2.7302 | ds_loss: 0.0000 | lr: 4.9676e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   0 | Iter:    687/ 13000 | global iter:    687/ 13000 | loss: 2.7581 | ds_loss: 0.0000 | lr: 4.9675e-05 | scale: 16384.0000 | micro time: 1.725 | step time: 0.000
train | epoch   0 | Iter:    688/ 13000 | global iter:    688/ 13000 | loss: 2.5489 | ds_loss: 0.0000 | lr: 4.9674e-05 | scale: 16384.0000 | micro time: 1.893 | step time: 0.000
train | epoch   0 | Iter:    689/ 13000 | global iter:    689/ 13000 | loss: 2.2295 | ds_loss: 0.0000 | lr: 4.9673e-05 | scale: 16384.0000 | micro time: 1.813 | step time: 0.000
train | epoch   0 | Iter:    690/ 13000 | global iter:    690/ 13000 | loss: 3.1417 | ds_loss: 0.0000 | lr: 4.9672e-05 | scale: 16384.0000 | micro time: 1.790 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    690/ 13000 | global iter:    690/ 13000 | loss: 2.5483 | ds_loss: 0.0000 | lr: 4.9672e-05 | scale: 16384.0000 | micro time: 1.790 | step time: 1.809
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    691/ 13000 | global iter:    691/ 13000 | loss: 2.6321 | ds_loss: 0.0000 | lr: 4.9671e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   0 | Iter:    692/ 13000 | global iter:    692/ 13000 | loss: 2.4934 | ds_loss: 0.0000 | lr: 4.9670e-05 | scale: 16384.0000 | micro time: 1.759 | step time: 0.000
train | epoch   0 | Iter:    693/ 13000 | global iter:    693/ 13000 | loss: 2.2921 | ds_loss: 0.0000 | lr: 4.9669e-05 | scale: 16384.0000 | micro time: 1.830 | step time: 0.000
train | epoch   0 | Iter:    694/ 13000 | global iter:    694/ 13000 | loss: 2.8190 | ds_loss: 0.0000 | lr: 4.9668e-05 | scale: 16384.0000 | micro time: 1.714 | step time: 0.000
train | epoch   0 | Iter:    695/ 13000 | global iter:    695/ 13000 | loss: 2.4837 | ds_loss: 0.0000 | lr: 4.9667e-05 | scale: 16384.0000 | micro time: 1.737 | step time: 0.000
train | epoch   0 | Iter:    696/ 13000 | global iter:    696/ 13000 | loss: 2.0244 | ds_loss: 0.0000 | lr: 4.9666e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   0 | Iter:    697/ 13000 | global iter:    697/ 13000 | loss: 2.5295 | ds_loss: 0.0000 | lr: 4.9665e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   0 | Iter:    698/ 13000 | global iter:    698/ 13000 | loss: 2.7812 | ds_loss: 0.0000 | lr: 4.9664e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   0 | Iter:    699/ 13000 | global iter:    699/ 13000 | loss: 2.4420 | ds_loss: 0.0000 | lr: 4.9663e-05 | scale: 16384.0000 | micro time: 1.812 | step time: 0.000
train | epoch   0 | Iter:    700/ 13000 | global iter:    700/ 13000 | loss: 2.7716 | ds_loss: 0.0000 | lr: 4.9662e-05 | scale: 16384.0000 | micro time: 1.770 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    700/ 13000 | global iter:    700/ 13000 | loss: 2.5269 | ds_loss: 0.0000 | lr: 4.9662e-05 | scale: 16384.0000 | micro time: 1.770 | step time: 1.784
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    701/ 13000 | global iter:    701/ 13000 | loss: 2.4314 | ds_loss: 0.0000 | lr: 4.9661e-05 | scale: 16384.0000 | micro time: 1.801 | step time: 0.000
train | epoch   0 | Iter:    702/ 13000 | global iter:    702/ 13000 | loss: 2.9493 | ds_loss: 0.0000 | lr: 4.9660e-05 | scale: 16384.0000 | micro time: 1.869 | step time: 0.000
train | epoch   0 | Iter:    703/ 13000 | global iter:    703/ 13000 | loss: 2.9597 | ds_loss: 0.0000 | lr: 4.9659e-05 | scale: 16384.0000 | micro time: 1.774 | step time: 0.000
train | epoch   0 | Iter:    704/ 13000 | global iter:    704/ 13000 | loss: 2.7628 | ds_loss: 0.0000 | lr: 4.9658e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   0 | Iter:    705/ 13000 | global iter:    705/ 13000 | loss: 1.7752 | ds_loss: 0.0000 | lr: 4.9657e-05 | scale: 16384.0000 | micro time: 1.808 | step time: 0.000
train | epoch   0 | Iter:    706/ 13000 | global iter:    706/ 13000 | loss: 2.6833 | ds_loss: 0.0000 | lr: 4.9656e-05 | scale: 16384.0000 | micro time: 1.781 | step time: 0.000
train | epoch   0 | Iter:    707/ 13000 | global iter:    707/ 13000 | loss: 2.5534 | ds_loss: 0.0000 | lr: 4.9655e-05 | scale: 16384.0000 | micro time: 1.844 | step time: 0.000
train | epoch   0 | Iter:    708/ 13000 | global iter:    708/ 13000 | loss: 2.2908 | ds_loss: 0.0000 | lr: 4.9654e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   0 | Iter:    709/ 13000 | global iter:    709/ 13000 | loss: 2.7323 | ds_loss: 0.0000 | lr: 4.9653e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   0 | Iter:    710/ 13000 | global iter:    710/ 13000 | loss: 2.3644 | ds_loss: 0.0000 | lr: 4.9652e-05 | scale: 16384.0000 | micro time: 1.757 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    710/ 13000 | global iter:    710/ 13000 | loss: 2.5503 | ds_loss: 0.0000 | lr: 4.9652e-05 | scale: 16384.0000 | micro time: 1.757 | step time: 1.818
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    711/ 13000 | global iter:    711/ 13000 | loss: 2.6747 | ds_loss: 0.0000 | lr: 4.9651e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 0.000
train | epoch   0 | Iter:    712/ 13000 | global iter:    712/ 13000 | loss: 1.8846 | ds_loss: 0.0000 | lr: 4.9650e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   0 | Iter:    713/ 13000 | global iter:    713/ 13000 | loss: 2.7701 | ds_loss: 0.0000 | lr: 4.9649e-05 | scale: 16384.0000 | micro time: 1.778 | step time: 0.000
train | epoch   0 | Iter:    714/ 13000 | global iter:    714/ 13000 | loss: 2.6920 | ds_loss: 0.0000 | lr: 4.9648e-05 | scale: 16384.0000 | micro time: 1.872 | step time: 0.000
train | epoch   0 | Iter:    715/ 13000 | global iter:    715/ 13000 | loss: 2.6516 | ds_loss: 0.0000 | lr: 4.9647e-05 | scale: 16384.0000 | micro time: 1.802 | step time: 0.000
train | epoch   0 | Iter:    716/ 13000 | global iter:    716/ 13000 | loss: 2.4620 | ds_loss: 0.0000 | lr: 4.9646e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   0 | Iter:    717/ 13000 | global iter:    717/ 13000 | loss: 1.8498 | ds_loss: 0.0000 | lr: 4.9645e-05 | scale: 16384.0000 | micro time: 1.820 | step time: 0.000
train | epoch   0 | Iter:    718/ 13000 | global iter:    718/ 13000 | loss: 2.4084 | ds_loss: 0.0000 | lr: 4.9644e-05 | scale: 16384.0000 | micro time: 1.822 | step time: 0.000
train | epoch   0 | Iter:    719/ 13000 | global iter:    719/ 13000 | loss: 2.3896 | ds_loss: 0.0000 | lr: 4.9643e-05 | scale: 16384.0000 | micro time: 1.833 | step time: 0.000
train | epoch   0 | Iter:    720/ 13000 | global iter:    720/ 13000 | loss: 2.7698 | ds_loss: 0.0000 | lr: 4.9642e-05 | scale: 16384.0000 | micro time: 1.820 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    720/ 13000 | global iter:    720/ 13000 | loss: 2.4553 | ds_loss: 0.0000 | lr: 4.9642e-05 | scale: 16384.0000 | micro time: 1.820 | step time: 1.823
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    721/ 13000 | global iter:    721/ 13000 | loss: 2.6811 | ds_loss: 0.0000 | lr: 4.9641e-05 | scale: 16384.0000 | micro time: 1.633 | step time: 0.000
train | epoch   0 | Iter:    722/ 13000 | global iter:    722/ 13000 | loss: 2.8271 | ds_loss: 0.0000 | lr: 4.9640e-05 | scale: 16384.0000 | micro time: 1.860 | step time: 0.000
train | epoch   0 | Iter:    723/ 13000 | global iter:    723/ 13000 | loss: 2.0344 | ds_loss: 0.0000 | lr: 4.9639e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 0.000
train | epoch   0 | Iter:    724/ 13000 | global iter:    724/ 13000 | loss: 2.6736 | ds_loss: 0.0000 | lr: 4.9638e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 0.000
train | epoch   0 | Iter:    725/ 13000 | global iter:    725/ 13000 | loss: 2.6176 | ds_loss: 0.0000 | lr: 4.9637e-05 | scale: 16384.0000 | micro time: 1.879 | step time: 0.000
train | epoch   0 | Iter:    726/ 13000 | global iter:    726/ 13000 | loss: 2.8158 | ds_loss: 0.0000 | lr: 4.9636e-05 | scale: 16384.0000 | micro time: 1.891 | step time: 0.000
train | epoch   0 | Iter:    727/ 13000 | global iter:    727/ 13000 | loss: 2.6373 | ds_loss: 0.0000 | lr: 4.9635e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   0 | Iter:    728/ 13000 | global iter:    728/ 13000 | loss: 2.5052 | ds_loss: 0.0000 | lr: 4.9634e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   0 | Iter:    729/ 13000 | global iter:    729/ 13000 | loss: 2.6660 | ds_loss: 0.0000 | lr: 4.9633e-05 | scale: 16384.0000 | micro time: 1.893 | step time: 0.000
train | epoch   0 | Iter:    730/ 13000 | global iter:    730/ 13000 | loss: 2.7216 | ds_loss: 0.0000 | lr: 4.9632e-05 | scale: 16384.0000 | micro time: 1.869 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    730/ 13000 | global iter:    730/ 13000 | loss: 2.6180 | ds_loss: 0.0000 | lr: 4.9632e-05 | scale: 16384.0000 | micro time: 1.869 | step time: 1.814
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    731/ 13000 | global iter:    731/ 13000 | loss: 2.6414 | ds_loss: 0.0000 | lr: 4.9631e-05 | scale: 16384.0000 | micro time: 1.733 | step time: 0.000
train | epoch   0 | Iter:    732/ 13000 | global iter:    732/ 13000 | loss: 2.5965 | ds_loss: 0.0000 | lr: 4.9630e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   0 | Iter:    733/ 13000 | global iter:    733/ 13000 | loss: 3.0665 | ds_loss: 0.0000 | lr: 4.9628e-05 | scale: 16384.0000 | micro time: 1.812 | step time: 0.000
train | epoch   0 | Iter:    734/ 13000 | global iter:    734/ 13000 | loss: 2.9928 | ds_loss: 0.0000 | lr: 4.9627e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   0 | Iter:    735/ 13000 | global iter:    735/ 13000 | loss: 2.5156 | ds_loss: 0.0000 | lr: 4.9626e-05 | scale: 16384.0000 | micro time: 1.880 | step time: 0.000
train | epoch   0 | Iter:    736/ 13000 | global iter:    736/ 13000 | loss: 2.9648 | ds_loss: 0.0000 | lr: 4.9625e-05 | scale: 16384.0000 | micro time: 1.862 | step time: 0.000
train | epoch   0 | Iter:    737/ 13000 | global iter:    737/ 13000 | loss: 2.2976 | ds_loss: 0.0000 | lr: 4.9624e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   0 | Iter:    738/ 13000 | global iter:    738/ 13000 | loss: 2.4035 | ds_loss: 0.0000 | lr: 4.9623e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   0 | Iter:    739/ 13000 | global iter:    739/ 13000 | loss: 1.1755 | ds_loss: 0.0000 | lr: 4.9622e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   0 | Iter:    740/ 13000 | global iter:    740/ 13000 | loss: 2.4408 | ds_loss: 0.0000 | lr: 4.9621e-05 | scale: 16384.0000 | micro time: 1.749 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    740/ 13000 | global iter:    740/ 13000 | loss: 2.5095 | ds_loss: 0.0000 | lr: 4.9621e-05 | scale: 16384.0000 | micro time: 1.749 | step time: 1.816
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    741/ 13000 | global iter:    741/ 13000 | loss: 2.6686 | ds_loss: 0.0000 | lr: 4.9620e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   0 | Iter:    742/ 13000 | global iter:    742/ 13000 | loss: 2.5178 | ds_loss: 0.0000 | lr: 4.9619e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   0 | Iter:    743/ 13000 | global iter:    743/ 13000 | loss: 2.7595 | ds_loss: 0.0000 | lr: 4.9618e-05 | scale: 16384.0000 | micro time: 1.719 | step time: 0.000
train | epoch   0 | Iter:    744/ 13000 | global iter:    744/ 13000 | loss: 2.8534 | ds_loss: 0.0000 | lr: 4.9617e-05 | scale: 16384.0000 | micro time: 1.759 | step time: 0.000
train | epoch   0 | Iter:    745/ 13000 | global iter:    745/ 13000 | loss: 2.6909 | ds_loss: 0.0000 | lr: 4.9616e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   0 | Iter:    746/ 13000 | global iter:    746/ 13000 | loss: 2.4401 | ds_loss: 0.0000 | lr: 4.9615e-05 | scale: 16384.0000 | micro time: 1.687 | step time: 0.000
train | epoch   0 | Iter:    747/ 13000 | global iter:    747/ 13000 | loss: 2.7371 | ds_loss: 0.0000 | lr: 4.9614e-05 | scale: 16384.0000 | micro time: 1.700 | step time: 0.000
train | epoch   0 | Iter:    748/ 13000 | global iter:    748/ 13000 | loss: 1.9984 | ds_loss: 0.0000 | lr: 4.9613e-05 | scale: 16384.0000 | micro time: 1.745 | step time: 0.000
train | epoch   0 | Iter:    749/ 13000 | global iter:    749/ 13000 | loss: 2.8546 | ds_loss: 0.0000 | lr: 4.9612e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   0 | Iter:    750/ 13000 | global iter:    750/ 13000 | loss: 2.2111 | ds_loss: 0.0000 | lr: 4.9611e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    750/ 13000 | global iter:    750/ 13000 | loss: 2.5732 | ds_loss: 0.0000 | lr: 4.9611e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 1.778
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    751/ 13000 | global iter:    751/ 13000 | loss: 2.6810 | ds_loss: 0.0000 | lr: 4.9610e-05 | scale: 16384.0000 | micro time: 1.698 | step time: 0.000
train | epoch   0 | Iter:    752/ 13000 | global iter:    752/ 13000 | loss: 2.7636 | ds_loss: 0.0000 | lr: 4.9609e-05 | scale: 16384.0000 | micro time: 1.881 | step time: 0.000
train | epoch   0 | Iter:    753/ 13000 | global iter:    753/ 13000 | loss: 2.5467 | ds_loss: 0.0000 | lr: 4.9607e-05 | scale: 16384.0000 | micro time: 1.849 | step time: 0.000
train | epoch   0 | Iter:    754/ 13000 | global iter:    754/ 13000 | loss: 2.3872 | ds_loss: 0.0000 | lr: 4.9606e-05 | scale: 16384.0000 | micro time: 1.857 | step time: 0.000
train | epoch   0 | Iter:    755/ 13000 | global iter:    755/ 13000 | loss: 2.3256 | ds_loss: 0.0000 | lr: 4.9605e-05 | scale: 16384.0000 | micro time: 1.817 | step time: 0.000
train | epoch   0 | Iter:    756/ 13000 | global iter:    756/ 13000 | loss: 2.2075 | ds_loss: 0.0000 | lr: 4.9604e-05 | scale: 16384.0000 | micro time: 1.801 | step time: 0.000
train | epoch   0 | Iter:    757/ 13000 | global iter:    757/ 13000 | loss: 1.9402 | ds_loss: 0.0000 | lr: 4.9603e-05 | scale: 16384.0000 | micro time: 1.741 | step time: 0.000
train | epoch   0 | Iter:    758/ 13000 | global iter:    758/ 13000 | loss: 3.1716 | ds_loss: 0.0000 | lr: 4.9602e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 0.000
train | epoch   0 | Iter:    759/ 13000 | global iter:    759/ 13000 | loss: 2.2904 | ds_loss: 0.0000 | lr: 4.9601e-05 | scale: 16384.0000 | micro time: 1.816 | step time: 0.000
train | epoch   0 | Iter:    760/ 13000 | global iter:    760/ 13000 | loss: 1.8888 | ds_loss: 0.0000 | lr: 4.9600e-05 | scale: 16384.0000 | micro time: 1.668 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    760/ 13000 | global iter:    760/ 13000 | loss: 2.4203 | ds_loss: 0.0000 | lr: 4.9600e-05 | scale: 16384.0000 | micro time: 1.668 | step time: 1.787
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    761/ 13000 | global iter:    761/ 13000 | loss: 1.9689 | ds_loss: 0.0000 | lr: 4.9599e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 0.000
train | epoch   0 | Iter:    762/ 13000 | global iter:    762/ 13000 | loss: 2.2812 | ds_loss: 0.0000 | lr: 4.9598e-05 | scale: 16384.0000 | micro time: 1.879 | step time: 0.000
train | epoch   0 | Iter:    763/ 13000 | global iter:    763/ 13000 | loss: 2.4879 | ds_loss: 0.0000 | lr: 4.9597e-05 | scale: 16384.0000 | micro time: 1.776 | step time: 0.000
train | epoch   0 | Iter:    764/ 13000 | global iter:    764/ 13000 | loss: 2.5209 | ds_loss: 0.0000 | lr: 4.9596e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   0 | Iter:    765/ 13000 | global iter:    765/ 13000 | loss: 2.7109 | ds_loss: 0.0000 | lr: 4.9595e-05 | scale: 16384.0000 | micro time: 1.723 | step time: 0.000
train | epoch   0 | Iter:    766/ 13000 | global iter:    766/ 13000 | loss: 2.2954 | ds_loss: 0.0000 | lr: 4.9593e-05 | scale: 16384.0000 | micro time: 1.798 | step time: 0.000
train | epoch   0 | Iter:    767/ 13000 | global iter:    767/ 13000 | loss: 2.8487 | ds_loss: 0.0000 | lr: 4.9592e-05 | scale: 16384.0000 | micro time: 1.714 | step time: 0.000
train | epoch   0 | Iter:    768/ 13000 | global iter:    768/ 13000 | loss: 2.3619 | ds_loss: 0.0000 | lr: 4.9591e-05 | scale: 16384.0000 | micro time: 1.765 | step time: 0.000
train | epoch   0 | Iter:    769/ 13000 | global iter:    769/ 13000 | loss: 2.5961 | ds_loss: 0.0000 | lr: 4.9590e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   0 | Iter:    770/ 13000 | global iter:    770/ 13000 | loss: 2.8988 | ds_loss: 0.0000 | lr: 4.9589e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    770/ 13000 | global iter:    770/ 13000 | loss: 2.4971 | ds_loss: 0.0000 | lr: 4.9589e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 1.794
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    771/ 13000 | global iter:    771/ 13000 | loss: 2.6064 | ds_loss: 0.0000 | lr: 4.9588e-05 | scale: 16384.0000 | micro time: 1.770 | step time: 0.000
train | epoch   0 | Iter:    772/ 13000 | global iter:    772/ 13000 | loss: 2.7809 | ds_loss: 0.0000 | lr: 4.9587e-05 | scale: 16384.0000 | micro time: 1.842 | step time: 0.000
train | epoch   0 | Iter:    773/ 13000 | global iter:    773/ 13000 | loss: 2.8495 | ds_loss: 0.0000 | lr: 4.9586e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   0 | Iter:    774/ 13000 | global iter:    774/ 13000 | loss: 2.3408 | ds_loss: 0.0000 | lr: 4.9585e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   0 | Iter:    775/ 13000 | global iter:    775/ 13000 | loss: 2.8273 | ds_loss: 0.0000 | lr: 4.9584e-05 | scale: 16384.0000 | micro time: 1.850 | step time: 0.000
train | epoch   0 | Iter:    776/ 13000 | global iter:    776/ 13000 | loss: 2.3483 | ds_loss: 0.0000 | lr: 4.9583e-05 | scale: 16384.0000 | micro time: 1.816 | step time: 0.000
train | epoch   0 | Iter:    777/ 13000 | global iter:    777/ 13000 | loss: 2.4689 | ds_loss: 0.0000 | lr: 4.9581e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 0.000
train | epoch   0 | Iter:    778/ 13000 | global iter:    778/ 13000 | loss: 2.5972 | ds_loss: 0.0000 | lr: 4.9580e-05 | scale: 16384.0000 | micro time: 1.903 | step time: 0.000
train | epoch   0 | Iter:    779/ 13000 | global iter:    779/ 13000 | loss: 2.6204 | ds_loss: 0.0000 | lr: 4.9579e-05 | scale: 16384.0000 | micro time: 1.719 | step time: 0.000
train | epoch   0 | Iter:    780/ 13000 | global iter:    780/ 13000 | loss: 2.2306 | ds_loss: 0.0000 | lr: 4.9578e-05 | scale: 16384.0000 | micro time: 1.814 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    780/ 13000 | global iter:    780/ 13000 | loss: 2.5670 | ds_loss: 0.0000 | lr: 4.9578e-05 | scale: 16384.0000 | micro time: 1.814 | step time: 1.815
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    781/ 13000 | global iter:    781/ 13000 | loss: 2.8482 | ds_loss: 0.0000 | lr: 4.9577e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   0 | Iter:    782/ 13000 | global iter:    782/ 13000 | loss: 2.8358 | ds_loss: 0.0000 | lr: 4.9576e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   0 | Iter:    783/ 13000 | global iter:    783/ 13000 | loss: 2.8836 | ds_loss: 0.0000 | lr: 4.9575e-05 | scale: 16384.0000 | micro time: 1.804 | step time: 0.000
train | epoch   0 | Iter:    784/ 13000 | global iter:    784/ 13000 | loss: 2.5808 | ds_loss: 0.0000 | lr: 4.9574e-05 | scale: 16384.0000 | micro time: 1.782 | step time: 0.000
train | epoch   0 | Iter:    785/ 13000 | global iter:    785/ 13000 | loss: 2.5059 | ds_loss: 0.0000 | lr: 4.9573e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   0 | Iter:    786/ 13000 | global iter:    786/ 13000 | loss: 3.0062 | ds_loss: 0.0000 | lr: 4.9572e-05 | scale: 16384.0000 | micro time: 1.709 | step time: 0.000
train | epoch   0 | Iter:    787/ 13000 | global iter:    787/ 13000 | loss: 2.6242 | ds_loss: 0.0000 | lr: 4.9570e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   0 | Iter:    788/ 13000 | global iter:    788/ 13000 | loss: 2.3700 | ds_loss: 0.0000 | lr: 4.9569e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   0 | Iter:    789/ 13000 | global iter:    789/ 13000 | loss: 2.3506 | ds_loss: 0.0000 | lr: 4.9568e-05 | scale: 16384.0000 | micro time: 1.778 | step time: 0.000
train | epoch   0 | Iter:    790/ 13000 | global iter:    790/ 13000 | loss: 2.7237 | ds_loss: 0.0000 | lr: 4.9567e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    790/ 13000 | global iter:    790/ 13000 | loss: 2.6729 | ds_loss: 0.0000 | lr: 4.9567e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 1.805
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    791/ 13000 | global iter:    791/ 13000 | loss: 2.9745 | ds_loss: 0.0000 | lr: 4.9566e-05 | scale: 16384.0000 | micro time: 1.780 | step time: 0.000
train | epoch   0 | Iter:    792/ 13000 | global iter:    792/ 13000 | loss: 2.4869 | ds_loss: 0.0000 | lr: 4.9565e-05 | scale: 16384.0000 | micro time: 1.940 | step time: 0.000
train | epoch   0 | Iter:    793/ 13000 | global iter:    793/ 13000 | loss: 2.9252 | ds_loss: 0.0000 | lr: 4.9564e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   0 | Iter:    794/ 13000 | global iter:    794/ 13000 | loss: 2.6312 | ds_loss: 0.0000 | lr: 4.9563e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   0 | Iter:    795/ 13000 | global iter:    795/ 13000 | loss: 2.4155 | ds_loss: 0.0000 | lr: 4.9561e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   0 | Iter:    796/ 13000 | global iter:    796/ 13000 | loss: 2.3422 | ds_loss: 0.0000 | lr: 4.9560e-05 | scale: 16384.0000 | micro time: 1.896 | step time: 0.000
train | epoch   0 | Iter:    797/ 13000 | global iter:    797/ 13000 | loss: 2.9336 | ds_loss: 0.0000 | lr: 4.9559e-05 | scale: 16384.0000 | micro time: 1.886 | step time: 0.000
train | epoch   0 | Iter:    798/ 13000 | global iter:    798/ 13000 | loss: 2.8131 | ds_loss: 0.0000 | lr: 4.9558e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   0 | Iter:    799/ 13000 | global iter:    799/ 13000 | loss: 2.2263 | ds_loss: 0.0000 | lr: 4.9557e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   0 | Iter:    800/ 13000 | global iter:    800/ 13000 | loss: 2.4219 | ds_loss: 0.0000 | lr: 4.9556e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    800/ 13000 | global iter:    800/ 13000 | loss: 2.6170 | ds_loss: 0.0000 | lr: 4.9556e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 1.838
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    801/ 13000 | global iter:    801/ 13000 | loss: 2.6834 | ds_loss: 0.0000 | lr: 4.9555e-05 | scale: 16384.0000 | micro time: 1.812 | step time: 0.000
train | epoch   0 | Iter:    802/ 13000 | global iter:    802/ 13000 | loss: 2.6398 | ds_loss: 0.0000 | lr: 4.9554e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   0 | Iter:    803/ 13000 | global iter:    803/ 13000 | loss: 1.9659 | ds_loss: 0.0000 | lr: 4.9552e-05 | scale: 16384.0000 | micro time: 1.809 | step time: 0.000
train | epoch   0 | Iter:    804/ 13000 | global iter:    804/ 13000 | loss: 2.2372 | ds_loss: 0.0000 | lr: 4.9551e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   0 | Iter:    805/ 13000 | global iter:    805/ 13000 | loss: 2.8632 | ds_loss: 0.0000 | lr: 4.9550e-05 | scale: 16384.0000 | micro time: 1.853 | step time: 0.000
train | epoch   0 | Iter:    806/ 13000 | global iter:    806/ 13000 | loss: 2.5350 | ds_loss: 0.0000 | lr: 4.9549e-05 | scale: 16384.0000 | micro time: 1.786 | step time: 0.000
train | epoch   0 | Iter:    807/ 13000 | global iter:    807/ 13000 | loss: 2.3325 | ds_loss: 0.0000 | lr: 4.9548e-05 | scale: 16384.0000 | micro time: 1.816 | step time: 0.000
train | epoch   0 | Iter:    808/ 13000 | global iter:    808/ 13000 | loss: 2.3305 | ds_loss: 0.0000 | lr: 4.9547e-05 | scale: 16384.0000 | micro time: 1.719 | step time: 0.000
train | epoch   0 | Iter:    809/ 13000 | global iter:    809/ 13000 | loss: 2.7977 | ds_loss: 0.0000 | lr: 4.9546e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   0 | Iter:    810/ 13000 | global iter:    810/ 13000 | loss: 2.5846 | ds_loss: 0.0000 | lr: 4.9544e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    810/ 13000 | global iter:    810/ 13000 | loss: 2.4970 | ds_loss: 0.0000 | lr: 4.9544e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 1.815
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    811/ 13000 | global iter:    811/ 13000 | loss: 2.2523 | ds_loss: 0.0000 | lr: 4.9543e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   0 | Iter:    812/ 13000 | global iter:    812/ 13000 | loss: 2.4988 | ds_loss: 0.0000 | lr: 4.9542e-05 | scale: 16384.0000 | micro time: 1.840 | step time: 0.000
train | epoch   0 | Iter:    813/ 13000 | global iter:    813/ 13000 | loss: 1.9323 | ds_loss: 0.0000 | lr: 4.9541e-05 | scale: 16384.0000 | micro time: 1.861 | step time: 0.000
train | epoch   0 | Iter:    814/ 13000 | global iter:    814/ 13000 | loss: 2.1491 | ds_loss: 0.0000 | lr: 4.9540e-05 | scale: 16384.0000 | micro time: 1.776 | step time: 0.000
train | epoch   0 | Iter:    815/ 13000 | global iter:    815/ 13000 | loss: 2.7975 | ds_loss: 0.0000 | lr: 4.9539e-05 | scale: 16384.0000 | micro time: 1.752 | step time: 0.000
train | epoch   0 | Iter:    816/ 13000 | global iter:    816/ 13000 | loss: 2.8925 | ds_loss: 0.0000 | lr: 4.9537e-05 | scale: 16384.0000 | micro time: 1.862 | step time: 0.000
train | epoch   0 | Iter:    817/ 13000 | global iter:    817/ 13000 | loss: 2.4809 | ds_loss: 0.0000 | lr: 4.9536e-05 | scale: 16384.0000 | micro time: 1.759 | step time: 0.000
train | epoch   0 | Iter:    818/ 13000 | global iter:    818/ 13000 | loss: 2.3184 | ds_loss: 0.0000 | lr: 4.9535e-05 | scale: 16384.0000 | micro time: 1.909 | step time: 0.000
train | epoch   0 | Iter:    819/ 13000 | global iter:    819/ 13000 | loss: 1.8917 | ds_loss: 0.0000 | lr: 4.9534e-05 | scale: 16384.0000 | micro time: 1.748 | step time: 0.000
train | epoch   0 | Iter:    820/ 13000 | global iter:    820/ 13000 | loss: 2.9026 | ds_loss: 0.0000 | lr: 4.9533e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    820/ 13000 | global iter:    820/ 13000 | loss: 2.4116 | ds_loss: 0.0000 | lr: 4.9533e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 1.805
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    821/ 13000 | global iter:    821/ 13000 | loss: 3.1312 | ds_loss: 0.0000 | lr: 4.9532e-05 | scale: 16384.0000 | micro time: 1.852 | step time: 0.000
train | epoch   0 | Iter:    822/ 13000 | global iter:    822/ 13000 | loss: 2.2933 | ds_loss: 0.0000 | lr: 4.9531e-05 | scale: 16384.0000 | micro time: 1.825 | step time: 0.000
train | epoch   0 | Iter:    823/ 13000 | global iter:    823/ 13000 | loss: 2.4959 | ds_loss: 0.0000 | lr: 4.9529e-05 | scale: 16384.0000 | micro time: 1.749 | step time: 0.000
train | epoch   0 | Iter:    824/ 13000 | global iter:    824/ 13000 | loss: 2.4054 | ds_loss: 0.0000 | lr: 4.9528e-05 | scale: 16384.0000 | micro time: 1.741 | step time: 0.000
train | epoch   0 | Iter:    825/ 13000 | global iter:    825/ 13000 | loss: 1.9556 | ds_loss: 0.0000 | lr: 4.9527e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   0 | Iter:    826/ 13000 | global iter:    826/ 13000 | loss: 2.4051 | ds_loss: 0.0000 | lr: 4.9526e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   0 | Iter:    827/ 13000 | global iter:    827/ 13000 | loss: 2.9299 | ds_loss: 0.0000 | lr: 4.9525e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   0 | Iter:    828/ 13000 | global iter:    828/ 13000 | loss: 2.5144 | ds_loss: 0.0000 | lr: 4.9524e-05 | scale: 16384.0000 | micro time: 1.669 | step time: 0.000
train | epoch   0 | Iter:    829/ 13000 | global iter:    829/ 13000 | loss: 2.1655 | ds_loss: 0.0000 | lr: 4.9522e-05 | scale: 16384.0000 | micro time: 1.764 | step time: 0.000
train | epoch   0 | Iter:    830/ 13000 | global iter:    830/ 13000 | loss: 2.3957 | ds_loss: 0.0000 | lr: 4.9521e-05 | scale: 16384.0000 | micro time: 1.860 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    830/ 13000 | global iter:    830/ 13000 | loss: 2.4692 | ds_loss: 0.0000 | lr: 4.9521e-05 | scale: 16384.0000 | micro time: 1.860 | step time: 1.791
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    831/ 13000 | global iter:    831/ 13000 | loss: 2.0934 | ds_loss: 0.0000 | lr: 4.9520e-05 | scale: 16384.0000 | micro time: 1.808 | step time: 0.000
train | epoch   0 | Iter:    832/ 13000 | global iter:    832/ 13000 | loss: 2.1692 | ds_loss: 0.0000 | lr: 4.9519e-05 | scale: 16384.0000 | micro time: 1.784 | step time: 0.000
train | epoch   0 | Iter:    833/ 13000 | global iter:    833/ 13000 | loss: 3.0068 | ds_loss: 0.0000 | lr: 4.9518e-05 | scale: 16384.0000 | micro time: 1.817 | step time: 0.000
train | epoch   0 | Iter:    834/ 13000 | global iter:    834/ 13000 | loss: 2.3685 | ds_loss: 0.0000 | lr: 4.9516e-05 | scale: 16384.0000 | micro time: 1.796 | step time: 0.000
train | epoch   0 | Iter:    835/ 13000 | global iter:    835/ 13000 | loss: 2.7044 | ds_loss: 0.0000 | lr: 4.9515e-05 | scale: 16384.0000 | micro time: 1.833 | step time: 0.000
train | epoch   0 | Iter:    836/ 13000 | global iter:    836/ 13000 | loss: 2.3694 | ds_loss: 0.0000 | lr: 4.9514e-05 | scale: 16384.0000 | micro time: 1.754 | step time: 0.000
train | epoch   0 | Iter:    837/ 13000 | global iter:    837/ 13000 | loss: 2.8385 | ds_loss: 0.0000 | lr: 4.9513e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   0 | Iter:    838/ 13000 | global iter:    838/ 13000 | loss: 2.1367 | ds_loss: 0.0000 | lr: 4.9512e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   0 | Iter:    839/ 13000 | global iter:    839/ 13000 | loss: 3.0009 | ds_loss: 0.0000 | lr: 4.9511e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   0 | Iter:    840/ 13000 | global iter:    840/ 13000 | loss: 2.3927 | ds_loss: 0.0000 | lr: 4.9509e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    840/ 13000 | global iter:    840/ 13000 | loss: 2.5080 | ds_loss: 0.0000 | lr: 4.9509e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 1.803
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    841/ 13000 | global iter:    841/ 13000 | loss: 2.6440 | ds_loss: 0.0000 | lr: 4.9508e-05 | scale: 16384.0000 | micro time: 1.749 | step time: 0.000
train | epoch   0 | Iter:    842/ 13000 | global iter:    842/ 13000 | loss: 2.7772 | ds_loss: 0.0000 | lr: 4.9507e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   0 | Iter:    843/ 13000 | global iter:    843/ 13000 | loss: 1.6483 | ds_loss: 0.0000 | lr: 4.9506e-05 | scale: 16384.0000 | micro time: 1.694 | step time: 0.000
train | epoch   0 | Iter:    844/ 13000 | global iter:    844/ 13000 | loss: 2.1072 | ds_loss: 0.0000 | lr: 4.9505e-05 | scale: 16384.0000 | micro time: 1.786 | step time: 0.000
train | epoch   0 | Iter:    845/ 13000 | global iter:    845/ 13000 | loss: 2.7338 | ds_loss: 0.0000 | lr: 4.9503e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   0 | Iter:    846/ 13000 | global iter:    846/ 13000 | loss: 2.3772 | ds_loss: 0.0000 | lr: 4.9502e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   0 | Iter:    847/ 13000 | global iter:    847/ 13000 | loss: 3.1195 | ds_loss: 0.0000 | lr: 4.9501e-05 | scale: 16384.0000 | micro time: 1.879 | step time: 0.000
train | epoch   0 | Iter:    848/ 13000 | global iter:    848/ 13000 | loss: 2.4948 | ds_loss: 0.0000 | lr: 4.9500e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   0 | Iter:    849/ 13000 | global iter:    849/ 13000 | loss: 2.9630 | ds_loss: 0.0000 | lr: 4.9499e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   0 | Iter:    850/ 13000 | global iter:    850/ 13000 | loss: 2.2408 | ds_loss: 0.0000 | lr: 4.9497e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    850/ 13000 | global iter:    850/ 13000 | loss: 2.5106 | ds_loss: 0.0000 | lr: 4.9497e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 1.785
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    851/ 13000 | global iter:    851/ 13000 | loss: 2.2233 | ds_loss: 0.0000 | lr: 4.9496e-05 | scale: 16384.0000 | micro time: 1.861 | step time: 0.000
train | epoch   0 | Iter:    852/ 13000 | global iter:    852/ 13000 | loss: 2.7776 | ds_loss: 0.0000 | lr: 4.9495e-05 | scale: 16384.0000 | micro time: 1.687 | step time: 0.000
train | epoch   0 | Iter:    853/ 13000 | global iter:    853/ 13000 | loss: 2.9003 | ds_loss: 0.0000 | lr: 4.9494e-05 | scale: 16384.0000 | micro time: 1.760 | step time: 0.000
train | epoch   0 | Iter:    854/ 13000 | global iter:    854/ 13000 | loss: 2.6186 | ds_loss: 0.0000 | lr: 4.9493e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   0 | Iter:    855/ 13000 | global iter:    855/ 13000 | loss: 2.6403 | ds_loss: 0.0000 | lr: 4.9491e-05 | scale: 16384.0000 | micro time: 1.687 | step time: 0.000
train | epoch   0 | Iter:    856/ 13000 | global iter:    856/ 13000 | loss: 2.7936 | ds_loss: 0.0000 | lr: 4.9490e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   0 | Iter:    857/ 13000 | global iter:    857/ 13000 | loss: 2.4584 | ds_loss: 0.0000 | lr: 4.9489e-05 | scale: 16384.0000 | micro time: 1.731 | step time: 0.000
train | epoch   0 | Iter:    858/ 13000 | global iter:    858/ 13000 | loss: 2.5358 | ds_loss: 0.0000 | lr: 4.9488e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   0 | Iter:    859/ 13000 | global iter:    859/ 13000 | loss: 2.7233 | ds_loss: 0.0000 | lr: 4.9486e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   0 | Iter:    860/ 13000 | global iter:    860/ 13000 | loss: 2.5161 | ds_loss: 0.0000 | lr: 4.9485e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    860/ 13000 | global iter:    860/ 13000 | loss: 2.6187 | ds_loss: 0.0000 | lr: 4.9485e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 1.777
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    861/ 13000 | global iter:    861/ 13000 | loss: 2.7110 | ds_loss: 0.0000 | lr: 4.9484e-05 | scale: 16384.0000 | micro time: 1.770 | step time: 0.000
train | epoch   0 | Iter:    862/ 13000 | global iter:    862/ 13000 | loss: 2.8909 | ds_loss: 0.0000 | lr: 4.9483e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   0 | Iter:    863/ 13000 | global iter:    863/ 13000 | loss: 2.5907 | ds_loss: 0.0000 | lr: 4.9482e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   0 | Iter:    864/ 13000 | global iter:    864/ 13000 | loss: 2.8310 | ds_loss: 0.0000 | lr: 4.9480e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   0 | Iter:    865/ 13000 | global iter:    865/ 13000 | loss: 2.2241 | ds_loss: 0.0000 | lr: 4.9479e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   0 | Iter:    866/ 13000 | global iter:    866/ 13000 | loss: 2.9293 | ds_loss: 0.0000 | lr: 4.9478e-05 | scale: 16384.0000 | micro time: 1.617 | step time: 0.000
train | epoch   0 | Iter:    867/ 13000 | global iter:    867/ 13000 | loss: 2.6082 | ds_loss: 0.0000 | lr: 4.9477e-05 | scale: 16384.0000 | micro time: 1.742 | step time: 0.000
train | epoch   0 | Iter:    868/ 13000 | global iter:    868/ 13000 | loss: 2.5199 | ds_loss: 0.0000 | lr: 4.9475e-05 | scale: 16384.0000 | micro time: 1.826 | step time: 0.000
train | epoch   0 | Iter:    869/ 13000 | global iter:    869/ 13000 | loss: 2.8801 | ds_loss: 0.0000 | lr: 4.9474e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   0 | Iter:    870/ 13000 | global iter:    870/ 13000 | loss: 2.4192 | ds_loss: 0.0000 | lr: 4.9473e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    870/ 13000 | global iter:    870/ 13000 | loss: 2.6604 | ds_loss: 0.0000 | lr: 4.9473e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 1.775
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    871/ 13000 | global iter:    871/ 13000 | loss: 2.0568 | ds_loss: 0.0000 | lr: 4.9472e-05 | scale: 16384.0000 | micro time: 1.802 | step time: 0.000
train | epoch   0 | Iter:    872/ 13000 | global iter:    872/ 13000 | loss: 2.1065 | ds_loss: 0.0000 | lr: 4.9471e-05 | scale: 16384.0000 | micro time: 1.738 | step time: 0.000
train | epoch   0 | Iter:    873/ 13000 | global iter:    873/ 13000 | loss: 2.2827 | ds_loss: 0.0000 | lr: 4.9469e-05 | scale: 16384.0000 | micro time: 1.812 | step time: 0.000
train | epoch   0 | Iter:    874/ 13000 | global iter:    874/ 13000 | loss: 2.0369 | ds_loss: 0.0000 | lr: 4.9468e-05 | scale: 16384.0000 | micro time: 1.812 | step time: 0.000
train | epoch   0 | Iter:    875/ 13000 | global iter:    875/ 13000 | loss: 2.2033 | ds_loss: 0.0000 | lr: 4.9467e-05 | scale: 16384.0000 | micro time: 1.814 | step time: 0.000
train | epoch   0 | Iter:    876/ 13000 | global iter:    876/ 13000 | loss: 2.4180 | ds_loss: 0.0000 | lr: 4.9466e-05 | scale: 16384.0000 | micro time: 1.732 | step time: 0.000
train | epoch   0 | Iter:    877/ 13000 | global iter:    877/ 13000 | loss: 2.7854 | ds_loss: 0.0000 | lr: 4.9464e-05 | scale: 16384.0000 | micro time: 1.862 | step time: 0.000
train | epoch   0 | Iter:    878/ 13000 | global iter:    878/ 13000 | loss: 1.5560 | ds_loss: 0.0000 | lr: 4.9463e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   0 | Iter:    879/ 13000 | global iter:    879/ 13000 | loss: 2.6496 | ds_loss: 0.0000 | lr: 4.9462e-05 | scale: 16384.0000 | micro time: 1.776 | step time: 0.000
train | epoch   0 | Iter:    880/ 13000 | global iter:    880/ 13000 | loss: 2.5295 | ds_loss: 0.0000 | lr: 4.9461e-05 | scale: 16384.0000 | micro time: 1.806 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    880/ 13000 | global iter:    880/ 13000 | loss: 2.2625 | ds_loss: 0.0000 | lr: 4.9461e-05 | scale: 16384.0000 | micro time: 1.806 | step time: 1.798
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    881/ 13000 | global iter:    881/ 13000 | loss: 2.7978 | ds_loss: 0.0000 | lr: 4.9459e-05 | scale: 16384.0000 | micro time: 1.749 | step time: 0.000
train | epoch   0 | Iter:    882/ 13000 | global iter:    882/ 13000 | loss: 1.9987 | ds_loss: 0.0000 | lr: 4.9458e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   0 | Iter:    883/ 13000 | global iter:    883/ 13000 | loss: 2.2195 | ds_loss: 0.0000 | lr: 4.9457e-05 | scale: 16384.0000 | micro time: 1.728 | step time: 0.000
train | epoch   0 | Iter:    884/ 13000 | global iter:    884/ 13000 | loss: 1.6830 | ds_loss: 0.0000 | lr: 4.9456e-05 | scale: 16384.0000 | micro time: 1.777 | step time: 0.000
train | epoch   0 | Iter:    885/ 13000 | global iter:    885/ 13000 | loss: 2.6464 | ds_loss: 0.0000 | lr: 4.9454e-05 | scale: 16384.0000 | micro time: 1.760 | step time: 0.000
train | epoch   0 | Iter:    886/ 13000 | global iter:    886/ 13000 | loss: 1.4652 | ds_loss: 0.0000 | lr: 4.9453e-05 | scale: 16384.0000 | micro time: 1.790 | step time: 0.000
train | epoch   0 | Iter:    887/ 13000 | global iter:    887/ 13000 | loss: 2.3946 | ds_loss: 0.0000 | lr: 4.9452e-05 | scale: 16384.0000 | micro time: 1.766 | step time: 0.000
train | epoch   0 | Iter:    888/ 13000 | global iter:    888/ 13000 | loss: 2.5612 | ds_loss: 0.0000 | lr: 4.9451e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   0 | Iter:    889/ 13000 | global iter:    889/ 13000 | loss: 2.1734 | ds_loss: 0.0000 | lr: 4.9449e-05 | scale: 16384.0000 | micro time: 1.713 | step time: 0.000
train | epoch   0 | Iter:    890/ 13000 | global iter:    890/ 13000 | loss: 1.6640 | ds_loss: 0.0000 | lr: 4.9448e-05 | scale: 16384.0000 | micro time: 1.733 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    890/ 13000 | global iter:    890/ 13000 | loss: 2.1604 | ds_loss: 0.0000 | lr: 4.9448e-05 | scale: 16384.0000 | micro time: 1.733 | step time: 1.760
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    891/ 13000 | global iter:    891/ 13000 | loss: 1.9679 | ds_loss: 0.0000 | lr: 4.9447e-05 | scale: 16384.0000 | micro time: 1.714 | step time: 0.000
train | epoch   0 | Iter:    892/ 13000 | global iter:    892/ 13000 | loss: 2.4116 | ds_loss: 0.0000 | lr: 4.9446e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
train | epoch   0 | Iter:    893/ 13000 | global iter:    893/ 13000 | loss: 2.8093 | ds_loss: 0.0000 | lr: 4.9444e-05 | scale: 16384.0000 | micro time: 1.821 | step time: 0.000
train | epoch   0 | Iter:    894/ 13000 | global iter:    894/ 13000 | loss: 2.6944 | ds_loss: 0.0000 | lr: 4.9443e-05 | scale: 16384.0000 | micro time: 1.769 | step time: 0.000
train | epoch   0 | Iter:    895/ 13000 | global iter:    895/ 13000 | loss: 2.1527 | ds_loss: 0.0000 | lr: 4.9442e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   0 | Iter:    896/ 13000 | global iter:    896/ 13000 | loss: 1.7724 | ds_loss: 0.0000 | lr: 4.9440e-05 | scale: 16384.0000 | micro time: 1.840 | step time: 0.000
train | epoch   0 | Iter:    897/ 13000 | global iter:    897/ 13000 | loss: 2.4642 | ds_loss: 0.0000 | lr: 4.9439e-05 | scale: 16384.0000 | micro time: 1.714 | step time: 0.000
train | epoch   0 | Iter:    898/ 13000 | global iter:    898/ 13000 | loss: 2.8610 | ds_loss: 0.0000 | lr: 4.9438e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   0 | Iter:    899/ 13000 | global iter:    899/ 13000 | loss: 2.5422 | ds_loss: 0.0000 | lr: 4.9437e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   0 | Iter:    900/ 13000 | global iter:    900/ 13000 | loss: 2.2934 | ds_loss: 0.0000 | lr: 4.9435e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    900/ 13000 | global iter:    900/ 13000 | loss: 2.3969 | ds_loss: 0.0000 | lr: 4.9435e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 1.798
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    901/ 13000 | global iter:    901/ 13000 | loss: 2.5727 | ds_loss: 0.0000 | lr: 4.9434e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   0 | Iter:    902/ 13000 | global iter:    902/ 13000 | loss: 2.2211 | ds_loss: 0.0000 | lr: 4.9433e-05 | scale: 16384.0000 | micro time: 1.758 | step time: 0.000
train | epoch   0 | Iter:    903/ 13000 | global iter:    903/ 13000 | loss: 2.2310 | ds_loss: 0.0000 | lr: 4.9432e-05 | scale: 16384.0000 | micro time: 1.716 | step time: 0.000
train | epoch   0 | Iter:    904/ 13000 | global iter:    904/ 13000 | loss: 2.4233 | ds_loss: 0.0000 | lr: 4.9430e-05 | scale: 16384.0000 | micro time: 1.836 | step time: 0.000
train | epoch   0 | Iter:    905/ 13000 | global iter:    905/ 13000 | loss: 2.2787 | ds_loss: 0.0000 | lr: 4.9429e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   0 | Iter:    906/ 13000 | global iter:    906/ 13000 | loss: 2.6952 | ds_loss: 0.0000 | lr: 4.9428e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 0.000
train | epoch   0 | Iter:    907/ 13000 | global iter:    907/ 13000 | loss: 1.8560 | ds_loss: 0.0000 | lr: 4.9426e-05 | scale: 16384.0000 | micro time: 1.717 | step time: 0.000
train | epoch   0 | Iter:    908/ 13000 | global iter:    908/ 13000 | loss: 2.7102 | ds_loss: 0.0000 | lr: 4.9425e-05 | scale: 16384.0000 | micro time: 1.772 | step time: 0.000
train | epoch   0 | Iter:    909/ 13000 | global iter:    909/ 13000 | loss: 2.5373 | ds_loss: 0.0000 | lr: 4.9424e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   0 | Iter:    910/ 13000 | global iter:    910/ 13000 | loss: 2.2772 | ds_loss: 0.0000 | lr: 4.9423e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    910/ 13000 | global iter:    910/ 13000 | loss: 2.3803 | ds_loss: 0.0000 | lr: 4.9423e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 1.774
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    911/ 13000 | global iter:    911/ 13000 | loss: 2.1433 | ds_loss: 0.0000 | lr: 4.9421e-05 | scale: 16384.0000 | micro time: 1.794 | step time: 0.000
train | epoch   0 | Iter:    912/ 13000 | global iter:    912/ 13000 | loss: 2.8854 | ds_loss: 0.0000 | lr: 4.9420e-05 | scale: 16384.0000 | micro time: 1.759 | step time: 0.000
train | epoch   0 | Iter:    913/ 13000 | global iter:    913/ 13000 | loss: 2.3655 | ds_loss: 0.0000 | lr: 4.9419e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   0 | Iter:    914/ 13000 | global iter:    914/ 13000 | loss: 2.3747 | ds_loss: 0.0000 | lr: 4.9417e-05 | scale: 16384.0000 | micro time: 1.848 | step time: 0.000
train | epoch   0 | Iter:    915/ 13000 | global iter:    915/ 13000 | loss: 2.8955 | ds_loss: 0.0000 | lr: 4.9416e-05 | scale: 16384.0000 | micro time: 1.650 | step time: 0.000
train | epoch   0 | Iter:    916/ 13000 | global iter:    916/ 13000 | loss: 2.2330 | ds_loss: 0.0000 | lr: 4.9415e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   0 | Iter:    917/ 13000 | global iter:    917/ 13000 | loss: 2.4022 | ds_loss: 0.0000 | lr: 4.9414e-05 | scale: 16384.0000 | micro time: 1.726 | step time: 0.000
train | epoch   0 | Iter:    918/ 13000 | global iter:    918/ 13000 | loss: 2.5461 | ds_loss: 0.0000 | lr: 4.9412e-05 | scale: 16384.0000 | micro time: 1.782 | step time: 0.000
train | epoch   0 | Iter:    919/ 13000 | global iter:    919/ 13000 | loss: 2.7735 | ds_loss: 0.0000 | lr: 4.9411e-05 | scale: 16384.0000 | micro time: 1.681 | step time: 0.000
train | epoch   0 | Iter:    920/ 13000 | global iter:    920/ 13000 | loss: 2.5779 | ds_loss: 0.0000 | lr: 4.9410e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    920/ 13000 | global iter:    920/ 13000 | loss: 2.5197 | ds_loss: 0.0000 | lr: 4.9410e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 1.774
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    921/ 13000 | global iter:    921/ 13000 | loss: 2.4669 | ds_loss: 0.0000 | lr: 4.9408e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   0 | Iter:    922/ 13000 | global iter:    922/ 13000 | loss: 2.9229 | ds_loss: 0.0000 | lr: 4.9407e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   0 | Iter:    923/ 13000 | global iter:    923/ 13000 | loss: 2.3554 | ds_loss: 0.0000 | lr: 4.9406e-05 | scale: 16384.0000 | micro time: 1.748 | step time: 0.000
train | epoch   0 | Iter:    924/ 13000 | global iter:    924/ 13000 | loss: 3.3859 | ds_loss: 0.0000 | lr: 4.9404e-05 | scale: 16384.0000 | micro time: 1.733 | step time: 0.000
train | epoch   0 | Iter:    925/ 13000 | global iter:    925/ 13000 | loss: 2.3773 | ds_loss: 0.0000 | lr: 4.9403e-05 | scale: 16384.0000 | micro time: 1.808 | step time: 0.000
train | epoch   0 | Iter:    926/ 13000 | global iter:    926/ 13000 | loss: 2.9724 | ds_loss: 0.0000 | lr: 4.9402e-05 | scale: 16384.0000 | micro time: 1.857 | step time: 0.000
train | epoch   0 | Iter:    927/ 13000 | global iter:    927/ 13000 | loss: 2.1581 | ds_loss: 0.0000 | lr: 4.9400e-05 | scale: 16384.0000 | micro time: 1.822 | step time: 0.000
train | epoch   0 | Iter:    928/ 13000 | global iter:    928/ 13000 | loss: 2.4730 | ds_loss: 0.0000 | lr: 4.9399e-05 | scale: 16384.0000 | micro time: 1.845 | step time: 0.000
train | epoch   0 | Iter:    929/ 13000 | global iter:    929/ 13000 | loss: 2.6799 | ds_loss: 0.0000 | lr: 4.9398e-05 | scale: 16384.0000 | micro time: 1.790 | step time: 0.000
train | epoch   0 | Iter:    930/ 13000 | global iter:    930/ 13000 | loss: 2.5649 | ds_loss: 0.0000 | lr: 4.9396e-05 | scale: 16384.0000 | micro time: 1.857 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    930/ 13000 | global iter:    930/ 13000 | loss: 2.6357 | ds_loss: 0.0000 | lr: 4.9396e-05 | scale: 16384.0000 | micro time: 1.857 | step time: 1.811
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    931/ 13000 | global iter:    931/ 13000 | loss: 2.1879 | ds_loss: 0.0000 | lr: 4.9395e-05 | scale: 16384.0000 | micro time: 1.630 | step time: 0.000
train | epoch   0 | Iter:    932/ 13000 | global iter:    932/ 13000 | loss: 2.9782 | ds_loss: 0.0000 | lr: 4.9394e-05 | scale: 16384.0000 | micro time: 1.722 | step time: 0.000
train | epoch   0 | Iter:    933/ 13000 | global iter:    933/ 13000 | loss: 2.6868 | ds_loss: 0.0000 | lr: 4.9393e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   0 | Iter:    934/ 13000 | global iter:    934/ 13000 | loss: 2.3120 | ds_loss: 0.0000 | lr: 4.9391e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   0 | Iter:    935/ 13000 | global iter:    935/ 13000 | loss: 2.7919 | ds_loss: 0.0000 | lr: 4.9390e-05 | scale: 16384.0000 | micro time: 1.830 | step time: 0.000
train | epoch   0 | Iter:    936/ 13000 | global iter:    936/ 13000 | loss: 1.8871 | ds_loss: 0.0000 | lr: 4.9389e-05 | scale: 16384.0000 | micro time: 2.003 | step time: 0.000
train | epoch   0 | Iter:    937/ 13000 | global iter:    937/ 13000 | loss: 2.5142 | ds_loss: 0.0000 | lr: 4.9387e-05 | scale: 16384.0000 | micro time: 1.700 | step time: 0.000
train | epoch   0 | Iter:    938/ 13000 | global iter:    938/ 13000 | loss: 2.3526 | ds_loss: 0.0000 | lr: 4.9386e-05 | scale: 16384.0000 | micro time: 1.838 | step time: 0.000
train | epoch   0 | Iter:    939/ 13000 | global iter:    939/ 13000 | loss: 3.0496 | ds_loss: 0.0000 | lr: 4.9385e-05 | scale: 16384.0000 | micro time: 1.850 | step time: 0.000
train | epoch   0 | Iter:    940/ 13000 | global iter:    940/ 13000 | loss: 2.7248 | ds_loss: 0.0000 | lr: 4.9383e-05 | scale: 16384.0000 | micro time: 1.764 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    940/ 13000 | global iter:    940/ 13000 | loss: 2.5485 | ds_loss: 0.0000 | lr: 4.9383e-05 | scale: 16384.0000 | micro time: 1.764 | step time: 1.794
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    941/ 13000 | global iter:    941/ 13000 | loss: 2.6572 | ds_loss: 0.0000 | lr: 4.9382e-05 | scale: 16384.0000 | micro time: 1.918 | step time: 0.000
train | epoch   0 | Iter:    942/ 13000 | global iter:    942/ 13000 | loss: 2.1845 | ds_loss: 0.0000 | lr: 4.9381e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   0 | Iter:    943/ 13000 | global iter:    943/ 13000 | loss: 2.5707 | ds_loss: 0.0000 | lr: 4.9379e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
train | epoch   0 | Iter:    944/ 13000 | global iter:    944/ 13000 | loss: 2.7601 | ds_loss: 0.0000 | lr: 4.9378e-05 | scale: 16384.0000 | micro time: 1.894 | step time: 0.000
train | epoch   0 | Iter:    945/ 13000 | global iter:    945/ 13000 | loss: 2.6960 | ds_loss: 0.0000 | lr: 4.9377e-05 | scale: 16384.0000 | micro time: 1.770 | step time: 0.000
train | epoch   0 | Iter:    946/ 13000 | global iter:    946/ 13000 | loss: 2.7695 | ds_loss: 0.0000 | lr: 4.9375e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   0 | Iter:    947/ 13000 | global iter:    947/ 13000 | loss: 2.2534 | ds_loss: 0.0000 | lr: 4.9374e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   0 | Iter:    948/ 13000 | global iter:    948/ 13000 | loss: 2.3063 | ds_loss: 0.0000 | lr: 4.9373e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   0 | Iter:    949/ 13000 | global iter:    949/ 13000 | loss: 2.2393 | ds_loss: 0.0000 | lr: 4.9371e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
train | epoch   0 | Iter:    950/ 13000 | global iter:    950/ 13000 | loss: 2.8229 | ds_loss: 0.0000 | lr: 4.9370e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    950/ 13000 | global iter:    950/ 13000 | loss: 2.5260 | ds_loss: 0.0000 | lr: 4.9370e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 1.836
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    951/ 13000 | global iter:    951/ 13000 | loss: 2.0727 | ds_loss: 0.0000 | lr: 4.9368e-05 | scale: 16384.0000 | micro time: 1.673 | step time: 0.000
train | epoch   0 | Iter:    952/ 13000 | global iter:    952/ 13000 | loss: 2.4198 | ds_loss: 0.0000 | lr: 4.9367e-05 | scale: 16384.0000 | micro time: 1.743 | step time: 0.000
train | epoch   0 | Iter:    953/ 13000 | global iter:    953/ 13000 | loss: 2.6657 | ds_loss: 0.0000 | lr: 4.9366e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   0 | Iter:    954/ 13000 | global iter:    954/ 13000 | loss: 2.2525 | ds_loss: 0.0000 | lr: 4.9364e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   0 | Iter:    955/ 13000 | global iter:    955/ 13000 | loss: 2.8868 | ds_loss: 0.0000 | lr: 4.9363e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   0 | Iter:    956/ 13000 | global iter:    956/ 13000 | loss: 1.7015 | ds_loss: 0.0000 | lr: 4.9362e-05 | scale: 16384.0000 | micro time: 1.792 | step time: 0.000
train | epoch   0 | Iter:    957/ 13000 | global iter:    957/ 13000 | loss: 2.2010 | ds_loss: 0.0000 | lr: 4.9360e-05 | scale: 16384.0000 | micro time: 1.818 | step time: 0.000
train | epoch   0 | Iter:    958/ 13000 | global iter:    958/ 13000 | loss: 2.2678 | ds_loss: 0.0000 | lr: 4.9359e-05 | scale: 16384.0000 | micro time: 1.735 | step time: 0.000
train | epoch   0 | Iter:    959/ 13000 | global iter:    959/ 13000 | loss: 2.3264 | ds_loss: 0.0000 | lr: 4.9358e-05 | scale: 16384.0000 | micro time: 1.844 | step time: 0.000
train | epoch   0 | Iter:    960/ 13000 | global iter:    960/ 13000 | loss: 2.5785 | ds_loss: 0.0000 | lr: 4.9356e-05 | scale: 16384.0000 | micro time: 1.826 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    960/ 13000 | global iter:    960/ 13000 | loss: 2.3373 | ds_loss: 0.0000 | lr: 4.9356e-05 | scale: 16384.0000 | micro time: 1.826 | step time: 1.781
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    961/ 13000 | global iter:    961/ 13000 | loss: 2.6426 | ds_loss: 0.0000 | lr: 4.9355e-05 | scale: 16384.0000 | micro time: 1.734 | step time: 0.000
train | epoch   0 | Iter:    962/ 13000 | global iter:    962/ 13000 | loss: 2.7909 | ds_loss: 0.0000 | lr: 4.9354e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   0 | Iter:    963/ 13000 | global iter:    963/ 13000 | loss: 2.4850 | ds_loss: 0.0000 | lr: 4.9352e-05 | scale: 16384.0000 | micro time: 1.850 | step time: 0.000
train | epoch   0 | Iter:    964/ 13000 | global iter:    964/ 13000 | loss: 2.0660 | ds_loss: 0.0000 | lr: 4.9351e-05 | scale: 16384.0000 | micro time: 1.679 | step time: 0.000
train | epoch   0 | Iter:    965/ 13000 | global iter:    965/ 13000 | loss: 2.6064 | ds_loss: 0.0000 | lr: 4.9349e-05 | scale: 16384.0000 | micro time: 1.726 | step time: 0.000
train | epoch   0 | Iter:    966/ 13000 | global iter:    966/ 13000 | loss: 2.3691 | ds_loss: 0.0000 | lr: 4.9348e-05 | scale: 16384.0000 | micro time: 1.784 | step time: 0.000
train | epoch   0 | Iter:    967/ 13000 | global iter:    967/ 13000 | loss: 2.0116 | ds_loss: 0.0000 | lr: 4.9347e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   0 | Iter:    968/ 13000 | global iter:    968/ 13000 | loss: 2.9391 | ds_loss: 0.0000 | lr: 4.9345e-05 | scale: 16384.0000 | micro time: 1.786 | step time: 0.000
train | epoch   0 | Iter:    969/ 13000 | global iter:    969/ 13000 | loss: 2.5649 | ds_loss: 0.0000 | lr: 4.9344e-05 | scale: 16384.0000 | micro time: 1.749 | step time: 0.000
train | epoch   0 | Iter:    970/ 13000 | global iter:    970/ 13000 | loss: 2.1760 | ds_loss: 0.0000 | lr: 4.9343e-05 | scale: 16384.0000 | micro time: 1.789 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    970/ 13000 | global iter:    970/ 13000 | loss: 2.4652 | ds_loss: 0.0000 | lr: 4.9343e-05 | scale: 16384.0000 | micro time: 1.789 | step time: 1.774
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    971/ 13000 | global iter:    971/ 13000 | loss: 2.6604 | ds_loss: 0.0000 | lr: 4.9341e-05 | scale: 16384.0000 | micro time: 1.879 | step time: 0.000
train | epoch   0 | Iter:    972/ 13000 | global iter:    972/ 13000 | loss: 2.9220 | ds_loss: 0.0000 | lr: 4.9340e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   0 | Iter:    973/ 13000 | global iter:    973/ 13000 | loss: 2.3388 | ds_loss: 0.0000 | lr: 4.9338e-05 | scale: 16384.0000 | micro time: 1.733 | step time: 0.000
train | epoch   0 | Iter:    974/ 13000 | global iter:    974/ 13000 | loss: 2.1559 | ds_loss: 0.0000 | lr: 4.9337e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   0 | Iter:    975/ 13000 | global iter:    975/ 13000 | loss: 2.7184 | ds_loss: 0.0000 | lr: 4.9336e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   0 | Iter:    976/ 13000 | global iter:    976/ 13000 | loss: 2.1088 | ds_loss: 0.0000 | lr: 4.9334e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   0 | Iter:    977/ 13000 | global iter:    977/ 13000 | loss: 2.4876 | ds_loss: 0.0000 | lr: 4.9333e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   0 | Iter:    978/ 13000 | global iter:    978/ 13000 | loss: 2.7139 | ds_loss: 0.0000 | lr: 4.9332e-05 | scale: 16384.0000 | micro time: 2.219 | step time: 0.000
train | epoch   0 | Iter:    979/ 13000 | global iter:    979/ 13000 | loss: 2.7386 | ds_loss: 0.0000 | lr: 4.9330e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   0 | Iter:    980/ 13000 | global iter:    980/ 13000 | loss: 2.5503 | ds_loss: 0.0000 | lr: 4.9329e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    980/ 13000 | global iter:    980/ 13000 | loss: 2.5395 | ds_loss: 0.0000 | lr: 4.9329e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 1.854
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    981/ 13000 | global iter:    981/ 13000 | loss: 2.9235 | ds_loss: 0.0000 | lr: 4.9327e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   0 | Iter:    982/ 13000 | global iter:    982/ 13000 | loss: 2.4582 | ds_loss: 0.0000 | lr: 4.9326e-05 | scale: 16384.0000 | micro time: 1.790 | step time: 0.000
train | epoch   0 | Iter:    983/ 13000 | global iter:    983/ 13000 | loss: 2.0868 | ds_loss: 0.0000 | lr: 4.9325e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   0 | Iter:    984/ 13000 | global iter:    984/ 13000 | loss: 2.4713 | ds_loss: 0.0000 | lr: 4.9323e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   0 | Iter:    985/ 13000 | global iter:    985/ 13000 | loss: 2.0061 | ds_loss: 0.0000 | lr: 4.9322e-05 | scale: 16384.0000 | micro time: 1.719 | step time: 0.000
train | epoch   0 | Iter:    986/ 13000 | global iter:    986/ 13000 | loss: 2.4029 | ds_loss: 0.0000 | lr: 4.9320e-05 | scale: 16384.0000 | micro time: 1.820 | step time: 0.000
train | epoch   0 | Iter:    987/ 13000 | global iter:    987/ 13000 | loss: 2.2356 | ds_loss: 0.0000 | lr: 4.9319e-05 | scale: 16384.0000 | micro time: 1.838 | step time: 0.000
train | epoch   0 | Iter:    988/ 13000 | global iter:    988/ 13000 | loss: 2.7000 | ds_loss: 0.0000 | lr: 4.9318e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   0 | Iter:    989/ 13000 | global iter:    989/ 13000 | loss: 3.4342 | ds_loss: 0.0000 | lr: 4.9316e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   0 | Iter:    990/ 13000 | global iter:    990/ 13000 | loss: 2.6995 | ds_loss: 0.0000 | lr: 4.9315e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:    990/ 13000 | global iter:    990/ 13000 | loss: 2.5418 | ds_loss: 0.0000 | lr: 4.9315e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 1.803
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:    991/ 13000 | global iter:    991/ 13000 | loss: 2.5363 | ds_loss: 0.0000 | lr: 4.9313e-05 | scale: 16384.0000 | micro time: 1.661 | step time: 0.000
train | epoch   0 | Iter:    992/ 13000 | global iter:    992/ 13000 | loss: 2.6935 | ds_loss: 0.0000 | lr: 4.9312e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   0 | Iter:    993/ 13000 | global iter:    993/ 13000 | loss: 2.2742 | ds_loss: 0.0000 | lr: 4.9311e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   0 | Iter:    994/ 13000 | global iter:    994/ 13000 | loss: 2.8176 | ds_loss: 0.0000 | lr: 4.9309e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 0.000
train | epoch   0 | Iter:    995/ 13000 | global iter:    995/ 13000 | loss: 2.7074 | ds_loss: 0.0000 | lr: 4.9308e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   0 | Iter:    996/ 13000 | global iter:    996/ 13000 | loss: 2.5042 | ds_loss: 0.0000 | lr: 4.9306e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   0 | Iter:    997/ 13000 | global iter:    997/ 13000 | loss: 2.2217 | ds_loss: 0.0000 | lr: 4.9305e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   0 | Iter:    998/ 13000 | global iter:    998/ 13000 | loss: 1.8826 | ds_loss: 0.0000 | lr: 4.9304e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   0 | Iter:    999/ 13000 | global iter:    999/ 13000 | loss: 2.9206 | ds_loss: 0.0000 | lr: 4.9302e-05 | scale: 16384.0000 | micro time: 1.703 | step time: 0.000
train | epoch   0 | Iter:   1000/ 13000 | global iter:   1000/ 13000 | loss: 2.0934 | ds_loss: 0.0000 | lr: 4.9301e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:   1000/ 13000 | global iter:   1000/ 13000 | loss: 2.4652 | ds_loss: 0.0000 | lr: 4.9301e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 1.786
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:   1001/ 13000 | global iter:   1001/ 13000 | loss: 2.1301 | ds_loss: 0.0000 | lr: 4.9299e-05 | scale: 16384.0000 | micro time: 1.809 | step time: 0.000
train | epoch   0 | Iter:   1002/ 13000 | global iter:   1002/ 13000 | loss: 2.1017 | ds_loss: 0.0000 | lr: 4.9298e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   0 | Iter:   1003/ 13000 | global iter:   1003/ 13000 | loss: 2.3159 | ds_loss: 0.0000 | lr: 4.9296e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   0 | Iter:   1004/ 13000 | global iter:   1004/ 13000 | loss: 2.9974 | ds_loss: 0.0000 | lr: 4.9295e-05 | scale: 16384.0000 | micro time: 1.845 | step time: 0.000
train | epoch   0 | Iter:   1005/ 13000 | global iter:   1005/ 13000 | loss: 2.2379 | ds_loss: 0.0000 | lr: 4.9294e-05 | scale: 16384.0000 | micro time: 1.845 | step time: 0.000
train | epoch   0 | Iter:   1006/ 13000 | global iter:   1006/ 13000 | loss: 2.0445 | ds_loss: 0.0000 | lr: 4.9292e-05 | scale: 16384.0000 | micro time: 1.816 | step time: 0.000
train | epoch   0 | Iter:   1007/ 13000 | global iter:   1007/ 13000 | loss: 2.1958 | ds_loss: 0.0000 | lr: 4.9291e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   0 | Iter:   1008/ 13000 | global iter:   1008/ 13000 | loss: 2.4714 | ds_loss: 0.0000 | lr: 4.9289e-05 | scale: 16384.0000 | micro time: 1.850 | step time: 0.000
train | epoch   0 | Iter:   1009/ 13000 | global iter:   1009/ 13000 | loss: 2.6762 | ds_loss: 0.0000 | lr: 4.9288e-05 | scale: 16384.0000 | micro time: 1.879 | step time: 0.000
train | epoch   0 | Iter:   1010/ 13000 | global iter:   1010/ 13000 | loss: 2.8515 | ds_loss: 0.0000 | lr: 4.9286e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:   1010/ 13000 | global iter:   1010/ 13000 | loss: 2.4022 | ds_loss: 0.0000 | lr: 4.9286e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 1.828
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:   1011/ 13000 | global iter:   1011/ 13000 | loss: 2.7977 | ds_loss: 0.0000 | lr: 4.9285e-05 | scale: 16384.0000 | micro time: 1.846 | step time: 0.000
train | epoch   0 | Iter:   1012/ 13000 | global iter:   1012/ 13000 | loss: 2.9294 | ds_loss: 0.0000 | lr: 4.9284e-05 | scale: 16384.0000 | micro time: 1.719 | step time: 0.000
train | epoch   0 | Iter:   1013/ 13000 | global iter:   1013/ 13000 | loss: 3.0163 | ds_loss: 0.0000 | lr: 4.9282e-05 | scale: 16384.0000 | micro time: 1.746 | step time: 0.000
train | epoch   0 | Iter:   1014/ 13000 | global iter:   1014/ 13000 | loss: 2.5347 | ds_loss: 0.0000 | lr: 4.9281e-05 | scale: 16384.0000 | micro time: 1.776 | step time: 0.000
train | epoch   0 | Iter:   1015/ 13000 | global iter:   1015/ 13000 | loss: 2.6877 | ds_loss: 0.0000 | lr: 4.9279e-05 | scale: 16384.0000 | micro time: 1.802 | step time: 0.000
train | epoch   0 | Iter:   1016/ 13000 | global iter:   1016/ 13000 | loss: 1.9093 | ds_loss: 0.0000 | lr: 4.9278e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   0 | Iter:   1017/ 13000 | global iter:   1017/ 13000 | loss: 2.7273 | ds_loss: 0.0000 | lr: 4.9276e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   0 | Iter:   1018/ 13000 | global iter:   1018/ 13000 | loss: 2.7893 | ds_loss: 0.0000 | lr: 4.9275e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   0 | Iter:   1019/ 13000 | global iter:   1019/ 13000 | loss: 2.4971 | ds_loss: 0.0000 | lr: 4.9274e-05 | scale: 16384.0000 | micro time: 1.695 | step time: 0.000
train | epoch   0 | Iter:   1020/ 13000 | global iter:   1020/ 13000 | loss: 3.0169 | ds_loss: 0.0000 | lr: 4.9272e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:   1020/ 13000 | global iter:   1020/ 13000 | loss: 2.6906 | ds_loss: 0.0000 | lr: 4.9272e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 1.786
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:   1021/ 13000 | global iter:   1021/ 13000 | loss: 2.8664 | ds_loss: 0.0000 | lr: 4.9271e-05 | scale: 16384.0000 | micro time: 1.759 | step time: 0.000
train | epoch   0 | Iter:   1022/ 13000 | global iter:   1022/ 13000 | loss: 2.4418 | ds_loss: 0.0000 | lr: 4.9269e-05 | scale: 16384.0000 | micro time: 1.866 | step time: 0.000
train | epoch   0 | Iter:   1023/ 13000 | global iter:   1023/ 13000 | loss: 2.6173 | ds_loss: 0.0000 | lr: 4.9268e-05 | scale: 16384.0000 | micro time: 1.754 | step time: 0.000
train | epoch   0 | Iter:   1024/ 13000 | global iter:   1024/ 13000 | loss: 2.8871 | ds_loss: 0.0000 | lr: 4.9266e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   0 | Iter:   1025/ 13000 | global iter:   1025/ 13000 | loss: 1.9744 | ds_loss: 0.0000 | lr: 4.9265e-05 | scale: 16384.0000 | micro time: 1.770 | step time: 0.000
train | epoch   0 | Iter:   1026/ 13000 | global iter:   1026/ 13000 | loss: 2.5430 | ds_loss: 0.0000 | lr: 4.9263e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   0 | Iter:   1027/ 13000 | global iter:   1027/ 13000 | loss: 2.6162 | ds_loss: 0.0000 | lr: 4.9262e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   0 | Iter:   1028/ 13000 | global iter:   1028/ 13000 | loss: 2.4189 | ds_loss: 0.0000 | lr: 4.9260e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   0 | Iter:   1029/ 13000 | global iter:   1029/ 13000 | loss: 2.2278 | ds_loss: 0.0000 | lr: 4.9259e-05 | scale: 16384.0000 | micro time: 1.796 | step time: 0.000
train | epoch   0 | Iter:   1030/ 13000 | global iter:   1030/ 13000 | loss: 2.6029 | ds_loss: 0.0000 | lr: 4.9258e-05 | scale: 16384.0000 | micro time: 1.857 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:   1030/ 13000 | global iter:   1030/ 13000 | loss: 2.5196 | ds_loss: 0.0000 | lr: 4.9258e-05 | scale: 16384.0000 | micro time: 1.857 | step time: 1.801
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:   1031/ 13000 | global iter:   1031/ 13000 | loss: 2.6580 | ds_loss: 0.0000 | lr: 4.9256e-05 | scale: 16384.0000 | micro time: 1.813 | step time: 0.000
train | epoch   0 | Iter:   1032/ 13000 | global iter:   1032/ 13000 | loss: 2.1985 | ds_loss: 0.0000 | lr: 4.9255e-05 | scale: 16384.0000 | micro time: 1.748 | step time: 0.000
train | epoch   0 | Iter:   1033/ 13000 | global iter:   1033/ 13000 | loss: 2.1518 | ds_loss: 0.0000 | lr: 4.9253e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   0 | Iter:   1034/ 13000 | global iter:   1034/ 13000 | loss: 1.1554 | ds_loss: 0.0000 | lr: 4.9252e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   0 | Iter:   1035/ 13000 | global iter:   1035/ 13000 | loss: 2.5403 | ds_loss: 0.0000 | lr: 4.9250e-05 | scale: 16384.0000 | micro time: 1.759 | step time: 0.000
train | epoch   0 | Iter:   1036/ 13000 | global iter:   1036/ 13000 | loss: 2.6137 | ds_loss: 0.0000 | lr: 4.9249e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   0 | Iter:   1037/ 13000 | global iter:   1037/ 13000 | loss: 2.5978 | ds_loss: 0.0000 | lr: 4.9247e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   0 | Iter:   1038/ 13000 | global iter:   1038/ 13000 | loss: 2.7185 | ds_loss: 0.0000 | lr: 4.9246e-05 | scale: 16384.0000 | micro time: 1.759 | step time: 0.000
train | epoch   0 | Iter:   1039/ 13000 | global iter:   1039/ 13000 | loss: 2.1816 | ds_loss: 0.0000 | lr: 4.9244e-05 | scale: 16384.0000 | micro time: 1.731 | step time: 0.000
train | epoch   0 | Iter:   1040/ 13000 | global iter:   1040/ 13000 | loss: 2.6122 | ds_loss: 0.0000 | lr: 4.9243e-05 | scale: 16384.0000 | micro time: 1.760 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:   1040/ 13000 | global iter:   1040/ 13000 | loss: 2.3428 | ds_loss: 0.0000 | lr: 4.9243e-05 | scale: 16384.0000 | micro time: 1.760 | step time: 1.787
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:   1041/ 13000 | global iter:   1041/ 13000 | loss: 2.6730 | ds_loss: 0.0000 | lr: 4.9241e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   0 | Iter:   1042/ 13000 | global iter:   1042/ 13000 | loss: 2.6112 | ds_loss: 0.0000 | lr: 4.9240e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   0 | Iter:   1043/ 13000 | global iter:   1043/ 13000 | loss: 2.5695 | ds_loss: 0.0000 | lr: 4.9238e-05 | scale: 16384.0000 | micro time: 1.802 | step time: 0.000
train | epoch   0 | Iter:   1044/ 13000 | global iter:   1044/ 13000 | loss: 2.3455 | ds_loss: 0.0000 | lr: 4.9237e-05 | scale: 16384.0000 | micro time: 1.856 | step time: 0.000
train | epoch   0 | Iter:   1045/ 13000 | global iter:   1045/ 13000 | loss: 2.2257 | ds_loss: 0.0000 | lr: 4.9236e-05 | scale: 16384.0000 | micro time: 1.829 | step time: 0.000
train | epoch   0 | Iter:   1046/ 13000 | global iter:   1046/ 13000 | loss: 2.0836 | ds_loss: 0.0000 | lr: 4.9234e-05 | scale: 16384.0000 | micro time: 1.814 | step time: 0.000
train | epoch   0 | Iter:   1047/ 13000 | global iter:   1047/ 13000 | loss: 2.4740 | ds_loss: 0.0000 | lr: 4.9233e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   0 | Iter:   1048/ 13000 | global iter:   1048/ 13000 | loss: 1.7401 | ds_loss: 0.0000 | lr: 4.9231e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   0 | Iter:   1049/ 13000 | global iter:   1049/ 13000 | loss: 2.1849 | ds_loss: 0.0000 | lr: 4.9230e-05 | scale: 16384.0000 | micro time: 1.727 | step time: 0.000
train | epoch   0 | Iter:   1050/ 13000 | global iter:   1050/ 13000 | loss: 2.3083 | ds_loss: 0.0000 | lr: 4.9228e-05 | scale: 16384.0000 | micro time: 1.711 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:   1050/ 13000 | global iter:   1050/ 13000 | loss: 2.3216 | ds_loss: 0.0000 | lr: 4.9228e-05 | scale: 16384.0000 | micro time: 1.711 | step time: 1.794
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:   1051/ 13000 | global iter:   1051/ 13000 | loss: 2.6870 | ds_loss: 0.0000 | lr: 4.9227e-05 | scale: 16384.0000 | micro time: 1.848 | step time: 0.000
train | epoch   0 | Iter:   1052/ 13000 | global iter:   1052/ 13000 | loss: 2.4671 | ds_loss: 0.0000 | lr: 4.9225e-05 | scale: 16384.0000 | micro time: 1.761 | step time: 0.000
train | epoch   0 | Iter:   1053/ 13000 | global iter:   1053/ 13000 | loss: 2.5610 | ds_loss: 0.0000 | lr: 4.9224e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   0 | Iter:   1054/ 13000 | global iter:   1054/ 13000 | loss: 1.9846 | ds_loss: 0.0000 | lr: 4.9222e-05 | scale: 16384.0000 | micro time: 1.741 | step time: 0.000
train | epoch   0 | Iter:   1055/ 13000 | global iter:   1055/ 13000 | loss: 2.7471 | ds_loss: 0.0000 | lr: 4.9221e-05 | scale: 16384.0000 | micro time: 1.794 | step time: 0.000
train | epoch   0 | Iter:   1056/ 13000 | global iter:   1056/ 13000 | loss: 2.2538 | ds_loss: 0.0000 | lr: 4.9219e-05 | scale: 16384.0000 | micro time: 1.806 | step time: 0.000
train | epoch   0 | Iter:   1057/ 13000 | global iter:   1057/ 13000 | loss: 2.3277 | ds_loss: 0.0000 | lr: 4.9218e-05 | scale: 16384.0000 | micro time: 1.742 | step time: 0.000
train | epoch   0 | Iter:   1058/ 13000 | global iter:   1058/ 13000 | loss: 2.8037 | ds_loss: 0.0000 | lr: 4.9216e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   0 | Iter:   1059/ 13000 | global iter:   1059/ 13000 | loss: 2.4148 | ds_loss: 0.0000 | lr: 4.9215e-05 | scale: 16384.0000 | micro time: 1.882 | step time: 0.000
train | epoch   0 | Iter:   1060/ 13000 | global iter:   1060/ 13000 | loss: 2.0611 | ds_loss: 0.0000 | lr: 4.9213e-05 | scale: 16384.0000 | micro time: 1.797 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:   1060/ 13000 | global iter:   1060/ 13000 | loss: 2.4308 | ds_loss: 0.0000 | lr: 4.9213e-05 | scale: 16384.0000 | micro time: 1.797 | step time: 1.798
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:   1061/ 13000 | global iter:   1061/ 13000 | loss: 2.1812 | ds_loss: 0.0000 | lr: 4.9212e-05 | scale: 16384.0000 | micro time: 1.828 | step time: 0.000
train | epoch   0 | Iter:   1062/ 13000 | global iter:   1062/ 13000 | loss: 2.5833 | ds_loss: 0.0000 | lr: 4.9210e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   0 | Iter:   1063/ 13000 | global iter:   1063/ 13000 | loss: 2.4750 | ds_loss: 0.0000 | lr: 4.9209e-05 | scale: 16384.0000 | micro time: 1.824 | step time: 0.000
train | epoch   0 | Iter:   1064/ 13000 | global iter:   1064/ 13000 | loss: 2.8118 | ds_loss: 0.0000 | lr: 4.9207e-05 | scale: 16384.0000 | micro time: 1.798 | step time: 0.000
train | epoch   0 | Iter:   1065/ 13000 | global iter:   1065/ 13000 | loss: 2.4300 | ds_loss: 0.0000 | lr: 4.9206e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   0 | Iter:   1066/ 13000 | global iter:   1066/ 13000 | loss: 2.0417 | ds_loss: 0.0000 | lr: 4.9204e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   0 | Iter:   1067/ 13000 | global iter:   1067/ 13000 | loss: 2.7614 | ds_loss: 0.0000 | lr: 4.9203e-05 | scale: 16384.0000 | micro time: 1.821 | step time: 0.000
train | epoch   0 | Iter:   1068/ 13000 | global iter:   1068/ 13000 | loss: 2.2468 | ds_loss: 0.0000 | lr: 4.9201e-05 | scale: 16384.0000 | micro time: 1.857 | step time: 0.000
train | epoch   0 | Iter:   1069/ 13000 | global iter:   1069/ 13000 | loss: 2.1167 | ds_loss: 0.0000 | lr: 4.9200e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   0 | Iter:   1070/ 13000 | global iter:   1070/ 13000 | loss: 2.6416 | ds_loss: 0.0000 | lr: 4.9198e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:   1070/ 13000 | global iter:   1070/ 13000 | loss: 2.4290 | ds_loss: 0.0000 | lr: 4.9198e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 1.820
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:   1071/ 13000 | global iter:   1071/ 13000 | loss: 2.4553 | ds_loss: 0.0000 | lr: 4.9197e-05 | scale: 16384.0000 | micro time: 1.824 | step time: 0.000
train | epoch   0 | Iter:   1072/ 13000 | global iter:   1072/ 13000 | loss: 2.3825 | ds_loss: 0.0000 | lr: 4.9195e-05 | scale: 16384.0000 | micro time: 1.852 | step time: 0.000
train | epoch   0 | Iter:   1073/ 13000 | global iter:   1073/ 13000 | loss: 2.9025 | ds_loss: 0.0000 | lr: 4.9193e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   0 | Iter:   1074/ 13000 | global iter:   1074/ 13000 | loss: 2.2004 | ds_loss: 0.0000 | lr: 4.9192e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   0 | Iter:   1075/ 13000 | global iter:   1075/ 13000 | loss: 2.6841 | ds_loss: 0.0000 | lr: 4.9190e-05 | scale: 16384.0000 | micro time: 1.879 | step time: 0.000
train | epoch   0 | Iter:   1076/ 13000 | global iter:   1076/ 13000 | loss: 2.4520 | ds_loss: 0.0000 | lr: 4.9189e-05 | scale: 16384.0000 | micro time: 1.814 | step time: 0.000
train | epoch   0 | Iter:   1077/ 13000 | global iter:   1077/ 13000 | loss: 2.6641 | ds_loss: 0.0000 | lr: 4.9187e-05 | scale: 16384.0000 | micro time: 1.840 | step time: 0.000
train | epoch   0 | Iter:   1078/ 13000 | global iter:   1078/ 13000 | loss: 2.5419 | ds_loss: 0.0000 | lr: 4.9186e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   0 | Iter:   1079/ 13000 | global iter:   1079/ 13000 | loss: 2.8429 | ds_loss: 0.0000 | lr: 4.9184e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   0 | Iter:   1080/ 13000 | global iter:   1080/ 13000 | loss: 2.7574 | ds_loss: 0.0000 | lr: 4.9183e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:   1080/ 13000 | global iter:   1080/ 13000 | loss: 2.5883 | ds_loss: 0.0000 | lr: 4.9183e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 1.841
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:   1081/ 13000 | global iter:   1081/ 13000 | loss: 2.4996 | ds_loss: 0.0000 | lr: 4.9181e-05 | scale: 16384.0000 | micro time: 1.810 | step time: 0.000
train | epoch   0 | Iter:   1082/ 13000 | global iter:   1082/ 13000 | loss: 2.4445 | ds_loss: 0.0000 | lr: 4.9180e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   0 | Iter:   1083/ 13000 | global iter:   1083/ 13000 | loss: 2.8337 | ds_loss: 0.0000 | lr: 4.9178e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   0 | Iter:   1084/ 13000 | global iter:   1084/ 13000 | loss: 2.6633 | ds_loss: 0.0000 | lr: 4.9177e-05 | scale: 16384.0000 | micro time: 1.899 | step time: 0.000
train | epoch   0 | Iter:   1085/ 13000 | global iter:   1085/ 13000 | loss: 2.2240 | ds_loss: 0.0000 | lr: 4.9175e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   0 | Iter:   1086/ 13000 | global iter:   1086/ 13000 | loss: 2.5760 | ds_loss: 0.0000 | lr: 4.9174e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   0 | Iter:   1087/ 13000 | global iter:   1087/ 13000 | loss: 2.1314 | ds_loss: 0.0000 | lr: 4.9172e-05 | scale: 16384.0000 | micro time: 1.727 | step time: 0.000
train | epoch   0 | Iter:   1088/ 13000 | global iter:   1088/ 13000 | loss: 2.0875 | ds_loss: 0.0000 | lr: 4.9171e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   0 | Iter:   1089/ 13000 | global iter:   1089/ 13000 | loss: 2.1336 | ds_loss: 0.0000 | lr: 4.9169e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   0 | Iter:   1090/ 13000 | global iter:   1090/ 13000 | loss: 2.5524 | ds_loss: 0.0000 | lr: 4.9167e-05 | scale: 16384.0000 | micro time: 1.711 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:   1090/ 13000 | global iter:   1090/ 13000 | loss: 2.4146 | ds_loss: 0.0000 | lr: 4.9167e-05 | scale: 16384.0000 | micro time: 1.711 | step time: 1.788
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:   1091/ 13000 | global iter:   1091/ 13000 | loss: 2.8995 | ds_loss: 0.0000 | lr: 4.9166e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 0.000
train | epoch   0 | Iter:   1092/ 13000 | global iter:   1092/ 13000 | loss: 2.4180 | ds_loss: 0.0000 | lr: 4.9164e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   0 | Iter:   1093/ 13000 | global iter:   1093/ 13000 | loss: 2.3705 | ds_loss: 0.0000 | lr: 4.9163e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   0 | Iter:   1094/ 13000 | global iter:   1094/ 13000 | loss: 2.0701 | ds_loss: 0.0000 | lr: 4.9161e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   0 | Iter:   1095/ 13000 | global iter:   1095/ 13000 | loss: 2.0165 | ds_loss: 0.0000 | lr: 4.9160e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   0 | Iter:   1096/ 13000 | global iter:   1096/ 13000 | loss: 2.6489 | ds_loss: 0.0000 | lr: 4.9158e-05 | scale: 16384.0000 | micro time: 1.728 | step time: 0.000
train | epoch   0 | Iter:   1097/ 13000 | global iter:   1097/ 13000 | loss: 2.0248 | ds_loss: 0.0000 | lr: 4.9157e-05 | scale: 16384.0000 | micro time: 1.729 | step time: 0.000
train | epoch   0 | Iter:   1098/ 13000 | global iter:   1098/ 13000 | loss: 2.4661 | ds_loss: 0.0000 | lr: 4.9155e-05 | scale: 16384.0000 | micro time: 1.798 | step time: 0.000
train | epoch   0 | Iter:   1099/ 13000 | global iter:   1099/ 13000 | loss: 2.1341 | ds_loss: 0.0000 | lr: 4.9153e-05 | scale: 16384.0000 | micro time: 1.850 | step time: 0.000
train | epoch   0 | Iter:   1100/ 13000 | global iter:   1100/ 13000 | loss: 2.5602 | ds_loss: 0.0000 | lr: 4.9152e-05 | scale: 16384.0000 | micro time: 1.838 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:   1100/ 13000 | global iter:   1100/ 13000 | loss: 2.3609 | ds_loss: 0.0000 | lr: 4.9152e-05 | scale: 16384.0000 | micro time: 1.838 | step time: 1.787
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:   1101/ 13000 | global iter:   1101/ 13000 | loss: 2.8349 | ds_loss: 0.0000 | lr: 4.9150e-05 | scale: 16384.0000 | micro time: 1.852 | step time: 0.000
train | epoch   0 | Iter:   1102/ 13000 | global iter:   1102/ 13000 | loss: 2.2348 | ds_loss: 0.0000 | lr: 4.9149e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   0 | Iter:   1103/ 13000 | global iter:   1103/ 13000 | loss: 2.7503 | ds_loss: 0.0000 | lr: 4.9147e-05 | scale: 16384.0000 | micro time: 1.748 | step time: 0.000
train | epoch   0 | Iter:   1104/ 13000 | global iter:   1104/ 13000 | loss: 2.5840 | ds_loss: 0.0000 | lr: 4.9146e-05 | scale: 16384.0000 | micro time: 1.840 | step time: 0.000
train | epoch   0 | Iter:   1105/ 13000 | global iter:   1105/ 13000 | loss: 2.5639 | ds_loss: 0.0000 | lr: 4.9144e-05 | scale: 16384.0000 | micro time: 1.864 | step time: 0.000
train | epoch   0 | Iter:   1106/ 13000 | global iter:   1106/ 13000 | loss: 2.5998 | ds_loss: 0.0000 | lr: 4.9143e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   0 | Iter:   1107/ 13000 | global iter:   1107/ 13000 | loss: 2.8460 | ds_loss: 0.0000 | lr: 4.9141e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   0 | Iter:   1108/ 13000 | global iter:   1108/ 13000 | loss: 2.1815 | ds_loss: 0.0000 | lr: 4.9139e-05 | scale: 16384.0000 | micro time: 1.890 | step time: 0.000
train | epoch   0 | Iter:   1109/ 13000 | global iter:   1109/ 13000 | loss: 1.9684 | ds_loss: 0.0000 | lr: 4.9138e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   0 | Iter:   1110/ 13000 | global iter:   1110/ 13000 | loss: 3.2752 | ds_loss: 0.0000 | lr: 4.9136e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:   1110/ 13000 | global iter:   1110/ 13000 | loss: 2.5839 | ds_loss: 0.0000 | lr: 4.9136e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 1.817
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:   1111/ 13000 | global iter:   1111/ 13000 | loss: 2.9751 | ds_loss: 0.0000 | lr: 4.9135e-05 | scale: 16384.0000 | micro time: 1.813 | step time: 0.000
train | epoch   0 | Iter:   1112/ 13000 | global iter:   1112/ 13000 | loss: 3.0034 | ds_loss: 0.0000 | lr: 4.9133e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   0 | Iter:   1113/ 13000 | global iter:   1113/ 13000 | loss: 2.5202 | ds_loss: 0.0000 | lr: 4.9132e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   0 | Iter:   1114/ 13000 | global iter:   1114/ 13000 | loss: 2.0436 | ds_loss: 0.0000 | lr: 4.9130e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   0 | Iter:   1115/ 13000 | global iter:   1115/ 13000 | loss: 3.0897 | ds_loss: 0.0000 | lr: 4.9128e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   0 | Iter:   1116/ 13000 | global iter:   1116/ 13000 | loss: 2.6537 | ds_loss: 0.0000 | lr: 4.9127e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   0 | Iter:   1117/ 13000 | global iter:   1117/ 13000 | loss: 2.5272 | ds_loss: 0.0000 | lr: 4.9125e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   0 | Iter:   1118/ 13000 | global iter:   1118/ 13000 | loss: 2.3065 | ds_loss: 0.0000 | lr: 4.9124e-05 | scale: 16384.0000 | micro time: 1.731 | step time: 0.000
train | epoch   0 | Iter:   1119/ 13000 | global iter:   1119/ 13000 | loss: 2.5441 | ds_loss: 0.0000 | lr: 4.9122e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   0 | Iter:   1120/ 13000 | global iter:   1120/ 13000 | loss: 2.6566 | ds_loss: 0.0000 | lr: 4.9120e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:   1120/ 13000 | global iter:   1120/ 13000 | loss: 2.6320 | ds_loss: 0.0000 | lr: 4.9120e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 1.790
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:   1121/ 13000 | global iter:   1121/ 13000 | loss: 2.8003 | ds_loss: 0.0000 | lr: 4.9119e-05 | scale: 16384.0000 | micro time: 1.817 | step time: 0.000
train | epoch   0 | Iter:   1122/ 13000 | global iter:   1122/ 13000 | loss: 1.8267 | ds_loss: 0.0000 | lr: 4.9117e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   0 | Iter:   1123/ 13000 | global iter:   1123/ 13000 | loss: 2.3610 | ds_loss: 0.0000 | lr: 4.9116e-05 | scale: 16384.0000 | micro time: 1.794 | step time: 0.000
train | epoch   0 | Iter:   1124/ 13000 | global iter:   1124/ 13000 | loss: 2.7249 | ds_loss: 0.0000 | lr: 4.9114e-05 | scale: 16384.0000 | micro time: 1.800 | step time: 0.000
train | epoch   0 | Iter:   1125/ 13000 | global iter:   1125/ 13000 | loss: 2.3015 | ds_loss: 0.0000 | lr: 4.9113e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   0 | Iter:   1126/ 13000 | global iter:   1126/ 13000 | loss: 2.5353 | ds_loss: 0.0000 | lr: 4.9111e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   0 | Iter:   1127/ 13000 | global iter:   1127/ 13000 | loss: 2.5273 | ds_loss: 0.0000 | lr: 4.9109e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   0 | Iter:   1128/ 13000 | global iter:   1128/ 13000 | loss: 2.5976 | ds_loss: 0.0000 | lr: 4.9108e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   0 | Iter:   1129/ 13000 | global iter:   1129/ 13000 | loss: 2.2945 | ds_loss: 0.0000 | lr: 4.9106e-05 | scale: 16384.0000 | micro time: 1.850 | step time: 0.000
train | epoch   0 | Iter:   1130/ 13000 | global iter:   1130/ 13000 | loss: 2.8957 | ds_loss: 0.0000 | lr: 4.9105e-05 | scale: 16384.0000 | micro time: 1.792 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:   1130/ 13000 | global iter:   1130/ 13000 | loss: 2.4865 | ds_loss: 0.0000 | lr: 4.9105e-05 | scale: 16384.0000 | micro time: 1.792 | step time: 1.811
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:   1131/ 13000 | global iter:   1131/ 13000 | loss: 2.2422 | ds_loss: 0.0000 | lr: 4.9103e-05 | scale: 16384.0000 | micro time: 1.726 | step time: 0.000
train | epoch   0 | Iter:   1132/ 13000 | global iter:   1132/ 13000 | loss: 2.3029 | ds_loss: 0.0000 | lr: 4.9101e-05 | scale: 16384.0000 | micro time: 1.768 | step time: 0.000
train | epoch   0 | Iter:   1133/ 13000 | global iter:   1133/ 13000 | loss: 2.5679 | ds_loss: 0.0000 | lr: 4.9100e-05 | scale: 16384.0000 | micro time: 1.826 | step time: 0.000
train | epoch   0 | Iter:   1134/ 13000 | global iter:   1134/ 13000 | loss: 2.0644 | ds_loss: 0.0000 | lr: 4.9098e-05 | scale: 16384.0000 | micro time: 1.711 | step time: 0.000
train | epoch   0 | Iter:   1135/ 13000 | global iter:   1135/ 13000 | loss: 1.9706 | ds_loss: 0.0000 | lr: 4.9097e-05 | scale: 16384.0000 | micro time: 1.723 | step time: 0.000
train | epoch   0 | Iter:   1136/ 13000 | global iter:   1136/ 13000 | loss: 2.3290 | ds_loss: 0.0000 | lr: 4.9095e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   0 | Iter:   1137/ 13000 | global iter:   1137/ 13000 | loss: 2.5497 | ds_loss: 0.0000 | lr: 4.9093e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   0 | Iter:   1138/ 13000 | global iter:   1138/ 13000 | loss: 2.7488 | ds_loss: 0.0000 | lr: 4.9092e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   0 | Iter:   1139/ 13000 | global iter:   1139/ 13000 | loss: 2.6905 | ds_loss: 0.0000 | lr: 4.9090e-05 | scale: 16384.0000 | micro time: 1.731 | step time: 0.000
train | epoch   0 | Iter:   1140/ 13000 | global iter:   1140/ 13000 | loss: 2.9520 | ds_loss: 0.0000 | lr: 4.9088e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:   1140/ 13000 | global iter:   1140/ 13000 | loss: 2.4418 | ds_loss: 0.0000 | lr: 4.9088e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 1.784
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:   1141/ 13000 | global iter:   1141/ 13000 | loss: 2.5740 | ds_loss: 0.0000 | lr: 4.9087e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 0.000
train | epoch   0 | Iter:   1142/ 13000 | global iter:   1142/ 13000 | loss: 2.1489 | ds_loss: 0.0000 | lr: 4.9085e-05 | scale: 16384.0000 | micro time: 1.899 | step time: 0.000
train | epoch   0 | Iter:   1143/ 13000 | global iter:   1143/ 13000 | loss: 2.7148 | ds_loss: 0.0000 | lr: 4.9084e-05 | scale: 16384.0000 | micro time: 1.809 | step time: 0.000
train | epoch   0 | Iter:   1144/ 13000 | global iter:   1144/ 13000 | loss: 2.5795 | ds_loss: 0.0000 | lr: 4.9082e-05 | scale: 16384.0000 | micro time: 1.848 | step time: 0.000
train | epoch   0 | Iter:   1145/ 13000 | global iter:   1145/ 13000 | loss: 2.5403 | ds_loss: 0.0000 | lr: 4.9080e-05 | scale: 16384.0000 | micro time: 1.806 | step time: 0.000
train | epoch   0 | Iter:   1146/ 13000 | global iter:   1146/ 13000 | loss: 2.5028 | ds_loss: 0.0000 | lr: 4.9079e-05 | scale: 16384.0000 | micro time: 1.758 | step time: 0.000
train | epoch   0 | Iter:   1147/ 13000 | global iter:   1147/ 13000 | loss: 2.3848 | ds_loss: 0.0000 | lr: 4.9077e-05 | scale: 16384.0000 | micro time: 1.834 | step time: 0.000
train | epoch   0 | Iter:   1148/ 13000 | global iter:   1148/ 13000 | loss: 2.2646 | ds_loss: 0.0000 | lr: 4.9075e-05 | scale: 16384.0000 | micro time: 1.854 | step time: 0.000
train | epoch   0 | Iter:   1149/ 13000 | global iter:   1149/ 13000 | loss: 2.2484 | ds_loss: 0.0000 | lr: 4.9074e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   0 | Iter:   1150/ 13000 | global iter:   1150/ 13000 | loss: 2.0786 | ds_loss: 0.0000 | lr: 4.9072e-05 | scale: 16384.0000 | micro time: 1.882 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:   1150/ 13000 | global iter:   1150/ 13000 | loss: 2.4037 | ds_loss: 0.0000 | lr: 4.9072e-05 | scale: 16384.0000 | micro time: 1.882 | step time: 1.818
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:   1151/ 13000 | global iter:   1151/ 13000 | loss: 2.4674 | ds_loss: 0.0000 | lr: 4.9071e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   0 | Iter:   1152/ 13000 | global iter:   1152/ 13000 | loss: 2.3301 | ds_loss: 0.0000 | lr: 4.9069e-05 | scale: 16384.0000 | micro time: 1.833 | step time: 0.000
train | epoch   0 | Iter:   1153/ 13000 | global iter:   1153/ 13000 | loss: 2.2999 | ds_loss: 0.0000 | lr: 4.9067e-05 | scale: 16384.0000 | micro time: 1.773 | step time: 0.000
train | epoch   0 | Iter:   1154/ 13000 | global iter:   1154/ 13000 | loss: 2.1848 | ds_loss: 0.0000 | lr: 4.9066e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   0 | Iter:   1155/ 13000 | global iter:   1155/ 13000 | loss: 2.4139 | ds_loss: 0.0000 | lr: 4.9064e-05 | scale: 16384.0000 | micro time: 1.723 | step time: 0.000
train | epoch   0 | Iter:   1156/ 13000 | global iter:   1156/ 13000 | loss: 2.5767 | ds_loss: 0.0000 | lr: 4.9062e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   0 | Iter:   1157/ 13000 | global iter:   1157/ 13000 | loss: 2.6915 | ds_loss: 0.0000 | lr: 4.9061e-05 | scale: 16384.0000 | micro time: 1.719 | step time: 0.000
train | epoch   0 | Iter:   1158/ 13000 | global iter:   1158/ 13000 | loss: 2.2113 | ds_loss: 0.0000 | lr: 4.9059e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   0 | Iter:   1159/ 13000 | global iter:   1159/ 13000 | loss: 2.2490 | ds_loss: 0.0000 | lr: 4.9058e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   0 | Iter:   1160/ 13000 | global iter:   1160/ 13000 | loss: 2.3601 | ds_loss: 0.0000 | lr: 4.9056e-05 | scale: 16384.0000 | micro time: 1.841 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:   1160/ 13000 | global iter:   1160/ 13000 | loss: 2.3785 | ds_loss: 0.0000 | lr: 4.9056e-05 | scale: 16384.0000 | micro time: 1.841 | step time: 1.803
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:   1161/ 13000 | global iter:   1161/ 13000 | loss: 3.0437 | ds_loss: 0.0000 | lr: 4.9054e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   0 | Iter:   1162/ 13000 | global iter:   1162/ 13000 | loss: 2.4629 | ds_loss: 0.0000 | lr: 4.9053e-05 | scale: 16384.0000 | micro time: 1.631 | step time: 0.000
train | epoch   0 | Iter:   1163/ 13000 | global iter:   1163/ 13000 | loss: 2.5655 | ds_loss: 0.0000 | lr: 4.9051e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   0 | Iter:   1164/ 13000 | global iter:   1164/ 13000 | loss: 2.3873 | ds_loss: 0.0000 | lr: 4.9049e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   0 | Iter:   1165/ 13000 | global iter:   1165/ 13000 | loss: 2.5898 | ds_loss: 0.0000 | lr: 4.9048e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   0 | Iter:   1166/ 13000 | global iter:   1166/ 13000 | loss: 2.5073 | ds_loss: 0.0000 | lr: 4.9046e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   0 | Iter:   1167/ 13000 | global iter:   1167/ 13000 | loss: 2.3332 | ds_loss: 0.0000 | lr: 4.9044e-05 | scale: 16384.0000 | micro time: 1.785 | step time: 0.000
train | epoch   0 | Iter:   1168/ 13000 | global iter:   1168/ 13000 | loss: 2.7838 | ds_loss: 0.0000 | lr: 4.9043e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   0 | Iter:   1169/ 13000 | global iter:   1169/ 13000 | loss: 2.6082 | ds_loss: 0.0000 | lr: 4.9041e-05 | scale: 16384.0000 | micro time: 1.713 | step time: 0.000
train | epoch   0 | Iter:   1170/ 13000 | global iter:   1170/ 13000 | loss: 2.0743 | ds_loss: 0.0000 | lr: 4.9039e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:   1170/ 13000 | global iter:   1170/ 13000 | loss: 2.5356 | ds_loss: 0.0000 | lr: 4.9039e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 1.775
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:   1171/ 13000 | global iter:   1171/ 13000 | loss: 2.1108 | ds_loss: 0.0000 | lr: 4.9038e-05 | scale: 16384.0000 | micro time: 1.874 | step time: 0.000
train | epoch   0 | Iter:   1172/ 13000 | global iter:   1172/ 13000 | loss: 2.1715 | ds_loss: 0.0000 | lr: 4.9036e-05 | scale: 16384.0000 | micro time: 1.810 | step time: 0.000
train | epoch   0 | Iter:   1173/ 13000 | global iter:   1173/ 13000 | loss: 2.7009 | ds_loss: 0.0000 | lr: 4.9034e-05 | scale: 16384.0000 | micro time: 1.793 | step time: 0.000
train | epoch   0 | Iter:   1174/ 13000 | global iter:   1174/ 13000 | loss: 2.2987 | ds_loss: 0.0000 | lr: 4.9033e-05 | scale: 16384.0000 | micro time: 1.762 | step time: 0.000
train | epoch   0 | Iter:   1175/ 13000 | global iter:   1175/ 13000 | loss: 2.3554 | ds_loss: 0.0000 | lr: 4.9031e-05 | scale: 16384.0000 | micro time: 1.742 | step time: 0.000
train | epoch   0 | Iter:   1176/ 13000 | global iter:   1176/ 13000 | loss: 2.5562 | ds_loss: 0.0000 | lr: 4.9029e-05 | scale: 16384.0000 | micro time: 1.756 | step time: 0.000
train | epoch   0 | Iter:   1177/ 13000 | global iter:   1177/ 13000 | loss: 2.2674 | ds_loss: 0.0000 | lr: 4.9028e-05 | scale: 16384.0000 | micro time: 1.717 | step time: 0.000
train | epoch   0 | Iter:   1178/ 13000 | global iter:   1178/ 13000 | loss: 2.8550 | ds_loss: 0.0000 | lr: 4.9026e-05 | scale: 16384.0000 | micro time: 1.727 | step time: 0.000
train | epoch   0 | Iter:   1179/ 13000 | global iter:   1179/ 13000 | loss: 2.4147 | ds_loss: 0.0000 | lr: 4.9024e-05 | scale: 16384.0000 | micro time: 1.781 | step time: 0.000
train | epoch   0 | Iter:   1180/ 13000 | global iter:   1180/ 13000 | loss: 2.5583 | ds_loss: 0.0000 | lr: 4.9023e-05 | scale: 16384.0000 | micro time: 1.758 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:   1180/ 13000 | global iter:   1180/ 13000 | loss: 2.4289 | ds_loss: 0.0000 | lr: 4.9023e-05 | scale: 16384.0000 | micro time: 1.758 | step time: 1.772
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:   1181/ 13000 | global iter:   1181/ 13000 | loss: 1.8388 | ds_loss: 0.0000 | lr: 4.9021e-05 | scale: 16384.0000 | micro time: 1.822 | step time: 0.000
train | epoch   0 | Iter:   1182/ 13000 | global iter:   1182/ 13000 | loss: 2.3802 | ds_loss: 0.0000 | lr: 4.9019e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   0 | Iter:   1183/ 13000 | global iter:   1183/ 13000 | loss: 2.3422 | ds_loss: 0.0000 | lr: 4.9018e-05 | scale: 16384.0000 | micro time: 1.830 | step time: 0.000
train | epoch   0 | Iter:   1184/ 13000 | global iter:   1184/ 13000 | loss: 2.6124 | ds_loss: 0.0000 | lr: 4.9016e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   0 | Iter:   1185/ 13000 | global iter:   1185/ 13000 | loss: 2.5680 | ds_loss: 0.0000 | lr: 4.9014e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   0 | Iter:   1186/ 13000 | global iter:   1186/ 13000 | loss: 1.8478 | ds_loss: 0.0000 | lr: 4.9013e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   0 | Iter:   1187/ 13000 | global iter:   1187/ 13000 | loss: 2.9464 | ds_loss: 0.0000 | lr: 4.9011e-05 | scale: 16384.0000 | micro time: 1.776 | step time: 0.000
train | epoch   0 | Iter:   1188/ 13000 | global iter:   1188/ 13000 | loss: 2.5952 | ds_loss: 0.0000 | lr: 4.9009e-05 | scale: 16384.0000 | micro time: 1.758 | step time: 0.000
train | epoch   0 | Iter:   1189/ 13000 | global iter:   1189/ 13000 | loss: 2.7215 | ds_loss: 0.0000 | lr: 4.9008e-05 | scale: 16384.0000 | micro time: 1.748 | step time: 0.000
train | epoch   0 | Iter:   1190/ 13000 | global iter:   1190/ 13000 | loss: 2.1177 | ds_loss: 0.0000 | lr: 4.9006e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:   1190/ 13000 | global iter:   1190/ 13000 | loss: 2.3970 | ds_loss: 0.0000 | lr: 4.9006e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 1.802
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:   1191/ 13000 | global iter:   1191/ 13000 | loss: 2.7825 | ds_loss: 0.0000 | lr: 4.9004e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   0 | Iter:   1192/ 13000 | global iter:   1192/ 13000 | loss: 2.3898 | ds_loss: 0.0000 | lr: 4.9003e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 0.000
train | epoch   0 | Iter:   1193/ 13000 | global iter:   1193/ 13000 | loss: 2.3730 | ds_loss: 0.0000 | lr: 4.9001e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   0 | Iter:   1194/ 13000 | global iter:   1194/ 13000 | loss: 2.4479 | ds_loss: 0.0000 | lr: 4.8999e-05 | scale: 16384.0000 | micro time: 1.728 | step time: 0.000
train | epoch   0 | Iter:   1195/ 13000 | global iter:   1195/ 13000 | loss: 2.0962 | ds_loss: 0.0000 | lr: 4.8998e-05 | scale: 16384.0000 | micro time: 1.842 | step time: 0.000
train | epoch   0 | Iter:   1196/ 13000 | global iter:   1196/ 13000 | loss: 3.3220 | ds_loss: 0.0000 | lr: 4.8996e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   0 | Iter:   1197/ 13000 | global iter:   1197/ 13000 | loss: 2.4656 | ds_loss: 0.0000 | lr: 4.8994e-05 | scale: 16384.0000 | micro time: 1.833 | step time: 0.000
train | epoch   0 | Iter:   1198/ 13000 | global iter:   1198/ 13000 | loss: 2.3000 | ds_loss: 0.0000 | lr: 4.8992e-05 | scale: 16384.0000 | micro time: 1.866 | step time: 0.000
train | epoch   0 | Iter:   1199/ 13000 | global iter:   1199/ 13000 | loss: 2.5735 | ds_loss: 0.0000 | lr: 4.8991e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   0 | Iter:   1200/ 13000 | global iter:   1200/ 13000 | loss: 2.3184 | ds_loss: 0.0000 | lr: 4.8989e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:   1200/ 13000 | global iter:   1200/ 13000 | loss: 2.5069 | ds_loss: 0.0000 | lr: 4.8989e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 1.802
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:   1201/ 13000 | global iter:   1201/ 13000 | loss: 2.5928 | ds_loss: 0.0000 | lr: 4.8987e-05 | scale: 16384.0000 | micro time: 1.825 | step time: 0.000
train | epoch   0 | Iter:   1202/ 13000 | global iter:   1202/ 13000 | loss: 2.3452 | ds_loss: 0.0000 | lr: 4.8986e-05 | scale: 16384.0000 | micro time: 1.818 | step time: 0.000
train | epoch   0 | Iter:   1203/ 13000 | global iter:   1203/ 13000 | loss: 2.4397 | ds_loss: 0.0000 | lr: 4.8984e-05 | scale: 16384.0000 | micro time: 1.696 | step time: 0.000
train | epoch   0 | Iter:   1204/ 13000 | global iter:   1204/ 13000 | loss: 2.6400 | ds_loss: 0.0000 | lr: 4.8982e-05 | scale: 16384.0000 | micro time: 1.770 | step time: 0.000
train | epoch   0 | Iter:   1205/ 13000 | global iter:   1205/ 13000 | loss: 2.6866 | ds_loss: 0.0000 | lr: 4.8981e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   0 | Iter:   1206/ 13000 | global iter:   1206/ 13000 | loss: 2.1987 | ds_loss: 0.0000 | lr: 4.8979e-05 | scale: 16384.0000 | micro time: 1.743 | step time: 0.000
train | epoch   0 | Iter:   1207/ 13000 | global iter:   1207/ 13000 | loss: 2.0671 | ds_loss: 0.0000 | lr: 4.8977e-05 | scale: 16384.0000 | micro time: 1.758 | step time: 0.000
train | epoch   0 | Iter:   1208/ 13000 | global iter:   1208/ 13000 | loss: 2.2441 | ds_loss: 0.0000 | lr: 4.8975e-05 | scale: 16384.0000 | micro time: 1.852 | step time: 0.000
train | epoch   0 | Iter:   1209/ 13000 | global iter:   1209/ 13000 | loss: 2.1337 | ds_loss: 0.0000 | lr: 4.8974e-05 | scale: 16384.0000 | micro time: 1.798 | step time: 0.000
train | epoch   0 | Iter:   1210/ 13000 | global iter:   1210/ 13000 | loss: 2.4420 | ds_loss: 0.0000 | lr: 4.8972e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:   1210/ 13000 | global iter:   1210/ 13000 | loss: 2.3790 | ds_loss: 0.0000 | lr: 4.8972e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 1.789
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:   1211/ 13000 | global iter:   1211/ 13000 | loss: 2.6920 | ds_loss: 0.0000 | lr: 4.8970e-05 | scale: 16384.0000 | micro time: 1.830 | step time: 0.000
train | epoch   0 | Iter:   1212/ 13000 | global iter:   1212/ 13000 | loss: 1.9341 | ds_loss: 0.0000 | lr: 4.8969e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   0 | Iter:   1213/ 13000 | global iter:   1213/ 13000 | loss: 2.6365 | ds_loss: 0.0000 | lr: 4.8967e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   0 | Iter:   1214/ 13000 | global iter:   1214/ 13000 | loss: 2.7104 | ds_loss: 0.0000 | lr: 4.8965e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   0 | Iter:   1215/ 13000 | global iter:   1215/ 13000 | loss: 2.5894 | ds_loss: 0.0000 | lr: 4.8963e-05 | scale: 16384.0000 | micro time: 1.749 | step time: 0.000
train | epoch   0 | Iter:   1216/ 13000 | global iter:   1216/ 13000 | loss: 2.3984 | ds_loss: 0.0000 | lr: 4.8962e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   0 | Iter:   1217/ 13000 | global iter:   1217/ 13000 | loss: 2.1345 | ds_loss: 0.0000 | lr: 4.8960e-05 | scale: 16384.0000 | micro time: 1.797 | step time: 0.000
train | epoch   0 | Iter:   1218/ 13000 | global iter:   1218/ 13000 | loss: 2.8682 | ds_loss: 0.0000 | lr: 4.8958e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   0 | Iter:   1219/ 13000 | global iter:   1219/ 13000 | loss: 2.6708 | ds_loss: 0.0000 | lr: 4.8957e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 0.000
train | epoch   0 | Iter:   1220/ 13000 | global iter:   1220/ 13000 | loss: 2.4576 | ds_loss: 0.0000 | lr: 4.8955e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:   1220/ 13000 | global iter:   1220/ 13000 | loss: 2.5092 | ds_loss: 0.0000 | lr: 4.8955e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 1.794
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:   1221/ 13000 | global iter:   1221/ 13000 | loss: 2.5484 | ds_loss: 0.0000 | lr: 4.8953e-05 | scale: 16384.0000 | micro time: 1.714 | step time: 0.000
train | epoch   0 | Iter:   1222/ 13000 | global iter:   1222/ 13000 | loss: 2.3878 | ds_loss: 0.0000 | lr: 4.8951e-05 | scale: 16384.0000 | micro time: 1.727 | step time: 0.000
train | epoch   0 | Iter:   1223/ 13000 | global iter:   1223/ 13000 | loss: 2.8010 | ds_loss: 0.0000 | lr: 4.8950e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   0 | Iter:   1224/ 13000 | global iter:   1224/ 13000 | loss: 2.8178 | ds_loss: 0.0000 | lr: 4.8948e-05 | scale: 16384.0000 | micro time: 1.826 | step time: 0.000
train | epoch   0 | Iter:   1225/ 13000 | global iter:   1225/ 13000 | loss: 1.9354 | ds_loss: 0.0000 | lr: 4.8946e-05 | scale: 16384.0000 | micro time: 1.856 | step time: 0.000
train | epoch   0 | Iter:   1226/ 13000 | global iter:   1226/ 13000 | loss: 2.5431 | ds_loss: 0.0000 | lr: 4.8944e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   0 | Iter:   1227/ 13000 | global iter:   1227/ 13000 | loss: 2.6418 | ds_loss: 0.0000 | lr: 4.8943e-05 | scale: 16384.0000 | micro time: 1.898 | step time: 0.000
train | epoch   0 | Iter:   1228/ 13000 | global iter:   1228/ 13000 | loss: 2.8682 | ds_loss: 0.0000 | lr: 4.8941e-05 | scale: 16384.0000 | micro time: 1.784 | step time: 0.000
train | epoch   0 | Iter:   1229/ 13000 | global iter:   1229/ 13000 | loss: 2.5825 | ds_loss: 0.0000 | lr: 4.8939e-05 | scale: 16384.0000 | micro time: 1.852 | step time: 0.000
train | epoch   0 | Iter:   1230/ 13000 | global iter:   1230/ 13000 | loss: 2.4150 | ds_loss: 0.0000 | lr: 4.8937e-05 | scale: 16384.0000 | micro time: 1.818 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:   1230/ 13000 | global iter:   1230/ 13000 | loss: 2.5541 | ds_loss: 0.0000 | lr: 4.8937e-05 | scale: 16384.0000 | micro time: 1.818 | step time: 1.804
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:   1231/ 13000 | global iter:   1231/ 13000 | loss: 1.9511 | ds_loss: 0.0000 | lr: 4.8936e-05 | scale: 16384.0000 | micro time: 1.861 | step time: 0.000
train | epoch   0 | Iter:   1232/ 13000 | global iter:   1232/ 13000 | loss: 2.1453 | ds_loss: 0.0000 | lr: 4.8934e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   0 | Iter:   1233/ 13000 | global iter:   1233/ 13000 | loss: 2.9085 | ds_loss: 0.0000 | lr: 4.8932e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   0 | Iter:   1234/ 13000 | global iter:   1234/ 13000 | loss: 2.4006 | ds_loss: 0.0000 | lr: 4.8930e-05 | scale: 16384.0000 | micro time: 1.840 | step time: 0.000
train | epoch   0 | Iter:   1235/ 13000 | global iter:   1235/ 13000 | loss: 2.0664 | ds_loss: 0.0000 | lr: 4.8929e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   0 | Iter:   1236/ 13000 | global iter:   1236/ 13000 | loss: 2.6250 | ds_loss: 0.0000 | lr: 4.8927e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   0 | Iter:   1237/ 13000 | global iter:   1237/ 13000 | loss: 2.1528 | ds_loss: 0.0000 | lr: 4.8925e-05 | scale: 16384.0000 | micro time: 1.786 | step time: 0.000
train | epoch   0 | Iter:   1238/ 13000 | global iter:   1238/ 13000 | loss: 2.2509 | ds_loss: 0.0000 | lr: 4.8923e-05 | scale: 16384.0000 | micro time: 1.760 | step time: 0.000
train | epoch   0 | Iter:   1239/ 13000 | global iter:   1239/ 13000 | loss: 1.3745 | ds_loss: 0.0000 | lr: 4.8922e-05 | scale: 16384.0000 | micro time: 1.717 | step time: 0.000
train | epoch   0 | Iter:   1240/ 13000 | global iter:   1240/ 13000 | loss: 2.4777 | ds_loss: 0.0000 | lr: 4.8920e-05 | scale: 16384.0000 | micro time: 1.801 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:   1240/ 13000 | global iter:   1240/ 13000 | loss: 2.2353 | ds_loss: 0.0000 | lr: 4.8920e-05 | scale: 16384.0000 | micro time: 1.801 | step time: 1.802
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:   1241/ 13000 | global iter:   1241/ 13000 | loss: 1.7233 | ds_loss: 0.0000 | lr: 4.8918e-05 | scale: 16384.0000 | micro time: 1.736 | step time: 0.000
train | epoch   0 | Iter:   1242/ 13000 | global iter:   1242/ 13000 | loss: 2.9217 | ds_loss: 0.0000 | lr: 4.8916e-05 | scale: 16384.0000 | micro time: 1.770 | step time: 0.000
train | epoch   0 | Iter:   1243/ 13000 | global iter:   1243/ 13000 | loss: 1.7511 | ds_loss: 0.0000 | lr: 4.8915e-05 | scale: 16384.0000 | micro time: 1.828 | step time: 0.000
train | epoch   0 | Iter:   1244/ 13000 | global iter:   1244/ 13000 | loss: 2.5652 | ds_loss: 0.0000 | lr: 4.8913e-05 | scale: 16384.0000 | micro time: 1.833 | step time: 0.000
train | epoch   0 | Iter:   1245/ 13000 | global iter:   1245/ 13000 | loss: 1.8963 | ds_loss: 0.0000 | lr: 4.8911e-05 | scale: 16384.0000 | micro time: 1.785 | step time: 0.000
train | epoch   0 | Iter:   1246/ 13000 | global iter:   1246/ 13000 | loss: 2.2093 | ds_loss: 0.0000 | lr: 4.8909e-05 | scale: 16384.0000 | micro time: 1.772 | step time: 0.000
train | epoch   0 | Iter:   1247/ 13000 | global iter:   1247/ 13000 | loss: 2.2442 | ds_loss: 0.0000 | lr: 4.8908e-05 | scale: 16384.0000 | micro time: 1.849 | step time: 0.000
train | epoch   0 | Iter:   1248/ 13000 | global iter:   1248/ 13000 | loss: 2.2208 | ds_loss: 0.0000 | lr: 4.8906e-05 | scale: 16384.0000 | micro time: 1.901 | step time: 0.000
train | epoch   0 | Iter:   1249/ 13000 | global iter:   1249/ 13000 | loss: 2.7557 | ds_loss: 0.0000 | lr: 4.8904e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   0 | Iter:   1250/ 13000 | global iter:   1250/ 13000 | loss: 2.8567 | ds_loss: 0.0000 | lr: 4.8902e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:   1250/ 13000 | global iter:   1250/ 13000 | loss: 2.3144 | ds_loss: 0.0000 | lr: 4.8902e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 1.804
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:   1251/ 13000 | global iter:   1251/ 13000 | loss: 2.5838 | ds_loss: 0.0000 | lr: 4.8901e-05 | scale: 16384.0000 | micro time: 1.820 | step time: 0.000
train | epoch   0 | Iter:   1252/ 13000 | global iter:   1252/ 13000 | loss: 1.6476 | ds_loss: 0.0000 | lr: 4.8899e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   0 | Iter:   1253/ 13000 | global iter:   1253/ 13000 | loss: 1.5522 | ds_loss: 0.0000 | lr: 4.8897e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   0 | Iter:   1254/ 13000 | global iter:   1254/ 13000 | loss: 2.1524 | ds_loss: 0.0000 | lr: 4.8895e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   0 | Iter:   1255/ 13000 | global iter:   1255/ 13000 | loss: 2.3539 | ds_loss: 0.0000 | lr: 4.8893e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   0 | Iter:   1256/ 13000 | global iter:   1256/ 13000 | loss: 2.2273 | ds_loss: 0.0000 | lr: 4.8892e-05 | scale: 16384.0000 | micro time: 1.761 | step time: 0.000
train | epoch   0 | Iter:   1257/ 13000 | global iter:   1257/ 13000 | loss: 1.9533 | ds_loss: 0.0000 | lr: 4.8890e-05 | scale: 16384.0000 | micro time: 1.749 | step time: 0.000
train | epoch   0 | Iter:   1258/ 13000 | global iter:   1258/ 13000 | loss: 2.4319 | ds_loss: 0.0000 | lr: 4.8888e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   0 | Iter:   1259/ 13000 | global iter:   1259/ 13000 | loss: 2.9650 | ds_loss: 0.0000 | lr: 4.8886e-05 | scale: 16384.0000 | micro time: 1.655 | step time: 0.000
train | epoch   0 | Iter:   1260/ 13000 | global iter:   1260/ 13000 | loss: 2.7815 | ds_loss: 0.0000 | lr: 4.8885e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:   1260/ 13000 | global iter:   1260/ 13000 | loss: 2.2649 | ds_loss: 0.0000 | lr: 4.8885e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 1.780
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:   1261/ 13000 | global iter:   1261/ 13000 | loss: 2.4441 | ds_loss: 0.0000 | lr: 4.8883e-05 | scale: 16384.0000 | micro time: 1.770 | step time: 0.000
train | epoch   0 | Iter:   1262/ 13000 | global iter:   1262/ 13000 | loss: 2.9939 | ds_loss: 0.0000 | lr: 4.8881e-05 | scale: 16384.0000 | micro time: 1.794 | step time: 0.000
train | epoch   0 | Iter:   1263/ 13000 | global iter:   1263/ 13000 | loss: 2.8630 | ds_loss: 0.0000 | lr: 4.8879e-05 | scale: 16384.0000 | micro time: 1.707 | step time: 0.000
train | epoch   0 | Iter:   1264/ 13000 | global iter:   1264/ 13000 | loss: 2.1562 | ds_loss: 0.0000 | lr: 4.8877e-05 | scale: 16384.0000 | micro time: 1.810 | step time: 0.000
train | epoch   0 | Iter:   1265/ 13000 | global iter:   1265/ 13000 | loss: 2.3122 | ds_loss: 0.0000 | lr: 4.8876e-05 | scale: 16384.0000 | micro time: 1.848 | step time: 0.000
train | epoch   0 | Iter:   1266/ 13000 | global iter:   1266/ 13000 | loss: 2.9470 | ds_loss: 0.0000 | lr: 4.8874e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   0 | Iter:   1267/ 13000 | global iter:   1267/ 13000 | loss: 2.3719 | ds_loss: 0.0000 | lr: 4.8872e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   0 | Iter:   1268/ 13000 | global iter:   1268/ 13000 | loss: 2.5026 | ds_loss: 0.0000 | lr: 4.8870e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   0 | Iter:   1269/ 13000 | global iter:   1269/ 13000 | loss: 2.3383 | ds_loss: 0.0000 | lr: 4.8868e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   0 | Iter:   1270/ 13000 | global iter:   1270/ 13000 | loss: 2.1200 | ds_loss: 0.0000 | lr: 4.8867e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:   1270/ 13000 | global iter:   1270/ 13000 | loss: 2.5049 | ds_loss: 0.0000 | lr: 4.8867e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 1.799
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:   1271/ 13000 | global iter:   1271/ 13000 | loss: 1.8908 | ds_loss: 0.0000 | lr: 4.8865e-05 | scale: 16384.0000 | micro time: 1.710 | step time: 0.000
train | epoch   0 | Iter:   1272/ 13000 | global iter:   1272/ 13000 | loss: 2.1115 | ds_loss: 0.0000 | lr: 4.8863e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   0 | Iter:   1273/ 13000 | global iter:   1273/ 13000 | loss: 1.7244 | ds_loss: 0.0000 | lr: 4.8861e-05 | scale: 16384.0000 | micro time: 1.695 | step time: 0.000
train | epoch   0 | Iter:   1274/ 13000 | global iter:   1274/ 13000 | loss: 1.7384 | ds_loss: 0.0000 | lr: 4.8859e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   0 | Iter:   1275/ 13000 | global iter:   1275/ 13000 | loss: 1.9227 | ds_loss: 0.0000 | lr: 4.8858e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   0 | Iter:   1276/ 13000 | global iter:   1276/ 13000 | loss: 2.9422 | ds_loss: 0.0000 | lr: 4.8856e-05 | scale: 16384.0000 | micro time: 1.699 | step time: 0.000
train | epoch   0 | Iter:   1277/ 13000 | global iter:   1277/ 13000 | loss: 2.6464 | ds_loss: 0.0000 | lr: 4.8854e-05 | scale: 16384.0000 | micro time: 1.738 | step time: 0.000
train | epoch   0 | Iter:   1278/ 13000 | global iter:   1278/ 13000 | loss: 2.2404 | ds_loss: 0.0000 | lr: 4.8852e-05 | scale: 16384.0000 | micro time: 1.676 | step time: 0.000
train | epoch   0 | Iter:   1279/ 13000 | global iter:   1279/ 13000 | loss: 2.6551 | ds_loss: 0.0000 | lr: 4.8850e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   0 | Iter:   1280/ 13000 | global iter:   1280/ 13000 | loss: 2.5401 | ds_loss: 0.0000 | lr: 4.8849e-05 | scale: 16384.0000 | micro time: 1.734 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:   1280/ 13000 | global iter:   1280/ 13000 | loss: 2.2412 | ds_loss: 0.0000 | lr: 4.8849e-05 | scale: 16384.0000 | micro time: 1.734 | step time: 1.756
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:   1281/ 13000 | global iter:   1281/ 13000 | loss: 3.1612 | ds_loss: 0.0000 | lr: 4.8847e-05 | scale: 16384.0000 | micro time: 1.738 | step time: 0.000
train | epoch   0 | Iter:   1282/ 13000 | global iter:   1282/ 13000 | loss: 2.6248 | ds_loss: 0.0000 | lr: 4.8845e-05 | scale: 16384.0000 | micro time: 1.838 | step time: 0.000
train | epoch   0 | Iter:   1283/ 13000 | global iter:   1283/ 13000 | loss: 2.3617 | ds_loss: 0.0000 | lr: 4.8843e-05 | scale: 16384.0000 | micro time: 1.844 | step time: 0.000
train | epoch   0 | Iter:   1284/ 13000 | global iter:   1284/ 13000 | loss: 2.3959 | ds_loss: 0.0000 | lr: 4.8841e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   0 | Iter:   1285/ 13000 | global iter:   1285/ 13000 | loss: 2.2551 | ds_loss: 0.0000 | lr: 4.8840e-05 | scale: 16384.0000 | micro time: 1.707 | step time: 0.000
train | epoch   0 | Iter:   1286/ 13000 | global iter:   1286/ 13000 | loss: 2.7331 | ds_loss: 0.0000 | lr: 4.8838e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   0 | Iter:   1287/ 13000 | global iter:   1287/ 13000 | loss: 2.4977 | ds_loss: 0.0000 | lr: 4.8836e-05 | scale: 16384.0000 | micro time: 1.695 | step time: 0.000
train | epoch   0 | Iter:   1288/ 13000 | global iter:   1288/ 13000 | loss: 2.6877 | ds_loss: 0.0000 | lr: 4.8834e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   0 | Iter:   1289/ 13000 | global iter:   1289/ 13000 | loss: 2.2994 | ds_loss: 0.0000 | lr: 4.8832e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   0 | Iter:   1290/ 13000 | global iter:   1290/ 13000 | loss: 2.3613 | ds_loss: 0.0000 | lr: 4.8830e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:   1290/ 13000 | global iter:   1290/ 13000 | loss: 2.5378 | ds_loss: 0.0000 | lr: 4.8830e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 1.786
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:   1291/ 13000 | global iter:   1291/ 13000 | loss: 2.8922 | ds_loss: 0.0000 | lr: 4.8829e-05 | scale: 16384.0000 | micro time: 1.838 | step time: 0.000
train | epoch   0 | Iter:   1292/ 13000 | global iter:   1292/ 13000 | loss: 2.1063 | ds_loss: 0.0000 | lr: 4.8827e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   0 | Iter:   1293/ 13000 | global iter:   1293/ 13000 | loss: 2.5546 | ds_loss: 0.0000 | lr: 4.8825e-05 | scale: 16384.0000 | micro time: 1.778 | step time: 0.000
train | epoch   0 | Iter:   1294/ 13000 | global iter:   1294/ 13000 | loss: 2.6958 | ds_loss: 0.0000 | lr: 4.8823e-05 | scale: 16384.0000 | micro time: 1.832 | step time: 0.000
train | epoch   0 | Iter:   1295/ 13000 | global iter:   1295/ 13000 | loss: 2.8168 | ds_loss: 0.0000 | lr: 4.8821e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   0 | Iter:   1296/ 13000 | global iter:   1296/ 13000 | loss: 2.5224 | ds_loss: 0.0000 | lr: 4.8820e-05 | scale: 16384.0000 | micro time: 1.833 | step time: 0.000
train | epoch   0 | Iter:   1297/ 13000 | global iter:   1297/ 13000 | loss: 2.6259 | ds_loss: 0.0000 | lr: 4.8818e-05 | scale: 16384.0000 | micro time: 1.845 | step time: 0.000
train | epoch   0 | Iter:   1298/ 13000 | global iter:   1298/ 13000 | loss: 2.6288 | ds_loss: 0.0000 | lr: 4.8816e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   0 | Iter:   1299/ 13000 | global iter:   1299/ 13000 | loss: 1.9286 | ds_loss: 0.0000 | lr: 4.8814e-05 | scale: 16384.0000 | micro time: 1.591 | step time: 0.000
train | epoch   0 | Iter:   1300/ 13000 | global iter:   1300/ 13000 | loss: 1.7880 | ds_loss: 0.0000 | lr: 4.8812e-05 | scale: 16384.0000 | micro time: 1.693 | step time: 0.000
****************************************************************************************************
train | epoch   0 | Iter:   1300/ 13000 | global iter:   1300/ 13000 | loss: 2.4559 | ds_loss: 0.0000 | lr: 4.8812e-05 | scale: 16384.0000 | micro time: 1.693 | step time: 1.784
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   0 | Iter:   1301/ 13000 | global iter:   1301/ 13000 | loss: 2.5079 | ds_loss: 0.0000 | lr: 4.8810e-05 | scale: 16384.0000 | micro time: 1.838 | step time: 0.000
Sat Apr 19 11:47:32 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   40C    P2            104W /  230W |   21781MiB /  23028MiB |     75%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   44C    P2            124W /  230W |   22469MiB /  23028MiB |     97%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   42C    P2            117W /  230W |   21741MiB /  23028MiB |     97%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   40C    P2            122W /  230W |   21057MiB /  23028MiB |     75%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3      21774MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3      22462MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3      21734MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3      21050MiB |
+-----------------------------------------------------------------------------------------+

Sat Apr 19 11:47:32 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   40C    P2            104W /  230W |   21781MiB /  23028MiB |     75%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   44C    P2            124W /  230W |   22469MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   42C    P2            107W /  230W |   21741MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   40C    P2            114W /  230W |   21057MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3      21774MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3      22462MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3      21734MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3      21050MiB |
+-----------------------------------------------------------------------------------------+

Sat Apr 19 11:47:32 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   40C    P2             97W /  230W |   21781MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   43C    P2            114W /  230W |   22469MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   42C    P2            106W /  230W |   21741MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   40C    P2            114W /  230W |   21057MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3      21774MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3      22462MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3      21734MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3      21050MiB |
+-----------------------------------------------------------------------------------------+

Sat Apr 19 11:47:32 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   40C    P2             97W /  230W |   21781MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   46C    P2            125W /  230W |   22469MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   41C    P2            101W /  230W |   21741MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   39C    P2             90W /  230W |   21057MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3      21774MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3      22462MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3      21734MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3      21050MiB |
+-----------------------------------------------------------------------------------------+

train | epoch   1 | Iter:   1302/ 13000 | global iter:   1302/ 13000 | loss: 2.4688 | ds_loss: 0.0000 | lr: 4.8808e-05 | scale: 16384.0000 | micro time: 1.887 | step time: 0.000
train | epoch   1 | Iter:   1303/ 13000 | global iter:   1303/ 13000 | loss: 2.0388 | ds_loss: 0.0000 | lr: 4.8807e-05 | scale: 16384.0000 | micro time: 1.826 | step time: 0.000
train | epoch   1 | Iter:   1304/ 13000 | global iter:   1304/ 13000 | loss: 1.6644 | ds_loss: 0.0000 | lr: 4.8805e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   1 | Iter:   1305/ 13000 | global iter:   1305/ 13000 | loss: 1.7968 | ds_loss: 0.0000 | lr: 4.8803e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   1 | Iter:   1306/ 13000 | global iter:   1306/ 13000 | loss: 1.7708 | ds_loss: 0.0000 | lr: 4.8801e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   1 | Iter:   1307/ 13000 | global iter:   1307/ 13000 | loss: 2.1037 | ds_loss: 0.0000 | lr: 4.8799e-05 | scale: 16384.0000 | micro time: 1.780 | step time: 0.000
train | epoch   1 | Iter:   1308/ 13000 | global iter:   1308/ 13000 | loss: 2.0596 | ds_loss: 0.0000 | lr: 4.8797e-05 | scale: 16384.0000 | micro time: 1.950 | step time: 0.000
train | epoch   1 | Iter:   1309/ 13000 | global iter:   1309/ 13000 | loss: 2.2316 | ds_loss: 0.0000 | lr: 4.8796e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   1 | Iter:   1310/ 13000 | global iter:   1310/ 13000 | loss: 1.3656 | ds_loss: 0.0000 | lr: 4.8794e-05 | scale: 16384.0000 | micro time: 1.758 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1310/ 13000 | global iter:   1310/ 13000 | loss: 2.0008 | ds_loss: 0.0000 | lr: 4.8794e-05 | scale: 16384.0000 | micro time: 1.758 | step time: 1.835
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1311/ 13000 | global iter:   1311/ 13000 | loss: 1.9616 | ds_loss: 0.0000 | lr: 4.8792e-05 | scale: 16384.0000 | micro time: 1.735 | step time: 0.000
train | epoch   1 | Iter:   1312/ 13000 | global iter:   1312/ 13000 | loss: 2.3132 | ds_loss: 0.0000 | lr: 4.8790e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   1 | Iter:   1313/ 13000 | global iter:   1313/ 13000 | loss: 2.2785 | ds_loss: 0.0000 | lr: 4.8788e-05 | scale: 16384.0000 | micro time: 1.701 | step time: 0.000
train | epoch   1 | Iter:   1314/ 13000 | global iter:   1314/ 13000 | loss: 2.5247 | ds_loss: 0.0000 | lr: 4.8786e-05 | scale: 16384.0000 | micro time: 1.801 | step time: 0.000
train | epoch   1 | Iter:   1315/ 13000 | global iter:   1315/ 13000 | loss: 2.5062 | ds_loss: 0.0000 | lr: 4.8784e-05 | scale: 16384.0000 | micro time: 1.886 | step time: 0.000
train | epoch   1 | Iter:   1316/ 13000 | global iter:   1316/ 13000 | loss: 2.1387 | ds_loss: 0.0000 | lr: 4.8783e-05 | scale: 16384.0000 | micro time: 1.882 | step time: 0.000
train | epoch   1 | Iter:   1317/ 13000 | global iter:   1317/ 13000 | loss: 2.3357 | ds_loss: 0.0000 | lr: 4.8781e-05 | scale: 16384.0000 | micro time: 1.735 | step time: 0.000
train | epoch   1 | Iter:   1318/ 13000 | global iter:   1318/ 13000 | loss: 2.3792 | ds_loss: 0.0000 | lr: 4.8779e-05 | scale: 16384.0000 | micro time: 1.817 | step time: 0.000
train | epoch   1 | Iter:   1319/ 13000 | global iter:   1319/ 13000 | loss: 2.1751 | ds_loss: 0.0000 | lr: 4.8777e-05 | scale: 16384.0000 | micro time: 1.887 | step time: 0.000
train | epoch   1 | Iter:   1320/ 13000 | global iter:   1320/ 13000 | loss: 1.8449 | ds_loss: 0.0000 | lr: 4.8775e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1320/ 13000 | global iter:   1320/ 13000 | loss: 2.2458 | ds_loss: 0.0000 | lr: 4.8775e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 1.808
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1321/ 13000 | global iter:   1321/ 13000 | loss: 2.6391 | ds_loss: 0.0000 | lr: 4.8773e-05 | scale: 16384.0000 | micro time: 1.841 | step time: 0.000
train | epoch   1 | Iter:   1322/ 13000 | global iter:   1322/ 13000 | loss: 1.8734 | ds_loss: 0.0000 | lr: 4.8771e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   1 | Iter:   1323/ 13000 | global iter:   1323/ 13000 | loss: 2.0512 | ds_loss: 0.0000 | lr: 4.8770e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   1 | Iter:   1324/ 13000 | global iter:   1324/ 13000 | loss: 2.3109 | ds_loss: 0.0000 | lr: 4.8768e-05 | scale: 16384.0000 | micro time: 1.915 | step time: 0.000
train | epoch   1 | Iter:   1325/ 13000 | global iter:   1325/ 13000 | loss: 1.8461 | ds_loss: 0.0000 | lr: 4.8766e-05 | scale: 16384.0000 | micro time: 1.728 | step time: 0.000
train | epoch   1 | Iter:   1326/ 13000 | global iter:   1326/ 13000 | loss: 2.7150 | ds_loss: 0.0000 | lr: 4.8764e-05 | scale: 16384.0000 | micro time: 1.730 | step time: 0.000
train | epoch   1 | Iter:   1327/ 13000 | global iter:   1327/ 13000 | loss: 1.3077 | ds_loss: 0.0000 | lr: 4.8762e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   1 | Iter:   1328/ 13000 | global iter:   1328/ 13000 | loss: 1.9363 | ds_loss: 0.0000 | lr: 4.8760e-05 | scale: 16384.0000 | micro time: 1.764 | step time: 0.000
train | epoch   1 | Iter:   1329/ 13000 | global iter:   1329/ 13000 | loss: 2.2654 | ds_loss: 0.0000 | lr: 4.8758e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 0.000
train | epoch   1 | Iter:   1330/ 13000 | global iter:   1330/ 13000 | loss: 1.7359 | ds_loss: 0.0000 | lr: 4.8756e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1330/ 13000 | global iter:   1330/ 13000 | loss: 2.0681 | ds_loss: 0.0000 | lr: 4.8756e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 1.814
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1331/ 13000 | global iter:   1331/ 13000 | loss: 1.5924 | ds_loss: 0.0000 | lr: 4.8755e-05 | scale: 16384.0000 | micro time: 1.789 | step time: 0.000
train | epoch   1 | Iter:   1332/ 13000 | global iter:   1332/ 13000 | loss: 2.2226 | ds_loss: 0.0000 | lr: 4.8753e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   1 | Iter:   1333/ 13000 | global iter:   1333/ 13000 | loss: 3.0075 | ds_loss: 0.0000 | lr: 4.8751e-05 | scale: 16384.0000 | micro time: 1.869 | step time: 0.000
train | epoch   1 | Iter:   1334/ 13000 | global iter:   1334/ 13000 | loss: 1.7408 | ds_loss: 0.0000 | lr: 4.8749e-05 | scale: 16384.0000 | micro time: 1.801 | step time: 0.000
train | epoch   1 | Iter:   1335/ 13000 | global iter:   1335/ 13000 | loss: 1.9308 | ds_loss: 0.0000 | lr: 4.8747e-05 | scale: 16384.0000 | micro time: 1.774 | step time: 0.000
train | epoch   1 | Iter:   1336/ 13000 | global iter:   1336/ 13000 | loss: 2.2857 | ds_loss: 0.0000 | lr: 4.8745e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   1 | Iter:   1337/ 13000 | global iter:   1337/ 13000 | loss: 2.0517 | ds_loss: 0.0000 | lr: 4.8743e-05 | scale: 16384.0000 | micro time: 1.887 | step time: 0.000
train | epoch   1 | Iter:   1338/ 13000 | global iter:   1338/ 13000 | loss: 1.9858 | ds_loss: 0.0000 | lr: 4.8741e-05 | scale: 16384.0000 | micro time: 1.882 | step time: 0.000
train | epoch   1 | Iter:   1339/ 13000 | global iter:   1339/ 13000 | loss: 2.2337 | ds_loss: 0.0000 | lr: 4.8739e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   1 | Iter:   1340/ 13000 | global iter:   1340/ 13000 | loss: 1.6619 | ds_loss: 0.0000 | lr: 4.8738e-05 | scale: 16384.0000 | micro time: 1.789 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1340/ 13000 | global iter:   1340/ 13000 | loss: 2.0713 | ds_loss: 0.0000 | lr: 4.8738e-05 | scale: 16384.0000 | micro time: 1.789 | step time: 1.817
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1341/ 13000 | global iter:   1341/ 13000 | loss: 2.2358 | ds_loss: 0.0000 | lr: 4.8736e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   1 | Iter:   1342/ 13000 | global iter:   1342/ 13000 | loss: 1.7524 | ds_loss: 0.0000 | lr: 4.8734e-05 | scale: 16384.0000 | micro time: 1.730 | step time: 0.000
train | epoch   1 | Iter:   1343/ 13000 | global iter:   1343/ 13000 | loss: 2.2036 | ds_loss: 0.0000 | lr: 4.8732e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   1 | Iter:   1344/ 13000 | global iter:   1344/ 13000 | loss: 2.2972 | ds_loss: 0.0000 | lr: 4.8730e-05 | scale: 16384.0000 | micro time: 1.725 | step time: 0.000
train | epoch   1 | Iter:   1345/ 13000 | global iter:   1345/ 13000 | loss: 2.4449 | ds_loss: 0.0000 | lr: 4.8728e-05 | scale: 16384.0000 | micro time: 1.861 | step time: 0.000
train | epoch   1 | Iter:   1346/ 13000 | global iter:   1346/ 13000 | loss: 2.1568 | ds_loss: 0.0000 | lr: 4.8726e-05 | scale: 16384.0000 | micro time: 1.762 | step time: 0.000
train | epoch   1 | Iter:   1347/ 13000 | global iter:   1347/ 13000 | loss: 1.7782 | ds_loss: 0.0000 | lr: 4.8724e-05 | scale: 16384.0000 | micro time: 1.892 | step time: 0.000
train | epoch   1 | Iter:   1348/ 13000 | global iter:   1348/ 13000 | loss: 2.3183 | ds_loss: 0.0000 | lr: 4.8722e-05 | scale: 16384.0000 | micro time: 1.769 | step time: 0.000
train | epoch   1 | Iter:   1349/ 13000 | global iter:   1349/ 13000 | loss: 2.4859 | ds_loss: 0.0000 | lr: 4.8720e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   1 | Iter:   1350/ 13000 | global iter:   1350/ 13000 | loss: 2.4947 | ds_loss: 0.0000 | lr: 4.8719e-05 | scale: 16384.0000 | micro time: 1.812 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1350/ 13000 | global iter:   1350/ 13000 | loss: 2.2168 | ds_loss: 0.0000 | lr: 4.8719e-05 | scale: 16384.0000 | micro time: 1.812 | step time: 1.796
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1351/ 13000 | global iter:   1351/ 13000 | loss: 2.1357 | ds_loss: 0.0000 | lr: 4.8717e-05 | scale: 16384.0000 | micro time: 1.766 | step time: 0.000
train | epoch   1 | Iter:   1352/ 13000 | global iter:   1352/ 13000 | loss: 2.5631 | ds_loss: 0.0000 | lr: 4.8715e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   1 | Iter:   1353/ 13000 | global iter:   1353/ 13000 | loss: 1.9342 | ds_loss: 0.0000 | lr: 4.8713e-05 | scale: 16384.0000 | micro time: 1.915 | step time: 0.000
train | epoch   1 | Iter:   1354/ 13000 | global iter:   1354/ 13000 | loss: 2.3875 | ds_loss: 0.0000 | lr: 4.8711e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   1 | Iter:   1355/ 13000 | global iter:   1355/ 13000 | loss: 2.6946 | ds_loss: 0.0000 | lr: 4.8709e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   1 | Iter:   1356/ 13000 | global iter:   1356/ 13000 | loss: 2.3274 | ds_loss: 0.0000 | lr: 4.8707e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   1 | Iter:   1357/ 13000 | global iter:   1357/ 13000 | loss: 2.2589 | ds_loss: 0.0000 | lr: 4.8705e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   1 | Iter:   1358/ 13000 | global iter:   1358/ 13000 | loss: 1.9107 | ds_loss: 0.0000 | lr: 4.8703e-05 | scale: 16384.0000 | micro time: 1.911 | step time: 0.000
train | epoch   1 | Iter:   1359/ 13000 | global iter:   1359/ 13000 | loss: 2.0580 | ds_loss: 0.0000 | lr: 4.8701e-05 | scale: 16384.0000 | micro time: 1.792 | step time: 0.000
train | epoch   1 | Iter:   1360/ 13000 | global iter:   1360/ 13000 | loss: 1.8056 | ds_loss: 0.0000 | lr: 4.8699e-05 | scale: 16384.0000 | micro time: 1.821 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1360/ 13000 | global iter:   1360/ 13000 | loss: 2.2076 | ds_loss: 0.0000 | lr: 4.8699e-05 | scale: 16384.0000 | micro time: 1.821 | step time: 1.828
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1361/ 13000 | global iter:   1361/ 13000 | loss: 2.0773 | ds_loss: 0.0000 | lr: 4.8697e-05 | scale: 16384.0000 | micro time: 1.890 | step time: 0.000
train | epoch   1 | Iter:   1362/ 13000 | global iter:   1362/ 13000 | loss: 2.3829 | ds_loss: 0.0000 | lr: 4.8696e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   1 | Iter:   1363/ 13000 | global iter:   1363/ 13000 | loss: 2.6336 | ds_loss: 0.0000 | lr: 4.8694e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   1 | Iter:   1364/ 13000 | global iter:   1364/ 13000 | loss: 2.7781 | ds_loss: 0.0000 | lr: 4.8692e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   1 | Iter:   1365/ 13000 | global iter:   1365/ 13000 | loss: 2.3395 | ds_loss: 0.0000 | lr: 4.8690e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   1 | Iter:   1366/ 13000 | global iter:   1366/ 13000 | loss: 2.5487 | ds_loss: 0.0000 | lr: 4.8688e-05 | scale: 16384.0000 | micro time: 1.788 | step time: 0.000
train | epoch   1 | Iter:   1367/ 13000 | global iter:   1367/ 13000 | loss: 1.9809 | ds_loss: 0.0000 | lr: 4.8686e-05 | scale: 16384.0000 | micro time: 1.846 | step time: 0.000
train | epoch   1 | Iter:   1368/ 13000 | global iter:   1368/ 13000 | loss: 2.0504 | ds_loss: 0.0000 | lr: 4.8684e-05 | scale: 16384.0000 | micro time: 1.834 | step time: 0.000
train | epoch   1 | Iter:   1369/ 13000 | global iter:   1369/ 13000 | loss: 1.9764 | ds_loss: 0.0000 | lr: 4.8682e-05 | scale: 16384.0000 | micro time: 1.809 | step time: 0.000
train | epoch   1 | Iter:   1370/ 13000 | global iter:   1370/ 13000 | loss: 2.4739 | ds_loss: 0.0000 | lr: 4.8680e-05 | scale: 16384.0000 | micro time: 1.762 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1370/ 13000 | global iter:   1370/ 13000 | loss: 2.3242 | ds_loss: 0.0000 | lr: 4.8680e-05 | scale: 16384.0000 | micro time: 1.762 | step time: 1.808
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1371/ 13000 | global iter:   1371/ 13000 | loss: 2.4833 | ds_loss: 0.0000 | lr: 4.8678e-05 | scale: 16384.0000 | micro time: 1.853 | step time: 0.000
train | epoch   1 | Iter:   1372/ 13000 | global iter:   1372/ 13000 | loss: 1.9209 | ds_loss: 0.0000 | lr: 4.8676e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   1 | Iter:   1373/ 13000 | global iter:   1373/ 13000 | loss: 2.6106 | ds_loss: 0.0000 | lr: 4.8674e-05 | scale: 16384.0000 | micro time: 1.849 | step time: 0.000
train | epoch   1 | Iter:   1374/ 13000 | global iter:   1374/ 13000 | loss: 2.5405 | ds_loss: 0.0000 | lr: 4.8672e-05 | scale: 16384.0000 | micro time: 1.841 | step time: 0.000
train | epoch   1 | Iter:   1375/ 13000 | global iter:   1375/ 13000 | loss: 1.8668 | ds_loss: 0.0000 | lr: 4.8670e-05 | scale: 16384.0000 | micro time: 1.810 | step time: 0.000
train | epoch   1 | Iter:   1376/ 13000 | global iter:   1376/ 13000 | loss: 2.5459 | ds_loss: 0.0000 | lr: 4.8668e-05 | scale: 16384.0000 | micro time: 1.801 | step time: 0.000
train | epoch   1 | Iter:   1377/ 13000 | global iter:   1377/ 13000 | loss: 2.7178 | ds_loss: 0.0000 | lr: 4.8667e-05 | scale: 16384.0000 | micro time: 1.865 | step time: 0.000
train | epoch   1 | Iter:   1378/ 13000 | global iter:   1378/ 13000 | loss: 2.7503 | ds_loss: 0.0000 | lr: 4.8665e-05 | scale: 16384.0000 | micro time: 1.814 | step time: 0.000
train | epoch   1 | Iter:   1379/ 13000 | global iter:   1379/ 13000 | loss: 2.3574 | ds_loss: 0.0000 | lr: 4.8663e-05 | scale: 16384.0000 | micro time: 1.711 | step time: 0.000
train | epoch   1 | Iter:   1380/ 13000 | global iter:   1380/ 13000 | loss: 2.1400 | ds_loss: 0.0000 | lr: 4.8661e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1380/ 13000 | global iter:   1380/ 13000 | loss: 2.3933 | ds_loss: 0.0000 | lr: 4.8661e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 1.810
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1381/ 13000 | global iter:   1381/ 13000 | loss: 2.4882 | ds_loss: 0.0000 | lr: 4.8659e-05 | scale: 16384.0000 | micro time: 1.772 | step time: 0.000
train | epoch   1 | Iter:   1382/ 13000 | global iter:   1382/ 13000 | loss: 2.0211 | ds_loss: 0.0000 | lr: 4.8657e-05 | scale: 16384.0000 | micro time: 1.760 | step time: 0.000
train | epoch   1 | Iter:   1383/ 13000 | global iter:   1383/ 13000 | loss: 2.3635 | ds_loss: 0.0000 | lr: 4.8655e-05 | scale: 16384.0000 | micro time: 1.891 | step time: 0.000
train | epoch   1 | Iter:   1384/ 13000 | global iter:   1384/ 13000 | loss: 2.4084 | ds_loss: 0.0000 | lr: 4.8653e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   1 | Iter:   1385/ 13000 | global iter:   1385/ 13000 | loss: 2.3711 | ds_loss: 0.0000 | lr: 4.8651e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   1 | Iter:   1386/ 13000 | global iter:   1386/ 13000 | loss: 2.6085 | ds_loss: 0.0000 | lr: 4.8649e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   1 | Iter:   1387/ 13000 | global iter:   1387/ 13000 | loss: 2.1192 | ds_loss: 0.0000 | lr: 4.8647e-05 | scale: 16384.0000 | micro time: 1.764 | step time: 0.000
train | epoch   1 | Iter:   1388/ 13000 | global iter:   1388/ 13000 | loss: 2.4064 | ds_loss: 0.0000 | lr: 4.8645e-05 | scale: 16384.0000 | micro time: 1.814 | step time: 0.000
train | epoch   1 | Iter:   1389/ 13000 | global iter:   1389/ 13000 | loss: 1.5608 | ds_loss: 0.0000 | lr: 4.8643e-05 | scale: 16384.0000 | micro time: 1.735 | step time: 0.000
train | epoch   1 | Iter:   1390/ 13000 | global iter:   1390/ 13000 | loss: 2.3550 | ds_loss: 0.0000 | lr: 4.8641e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1390/ 13000 | global iter:   1390/ 13000 | loss: 2.2702 | ds_loss: 0.0000 | lr: 4.8641e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 1.809
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1391/ 13000 | global iter:   1391/ 13000 | loss: 1.4054 | ds_loss: 0.0000 | lr: 4.8639e-05 | scale: 16384.0000 | micro time: 1.726 | step time: 0.000
train | epoch   1 | Iter:   1392/ 13000 | global iter:   1392/ 13000 | loss: 2.3284 | ds_loss: 0.0000 | lr: 4.8637e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   1 | Iter:   1393/ 13000 | global iter:   1393/ 13000 | loss: 2.5427 | ds_loss: 0.0000 | lr: 4.8635e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   1 | Iter:   1394/ 13000 | global iter:   1394/ 13000 | loss: 1.7691 | ds_loss: 0.0000 | lr: 4.8633e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   1 | Iter:   1395/ 13000 | global iter:   1395/ 13000 | loss: 2.6628 | ds_loss: 0.0000 | lr: 4.8631e-05 | scale: 16384.0000 | micro time: 1.798 | step time: 0.000
train | epoch   1 | Iter:   1396/ 13000 | global iter:   1396/ 13000 | loss: 2.4409 | ds_loss: 0.0000 | lr: 4.8629e-05 | scale: 16384.0000 | micro time: 1.840 | step time: 0.000
train | epoch   1 | Iter:   1397/ 13000 | global iter:   1397/ 13000 | loss: 1.8147 | ds_loss: 0.0000 | lr: 4.8627e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   1 | Iter:   1398/ 13000 | global iter:   1398/ 13000 | loss: 1.9274 | ds_loss: 0.0000 | lr: 4.8625e-05 | scale: 16384.0000 | micro time: 1.801 | step time: 0.000
train | epoch   1 | Iter:   1399/ 13000 | global iter:   1399/ 13000 | loss: 2.2616 | ds_loss: 0.0000 | lr: 4.8623e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   1 | Iter:   1400/ 13000 | global iter:   1400/ 13000 | loss: 2.7132 | ds_loss: 0.0000 | lr: 4.8621e-05 | scale: 16384.0000 | micro time: 1.731 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1400/ 13000 | global iter:   1400/ 13000 | loss: 2.1866 | ds_loss: 0.0000 | lr: 4.8621e-05 | scale: 16384.0000 | micro time: 1.731 | step time: 1.808
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1401/ 13000 | global iter:   1401/ 13000 | loss: 1.8431 | ds_loss: 0.0000 | lr: 4.8619e-05 | scale: 16384.0000 | micro time: 1.873 | step time: 0.000
train | epoch   1 | Iter:   1402/ 13000 | global iter:   1402/ 13000 | loss: 2.4200 | ds_loss: 0.0000 | lr: 4.8617e-05 | scale: 16384.0000 | micro time: 1.891 | step time: 0.000
train | epoch   1 | Iter:   1403/ 13000 | global iter:   1403/ 13000 | loss: 2.1643 | ds_loss: 0.0000 | lr: 4.8615e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
train | epoch   1 | Iter:   1404/ 13000 | global iter:   1404/ 13000 | loss: 1.6636 | ds_loss: 0.0000 | lr: 4.8614e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   1 | Iter:   1405/ 13000 | global iter:   1405/ 13000 | loss: 2.1703 | ds_loss: 0.0000 | lr: 4.8612e-05 | scale: 16384.0000 | micro time: 1.659 | step time: 0.000
train | epoch   1 | Iter:   1406/ 13000 | global iter:   1406/ 13000 | loss: 2.0314 | ds_loss: 0.0000 | lr: 4.8610e-05 | scale: 16384.0000 | micro time: 1.879 | step time: 0.000
train | epoch   1 | Iter:   1407/ 13000 | global iter:   1407/ 13000 | loss: 2.2945 | ds_loss: 0.0000 | lr: 4.8608e-05 | scale: 16384.0000 | micro time: 1.939 | step time: 0.000
train | epoch   1 | Iter:   1408/ 13000 | global iter:   1408/ 13000 | loss: 2.2757 | ds_loss: 0.0000 | lr: 4.8606e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
train | epoch   1 | Iter:   1409/ 13000 | global iter:   1409/ 13000 | loss: 2.5429 | ds_loss: 0.0000 | lr: 4.8604e-05 | scale: 16384.0000 | micro time: 1.738 | step time: 0.000
train | epoch   1 | Iter:   1410/ 13000 | global iter:   1410/ 13000 | loss: 2.5113 | ds_loss: 0.0000 | lr: 4.8602e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1410/ 13000 | global iter:   1410/ 13000 | loss: 2.1917 | ds_loss: 0.0000 | lr: 4.8602e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 1.830
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1411/ 13000 | global iter:   1411/ 13000 | loss: 2.0617 | ds_loss: 0.0000 | lr: 4.8600e-05 | scale: 16384.0000 | micro time: 1.766 | step time: 0.000
train | epoch   1 | Iter:   1412/ 13000 | global iter:   1412/ 13000 | loss: 2.4263 | ds_loss: 0.0000 | lr: 4.8598e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   1 | Iter:   1413/ 13000 | global iter:   1413/ 13000 | loss: 1.3596 | ds_loss: 0.0000 | lr: 4.8596e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   1 | Iter:   1414/ 13000 | global iter:   1414/ 13000 | loss: 1.9273 | ds_loss: 0.0000 | lr: 4.8594e-05 | scale: 16384.0000 | micro time: 1.822 | step time: 0.000
train | epoch   1 | Iter:   1415/ 13000 | global iter:   1415/ 13000 | loss: 2.0304 | ds_loss: 0.0000 | lr: 4.8592e-05 | scale: 16384.0000 | micro time: 1.780 | step time: 0.000
train | epoch   1 | Iter:   1416/ 13000 | global iter:   1416/ 13000 | loss: 2.0267 | ds_loss: 0.0000 | lr: 4.8590e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   1 | Iter:   1417/ 13000 | global iter:   1417/ 13000 | loss: 2.4827 | ds_loss: 0.0000 | lr: 4.8588e-05 | scale: 16384.0000 | micro time: 1.650 | step time: 0.000
train | epoch   1 | Iter:   1418/ 13000 | global iter:   1418/ 13000 | loss: 2.0298 | ds_loss: 0.0000 | lr: 4.8586e-05 | scale: 16384.0000 | micro time: 1.880 | step time: 0.000
train | epoch   1 | Iter:   1419/ 13000 | global iter:   1419/ 13000 | loss: 2.0213 | ds_loss: 0.0000 | lr: 4.8584e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   1 | Iter:   1420/ 13000 | global iter:   1420/ 13000 | loss: 2.5070 | ds_loss: 0.0000 | lr: 4.8582e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1420/ 13000 | global iter:   1420/ 13000 | loss: 2.0873 | ds_loss: 0.0000 | lr: 4.8582e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 1.788
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1421/ 13000 | global iter:   1421/ 13000 | loss: 2.0175 | ds_loss: 0.0000 | lr: 4.8580e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 0.000
train | epoch   1 | Iter:   1422/ 13000 | global iter:   1422/ 13000 | loss: 2.8659 | ds_loss: 0.0000 | lr: 4.8578e-05 | scale: 16384.0000 | micro time: 1.923 | step time: 0.000
train | epoch   1 | Iter:   1423/ 13000 | global iter:   1423/ 13000 | loss: 2.0423 | ds_loss: 0.0000 | lr: 4.8576e-05 | scale: 16384.0000 | micro time: 1.828 | step time: 0.000
train | epoch   1 | Iter:   1424/ 13000 | global iter:   1424/ 13000 | loss: 1.9400 | ds_loss: 0.0000 | lr: 4.8574e-05 | scale: 16384.0000 | micro time: 1.778 | step time: 0.000
train | epoch   1 | Iter:   1425/ 13000 | global iter:   1425/ 13000 | loss: 1.8600 | ds_loss: 0.0000 | lr: 4.8572e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   1 | Iter:   1426/ 13000 | global iter:   1426/ 13000 | loss: 2.5550 | ds_loss: 0.0000 | lr: 4.8570e-05 | scale: 16384.0000 | micro time: 1.833 | step time: 0.000
train | epoch   1 | Iter:   1427/ 13000 | global iter:   1427/ 13000 | loss: 2.3649 | ds_loss: 0.0000 | lr: 4.8568e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   1 | Iter:   1428/ 13000 | global iter:   1428/ 13000 | loss: 2.0113 | ds_loss: 0.0000 | lr: 4.8566e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   1 | Iter:   1429/ 13000 | global iter:   1429/ 13000 | loss: 1.8402 | ds_loss: 0.0000 | lr: 4.8564e-05 | scale: 16384.0000 | micro time: 1.882 | step time: 0.000
train | epoch   1 | Iter:   1430/ 13000 | global iter:   1430/ 13000 | loss: 1.8466 | ds_loss: 0.0000 | lr: 4.8562e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1430/ 13000 | global iter:   1430/ 13000 | loss: 2.1344 | ds_loss: 0.0000 | lr: 4.8562e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 1.838
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1431/ 13000 | global iter:   1431/ 13000 | loss: 2.4802 | ds_loss: 0.0000 | lr: 4.8559e-05 | scale: 16384.0000 | micro time: 1.810 | step time: 0.000
train | epoch   1 | Iter:   1432/ 13000 | global iter:   1432/ 13000 | loss: 2.4705 | ds_loss: 0.0000 | lr: 4.8557e-05 | scale: 16384.0000 | micro time: 1.897 | step time: 0.000
train | epoch   1 | Iter:   1433/ 13000 | global iter:   1433/ 13000 | loss: 2.2043 | ds_loss: 0.0000 | lr: 4.8555e-05 | scale: 16384.0000 | micro time: 1.879 | step time: 0.000
train | epoch   1 | Iter:   1434/ 13000 | global iter:   1434/ 13000 | loss: 1.0834 | ds_loss: 0.0000 | lr: 4.8553e-05 | scale: 16384.0000 | micro time: 1.761 | step time: 0.000
train | epoch   1 | Iter:   1435/ 13000 | global iter:   1435/ 13000 | loss: 2.2066 | ds_loss: 0.0000 | lr: 4.8551e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   1 | Iter:   1436/ 13000 | global iter:   1436/ 13000 | loss: 2.6320 | ds_loss: 0.0000 | lr: 4.8549e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   1 | Iter:   1437/ 13000 | global iter:   1437/ 13000 | loss: 2.1741 | ds_loss: 0.0000 | lr: 4.8547e-05 | scale: 16384.0000 | micro time: 1.762 | step time: 0.000
train | epoch   1 | Iter:   1438/ 13000 | global iter:   1438/ 13000 | loss: 1.8814 | ds_loss: 0.0000 | lr: 4.8545e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   1 | Iter:   1439/ 13000 | global iter:   1439/ 13000 | loss: 2.1020 | ds_loss: 0.0000 | lr: 4.8543e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   1 | Iter:   1440/ 13000 | global iter:   1440/ 13000 | loss: 1.9481 | ds_loss: 0.0000 | lr: 4.8541e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1440/ 13000 | global iter:   1440/ 13000 | loss: 2.1183 | ds_loss: 0.0000 | lr: 4.8541e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 1.815
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1441/ 13000 | global iter:   1441/ 13000 | loss: 2.1038 | ds_loss: 0.0000 | lr: 4.8539e-05 | scale: 16384.0000 | micro time: 1.693 | step time: 0.000
train | epoch   1 | Iter:   1442/ 13000 | global iter:   1442/ 13000 | loss: 2.1225 | ds_loss: 0.0000 | lr: 4.8537e-05 | scale: 16384.0000 | micro time: 1.793 | step time: 0.000
train | epoch   1 | Iter:   1443/ 13000 | global iter:   1443/ 13000 | loss: 2.1683 | ds_loss: 0.0000 | lr: 4.8535e-05 | scale: 16384.0000 | micro time: 1.881 | step time: 0.000
train | epoch   1 | Iter:   1444/ 13000 | global iter:   1444/ 13000 | loss: 2.1620 | ds_loss: 0.0000 | lr: 4.8533e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   1 | Iter:   1445/ 13000 | global iter:   1445/ 13000 | loss: 2.3986 | ds_loss: 0.0000 | lr: 4.8531e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   1 | Iter:   1446/ 13000 | global iter:   1446/ 13000 | loss: 2.7007 | ds_loss: 0.0000 | lr: 4.8529e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   1 | Iter:   1447/ 13000 | global iter:   1447/ 13000 | loss: 1.9652 | ds_loss: 0.0000 | lr: 4.8527e-05 | scale: 16384.0000 | micro time: 1.786 | step time: 0.000
train | epoch   1 | Iter:   1448/ 13000 | global iter:   1448/ 13000 | loss: 2.2418 | ds_loss: 0.0000 | lr: 4.8525e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   1 | Iter:   1449/ 13000 | global iter:   1449/ 13000 | loss: 2.2089 | ds_loss: 0.0000 | lr: 4.8523e-05 | scale: 16384.0000 | micro time: 1.765 | step time: 0.000
train | epoch   1 | Iter:   1450/ 13000 | global iter:   1450/ 13000 | loss: 2.3278 | ds_loss: 0.0000 | lr: 4.8521e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1450/ 13000 | global iter:   1450/ 13000 | loss: 2.2400 | ds_loss: 0.0000 | lr: 4.8521e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 1.798
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1451/ 13000 | global iter:   1451/ 13000 | loss: 1.8089 | ds_loss: 0.0000 | lr: 4.8519e-05 | scale: 16384.0000 | micro time: 1.846 | step time: 0.000
train | epoch   1 | Iter:   1452/ 13000 | global iter:   1452/ 13000 | loss: 1.9062 | ds_loss: 0.0000 | lr: 4.8517e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   1 | Iter:   1453/ 13000 | global iter:   1453/ 13000 | loss: 2.2270 | ds_loss: 0.0000 | lr: 4.8515e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   1 | Iter:   1454/ 13000 | global iter:   1454/ 13000 | loss: 1.6746 | ds_loss: 0.0000 | lr: 4.8513e-05 | scale: 32768.0000 | micro time: 1.796 | step time: 0.000
train | epoch   1 | Iter:   1455/ 13000 | global iter:   1455/ 13000 | loss: 1.6582 | ds_loss: 0.0000 | lr: 4.8511e-05 | scale: 32768.0000 | micro time: 1.856 | step time: 0.000
train | epoch   1 | Iter:   1456/ 13000 | global iter:   1456/ 13000 | loss: 1.0289 | ds_loss: 0.0000 | lr: 4.8509e-05 | scale: 32768.0000 | micro time: 1.780 | step time: 0.000
train | epoch   1 | Iter:   1457/ 13000 | global iter:   1457/ 13000 | loss: 1.8760 | ds_loss: 0.0000 | lr: 4.8507e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   1 | Iter:   1458/ 13000 | global iter:   1458/ 13000 | loss: 2.0849 | ds_loss: 0.0000 | lr: 4.8504e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   1 | Iter:   1459/ 13000 | global iter:   1459/ 13000 | loss: 1.9512 | ds_loss: 0.0000 | lr: 4.8502e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   1 | Iter:   1460/ 13000 | global iter:   1460/ 13000 | loss: 2.3584 | ds_loss: 0.0000 | lr: 4.8500e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1460/ 13000 | global iter:   1460/ 13000 | loss: 1.8574 | ds_loss: 0.0000 | lr: 4.8500e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 1.823
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1461/ 13000 | global iter:   1461/ 13000 | loss: 1.4000 | ds_loss: 0.0000 | lr: 4.8498e-05 | scale: 32768.0000 | micro time: 1.834 | step time: 0.000
train | epoch   1 | Iter:   1462/ 13000 | global iter:   1462/ 13000 | loss: 2.1929 | ds_loss: 0.0000 | lr: 4.8496e-05 | scale: 32768.0000 | micro time: 1.878 | step time: 0.000
train | epoch   1 | Iter:   1463/ 13000 | global iter:   1463/ 13000 | loss: 1.5848 | ds_loss: 0.0000 | lr: 4.8494e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   1 | Iter:   1464/ 13000 | global iter:   1464/ 13000 | loss: 2.2553 | ds_loss: 0.0000 | lr: 4.8492e-05 | scale: 32768.0000 | micro time: 1.743 | step time: 0.000
train | epoch   1 | Iter:   1465/ 13000 | global iter:   1465/ 13000 | loss: 1.6004 | ds_loss: 0.0000 | lr: 4.8490e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 0.000
train | epoch   1 | Iter:   1466/ 13000 | global iter:   1466/ 13000 | loss: 1.8415 | ds_loss: 0.0000 | lr: 4.8488e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   1 | Iter:   1467/ 13000 | global iter:   1467/ 13000 | loss: 2.1940 | ds_loss: 0.0000 | lr: 4.8486e-05 | scale: 32768.0000 | micro time: 1.757 | step time: 0.000
train | epoch   1 | Iter:   1468/ 13000 | global iter:   1468/ 13000 | loss: 2.4201 | ds_loss: 0.0000 | lr: 4.8484e-05 | scale: 32768.0000 | micro time: 1.801 | step time: 0.000
train | epoch   1 | Iter:   1469/ 13000 | global iter:   1469/ 13000 | loss: 1.6614 | ds_loss: 0.0000 | lr: 4.8482e-05 | scale: 32768.0000 | micro time: 1.801 | step time: 0.000
train | epoch   1 | Iter:   1470/ 13000 | global iter:   1470/ 13000 | loss: 2.0761 | ds_loss: 0.0000 | lr: 4.8480e-05 | scale: 32768.0000 | micro time: 1.884 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1470/ 13000 | global iter:   1470/ 13000 | loss: 1.9226 | ds_loss: 0.0000 | lr: 4.8480e-05 | scale: 32768.0000 | micro time: 1.884 | step time: 1.814
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1471/ 13000 | global iter:   1471/ 13000 | loss: 2.5564 | ds_loss: 0.0000 | lr: 4.8478e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
train | epoch   1 | Iter:   1472/ 13000 | global iter:   1472/ 13000 | loss: 2.4374 | ds_loss: 0.0000 | lr: 4.8476e-05 | scale: 32768.0000 | micro time: 1.727 | step time: 0.000
train | epoch   1 | Iter:   1473/ 13000 | global iter:   1473/ 13000 | loss: 2.1850 | ds_loss: 0.0000 | lr: 4.8473e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   1 | Iter:   1474/ 13000 | global iter:   1474/ 13000 | loss: 2.5069 | ds_loss: 0.0000 | lr: 4.8471e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   1 | Iter:   1475/ 13000 | global iter:   1475/ 13000 | loss: 2.1605 | ds_loss: 0.0000 | lr: 4.8469e-05 | scale: 32768.0000 | micro time: 1.757 | step time: 0.000
train | epoch   1 | Iter:   1476/ 13000 | global iter:   1476/ 13000 | loss: 2.1943 | ds_loss: 0.0000 | lr: 4.8467e-05 | scale: 32768.0000 | micro time: 1.772 | step time: 0.000
train | epoch   1 | Iter:   1477/ 13000 | global iter:   1477/ 13000 | loss: 2.3163 | ds_loss: 0.0000 | lr: 4.8465e-05 | scale: 32768.0000 | micro time: 1.850 | step time: 0.000
train | epoch   1 | Iter:   1478/ 13000 | global iter:   1478/ 13000 | loss: 2.2695 | ds_loss: 0.0000 | lr: 4.8463e-05 | scale: 32768.0000 | micro time: 1.820 | step time: 0.000
train | epoch   1 | Iter:   1479/ 13000 | global iter:   1479/ 13000 | loss: 2.3618 | ds_loss: 0.0000 | lr: 4.8461e-05 | scale: 32768.0000 | micro time: 1.746 | step time: 0.000
train | epoch   1 | Iter:   1480/ 13000 | global iter:   1480/ 13000 | loss: 2.2269 | ds_loss: 0.0000 | lr: 4.8459e-05 | scale: 32768.0000 | micro time: 1.788 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1480/ 13000 | global iter:   1480/ 13000 | loss: 2.3215 | ds_loss: 0.0000 | lr: 4.8459e-05 | scale: 32768.0000 | micro time: 1.788 | step time: 1.789
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1481/ 13000 | global iter:   1481/ 13000 | loss: 2.2989 | ds_loss: 0.0000 | lr: 4.8457e-05 | scale: 32768.0000 | micro time: 1.721 | step time: 0.000
train | epoch   1 | Iter:   1482/ 13000 | global iter:   1482/ 13000 | loss: 2.1015 | ds_loss: 0.0000 | lr: 4.8455e-05 | scale: 32768.0000 | micro time: 1.675 | step time: 0.000
train | epoch   1 | Iter:   1483/ 13000 | global iter:   1483/ 13000 | loss: 2.6117 | ds_loss: 0.0000 | lr: 4.8453e-05 | scale: 32768.0000 | micro time: 1.899 | step time: 0.000
train | epoch   1 | Iter:   1484/ 13000 | global iter:   1484/ 13000 | loss: 1.1620 | ds_loss: 0.0000 | lr: 4.8451e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   1 | Iter:   1485/ 13000 | global iter:   1485/ 13000 | loss: 2.2422 | ds_loss: 0.0000 | lr: 4.8448e-05 | scale: 32768.0000 | micro time: 1.806 | step time: 0.000
train | epoch   1 | Iter:   1486/ 13000 | global iter:   1486/ 13000 | loss: 2.2676 | ds_loss: 0.0000 | lr: 4.8446e-05 | scale: 32768.0000 | micro time: 1.776 | step time: 0.000
train | epoch   1 | Iter:   1487/ 13000 | global iter:   1487/ 13000 | loss: 1.9926 | ds_loss: 0.0000 | lr: 4.8444e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   1 | Iter:   1488/ 13000 | global iter:   1488/ 13000 | loss: 1.8986 | ds_loss: 0.0000 | lr: 4.8442e-05 | scale: 32768.0000 | micro time: 1.780 | step time: 0.000
train | epoch   1 | Iter:   1489/ 13000 | global iter:   1489/ 13000 | loss: 2.2304 | ds_loss: 0.0000 | lr: 4.8440e-05 | scale: 32768.0000 | micro time: 1.737 | step time: 0.000
train | epoch   1 | Iter:   1490/ 13000 | global iter:   1490/ 13000 | loss: 2.3565 | ds_loss: 0.0000 | lr: 4.8438e-05 | scale: 32768.0000 | micro time: 1.703 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1490/ 13000 | global iter:   1490/ 13000 | loss: 2.1162 | ds_loss: 0.0000 | lr: 4.8438e-05 | scale: 32768.0000 | micro time: 1.703 | step time: 1.777
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1491/ 13000 | global iter:   1491/ 13000 | loss: 1.7581 | ds_loss: 0.0000 | lr: 4.8436e-05 | scale: 32768.0000 | micro time: 1.750 | step time: 0.000
train | epoch   1 | Iter:   1492/ 13000 | global iter:   1492/ 13000 | loss: 1.8821 | ds_loss: 0.0000 | lr: 4.8434e-05 | scale: 32768.0000 | micro time: 1.912 | step time: 0.000
train | epoch   1 | Iter:   1493/ 13000 | global iter:   1493/ 13000 | loss: 1.9918 | ds_loss: 0.0000 | lr: 4.8432e-05 | scale: 32768.0000 | micro time: 1.778 | step time: 0.000
train | epoch   1 | Iter:   1494/ 13000 | global iter:   1494/ 13000 | loss: 2.3982 | ds_loss: 0.0000 | lr: 4.8430e-05 | scale: 32768.0000 | micro time: 1.723 | step time: 0.000
train | epoch   1 | Iter:   1495/ 13000 | global iter:   1495/ 13000 | loss: 2.0402 | ds_loss: 0.0000 | lr: 4.8427e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   1 | Iter:   1496/ 13000 | global iter:   1496/ 13000 | loss: 2.3826 | ds_loss: 0.0000 | lr: 4.8425e-05 | scale: 32768.0000 | micro time: 1.806 | step time: 0.000
train | epoch   1 | Iter:   1497/ 13000 | global iter:   1497/ 13000 | loss: 2.3870 | ds_loss: 0.0000 | lr: 4.8423e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   1 | Iter:   1498/ 13000 | global iter:   1498/ 13000 | loss: 2.4628 | ds_loss: 0.0000 | lr: 4.8421e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   1 | Iter:   1499/ 13000 | global iter:   1499/ 13000 | loss: 2.5153 | ds_loss: 0.0000 | lr: 4.8419e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   1 | Iter:   1500/ 13000 | global iter:   1500/ 13000 | loss: 2.1633 | ds_loss: 0.0000 | lr: 4.8417e-05 | scale: 32768.0000 | micro time: 1.768 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1500/ 13000 | global iter:   1500/ 13000 | loss: 2.1981 | ds_loss: 0.0000 | lr: 4.8417e-05 | scale: 32768.0000 | micro time: 1.768 | step time: 1.797
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1501/ 13000 | global iter:   1501/ 13000 | loss: 2.1559 | ds_loss: 0.0000 | lr: 4.8415e-05 | scale: 32768.0000 | micro time: 1.793 | step time: 0.000
train | epoch   1 | Iter:   1502/ 13000 | global iter:   1502/ 13000 | loss: 1.6477 | ds_loss: 0.0000 | lr: 4.8413e-05 | scale: 32768.0000 | micro time: 1.695 | step time: 0.000
train | epoch   1 | Iter:   1503/ 13000 | global iter:   1503/ 13000 | loss: 2.4157 | ds_loss: 0.0000 | lr: 4.8411e-05 | scale: 32768.0000 | micro time: 1.841 | step time: 0.000
train | epoch   1 | Iter:   1504/ 13000 | global iter:   1504/ 13000 | loss: 1.9196 | ds_loss: 0.0000 | lr: 4.8408e-05 | scale: 32768.0000 | micro time: 1.862 | step time: 0.000
train | epoch   1 | Iter:   1505/ 13000 | global iter:   1505/ 13000 | loss: 1.9192 | ds_loss: 0.0000 | lr: 4.8406e-05 | scale: 32768.0000 | micro time: 1.733 | step time: 0.000
train | epoch   1 | Iter:   1506/ 13000 | global iter:   1506/ 13000 | loss: 2.3716 | ds_loss: 0.0000 | lr: 4.8404e-05 | scale: 32768.0000 | micro time: 1.731 | step time: 0.000
train | epoch   1 | Iter:   1507/ 13000 | global iter:   1507/ 13000 | loss: 2.2763 | ds_loss: 0.0000 | lr: 4.8402e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   1 | Iter:   1508/ 13000 | global iter:   1508/ 13000 | loss: 2.5397 | ds_loss: 0.0000 | lr: 4.8400e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   1 | Iter:   1509/ 13000 | global iter:   1509/ 13000 | loss: 2.3813 | ds_loss: 0.0000 | lr: 4.8398e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   1 | Iter:   1510/ 13000 | global iter:   1510/ 13000 | loss: 2.4607 | ds_loss: 0.0000 | lr: 4.8396e-05 | scale: 32768.0000 | micro time: 1.862 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1510/ 13000 | global iter:   1510/ 13000 | loss: 2.2088 | ds_loss: 0.0000 | lr: 4.8396e-05 | scale: 32768.0000 | micro time: 1.862 | step time: 1.797
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1511/ 13000 | global iter:   1511/ 13000 | loss: 1.8393 | ds_loss: 0.0000 | lr: 4.8394e-05 | scale: 32768.0000 | micro time: 1.795 | step time: 0.000
train | epoch   1 | Iter:   1512/ 13000 | global iter:   1512/ 13000 | loss: 1.6384 | ds_loss: 0.0000 | lr: 4.8391e-05 | scale: 32768.0000 | micro time: 1.873 | step time: 0.000
train | epoch   1 | Iter:   1513/ 13000 | global iter:   1513/ 13000 | loss: 2.3079 | ds_loss: 0.0000 | lr: 4.8389e-05 | scale: 32768.0000 | micro time: 1.741 | step time: 0.000
train | epoch   1 | Iter:   1514/ 13000 | global iter:   1514/ 13000 | loss: 2.3725 | ds_loss: 0.0000 | lr: 4.8387e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   1 | Iter:   1515/ 13000 | global iter:   1515/ 13000 | loss: 2.3946 | ds_loss: 0.0000 | lr: 4.8385e-05 | scale: 32768.0000 | micro time: 1.742 | step time: 0.000
train | epoch   1 | Iter:   1516/ 13000 | global iter:   1516/ 13000 | loss: 2.2187 | ds_loss: 0.0000 | lr: 4.8383e-05 | scale: 32768.0000 | micro time: 1.770 | step time: 0.000
train | epoch   1 | Iter:   1517/ 13000 | global iter:   1517/ 13000 | loss: 2.0560 | ds_loss: 0.0000 | lr: 4.8381e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   1 | Iter:   1518/ 13000 | global iter:   1518/ 13000 | loss: 2.0760 | ds_loss: 0.0000 | lr: 4.8379e-05 | scale: 32768.0000 | micro time: 1.856 | step time: 0.000
train | epoch   1 | Iter:   1519/ 13000 | global iter:   1519/ 13000 | loss: 1.5615 | ds_loss: 0.0000 | lr: 4.8377e-05 | scale: 32768.0000 | micro time: 1.755 | step time: 0.000
train | epoch   1 | Iter:   1520/ 13000 | global iter:   1520/ 13000 | loss: 2.1104 | ds_loss: 0.0000 | lr: 4.8374e-05 | scale: 32768.0000 | micro time: 1.745 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1520/ 13000 | global iter:   1520/ 13000 | loss: 2.0575 | ds_loss: 0.0000 | lr: 4.8374e-05 | scale: 32768.0000 | micro time: 1.745 | step time: 1.798
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1521/ 13000 | global iter:   1521/ 13000 | loss: 1.5743 | ds_loss: 0.0000 | lr: 4.8372e-05 | scale: 32768.0000 | micro time: 1.680 | step time: 0.000
train | epoch   1 | Iter:   1522/ 13000 | global iter:   1522/ 13000 | loss: 2.9332 | ds_loss: 0.0000 | lr: 4.8370e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   1 | Iter:   1523/ 13000 | global iter:   1523/ 13000 | loss: 1.8467 | ds_loss: 0.0000 | lr: 4.8368e-05 | scale: 32768.0000 | micro time: 1.841 | step time: 0.000
train | epoch   1 | Iter:   1524/ 13000 | global iter:   1524/ 13000 | loss: 1.8720 | ds_loss: 0.0000 | lr: 4.8366e-05 | scale: 32768.0000 | micro time: 1.739 | step time: 0.000
train | epoch   1 | Iter:   1525/ 13000 | global iter:   1525/ 13000 | loss: 2.1403 | ds_loss: 0.0000 | lr: 4.8364e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   1 | Iter:   1526/ 13000 | global iter:   1526/ 13000 | loss: 2.1666 | ds_loss: 0.0000 | lr: 4.8362e-05 | scale: 32768.0000 | micro time: 1.711 | step time: 0.000
train | epoch   1 | Iter:   1527/ 13000 | global iter:   1527/ 13000 | loss: 2.5804 | ds_loss: 0.0000 | lr: 4.8359e-05 | scale: 32768.0000 | micro time: 1.919 | step time: 0.000
train | epoch   1 | Iter:   1528/ 13000 | global iter:   1528/ 13000 | loss: 2.2626 | ds_loss: 0.0000 | lr: 4.8357e-05 | scale: 32768.0000 | micro time: 1.752 | step time: 0.000
train | epoch   1 | Iter:   1529/ 13000 | global iter:   1529/ 13000 | loss: 2.4135 | ds_loss: 0.0000 | lr: 4.8355e-05 | scale: 32768.0000 | micro time: 1.788 | step time: 0.000
train | epoch   1 | Iter:   1530/ 13000 | global iter:   1530/ 13000 | loss: 1.9138 | ds_loss: 0.0000 | lr: 4.8353e-05 | scale: 32768.0000 | micro time: 1.677 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1530/ 13000 | global iter:   1530/ 13000 | loss: 2.1703 | ds_loss: 0.0000 | lr: 4.8353e-05 | scale: 32768.0000 | micro time: 1.677 | step time: 1.776
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1531/ 13000 | global iter:   1531/ 13000 | loss: 2.0342 | ds_loss: 0.0000 | lr: 4.8351e-05 | scale: 32768.0000 | micro time: 1.829 | step time: 0.000
train | epoch   1 | Iter:   1532/ 13000 | global iter:   1532/ 13000 | loss: 2.0763 | ds_loss: 0.0000 | lr: 4.8349e-05 | scale: 32768.0000 | micro time: 1.745 | step time: 0.000
train | epoch   1 | Iter:   1533/ 13000 | global iter:   1533/ 13000 | loss: 2.1822 | ds_loss: 0.0000 | lr: 4.8346e-05 | scale: 32768.0000 | micro time: 1.769 | step time: 0.000
train | epoch   1 | Iter:   1534/ 13000 | global iter:   1534/ 13000 | loss: 2.0874 | ds_loss: 0.0000 | lr: 4.8344e-05 | scale: 32768.0000 | micro time: 1.707 | step time: 0.000
train | epoch   1 | Iter:   1535/ 13000 | global iter:   1535/ 13000 | loss: 2.1418 | ds_loss: 0.0000 | lr: 4.8342e-05 | scale: 32768.0000 | micro time: 1.711 | step time: 0.000
train | epoch   1 | Iter:   1536/ 13000 | global iter:   1536/ 13000 | loss: 2.2945 | ds_loss: 0.0000 | lr: 4.8340e-05 | scale: 32768.0000 | micro time: 1.751 | step time: 0.000
train | epoch   1 | Iter:   1537/ 13000 | global iter:   1537/ 13000 | loss: 2.5867 | ds_loss: 0.0000 | lr: 4.8338e-05 | scale: 32768.0000 | micro time: 1.777 | step time: 0.000
train | epoch   1 | Iter:   1538/ 13000 | global iter:   1538/ 13000 | loss: 1.9511 | ds_loss: 0.0000 | lr: 4.8336e-05 | scale: 32768.0000 | micro time: 1.741 | step time: 0.000
train | epoch   1 | Iter:   1539/ 13000 | global iter:   1539/ 13000 | loss: 1.7205 | ds_loss: 0.0000 | lr: 4.8333e-05 | scale: 32768.0000 | micro time: 1.781 | step time: 0.000
train | epoch   1 | Iter:   1540/ 13000 | global iter:   1540/ 13000 | loss: 2.3903 | ds_loss: 0.0000 | lr: 4.8331e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1540/ 13000 | global iter:   1540/ 13000 | loss: 2.1465 | ds_loss: 0.0000 | lr: 4.8331e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 1.763
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1541/ 13000 | global iter:   1541/ 13000 | loss: 2.0842 | ds_loss: 0.0000 | lr: 4.8329e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   1 | Iter:   1542/ 13000 | global iter:   1542/ 13000 | loss: 2.1211 | ds_loss: 0.0000 | lr: 4.8327e-05 | scale: 32768.0000 | micro time: 1.784 | step time: 0.000
train | epoch   1 | Iter:   1543/ 13000 | global iter:   1543/ 13000 | loss: 2.3412 | ds_loss: 0.0000 | lr: 4.8325e-05 | scale: 32768.0000 | micro time: 1.730 | step time: 0.000
train | epoch   1 | Iter:   1544/ 13000 | global iter:   1544/ 13000 | loss: 2.4513 | ds_loss: 0.0000 | lr: 4.8323e-05 | scale: 32768.0000 | micro time: 1.797 | step time: 0.000
train | epoch   1 | Iter:   1545/ 13000 | global iter:   1545/ 13000 | loss: 1.7832 | ds_loss: 0.0000 | lr: 4.8320e-05 | scale: 32768.0000 | micro time: 1.817 | step time: 0.000
train | epoch   1 | Iter:   1546/ 13000 | global iter:   1546/ 13000 | loss: 2.4029 | ds_loss: 0.0000 | lr: 4.8318e-05 | scale: 32768.0000 | micro time: 1.699 | step time: 0.000
train | epoch   1 | Iter:   1547/ 13000 | global iter:   1547/ 13000 | loss: 2.4915 | ds_loss: 0.0000 | lr: 4.8316e-05 | scale: 32768.0000 | micro time: 1.923 | step time: 0.000
train | epoch   1 | Iter:   1548/ 13000 | global iter:   1548/ 13000 | loss: 0.8933 | ds_loss: 0.0000 | lr: 4.8314e-05 | scale: 32768.0000 | micro time: 1.715 | step time: 0.000
train | epoch   1 | Iter:   1549/ 13000 | global iter:   1549/ 13000 | loss: 2.1372 | ds_loss: 0.0000 | lr: 4.8312e-05 | scale: 32768.0000 | micro time: 1.849 | step time: 0.000
train | epoch   1 | Iter:   1550/ 13000 | global iter:   1550/ 13000 | loss: 2.6004 | ds_loss: 0.0000 | lr: 4.8310e-05 | scale: 32768.0000 | micro time: 1.804 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1550/ 13000 | global iter:   1550/ 13000 | loss: 2.1306 | ds_loss: 0.0000 | lr: 4.8310e-05 | scale: 32768.0000 | micro time: 1.804 | step time: 1.788
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1551/ 13000 | global iter:   1551/ 13000 | loss: 2.1992 | ds_loss: 0.0000 | lr: 4.8307e-05 | scale: 32768.0000 | micro time: 1.804 | step time: 0.000
train | epoch   1 | Iter:   1552/ 13000 | global iter:   1552/ 13000 | loss: 2.2326 | ds_loss: 0.0000 | lr: 4.8305e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   1 | Iter:   1553/ 13000 | global iter:   1553/ 13000 | loss: 1.7351 | ds_loss: 0.0000 | lr: 4.8303e-05 | scale: 32768.0000 | micro time: 1.793 | step time: 0.000
train | epoch   1 | Iter:   1554/ 13000 | global iter:   1554/ 13000 | loss: 1.7954 | ds_loss: 0.0000 | lr: 4.8301e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   1 | Iter:   1555/ 13000 | global iter:   1555/ 13000 | loss: 2.3745 | ds_loss: 0.0000 | lr: 4.8299e-05 | scale: 32768.0000 | micro time: 1.788 | step time: 0.000
train | epoch   1 | Iter:   1556/ 13000 | global iter:   1556/ 13000 | loss: 2.4896 | ds_loss: 0.0000 | lr: 4.8296e-05 | scale: 32768.0000 | micro time: 1.806 | step time: 0.000
train | epoch   1 | Iter:   1557/ 13000 | global iter:   1557/ 13000 | loss: 2.4060 | ds_loss: 0.0000 | lr: 4.8294e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   1 | Iter:   1558/ 13000 | global iter:   1558/ 13000 | loss: 1.9405 | ds_loss: 0.0000 | lr: 4.8292e-05 | scale: 32768.0000 | micro time: 1.836 | step time: 0.000
train | epoch   1 | Iter:   1559/ 13000 | global iter:   1559/ 13000 | loss: 2.6720 | ds_loss: 0.0000 | lr: 4.8290e-05 | scale: 32768.0000 | micro time: 1.879 | step time: 0.000
train | epoch   1 | Iter:   1560/ 13000 | global iter:   1560/ 13000 | loss: 2.1214 | ds_loss: 0.0000 | lr: 4.8288e-05 | scale: 32768.0000 | micro time: 1.800 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1560/ 13000 | global iter:   1560/ 13000 | loss: 2.1966 | ds_loss: 0.0000 | lr: 4.8288e-05 | scale: 32768.0000 | micro time: 1.800 | step time: 1.820
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1561/ 13000 | global iter:   1561/ 13000 | loss: 2.0804 | ds_loss: 0.0000 | lr: 4.8285e-05 | scale: 32768.0000 | micro time: 1.742 | step time: 0.000
train | epoch   1 | Iter:   1562/ 13000 | global iter:   1562/ 13000 | loss: 1.9724 | ds_loss: 0.0000 | lr: 4.8283e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   1 | Iter:   1563/ 13000 | global iter:   1563/ 13000 | loss: 2.3515 | ds_loss: 0.0000 | lr: 4.8281e-05 | scale: 32768.0000 | micro time: 1.837 | step time: 0.000
train | epoch   1 | Iter:   1564/ 13000 | global iter:   1564/ 13000 | loss: 2.4142 | ds_loss: 0.0000 | lr: 4.8279e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 0.000
train | epoch   1 | Iter:   1565/ 13000 | global iter:   1565/ 13000 | loss: 2.6372 | ds_loss: 0.0000 | lr: 4.8277e-05 | scale: 32768.0000 | micro time: 1.813 | step time: 0.000
train | epoch   1 | Iter:   1566/ 13000 | global iter:   1566/ 13000 | loss: 2.4922 | ds_loss: 0.0000 | lr: 4.8274e-05 | scale: 32768.0000 | micro time: 1.741 | step time: 0.000
train | epoch   1 | Iter:   1567/ 13000 | global iter:   1567/ 13000 | loss: 2.6941 | ds_loss: 0.0000 | lr: 4.8272e-05 | scale: 32768.0000 | micro time: 1.797 | step time: 0.000
train | epoch   1 | Iter:   1568/ 13000 | global iter:   1568/ 13000 | loss: 2.3379 | ds_loss: 0.0000 | lr: 4.8270e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   1 | Iter:   1569/ 13000 | global iter:   1569/ 13000 | loss: 2.5178 | ds_loss: 0.0000 | lr: 4.8268e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   1 | Iter:   1570/ 13000 | global iter:   1570/ 13000 | loss: 1.8916 | ds_loss: 0.0000 | lr: 4.8266e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1570/ 13000 | global iter:   1570/ 13000 | loss: 2.3389 | ds_loss: 0.0000 | lr: 4.8266e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 1.815
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1571/ 13000 | global iter:   1571/ 13000 | loss: 2.5508 | ds_loss: 0.0000 | lr: 4.8263e-05 | scale: 32768.0000 | micro time: 1.665 | step time: 0.000
train | epoch   1 | Iter:   1572/ 13000 | global iter:   1572/ 13000 | loss: 2.1269 | ds_loss: 0.0000 | lr: 4.8261e-05 | scale: 32768.0000 | micro time: 1.764 | step time: 0.000
train | epoch   1 | Iter:   1573/ 13000 | global iter:   1573/ 13000 | loss: 2.6102 | ds_loss: 0.0000 | lr: 4.8259e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 0.000
train | epoch   1 | Iter:   1574/ 13000 | global iter:   1574/ 13000 | loss: 2.5717 | ds_loss: 0.0000 | lr: 4.8257e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   1 | Iter:   1575/ 13000 | global iter:   1575/ 13000 | loss: 2.1667 | ds_loss: 0.0000 | lr: 4.8255e-05 | scale: 32768.0000 | micro time: 1.760 | step time: 0.000
train | epoch   1 | Iter:   1576/ 13000 | global iter:   1576/ 13000 | loss: 2.6776 | ds_loss: 0.0000 | lr: 4.8252e-05 | scale: 32768.0000 | micro time: 1.906 | step time: 0.000
train | epoch   1 | Iter:   1577/ 13000 | global iter:   1577/ 13000 | loss: 2.0010 | ds_loss: 0.0000 | lr: 4.8250e-05 | scale: 32768.0000 | micro time: 1.674 | step time: 0.000
train | epoch   1 | Iter:   1578/ 13000 | global iter:   1578/ 13000 | loss: 1.5035 | ds_loss: 0.0000 | lr: 4.8248e-05 | scale: 32768.0000 | micro time: 1.822 | step time: 0.000
train | epoch   1 | Iter:   1579/ 13000 | global iter:   1579/ 13000 | loss: 2.1112 | ds_loss: 0.0000 | lr: 4.8246e-05 | scale: 32768.0000 | micro time: 1.769 | step time: 0.000
train | epoch   1 | Iter:   1580/ 13000 | global iter:   1580/ 13000 | loss: 2.2530 | ds_loss: 0.0000 | lr: 4.8243e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1580/ 13000 | global iter:   1580/ 13000 | loss: 2.2573 | ds_loss: 0.0000 | lr: 4.8243e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 1.785
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1581/ 13000 | global iter:   1581/ 13000 | loss: 2.2969 | ds_loss: 0.0000 | lr: 4.8241e-05 | scale: 32768.0000 | micro time: 1.754 | step time: 0.000
train | epoch   1 | Iter:   1582/ 13000 | global iter:   1582/ 13000 | loss: 2.3242 | ds_loss: 0.0000 | lr: 4.8239e-05 | scale: 32768.0000 | micro time: 1.895 | step time: 0.000
train | epoch   1 | Iter:   1583/ 13000 | global iter:   1583/ 13000 | loss: 2.0009 | ds_loss: 0.0000 | lr: 4.8237e-05 | scale: 32768.0000 | micro time: 1.798 | step time: 0.000
train | epoch   1 | Iter:   1584/ 13000 | global iter:   1584/ 13000 | loss: 2.6806 | ds_loss: 0.0000 | lr: 4.8235e-05 | scale: 32768.0000 | micro time: 1.784 | step time: 0.000
train | epoch   1 | Iter:   1585/ 13000 | global iter:   1585/ 13000 | loss: 2.0014 | ds_loss: 0.0000 | lr: 4.8232e-05 | scale: 32768.0000 | micro time: 1.778 | step time: 0.000
train | epoch   1 | Iter:   1586/ 13000 | global iter:   1586/ 13000 | loss: 2.2110 | ds_loss: 0.0000 | lr: 4.8230e-05 | scale: 32768.0000 | micro time: 1.684 | step time: 0.000
train | epoch   1 | Iter:   1587/ 13000 | global iter:   1587/ 13000 | loss: 1.8065 | ds_loss: 0.0000 | lr: 4.8228e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 0.000
train | epoch   1 | Iter:   1588/ 13000 | global iter:   1588/ 13000 | loss: 2.0928 | ds_loss: 0.0000 | lr: 4.8226e-05 | scale: 32768.0000 | micro time: 1.887 | step time: 0.000
train | epoch   1 | Iter:   1589/ 13000 | global iter:   1589/ 13000 | loss: 2.1920 | ds_loss: 0.0000 | lr: 4.8223e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   1 | Iter:   1590/ 13000 | global iter:   1590/ 13000 | loss: 1.9918 | ds_loss: 0.0000 | lr: 4.8221e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1590/ 13000 | global iter:   1590/ 13000 | loss: 2.1598 | ds_loss: 0.0000 | lr: 4.8221e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 1.814
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1591/ 13000 | global iter:   1591/ 13000 | loss: 2.3314 | ds_loss: 0.0000 | lr: 4.8219e-05 | scale: 32768.0000 | micro time: 1.731 | step time: 0.000
train | epoch   1 | Iter:   1592/ 13000 | global iter:   1592/ 13000 | loss: 2.2599 | ds_loss: 0.0000 | lr: 4.8217e-05 | scale: 32768.0000 | micro time: 1.860 | step time: 0.000
train | epoch   1 | Iter:   1593/ 13000 | global iter:   1593/ 13000 | loss: 2.4166 | ds_loss: 0.0000 | lr: 4.8214e-05 | scale: 32768.0000 | micro time: 1.834 | step time: 0.000
train | epoch   1 | Iter:   1594/ 13000 | global iter:   1594/ 13000 | loss: 2.2567 | ds_loss: 0.0000 | lr: 4.8212e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   1 | Iter:   1595/ 13000 | global iter:   1595/ 13000 | loss: 2.1621 | ds_loss: 0.0000 | lr: 4.8210e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   1 | Iter:   1596/ 13000 | global iter:   1596/ 13000 | loss: 1.7194 | ds_loss: 0.0000 | lr: 4.8208e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   1 | Iter:   1597/ 13000 | global iter:   1597/ 13000 | loss: 2.4966 | ds_loss: 0.0000 | lr: 4.8206e-05 | scale: 32768.0000 | micro time: 1.838 | step time: 0.000
train | epoch   1 | Iter:   1598/ 13000 | global iter:   1598/ 13000 | loss: 2.6180 | ds_loss: 0.0000 | lr: 4.8203e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   1 | Iter:   1599/ 13000 | global iter:   1599/ 13000 | loss: 2.2930 | ds_loss: 0.0000 | lr: 4.8201e-05 | scale: 32768.0000 | micro time: 1.784 | step time: 0.000
train | epoch   1 | Iter:   1600/ 13000 | global iter:   1600/ 13000 | loss: 2.0913 | ds_loss: 0.0000 | lr: 4.8199e-05 | scale: 32768.0000 | micro time: 1.841 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1600/ 13000 | global iter:   1600/ 13000 | loss: 2.2645 | ds_loss: 0.0000 | lr: 4.8199e-05 | scale: 32768.0000 | micro time: 1.841 | step time: 1.817
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1601/ 13000 | global iter:   1601/ 13000 | loss: 1.9655 | ds_loss: 0.0000 | lr: 4.8197e-05 | scale: 32768.0000 | micro time: 1.790 | step time: 0.000
train | epoch   1 | Iter:   1602/ 13000 | global iter:   1602/ 13000 | loss: 2.6248 | ds_loss: 0.0000 | lr: 4.8194e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   1 | Iter:   1603/ 13000 | global iter:   1603/ 13000 | loss: 1.9999 | ds_loss: 0.0000 | lr: 4.8192e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   1 | Iter:   1604/ 13000 | global iter:   1604/ 13000 | loss: 2.3303 | ds_loss: 0.0000 | lr: 4.8190e-05 | scale: 32768.0000 | micro time: 1.857 | step time: 0.000
train | epoch   1 | Iter:   1605/ 13000 | global iter:   1605/ 13000 | loss: 1.8356 | ds_loss: 0.0000 | lr: 4.8188e-05 | scale: 32768.0000 | micro time: 1.735 | step time: 0.000
train | epoch   1 | Iter:   1606/ 13000 | global iter:   1606/ 13000 | loss: 2.3696 | ds_loss: 0.0000 | lr: 4.8185e-05 | scale: 32768.0000 | micro time: 1.899 | step time: 0.000
train | epoch   1 | Iter:   1607/ 13000 | global iter:   1607/ 13000 | loss: 2.3045 | ds_loss: 0.0000 | lr: 4.8183e-05 | scale: 32768.0000 | micro time: 1.856 | step time: 0.000
train | epoch   1 | Iter:   1608/ 13000 | global iter:   1608/ 13000 | loss: 2.0465 | ds_loss: 0.0000 | lr: 4.8181e-05 | scale: 32768.0000 | micro time: 1.698 | step time: 0.000
train | epoch   1 | Iter:   1609/ 13000 | global iter:   1609/ 13000 | loss: 2.2594 | ds_loss: 0.0000 | lr: 4.8178e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   1 | Iter:   1610/ 13000 | global iter:   1610/ 13000 | loss: 2.0958 | ds_loss: 0.0000 | lr: 4.8176e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1610/ 13000 | global iter:   1610/ 13000 | loss: 2.1832 | ds_loss: 0.0000 | lr: 4.8176e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 1.809
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1611/ 13000 | global iter:   1611/ 13000 | loss: 2.1852 | ds_loss: 0.0000 | lr: 4.8174e-05 | scale: 32768.0000 | micro time: 1.762 | step time: 0.000
train | epoch   1 | Iter:   1612/ 13000 | global iter:   1612/ 13000 | loss: 2.1701 | ds_loss: 0.0000 | lr: 4.8172e-05 | scale: 32768.0000 | micro time: 1.887 | step time: 0.000
train | epoch   1 | Iter:   1613/ 13000 | global iter:   1613/ 13000 | loss: 2.1917 | ds_loss: 0.0000 | lr: 4.8169e-05 | scale: 32768.0000 | micro time: 1.755 | step time: 0.000
train | epoch   1 | Iter:   1614/ 13000 | global iter:   1614/ 13000 | loss: 1.9998 | ds_loss: 0.0000 | lr: 4.8167e-05 | scale: 32768.0000 | micro time: 1.688 | step time: 0.000
train | epoch   1 | Iter:   1615/ 13000 | global iter:   1615/ 13000 | loss: 2.0930 | ds_loss: 0.0000 | lr: 4.8165e-05 | scale: 32768.0000 | micro time: 1.818 | step time: 0.000
train | epoch   1 | Iter:   1616/ 13000 | global iter:   1616/ 13000 | loss: 1.9645 | ds_loss: 0.0000 | lr: 4.8163e-05 | scale: 32768.0000 | micro time: 1.643 | step time: 0.000
train | epoch   1 | Iter:   1617/ 13000 | global iter:   1617/ 13000 | loss: 2.4244 | ds_loss: 0.0000 | lr: 4.8160e-05 | scale: 32768.0000 | micro time: 1.927 | step time: 0.000
train | epoch   1 | Iter:   1618/ 13000 | global iter:   1618/ 13000 | loss: 2.1919 | ds_loss: 0.0000 | lr: 4.8158e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   1 | Iter:   1619/ 13000 | global iter:   1619/ 13000 | loss: 1.8650 | ds_loss: 0.0000 | lr: 4.8156e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   1 | Iter:   1620/ 13000 | global iter:   1620/ 13000 | loss: 2.0597 | ds_loss: 0.0000 | lr: 4.8154e-05 | scale: 32768.0000 | micro time: 1.896 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1620/ 13000 | global iter:   1620/ 13000 | loss: 2.1145 | ds_loss: 0.0000 | lr: 4.8154e-05 | scale: 32768.0000 | micro time: 1.896 | step time: 1.807
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1621/ 13000 | global iter:   1621/ 13000 | loss: 1.3054 | ds_loss: 0.0000 | lr: 4.8151e-05 | scale: 32768.0000 | micro time: 1.852 | step time: 0.000
train | epoch   1 | Iter:   1622/ 13000 | global iter:   1622/ 13000 | loss: 2.4664 | ds_loss: 0.0000 | lr: 4.8149e-05 | scale: 32768.0000 | micro time: 1.694 | step time: 0.000
train | epoch   1 | Iter:   1623/ 13000 | global iter:   1623/ 13000 | loss: 2.4837 | ds_loss: 0.0000 | lr: 4.8147e-05 | scale: 32768.0000 | micro time: 1.945 | step time: 0.000
train | epoch   1 | Iter:   1624/ 13000 | global iter:   1624/ 13000 | loss: 2.2124 | ds_loss: 0.0000 | lr: 4.8144e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   1 | Iter:   1625/ 13000 | global iter:   1625/ 13000 | loss: 2.5076 | ds_loss: 0.0000 | lr: 4.8142e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   1 | Iter:   1626/ 13000 | global iter:   1626/ 13000 | loss: 2.6044 | ds_loss: 0.0000 | lr: 4.8140e-05 | scale: 32768.0000 | micro time: 1.797 | step time: 0.000
train | epoch   1 | Iter:   1627/ 13000 | global iter:   1627/ 13000 | loss: 2.5158 | ds_loss: 0.0000 | lr: 4.8138e-05 | scale: 32768.0000 | micro time: 1.765 | step time: 0.000
train | epoch   1 | Iter:   1628/ 13000 | global iter:   1628/ 13000 | loss: 2.1169 | ds_loss: 0.0000 | lr: 4.8135e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   1 | Iter:   1629/ 13000 | global iter:   1629/ 13000 | loss: 2.2974 | ds_loss: 0.0000 | lr: 4.8133e-05 | scale: 32768.0000 | micro time: 1.864 | step time: 0.000
train | epoch   1 | Iter:   1630/ 13000 | global iter:   1630/ 13000 | loss: 2.5217 | ds_loss: 0.0000 | lr: 4.8131e-05 | scale: 32768.0000 | micro time: 1.852 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1630/ 13000 | global iter:   1630/ 13000 | loss: 2.3032 | ds_loss: 0.0000 | lr: 4.8131e-05 | scale: 32768.0000 | micro time: 1.852 | step time: 1.819
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1631/ 13000 | global iter:   1631/ 13000 | loss: 2.4482 | ds_loss: 0.0000 | lr: 4.8128e-05 | scale: 32768.0000 | micro time: 1.808 | step time: 0.000
train | epoch   1 | Iter:   1632/ 13000 | global iter:   1632/ 13000 | loss: 1.9600 | ds_loss: 0.0000 | lr: 4.8126e-05 | scale: 32768.0000 | micro time: 1.703 | step time: 0.000
train | epoch   1 | Iter:   1633/ 13000 | global iter:   1633/ 13000 | loss: 1.7509 | ds_loss: 0.0000 | lr: 4.8124e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   1 | Iter:   1634/ 13000 | global iter:   1634/ 13000 | loss: 1.7236 | ds_loss: 0.0000 | lr: 4.8122e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   1 | Iter:   1635/ 13000 | global iter:   1635/ 13000 | loss: 2.4374 | ds_loss: 0.0000 | lr: 4.8119e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   1 | Iter:   1636/ 13000 | global iter:   1636/ 13000 | loss: 2.6265 | ds_loss: 0.0000 | lr: 4.8117e-05 | scale: 32768.0000 | micro time: 1.845 | step time: 0.000
train | epoch   1 | Iter:   1637/ 13000 | global iter:   1637/ 13000 | loss: 1.4915 | ds_loss: 0.0000 | lr: 4.8115e-05 | scale: 32768.0000 | micro time: 1.853 | step time: 0.000
train | epoch   1 | Iter:   1638/ 13000 | global iter:   1638/ 13000 | loss: 2.0052 | ds_loss: 0.0000 | lr: 4.8112e-05 | scale: 32768.0000 | micro time: 1.713 | step time: 0.000
train | epoch   1 | Iter:   1639/ 13000 | global iter:   1639/ 13000 | loss: 1.7633 | ds_loss: 0.0000 | lr: 4.8110e-05 | scale: 32768.0000 | micro time: 1.776 | step time: 0.000
train | epoch   1 | Iter:   1640/ 13000 | global iter:   1640/ 13000 | loss: 2.0778 | ds_loss: 0.0000 | lr: 4.8108e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1640/ 13000 | global iter:   1640/ 13000 | loss: 2.0284 | ds_loss: 0.0000 | lr: 4.8108e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 1.790
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1641/ 13000 | global iter:   1641/ 13000 | loss: 1.8754 | ds_loss: 0.0000 | lr: 4.8105e-05 | scale: 32768.0000 | micro time: 1.765 | step time: 0.000
train | epoch   1 | Iter:   1642/ 13000 | global iter:   1642/ 13000 | loss: 2.4540 | ds_loss: 0.0000 | lr: 4.8103e-05 | scale: 32768.0000 | micro time: 1.786 | step time: 0.000
train | epoch   1 | Iter:   1643/ 13000 | global iter:   1643/ 13000 | loss: 1.8053 | ds_loss: 0.0000 | lr: 4.8101e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   1 | Iter:   1644/ 13000 | global iter:   1644/ 13000 | loss: 2.8925 | ds_loss: 0.0000 | lr: 4.8098e-05 | scale: 32768.0000 | micro time: 1.874 | step time: 0.000
train | epoch   1 | Iter:   1645/ 13000 | global iter:   1645/ 13000 | loss: 2.3174 | ds_loss: 0.0000 | lr: 4.8096e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   1 | Iter:   1646/ 13000 | global iter:   1646/ 13000 | loss: 2.3064 | ds_loss: 0.0000 | lr: 4.8094e-05 | scale: 32768.0000 | micro time: 1.743 | step time: 0.000
train | epoch   1 | Iter:   1647/ 13000 | global iter:   1647/ 13000 | loss: 1.7777 | ds_loss: 0.0000 | lr: 4.8092e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   1 | Iter:   1648/ 13000 | global iter:   1648/ 13000 | loss: 2.1341 | ds_loss: 0.0000 | lr: 4.8089e-05 | scale: 32768.0000 | micro time: 1.887 | step time: 0.000
train | epoch   1 | Iter:   1649/ 13000 | global iter:   1649/ 13000 | loss: 2.4100 | ds_loss: 0.0000 | lr: 4.8087e-05 | scale: 32768.0000 | micro time: 1.796 | step time: 0.000
train | epoch   1 | Iter:   1650/ 13000 | global iter:   1650/ 13000 | loss: 1.7344 | ds_loss: 0.0000 | lr: 4.8085e-05 | scale: 32768.0000 | micro time: 1.751 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1650/ 13000 | global iter:   1650/ 13000 | loss: 2.1707 | ds_loss: 0.0000 | lr: 4.8085e-05 | scale: 32768.0000 | micro time: 1.751 | step time: 1.812
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1651/ 13000 | global iter:   1651/ 13000 | loss: 2.5394 | ds_loss: 0.0000 | lr: 4.8082e-05 | scale: 32768.0000 | micro time: 1.797 | step time: 0.000
train | epoch   1 | Iter:   1652/ 13000 | global iter:   1652/ 13000 | loss: 1.9174 | ds_loss: 0.0000 | lr: 4.8080e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   1 | Iter:   1653/ 13000 | global iter:   1653/ 13000 | loss: 2.2846 | ds_loss: 0.0000 | lr: 4.8078e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   1 | Iter:   1654/ 13000 | global iter:   1654/ 13000 | loss: 2.1421 | ds_loss: 0.0000 | lr: 4.8075e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   1 | Iter:   1655/ 13000 | global iter:   1655/ 13000 | loss: 2.3791 | ds_loss: 0.0000 | lr: 4.8073e-05 | scale: 32768.0000 | micro time: 1.739 | step time: 0.000
train | epoch   1 | Iter:   1656/ 13000 | global iter:   1656/ 13000 | loss: 1.9006 | ds_loss: 0.0000 | lr: 4.8071e-05 | scale: 32768.0000 | micro time: 1.879 | step time: 0.000
train | epoch   1 | Iter:   1657/ 13000 | global iter:   1657/ 13000 | loss: 2.2050 | ds_loss: 0.0000 | lr: 4.8068e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   1 | Iter:   1658/ 13000 | global iter:   1658/ 13000 | loss: 1.8920 | ds_loss: 0.0000 | lr: 4.8066e-05 | scale: 32768.0000 | micro time: 1.792 | step time: 0.000
train | epoch   1 | Iter:   1659/ 13000 | global iter:   1659/ 13000 | loss: 2.5306 | ds_loss: 0.0000 | lr: 4.8064e-05 | scale: 32768.0000 | micro time: 1.790 | step time: 0.000
train | epoch   1 | Iter:   1660/ 13000 | global iter:   1660/ 13000 | loss: 2.3687 | ds_loss: 0.0000 | lr: 4.8061e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1660/ 13000 | global iter:   1660/ 13000 | loss: 2.2159 | ds_loss: 0.0000 | lr: 4.8061e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 1.809
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1661/ 13000 | global iter:   1661/ 13000 | loss: 2.2680 | ds_loss: 0.0000 | lr: 4.8059e-05 | scale: 32768.0000 | micro time: 1.814 | step time: 0.000
train | epoch   1 | Iter:   1662/ 13000 | global iter:   1662/ 13000 | loss: 2.2235 | ds_loss: 0.0000 | lr: 4.8057e-05 | scale: 32768.0000 | micro time: 1.747 | step time: 0.000
train | epoch   1 | Iter:   1663/ 13000 | global iter:   1663/ 13000 | loss: 2.1097 | ds_loss: 0.0000 | lr: 4.8054e-05 | scale: 32768.0000 | micro time: 1.824 | step time: 0.000
train | epoch   1 | Iter:   1664/ 13000 | global iter:   1664/ 13000 | loss: 2.1446 | ds_loss: 0.0000 | lr: 4.8052e-05 | scale: 32768.0000 | micro time: 1.725 | step time: 0.000
train | epoch   1 | Iter:   1665/ 13000 | global iter:   1665/ 13000 | loss: 2.7448 | ds_loss: 0.0000 | lr: 4.8050e-05 | scale: 32768.0000 | micro time: 1.748 | step time: 0.000
train | epoch   1 | Iter:   1666/ 13000 | global iter:   1666/ 13000 | loss: 2.6981 | ds_loss: 0.0000 | lr: 4.8047e-05 | scale: 32768.0000 | micro time: 1.784 | step time: 0.000
train | epoch   1 | Iter:   1667/ 13000 | global iter:   1667/ 13000 | loss: 2.3243 | ds_loss: 0.0000 | lr: 4.8045e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   1 | Iter:   1668/ 13000 | global iter:   1668/ 13000 | loss: 2.3306 | ds_loss: 0.0000 | lr: 4.8043e-05 | scale: 32768.0000 | micro time: 1.879 | step time: 0.000
train | epoch   1 | Iter:   1669/ 13000 | global iter:   1669/ 13000 | loss: 2.2480 | ds_loss: 0.0000 | lr: 4.8040e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   1 | Iter:   1670/ 13000 | global iter:   1670/ 13000 | loss: 1.6277 | ds_loss: 0.0000 | lr: 4.8038e-05 | scale: 32768.0000 | micro time: 1.732 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1670/ 13000 | global iter:   1670/ 13000 | loss: 2.2719 | ds_loss: 0.0000 | lr: 4.8038e-05 | scale: 32768.0000 | micro time: 1.732 | step time: 1.789
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1671/ 13000 | global iter:   1671/ 13000 | loss: 2.5159 | ds_loss: 0.0000 | lr: 4.8036e-05 | scale: 32768.0000 | micro time: 1.693 | step time: 0.000
train | epoch   1 | Iter:   1672/ 13000 | global iter:   1672/ 13000 | loss: 1.7979 | ds_loss: 0.0000 | lr: 4.8033e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   1 | Iter:   1673/ 13000 | global iter:   1673/ 13000 | loss: 2.0485 | ds_loss: 0.0000 | lr: 4.8031e-05 | scale: 32768.0000 | micro time: 1.832 | step time: 0.000
train | epoch   1 | Iter:   1674/ 13000 | global iter:   1674/ 13000 | loss: 1.9323 | ds_loss: 0.0000 | lr: 4.8029e-05 | scale: 32768.0000 | micro time: 1.690 | step time: 0.000
train | epoch   1 | Iter:   1675/ 13000 | global iter:   1675/ 13000 | loss: 2.1155 | ds_loss: 0.0000 | lr: 4.8026e-05 | scale: 32768.0000 | micro time: 1.751 | step time: 0.000
train | epoch   1 | Iter:   1676/ 13000 | global iter:   1676/ 13000 | loss: 2.1980 | ds_loss: 0.0000 | lr: 4.8024e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   1 | Iter:   1677/ 13000 | global iter:   1677/ 13000 | loss: 2.4847 | ds_loss: 0.0000 | lr: 4.8022e-05 | scale: 32768.0000 | micro time: 1.719 | step time: 0.000
train | epoch   1 | Iter:   1678/ 13000 | global iter:   1678/ 13000 | loss: 2.2112 | ds_loss: 0.0000 | lr: 4.8019e-05 | scale: 32768.0000 | micro time: 1.919 | step time: 0.000
train | epoch   1 | Iter:   1679/ 13000 | global iter:   1679/ 13000 | loss: 2.2282 | ds_loss: 0.0000 | lr: 4.8017e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   1 | Iter:   1680/ 13000 | global iter:   1680/ 13000 | loss: 2.5446 | ds_loss: 0.0000 | lr: 4.8015e-05 | scale: 32768.0000 | micro time: 1.689 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1680/ 13000 | global iter:   1680/ 13000 | loss: 2.2077 | ds_loss: 0.0000 | lr: 4.8015e-05 | scale: 32768.0000 | micro time: 1.689 | step time: 1.773
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1681/ 13000 | global iter:   1681/ 13000 | loss: 2.1565 | ds_loss: 0.0000 | lr: 4.8012e-05 | scale: 32768.0000 | micro time: 1.758 | step time: 0.000
train | epoch   1 | Iter:   1682/ 13000 | global iter:   1682/ 13000 | loss: 1.8025 | ds_loss: 0.0000 | lr: 4.8010e-05 | scale: 32768.0000 | micro time: 1.726 | step time: 0.000
train | epoch   1 | Iter:   1683/ 13000 | global iter:   1683/ 13000 | loss: 2.4271 | ds_loss: 0.0000 | lr: 4.8007e-05 | scale: 32768.0000 | micro time: 1.776 | step time: 0.000
train | epoch   1 | Iter:   1684/ 13000 | global iter:   1684/ 13000 | loss: 2.5058 | ds_loss: 0.0000 | lr: 4.8005e-05 | scale: 32768.0000 | micro time: 1.784 | step time: 0.000
train | epoch   1 | Iter:   1685/ 13000 | global iter:   1685/ 13000 | loss: 2.0377 | ds_loss: 0.0000 | lr: 4.8003e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   1 | Iter:   1686/ 13000 | global iter:   1686/ 13000 | loss: 2.0971 | ds_loss: 0.0000 | lr: 4.8000e-05 | scale: 32768.0000 | micro time: 1.762 | step time: 0.000
train | epoch   1 | Iter:   1687/ 13000 | global iter:   1687/ 13000 | loss: 2.5597 | ds_loss: 0.0000 | lr: 4.7998e-05 | scale: 32768.0000 | micro time: 1.840 | step time: 0.000
train | epoch   1 | Iter:   1688/ 13000 | global iter:   1688/ 13000 | loss: 2.2512 | ds_loss: 0.0000 | lr: 4.7996e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   1 | Iter:   1689/ 13000 | global iter:   1689/ 13000 | loss: 1.7122 | ds_loss: 0.0000 | lr: 4.7993e-05 | scale: 32768.0000 | micro time: 1.804 | step time: 0.000
train | epoch   1 | Iter:   1690/ 13000 | global iter:   1690/ 13000 | loss: 1.8143 | ds_loss: 0.0000 | lr: 4.7991e-05 | scale: 32768.0000 | micro time: 1.747 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1690/ 13000 | global iter:   1690/ 13000 | loss: 2.1364 | ds_loss: 0.0000 | lr: 4.7991e-05 | scale: 32768.0000 | micro time: 1.747 | step time: 1.780
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1691/ 13000 | global iter:   1691/ 13000 | loss: 2.1983 | ds_loss: 0.0000 | lr: 4.7988e-05 | scale: 32768.0000 | micro time: 1.789 | step time: 0.000
train | epoch   1 | Iter:   1692/ 13000 | global iter:   1692/ 13000 | loss: 2.2807 | ds_loss: 0.0000 | lr: 4.7986e-05 | scale: 32768.0000 | micro time: 1.770 | step time: 0.000
train | epoch   1 | Iter:   1693/ 13000 | global iter:   1693/ 13000 | loss: 1.7343 | ds_loss: 0.0000 | lr: 4.7984e-05 | scale: 32768.0000 | micro time: 1.741 | step time: 0.000
train | epoch   1 | Iter:   1694/ 13000 | global iter:   1694/ 13000 | loss: 2.2308 | ds_loss: 0.0000 | lr: 4.7981e-05 | scale: 32768.0000 | micro time: 1.743 | step time: 0.000
train | epoch   1 | Iter:   1695/ 13000 | global iter:   1695/ 13000 | loss: 2.1355 | ds_loss: 0.0000 | lr: 4.7979e-05 | scale: 32768.0000 | micro time: 1.802 | step time: 0.000
train | epoch   1 | Iter:   1696/ 13000 | global iter:   1696/ 13000 | loss: 1.8933 | ds_loss: 0.0000 | lr: 4.7977e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   1 | Iter:   1697/ 13000 | global iter:   1697/ 13000 | loss: 2.2761 | ds_loss: 0.0000 | lr: 4.7974e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   1 | Iter:   1698/ 13000 | global iter:   1698/ 13000 | loss: 2.1250 | ds_loss: 0.0000 | lr: 4.7972e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   1 | Iter:   1699/ 13000 | global iter:   1699/ 13000 | loss: 2.6016 | ds_loss: 0.0000 | lr: 4.7969e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   1 | Iter:   1700/ 13000 | global iter:   1700/ 13000 | loss: 1.9796 | ds_loss: 0.0000 | lr: 4.7967e-05 | scale: 32768.0000 | micro time: 1.887 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1700/ 13000 | global iter:   1700/ 13000 | loss: 2.1455 | ds_loss: 0.0000 | lr: 4.7967e-05 | scale: 32768.0000 | micro time: 1.887 | step time: 1.810
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1701/ 13000 | global iter:   1701/ 13000 | loss: 1.8838 | ds_loss: 0.0000 | lr: 4.7965e-05 | scale: 32768.0000 | micro time: 1.795 | step time: 0.000
train | epoch   1 | Iter:   1702/ 13000 | global iter:   1702/ 13000 | loss: 2.4193 | ds_loss: 0.0000 | lr: 4.7962e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   1 | Iter:   1703/ 13000 | global iter:   1703/ 13000 | loss: 2.2178 | ds_loss: 0.0000 | lr: 4.7960e-05 | scale: 32768.0000 | micro time: 1.853 | step time: 0.000
train | epoch   1 | Iter:   1704/ 13000 | global iter:   1704/ 13000 | loss: 2.5018 | ds_loss: 0.0000 | lr: 4.7958e-05 | scale: 32768.0000 | micro time: 1.770 | step time: 0.000
train | epoch   1 | Iter:   1705/ 13000 | global iter:   1705/ 13000 | loss: 1.6696 | ds_loss: 0.0000 | lr: 4.7955e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   1 | Iter:   1706/ 13000 | global iter:   1706/ 13000 | loss: 2.3766 | ds_loss: 0.0000 | lr: 4.7953e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   1 | Iter:   1707/ 13000 | global iter:   1707/ 13000 | loss: 2.2490 | ds_loss: 0.0000 | lr: 4.7950e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   1 | Iter:   1708/ 13000 | global iter:   1708/ 13000 | loss: 1.6748 | ds_loss: 0.0000 | lr: 4.7948e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   1 | Iter:   1709/ 13000 | global iter:   1709/ 13000 | loss: 2.6628 | ds_loss: 0.0000 | lr: 4.7946e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   1 | Iter:   1710/ 13000 | global iter:   1710/ 13000 | loss: 1.9326 | ds_loss: 0.0000 | lr: 4.7943e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1710/ 13000 | global iter:   1710/ 13000 | loss: 2.1588 | ds_loss: 0.0000 | lr: 4.7943e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 1.820
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1711/ 13000 | global iter:   1711/ 13000 | loss: 2.6818 | ds_loss: 0.0000 | lr: 4.7941e-05 | scale: 32768.0000 | micro time: 1.798 | step time: 0.000
train | epoch   1 | Iter:   1712/ 13000 | global iter:   1712/ 13000 | loss: 1.7138 | ds_loss: 0.0000 | lr: 4.7938e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   1 | Iter:   1713/ 13000 | global iter:   1713/ 13000 | loss: 1.9923 | ds_loss: 0.0000 | lr: 4.7936e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   1 | Iter:   1714/ 13000 | global iter:   1714/ 13000 | loss: 2.4653 | ds_loss: 0.0000 | lr: 4.7934e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   1 | Iter:   1715/ 13000 | global iter:   1715/ 13000 | loss: 2.1628 | ds_loss: 0.0000 | lr: 4.7931e-05 | scale: 32768.0000 | micro time: 1.881 | step time: 0.000
train | epoch   1 | Iter:   1716/ 13000 | global iter:   1716/ 13000 | loss: 2.7698 | ds_loss: 0.0000 | lr: 4.7929e-05 | scale: 32768.0000 | micro time: 1.856 | step time: 0.000
train | epoch   1 | Iter:   1717/ 13000 | global iter:   1717/ 13000 | loss: 1.7831 | ds_loss: 0.0000 | lr: 4.7926e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   1 | Iter:   1718/ 13000 | global iter:   1718/ 13000 | loss: 2.1646 | ds_loss: 0.0000 | lr: 4.7924e-05 | scale: 32768.0000 | micro time: 1.760 | step time: 0.000
train | epoch   1 | Iter:   1719/ 13000 | global iter:   1719/ 13000 | loss: 1.3813 | ds_loss: 0.0000 | lr: 4.7922e-05 | scale: 32768.0000 | micro time: 1.786 | step time: 0.000
train | epoch   1 | Iter:   1720/ 13000 | global iter:   1720/ 13000 | loss: 2.1452 | ds_loss: 0.0000 | lr: 4.7919e-05 | scale: 32768.0000 | micro time: 1.735 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1720/ 13000 | global iter:   1720/ 13000 | loss: 2.1260 | ds_loss: 0.0000 | lr: 4.7919e-05 | scale: 32768.0000 | micro time: 1.735 | step time: 1.808
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1721/ 13000 | global iter:   1721/ 13000 | loss: 1.8774 | ds_loss: 0.0000 | lr: 4.7917e-05 | scale: 32768.0000 | micro time: 1.925 | step time: 0.000
train | epoch   1 | Iter:   1722/ 13000 | global iter:   1722/ 13000 | loss: 1.7238 | ds_loss: 0.0000 | lr: 4.7914e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   1 | Iter:   1723/ 13000 | global iter:   1723/ 13000 | loss: 1.8458 | ds_loss: 0.0000 | lr: 4.7912e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   1 | Iter:   1724/ 13000 | global iter:   1724/ 13000 | loss: 2.2654 | ds_loss: 0.0000 | lr: 4.7909e-05 | scale: 32768.0000 | micro time: 1.712 | step time: 0.000
train | epoch   1 | Iter:   1725/ 13000 | global iter:   1725/ 13000 | loss: 2.9173 | ds_loss: 0.0000 | lr: 4.7907e-05 | scale: 32768.0000 | micro time: 1.825 | step time: 0.000
train | epoch   1 | Iter:   1726/ 13000 | global iter:   1726/ 13000 | loss: 2.2958 | ds_loss: 0.0000 | lr: 4.7905e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   1 | Iter:   1727/ 13000 | global iter:   1727/ 13000 | loss: 1.6179 | ds_loss: 0.0000 | lr: 4.7902e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   1 | Iter:   1728/ 13000 | global iter:   1728/ 13000 | loss: 1.7637 | ds_loss: 0.0000 | lr: 4.7900e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   1 | Iter:   1729/ 13000 | global iter:   1729/ 13000 | loss: 2.4923 | ds_loss: 0.0000 | lr: 4.7897e-05 | scale: 32768.0000 | micro time: 1.710 | step time: 0.000
train | epoch   1 | Iter:   1730/ 13000 | global iter:   1730/ 13000 | loss: 2.7505 | ds_loss: 0.0000 | lr: 4.7895e-05 | scale: 32768.0000 | micro time: 1.810 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1730/ 13000 | global iter:   1730/ 13000 | loss: 2.1550 | ds_loss: 0.0000 | lr: 4.7895e-05 | scale: 32768.0000 | micro time: 1.810 | step time: 1.814
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1731/ 13000 | global iter:   1731/ 13000 | loss: 2.6707 | ds_loss: 0.0000 | lr: 4.7893e-05 | scale: 32768.0000 | micro time: 1.746 | step time: 0.000
train | epoch   1 | Iter:   1732/ 13000 | global iter:   1732/ 13000 | loss: 1.9939 | ds_loss: 0.0000 | lr: 4.7890e-05 | scale: 32768.0000 | micro time: 1.877 | step time: 0.000
train | epoch   1 | Iter:   1733/ 13000 | global iter:   1733/ 13000 | loss: 2.4900 | ds_loss: 0.0000 | lr: 4.7888e-05 | scale: 32768.0000 | micro time: 1.756 | step time: 0.000
train | epoch   1 | Iter:   1734/ 13000 | global iter:   1734/ 13000 | loss: 1.8188 | ds_loss: 0.0000 | lr: 4.7885e-05 | scale: 32768.0000 | micro time: 1.844 | step time: 0.000
train | epoch   1 | Iter:   1735/ 13000 | global iter:   1735/ 13000 | loss: 2.0902 | ds_loss: 0.0000 | lr: 4.7883e-05 | scale: 32768.0000 | micro time: 1.805 | step time: 0.000
train | epoch   1 | Iter:   1736/ 13000 | global iter:   1736/ 13000 | loss: 1.4837 | ds_loss: 0.0000 | lr: 4.7880e-05 | scale: 32768.0000 | micro time: 1.744 | step time: 0.000
train | epoch   1 | Iter:   1737/ 13000 | global iter:   1737/ 13000 | loss: 2.3101 | ds_loss: 0.0000 | lr: 4.7878e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   1 | Iter:   1738/ 13000 | global iter:   1738/ 13000 | loss: 2.5081 | ds_loss: 0.0000 | lr: 4.7876e-05 | scale: 32768.0000 | micro time: 1.683 | step time: 0.000
train | epoch   1 | Iter:   1739/ 13000 | global iter:   1739/ 13000 | loss: 2.0354 | ds_loss: 0.0000 | lr: 4.7873e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 0.000
train | epoch   1 | Iter:   1740/ 13000 | global iter:   1740/ 13000 | loss: 2.6171 | ds_loss: 0.0000 | lr: 4.7871e-05 | scale: 32768.0000 | micro time: 1.717 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1740/ 13000 | global iter:   1740/ 13000 | loss: 2.2018 | ds_loss: 0.0000 | lr: 4.7871e-05 | scale: 32768.0000 | micro time: 1.717 | step time: 1.782
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1741/ 13000 | global iter:   1741/ 13000 | loss: 2.4493 | ds_loss: 0.0000 | lr: 4.7868e-05 | scale: 32768.0000 | micro time: 1.712 | step time: 0.000
train | epoch   1 | Iter:   1742/ 13000 | global iter:   1742/ 13000 | loss: 2.5608 | ds_loss: 0.0000 | lr: 4.7866e-05 | scale: 32768.0000 | micro time: 1.689 | step time: 0.000
train | epoch   1 | Iter:   1743/ 13000 | global iter:   1743/ 13000 | loss: 1.9804 | ds_loss: 0.0000 | lr: 4.7863e-05 | scale: 32768.0000 | micro time: 1.757 | step time: 0.000
train | epoch   1 | Iter:   1744/ 13000 | global iter:   1744/ 13000 | loss: 1.7850 | ds_loss: 0.0000 | lr: 4.7861e-05 | scale: 32768.0000 | micro time: 1.845 | step time: 0.000
train | epoch   1 | Iter:   1745/ 13000 | global iter:   1745/ 13000 | loss: 1.1878 | ds_loss: 0.0000 | lr: 4.7858e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   1 | Iter:   1746/ 13000 | global iter:   1746/ 13000 | loss: 1.7782 | ds_loss: 0.0000 | lr: 4.7856e-05 | scale: 32768.0000 | micro time: 1.876 | step time: 0.000
train | epoch   1 | Iter:   1747/ 13000 | global iter:   1747/ 13000 | loss: 1.1687 | ds_loss: 0.0000 | lr: 4.7854e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   1 | Iter:   1748/ 13000 | global iter:   1748/ 13000 | loss: 2.0051 | ds_loss: 0.0000 | lr: 4.7851e-05 | scale: 32768.0000 | micro time: 1.822 | step time: 0.000
train | epoch   1 | Iter:   1749/ 13000 | global iter:   1749/ 13000 | loss: 2.2612 | ds_loss: 0.0000 | lr: 4.7849e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   1 | Iter:   1750/ 13000 | global iter:   1750/ 13000 | loss: 2.0017 | ds_loss: 0.0000 | lr: 4.7846e-05 | scale: 32768.0000 | micro time: 1.669 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1750/ 13000 | global iter:   1750/ 13000 | loss: 1.9178 | ds_loss: 0.0000 | lr: 4.7846e-05 | scale: 32768.0000 | micro time: 1.669 | step time: 1.776
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1751/ 13000 | global iter:   1751/ 13000 | loss: 1.8978 | ds_loss: 0.0000 | lr: 4.7844e-05 | scale: 32768.0000 | micro time: 1.714 | step time: 0.000
train | epoch   1 | Iter:   1752/ 13000 | global iter:   1752/ 13000 | loss: 2.5304 | ds_loss: 0.0000 | lr: 4.7841e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   1 | Iter:   1753/ 13000 | global iter:   1753/ 13000 | loss: 2.0992 | ds_loss: 0.0000 | lr: 4.7839e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   1 | Iter:   1754/ 13000 | global iter:   1754/ 13000 | loss: 1.7499 | ds_loss: 0.0000 | lr: 4.7836e-05 | scale: 32768.0000 | micro time: 1.745 | step time: 0.000
train | epoch   1 | Iter:   1755/ 13000 | global iter:   1755/ 13000 | loss: 1.6414 | ds_loss: 0.0000 | lr: 4.7834e-05 | scale: 32768.0000 | micro time: 1.809 | step time: 0.000
train | epoch   1 | Iter:   1756/ 13000 | global iter:   1756/ 13000 | loss: 2.0123 | ds_loss: 0.0000 | lr: 4.7832e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   1 | Iter:   1757/ 13000 | global iter:   1757/ 13000 | loss: 1.6446 | ds_loss: 0.0000 | lr: 4.7829e-05 | scale: 32768.0000 | micro time: 1.716 | step time: 0.000
train | epoch   1 | Iter:   1758/ 13000 | global iter:   1758/ 13000 | loss: 1.8996 | ds_loss: 0.0000 | lr: 4.7827e-05 | scale: 32768.0000 | micro time: 1.739 | step time: 0.000
train | epoch   1 | Iter:   1759/ 13000 | global iter:   1759/ 13000 | loss: 2.0836 | ds_loss: 0.0000 | lr: 4.7824e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   1 | Iter:   1760/ 13000 | global iter:   1760/ 13000 | loss: 2.2945 | ds_loss: 0.0000 | lr: 4.7822e-05 | scale: 32768.0000 | micro time: 1.743 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1760/ 13000 | global iter:   1760/ 13000 | loss: 1.9853 | ds_loss: 0.0000 | lr: 4.7822e-05 | scale: 32768.0000 | micro time: 1.743 | step time: 1.785
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1761/ 13000 | global iter:   1761/ 13000 | loss: 1.8589 | ds_loss: 0.0000 | lr: 4.7819e-05 | scale: 32768.0000 | micro time: 1.786 | step time: 0.000
train | epoch   1 | Iter:   1762/ 13000 | global iter:   1762/ 13000 | loss: 2.2840 | ds_loss: 0.0000 | lr: 4.7817e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   1 | Iter:   1763/ 13000 | global iter:   1763/ 13000 | loss: 2.4308 | ds_loss: 0.0000 | lr: 4.7814e-05 | scale: 32768.0000 | micro time: 1.879 | step time: 0.000
train | epoch   1 | Iter:   1764/ 13000 | global iter:   1764/ 13000 | loss: 2.1746 | ds_loss: 0.0000 | lr: 4.7812e-05 | scale: 32768.0000 | micro time: 1.727 | step time: 0.000
train | epoch   1 | Iter:   1765/ 13000 | global iter:   1765/ 13000 | loss: 2.5764 | ds_loss: 0.0000 | lr: 4.7809e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   1 | Iter:   1766/ 13000 | global iter:   1766/ 13000 | loss: 2.7892 | ds_loss: 0.0000 | lr: 4.7807e-05 | scale: 32768.0000 | micro time: 1.667 | step time: 0.000
train | epoch   1 | Iter:   1767/ 13000 | global iter:   1767/ 13000 | loss: 2.2103 | ds_loss: 0.0000 | lr: 4.7804e-05 | scale: 32768.0000 | micro time: 1.735 | step time: 0.000
train | epoch   1 | Iter:   1768/ 13000 | global iter:   1768/ 13000 | loss: 2.4635 | ds_loss: 0.0000 | lr: 4.7802e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   1 | Iter:   1769/ 13000 | global iter:   1769/ 13000 | loss: 2.4640 | ds_loss: 0.0000 | lr: 4.7799e-05 | scale: 32768.0000 | micro time: 1.870 | step time: 0.000
train | epoch   1 | Iter:   1770/ 13000 | global iter:   1770/ 13000 | loss: 2.0037 | ds_loss: 0.0000 | lr: 4.7797e-05 | scale: 32768.0000 | micro time: 1.720 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1770/ 13000 | global iter:   1770/ 13000 | loss: 2.3255 | ds_loss: 0.0000 | lr: 4.7797e-05 | scale: 32768.0000 | micro time: 1.720 | step time: 1.790
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1771/ 13000 | global iter:   1771/ 13000 | loss: 1.6713 | ds_loss: 0.0000 | lr: 4.7794e-05 | scale: 32768.0000 | micro time: 1.774 | step time: 0.000
train | epoch   1 | Iter:   1772/ 13000 | global iter:   1772/ 13000 | loss: 1.9129 | ds_loss: 0.0000 | lr: 4.7792e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   1 | Iter:   1773/ 13000 | global iter:   1773/ 13000 | loss: 2.0057 | ds_loss: 0.0000 | lr: 4.7790e-05 | scale: 32768.0000 | micro time: 1.860 | step time: 0.000
train | epoch   1 | Iter:   1774/ 13000 | global iter:   1774/ 13000 | loss: 2.3110 | ds_loss: 0.0000 | lr: 4.7787e-05 | scale: 32768.0000 | micro time: 1.790 | step time: 0.000
train | epoch   1 | Iter:   1775/ 13000 | global iter:   1775/ 13000 | loss: 2.8151 | ds_loss: 0.0000 | lr: 4.7785e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   1 | Iter:   1776/ 13000 | global iter:   1776/ 13000 | loss: 2.1339 | ds_loss: 0.0000 | lr: 4.7782e-05 | scale: 32768.0000 | micro time: 1.853 | step time: 0.000
train | epoch   1 | Iter:   1777/ 13000 | global iter:   1777/ 13000 | loss: 2.4652 | ds_loss: 0.0000 | lr: 4.7780e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
train | epoch   1 | Iter:   1778/ 13000 | global iter:   1778/ 13000 | loss: 1.6248 | ds_loss: 0.0000 | lr: 4.7777e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   1 | Iter:   1779/ 13000 | global iter:   1779/ 13000 | loss: 2.1321 | ds_loss: 0.0000 | lr: 4.7775e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   1 | Iter:   1780/ 13000 | global iter:   1780/ 13000 | loss: 2.2192 | ds_loss: 0.0000 | lr: 4.7772e-05 | scale: 32768.0000 | micro time: 1.838 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1780/ 13000 | global iter:   1780/ 13000 | loss: 2.1291 | ds_loss: 0.0000 | lr: 4.7772e-05 | scale: 32768.0000 | micro time: 1.838 | step time: 1.821
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1781/ 13000 | global iter:   1781/ 13000 | loss: 1.5664 | ds_loss: 0.0000 | lr: 4.7770e-05 | scale: 32768.0000 | micro time: 1.707 | step time: 0.000
train | epoch   1 | Iter:   1782/ 13000 | global iter:   1782/ 13000 | loss: 2.4310 | ds_loss: 0.0000 | lr: 4.7767e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   1 | Iter:   1783/ 13000 | global iter:   1783/ 13000 | loss: 2.1088 | ds_loss: 0.0000 | lr: 4.7765e-05 | scale: 32768.0000 | micro time: 1.907 | step time: 0.000
train | epoch   1 | Iter:   1784/ 13000 | global iter:   1784/ 13000 | loss: 2.1821 | ds_loss: 0.0000 | lr: 4.7762e-05 | scale: 32768.0000 | micro time: 1.832 | step time: 0.000
train | epoch   1 | Iter:   1785/ 13000 | global iter:   1785/ 13000 | loss: 2.0224 | ds_loss: 0.0000 | lr: 4.7760e-05 | scale: 32768.0000 | micro time: 1.830 | step time: 0.000
train | epoch   1 | Iter:   1786/ 13000 | global iter:   1786/ 13000 | loss: 1.8811 | ds_loss: 0.0000 | lr: 4.7757e-05 | scale: 32768.0000 | micro time: 1.770 | step time: 0.000
train | epoch   1 | Iter:   1787/ 13000 | global iter:   1787/ 13000 | loss: 1.8278 | ds_loss: 0.0000 | lr: 4.7755e-05 | scale: 32768.0000 | micro time: 1.720 | step time: 0.000
train | epoch   1 | Iter:   1788/ 13000 | global iter:   1788/ 13000 | loss: 1.3437 | ds_loss: 0.0000 | lr: 4.7752e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 0.000
train | epoch   1 | Iter:   1789/ 13000 | global iter:   1789/ 13000 | loss: 2.1804 | ds_loss: 0.0000 | lr: 4.7750e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   1 | Iter:   1790/ 13000 | global iter:   1790/ 13000 | loss: 1.6751 | ds_loss: 0.0000 | lr: 4.7747e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1790/ 13000 | global iter:   1790/ 13000 | loss: 1.9219 | ds_loss: 0.0000 | lr: 4.7747e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 1.797
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1791/ 13000 | global iter:   1791/ 13000 | loss: 1.9916 | ds_loss: 0.0000 | lr: 4.7745e-05 | scale: 32768.0000 | micro time: 1.728 | step time: 0.000
train | epoch   1 | Iter:   1792/ 13000 | global iter:   1792/ 13000 | loss: 1.5312 | ds_loss: 0.0000 | lr: 4.7742e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   1 | Iter:   1793/ 13000 | global iter:   1793/ 13000 | loss: 1.9690 | ds_loss: 0.0000 | lr: 4.7740e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   1 | Iter:   1794/ 13000 | global iter:   1794/ 13000 | loss: 2.0927 | ds_loss: 0.0000 | lr: 4.7737e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   1 | Iter:   1795/ 13000 | global iter:   1795/ 13000 | loss: 2.0110 | ds_loss: 0.0000 | lr: 4.7735e-05 | scale: 32768.0000 | micro time: 1.809 | step time: 0.000
train | epoch   1 | Iter:   1796/ 13000 | global iter:   1796/ 13000 | loss: 1.0318 | ds_loss: 0.0000 | lr: 4.7732e-05 | scale: 32768.0000 | micro time: 1.840 | step time: 0.000
train | epoch   1 | Iter:   1797/ 13000 | global iter:   1797/ 13000 | loss: 2.7468 | ds_loss: 0.0000 | lr: 4.7730e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   1 | Iter:   1798/ 13000 | global iter:   1798/ 13000 | loss: 1.7616 | ds_loss: 0.0000 | lr: 4.7727e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   1 | Iter:   1799/ 13000 | global iter:   1799/ 13000 | loss: 2.2737 | ds_loss: 0.0000 | lr: 4.7725e-05 | scale: 32768.0000 | micro time: 1.770 | step time: 0.000
train | epoch   1 | Iter:   1800/ 13000 | global iter:   1800/ 13000 | loss: 2.0991 | ds_loss: 0.0000 | lr: 4.7722e-05 | scale: 32768.0000 | micro time: 1.856 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1800/ 13000 | global iter:   1800/ 13000 | loss: 1.9509 | ds_loss: 0.0000 | lr: 4.7722e-05 | scale: 32768.0000 | micro time: 1.856 | step time: 1.807
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1801/ 13000 | global iter:   1801/ 13000 | loss: 1.8261 | ds_loss: 0.0000 | lr: 4.7720e-05 | scale: 32768.0000 | micro time: 1.717 | step time: 0.000
train | epoch   1 | Iter:   1802/ 13000 | global iter:   1802/ 13000 | loss: 2.0148 | ds_loss: 0.0000 | lr: 4.7717e-05 | scale: 32768.0000 | micro time: 1.816 | step time: 0.000
train | epoch   1 | Iter:   1803/ 13000 | global iter:   1803/ 13000 | loss: 1.8397 | ds_loss: 0.0000 | lr: 4.7714e-05 | scale: 32768.0000 | micro time: 1.881 | step time: 0.000
train | epoch   1 | Iter:   1804/ 13000 | global iter:   1804/ 13000 | loss: 2.5094 | ds_loss: 0.0000 | lr: 4.7712e-05 | scale: 32768.0000 | micro time: 1.687 | step time: 0.000
train | epoch   1 | Iter:   1805/ 13000 | global iter:   1805/ 13000 | loss: 1.7640 | ds_loss: 0.0000 | lr: 4.7709e-05 | scale: 32768.0000 | micro time: 1.793 | step time: 0.000
train | epoch   1 | Iter:   1806/ 13000 | global iter:   1806/ 13000 | loss: 2.5312 | ds_loss: 0.0000 | lr: 4.7707e-05 | scale: 32768.0000 | micro time: 1.738 | step time: 0.000
train | epoch   1 | Iter:   1807/ 13000 | global iter:   1807/ 13000 | loss: 2.1441 | ds_loss: 0.0000 | lr: 4.7704e-05 | scale: 32768.0000 | micro time: 1.753 | step time: 0.000
train | epoch   1 | Iter:   1808/ 13000 | global iter:   1808/ 13000 | loss: 2.1583 | ds_loss: 0.0000 | lr: 4.7702e-05 | scale: 32768.0000 | micro time: 1.743 | step time: 0.000
train | epoch   1 | Iter:   1809/ 13000 | global iter:   1809/ 13000 | loss: 2.2774 | ds_loss: 0.0000 | lr: 4.7699e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   1 | Iter:   1810/ 13000 | global iter:   1810/ 13000 | loss: 2.4381 | ds_loss: 0.0000 | lr: 4.7697e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1810/ 13000 | global iter:   1810/ 13000 | loss: 2.1503 | ds_loss: 0.0000 | lr: 4.7697e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 1.776
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1811/ 13000 | global iter:   1811/ 13000 | loss: 2.2452 | ds_loss: 0.0000 | lr: 4.7694e-05 | scale: 32768.0000 | micro time: 1.830 | step time: 0.000
train | epoch   1 | Iter:   1812/ 13000 | global iter:   1812/ 13000 | loss: 2.4968 | ds_loss: 0.0000 | lr: 4.7692e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   1 | Iter:   1813/ 13000 | global iter:   1813/ 13000 | loss: 2.4277 | ds_loss: 0.0000 | lr: 4.7689e-05 | scale: 32768.0000 | micro time: 1.873 | step time: 0.000
train | epoch   1 | Iter:   1814/ 13000 | global iter:   1814/ 13000 | loss: 2.1707 | ds_loss: 0.0000 | lr: 4.7687e-05 | scale: 32768.0000 | micro time: 1.735 | step time: 0.000
train | epoch   1 | Iter:   1815/ 13000 | global iter:   1815/ 13000 | loss: 1.4206 | ds_loss: 0.0000 | lr: 4.7684e-05 | scale: 32768.0000 | micro time: 1.813 | step time: 0.000
train | epoch   1 | Iter:   1816/ 13000 | global iter:   1816/ 13000 | loss: 1.5709 | ds_loss: 0.0000 | lr: 4.7682e-05 | scale: 32768.0000 | micro time: 1.788 | step time: 0.000
train | epoch   1 | Iter:   1817/ 13000 | global iter:   1817/ 13000 | loss: 2.2793 | ds_loss: 0.0000 | lr: 4.7679e-05 | scale: 32768.0000 | micro time: 1.722 | step time: 0.000
train | epoch   1 | Iter:   1818/ 13000 | global iter:   1818/ 13000 | loss: 2.2779 | ds_loss: 0.0000 | lr: 4.7677e-05 | scale: 32768.0000 | micro time: 1.854 | step time: 0.000
train | epoch   1 | Iter:   1819/ 13000 | global iter:   1819/ 13000 | loss: 2.0689 | ds_loss: 0.0000 | lr: 4.7674e-05 | scale: 32768.0000 | micro time: 1.794 | step time: 0.000
train | epoch   1 | Iter:   1820/ 13000 | global iter:   1820/ 13000 | loss: 1.8960 | ds_loss: 0.0000 | lr: 4.7671e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1820/ 13000 | global iter:   1820/ 13000 | loss: 2.0854 | ds_loss: 0.0000 | lr: 4.7671e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 1.810
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1821/ 13000 | global iter:   1821/ 13000 | loss: 2.2547 | ds_loss: 0.0000 | lr: 4.7669e-05 | scale: 32768.0000 | micro time: 1.818 | step time: 0.000
train | epoch   1 | Iter:   1822/ 13000 | global iter:   1822/ 13000 | loss: 2.0512 | ds_loss: 0.0000 | lr: 4.7666e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   1 | Iter:   1823/ 13000 | global iter:   1823/ 13000 | loss: 2.1368 | ds_loss: 0.0000 | lr: 4.7664e-05 | scale: 32768.0000 | micro time: 1.814 | step time: 0.000
train | epoch   1 | Iter:   1824/ 13000 | global iter:   1824/ 13000 | loss: 1.9883 | ds_loss: 0.0000 | lr: 4.7661e-05 | scale: 32768.0000 | micro time: 1.680 | step time: 0.000
train | epoch   1 | Iter:   1825/ 13000 | global iter:   1825/ 13000 | loss: 1.9757 | ds_loss: 0.0000 | lr: 4.7659e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   1 | Iter:   1826/ 13000 | global iter:   1826/ 13000 | loss: 2.3127 | ds_loss: 0.0000 | lr: 4.7656e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   1 | Iter:   1827/ 13000 | global iter:   1827/ 13000 | loss: 2.1883 | ds_loss: 0.0000 | lr: 4.7654e-05 | scale: 32768.0000 | micro time: 1.858 | step time: 0.000
train | epoch   1 | Iter:   1828/ 13000 | global iter:   1828/ 13000 | loss: 2.5058 | ds_loss: 0.0000 | lr: 4.7651e-05 | scale: 32768.0000 | micro time: 1.712 | step time: 0.000
train | epoch   1 | Iter:   1829/ 13000 | global iter:   1829/ 13000 | loss: 2.0789 | ds_loss: 0.0000 | lr: 4.7648e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   1 | Iter:   1830/ 13000 | global iter:   1830/ 13000 | loss: 1.6752 | ds_loss: 0.0000 | lr: 4.7646e-05 | scale: 32768.0000 | micro time: 1.889 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1830/ 13000 | global iter:   1830/ 13000 | loss: 2.1168 | ds_loss: 0.0000 | lr: 4.7646e-05 | scale: 32768.0000 | micro time: 1.889 | step time: 1.801
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1831/ 13000 | global iter:   1831/ 13000 | loss: 2.5600 | ds_loss: 0.0000 | lr: 4.7643e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   1 | Iter:   1832/ 13000 | global iter:   1832/ 13000 | loss: 1.7850 | ds_loss: 0.0000 | lr: 4.7641e-05 | scale: 32768.0000 | micro time: 1.785 | step time: 0.000
train | epoch   1 | Iter:   1833/ 13000 | global iter:   1833/ 13000 | loss: 1.5457 | ds_loss: 0.0000 | lr: 4.7638e-05 | scale: 32768.0000 | micro time: 1.849 | step time: 0.000
train | epoch   1 | Iter:   1834/ 13000 | global iter:   1834/ 13000 | loss: 2.3042 | ds_loss: 0.0000 | lr: 4.7636e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   1 | Iter:   1835/ 13000 | global iter:   1835/ 13000 | loss: 2.2844 | ds_loss: 0.0000 | lr: 4.7633e-05 | scale: 32768.0000 | micro time: 1.740 | step time: 0.000
train | epoch   1 | Iter:   1836/ 13000 | global iter:   1836/ 13000 | loss: 1.8636 | ds_loss: 0.0000 | lr: 4.7631e-05 | scale: 32768.0000 | micro time: 1.766 | step time: 0.000
train | epoch   1 | Iter:   1837/ 13000 | global iter:   1837/ 13000 | loss: 2.0488 | ds_loss: 0.0000 | lr: 4.7628e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   1 | Iter:   1838/ 13000 | global iter:   1838/ 13000 | loss: 1.8376 | ds_loss: 0.0000 | lr: 4.7625e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   1 | Iter:   1839/ 13000 | global iter:   1839/ 13000 | loss: 2.2211 | ds_loss: 0.0000 | lr: 4.7623e-05 | scale: 32768.0000 | micro time: 1.792 | step time: 0.000
train | epoch   1 | Iter:   1840/ 13000 | global iter:   1840/ 13000 | loss: 2.8313 | ds_loss: 0.0000 | lr: 4.7620e-05 | scale: 32768.0000 | micro time: 1.813 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1840/ 13000 | global iter:   1840/ 13000 | loss: 2.1282 | ds_loss: 0.0000 | lr: 4.7620e-05 | scale: 32768.0000 | micro time: 1.813 | step time: 1.796
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1841/ 13000 | global iter:   1841/ 13000 | loss: 2.1850 | ds_loss: 0.0000 | lr: 4.7618e-05 | scale: 32768.0000 | micro time: 1.821 | step time: 0.000
train | epoch   1 | Iter:   1842/ 13000 | global iter:   1842/ 13000 | loss: 2.2886 | ds_loss: 0.0000 | lr: 4.7615e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   1 | Iter:   1843/ 13000 | global iter:   1843/ 13000 | loss: 2.1527 | ds_loss: 0.0000 | lr: 4.7613e-05 | scale: 32768.0000 | micro time: 1.795 | step time: 0.000
train | epoch   1 | Iter:   1844/ 13000 | global iter:   1844/ 13000 | loss: 2.1890 | ds_loss: 0.0000 | lr: 4.7610e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   1 | Iter:   1845/ 13000 | global iter:   1845/ 13000 | loss: 2.6353 | ds_loss: 0.0000 | lr: 4.7607e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   1 | Iter:   1846/ 13000 | global iter:   1846/ 13000 | loss: 1.5308 | ds_loss: 0.0000 | lr: 4.7605e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   1 | Iter:   1847/ 13000 | global iter:   1847/ 13000 | loss: 2.8105 | ds_loss: 0.0000 | lr: 4.7602e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   1 | Iter:   1848/ 13000 | global iter:   1848/ 13000 | loss: 2.5076 | ds_loss: 0.0000 | lr: 4.7600e-05 | scale: 32768.0000 | micro time: 1.747 | step time: 0.000
train | epoch   1 | Iter:   1849/ 13000 | global iter:   1849/ 13000 | loss: 1.8732 | ds_loss: 0.0000 | lr: 4.7597e-05 | scale: 32768.0000 | micro time: 1.837 | step time: 0.000
train | epoch   1 | Iter:   1850/ 13000 | global iter:   1850/ 13000 | loss: 2.5948 | ds_loss: 0.0000 | lr: 4.7595e-05 | scale: 32768.0000 | micro time: 1.751 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1850/ 13000 | global iter:   1850/ 13000 | loss: 2.2768 | ds_loss: 0.0000 | lr: 4.7595e-05 | scale: 32768.0000 | micro time: 1.751 | step time: 1.805
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1851/ 13000 | global iter:   1851/ 13000 | loss: 1.7900 | ds_loss: 0.0000 | lr: 4.7592e-05 | scale: 32768.0000 | micro time: 1.711 | step time: 0.000
train | epoch   1 | Iter:   1852/ 13000 | global iter:   1852/ 13000 | loss: 2.3695 | ds_loss: 0.0000 | lr: 4.7589e-05 | scale: 32768.0000 | micro time: 1.846 | step time: 0.000
train | epoch   1 | Iter:   1853/ 13000 | global iter:   1853/ 13000 | loss: 2.2438 | ds_loss: 0.0000 | lr: 4.7587e-05 | scale: 32768.0000 | micro time: 1.822 | step time: 0.000
train | epoch   1 | Iter:   1854/ 13000 | global iter:   1854/ 13000 | loss: 1.9622 | ds_loss: 0.0000 | lr: 4.7584e-05 | scale: 32768.0000 | micro time: 1.840 | step time: 0.000
train | epoch   1 | Iter:   1855/ 13000 | global iter:   1855/ 13000 | loss: 1.9691 | ds_loss: 0.0000 | lr: 4.7582e-05 | scale: 32768.0000 | micro time: 1.728 | step time: 0.000
train | epoch   1 | Iter:   1856/ 13000 | global iter:   1856/ 13000 | loss: 2.1067 | ds_loss: 0.0000 | lr: 4.7579e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   1 | Iter:   1857/ 13000 | global iter:   1857/ 13000 | loss: 2.2182 | ds_loss: 0.0000 | lr: 4.7576e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   1 | Iter:   1858/ 13000 | global iter:   1858/ 13000 | loss: 1.9856 | ds_loss: 0.0000 | lr: 4.7574e-05 | scale: 32768.0000 | micro time: 1.755 | step time: 0.000
train | epoch   1 | Iter:   1859/ 13000 | global iter:   1859/ 13000 | loss: 2.2825 | ds_loss: 0.0000 | lr: 4.7571e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   1 | Iter:   1860/ 13000 | global iter:   1860/ 13000 | loss: 2.1797 | ds_loss: 0.0000 | lr: 4.7569e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1860/ 13000 | global iter:   1860/ 13000 | loss: 2.1107 | ds_loss: 0.0000 | lr: 4.7569e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 1.789
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1861/ 13000 | global iter:   1861/ 13000 | loss: 2.1152 | ds_loss: 0.0000 | lr: 4.7566e-05 | scale: 32768.0000 | micro time: 1.794 | step time: 0.000
train | epoch   1 | Iter:   1862/ 13000 | global iter:   1862/ 13000 | loss: 1.9619 | ds_loss: 0.0000 | lr: 4.7563e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 0.000
train | epoch   1 | Iter:   1863/ 13000 | global iter:   1863/ 13000 | loss: 1.8960 | ds_loss: 0.0000 | lr: 4.7561e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   1 | Iter:   1864/ 13000 | global iter:   1864/ 13000 | loss: 1.9083 | ds_loss: 0.0000 | lr: 4.7558e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   1 | Iter:   1865/ 13000 | global iter:   1865/ 13000 | loss: 1.8483 | ds_loss: 0.0000 | lr: 4.7556e-05 | scale: 32768.0000 | micro time: 1.776 | step time: 0.000
train | epoch   1 | Iter:   1866/ 13000 | global iter:   1866/ 13000 | loss: 2.1115 | ds_loss: 0.0000 | lr: 4.7553e-05 | scale: 32768.0000 | micro time: 1.878 | step time: 0.000
train | epoch   1 | Iter:   1867/ 13000 | global iter:   1867/ 13000 | loss: 2.0123 | ds_loss: 0.0000 | lr: 4.7550e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
train | epoch   1 | Iter:   1868/ 13000 | global iter:   1868/ 13000 | loss: 2.3209 | ds_loss: 0.0000 | lr: 4.7548e-05 | scale: 32768.0000 | micro time: 1.647 | step time: 0.000
train | epoch   1 | Iter:   1869/ 13000 | global iter:   1869/ 13000 | loss: 2.3365 | ds_loss: 0.0000 | lr: 4.7545e-05 | scale: 32768.0000 | micro time: 1.895 | step time: 0.000
train | epoch   1 | Iter:   1870/ 13000 | global iter:   1870/ 13000 | loss: 2.3807 | ds_loss: 0.0000 | lr: 4.7543e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1870/ 13000 | global iter:   1870/ 13000 | loss: 2.0892 | ds_loss: 0.0000 | lr: 4.7543e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 1.806
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1871/ 13000 | global iter:   1871/ 13000 | loss: 2.2103 | ds_loss: 0.0000 | lr: 4.7540e-05 | scale: 32768.0000 | micro time: 1.838 | step time: 0.000
train | epoch   1 | Iter:   1872/ 13000 | global iter:   1872/ 13000 | loss: 2.0703 | ds_loss: 0.0000 | lr: 4.7537e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   1 | Iter:   1873/ 13000 | global iter:   1873/ 13000 | loss: 2.2989 | ds_loss: 0.0000 | lr: 4.7535e-05 | scale: 32768.0000 | micro time: 1.842 | step time: 0.000
train | epoch   1 | Iter:   1874/ 13000 | global iter:   1874/ 13000 | loss: 2.0300 | ds_loss: 0.0000 | lr: 4.7532e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   1 | Iter:   1875/ 13000 | global iter:   1875/ 13000 | loss: 1.9103 | ds_loss: 0.0000 | lr: 4.7530e-05 | scale: 32768.0000 | micro time: 1.739 | step time: 0.000
train | epoch   1 | Iter:   1876/ 13000 | global iter:   1876/ 13000 | loss: 1.7204 | ds_loss: 0.0000 | lr: 4.7527e-05 | scale: 32768.0000 | micro time: 1.727 | step time: 0.000
train | epoch   1 | Iter:   1877/ 13000 | global iter:   1877/ 13000 | loss: 1.7648 | ds_loss: 0.0000 | lr: 4.7524e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   1 | Iter:   1878/ 13000 | global iter:   1878/ 13000 | loss: 1.8691 | ds_loss: 0.0000 | lr: 4.7522e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   1 | Iter:   1879/ 13000 | global iter:   1879/ 13000 | loss: 2.3460 | ds_loss: 0.0000 | lr: 4.7519e-05 | scale: 32768.0000 | micro time: 1.719 | step time: 0.000
train | epoch   1 | Iter:   1880/ 13000 | global iter:   1880/ 13000 | loss: 2.4036 | ds_loss: 0.0000 | lr: 4.7516e-05 | scale: 32768.0000 | micro time: 1.694 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1880/ 13000 | global iter:   1880/ 13000 | loss: 2.0624 | ds_loss: 0.0000 | lr: 4.7516e-05 | scale: 32768.0000 | micro time: 1.694 | step time: 1.784
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1881/ 13000 | global iter:   1881/ 13000 | loss: 2.3062 | ds_loss: 0.0000 | lr: 4.7514e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   1 | Iter:   1882/ 13000 | global iter:   1882/ 13000 | loss: 1.8177 | ds_loss: 0.0000 | lr: 4.7511e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   1 | Iter:   1883/ 13000 | global iter:   1883/ 13000 | loss: 2.1718 | ds_loss: 0.0000 | lr: 4.7509e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   1 | Iter:   1884/ 13000 | global iter:   1884/ 13000 | loss: 2.0160 | ds_loss: 0.0000 | lr: 4.7506e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   1 | Iter:   1885/ 13000 | global iter:   1885/ 13000 | loss: 2.0371 | ds_loss: 0.0000 | lr: 4.7503e-05 | scale: 32768.0000 | micro time: 1.711 | step time: 0.000
train | epoch   1 | Iter:   1886/ 13000 | global iter:   1886/ 13000 | loss: 1.9713 | ds_loss: 0.0000 | lr: 4.7501e-05 | scale: 32768.0000 | micro time: 1.892 | step time: 0.000
train | epoch   1 | Iter:   1887/ 13000 | global iter:   1887/ 13000 | loss: 2.1876 | ds_loss: 0.0000 | lr: 4.7498e-05 | scale: 32768.0000 | micro time: 1.800 | step time: 0.000
train | epoch   1 | Iter:   1888/ 13000 | global iter:   1888/ 13000 | loss: 1.5444 | ds_loss: 0.0000 | lr: 4.7495e-05 | scale: 32768.0000 | micro time: 1.653 | step time: 0.000
train | epoch   1 | Iter:   1889/ 13000 | global iter:   1889/ 13000 | loss: 1.9939 | ds_loss: 0.0000 | lr: 4.7493e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   1 | Iter:   1890/ 13000 | global iter:   1890/ 13000 | loss: 2.4453 | ds_loss: 0.0000 | lr: 4.7490e-05 | scale: 32768.0000 | micro time: 1.899 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1890/ 13000 | global iter:   1890/ 13000 | loss: 2.0491 | ds_loss: 0.0000 | lr: 4.7490e-05 | scale: 32768.0000 | micro time: 1.899 | step time: 1.797
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1891/ 13000 | global iter:   1891/ 13000 | loss: 2.3992 | ds_loss: 0.0000 | lr: 4.7488e-05 | scale: 32768.0000 | micro time: 1.866 | step time: 0.000
train | epoch   1 | Iter:   1892/ 13000 | global iter:   1892/ 13000 | loss: 2.5948 | ds_loss: 0.0000 | lr: 4.7485e-05 | scale: 32768.0000 | micro time: 1.719 | step time: 0.000
train | epoch   1 | Iter:   1893/ 13000 | global iter:   1893/ 13000 | loss: 2.0917 | ds_loss: 0.0000 | lr: 4.7482e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   1 | Iter:   1894/ 13000 | global iter:   1894/ 13000 | loss: 2.0201 | ds_loss: 0.0000 | lr: 4.7480e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 0.000
train | epoch   1 | Iter:   1895/ 13000 | global iter:   1895/ 13000 | loss: 2.1364 | ds_loss: 0.0000 | lr: 4.7477e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   1 | Iter:   1896/ 13000 | global iter:   1896/ 13000 | loss: 1.9483 | ds_loss: 0.0000 | lr: 4.7474e-05 | scale: 32768.0000 | micro time: 1.735 | step time: 0.000
train | epoch   1 | Iter:   1897/ 13000 | global iter:   1897/ 13000 | loss: 2.5953 | ds_loss: 0.0000 | lr: 4.7472e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   1 | Iter:   1898/ 13000 | global iter:   1898/ 13000 | loss: 1.9581 | ds_loss: 0.0000 | lr: 4.7469e-05 | scale: 32768.0000 | micro time: 1.833 | step time: 0.000
train | epoch   1 | Iter:   1899/ 13000 | global iter:   1899/ 13000 | loss: 2.2011 | ds_loss: 0.0000 | lr: 4.7466e-05 | scale: 32768.0000 | micro time: 1.786 | step time: 0.000
train | epoch   1 | Iter:   1900/ 13000 | global iter:   1900/ 13000 | loss: 2.2385 | ds_loss: 0.0000 | lr: 4.7464e-05 | scale: 32768.0000 | micro time: 1.828 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1900/ 13000 | global iter:   1900/ 13000 | loss: 2.2184 | ds_loss: 0.0000 | lr: 4.7464e-05 | scale: 32768.0000 | micro time: 1.828 | step time: 1.817
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1901/ 13000 | global iter:   1901/ 13000 | loss: 2.4002 | ds_loss: 0.0000 | lr: 4.7461e-05 | scale: 32768.0000 | micro time: 1.837 | step time: 0.000
train | epoch   1 | Iter:   1902/ 13000 | global iter:   1902/ 13000 | loss: 2.5322 | ds_loss: 0.0000 | lr: 4.7458e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   1 | Iter:   1903/ 13000 | global iter:   1903/ 13000 | loss: 2.3776 | ds_loss: 0.0000 | lr: 4.7456e-05 | scale: 32768.0000 | micro time: 1.833 | step time: 0.000
train | epoch   1 | Iter:   1904/ 13000 | global iter:   1904/ 13000 | loss: 2.7203 | ds_loss: 0.0000 | lr: 4.7453e-05 | scale: 32768.0000 | micro time: 1.785 | step time: 0.000
train | epoch   1 | Iter:   1905/ 13000 | global iter:   1905/ 13000 | loss: 1.7555 | ds_loss: 0.0000 | lr: 4.7450e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   1 | Iter:   1906/ 13000 | global iter:   1906/ 13000 | loss: 2.2192 | ds_loss: 0.0000 | lr: 4.7448e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   1 | Iter:   1907/ 13000 | global iter:   1907/ 13000 | loss: 2.2230 | ds_loss: 0.0000 | lr: 4.7445e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   1 | Iter:   1908/ 13000 | global iter:   1908/ 13000 | loss: 2.1365 | ds_loss: 0.0000 | lr: 4.7443e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   1 | Iter:   1909/ 13000 | global iter:   1909/ 13000 | loss: 2.6835 | ds_loss: 0.0000 | lr: 4.7440e-05 | scale: 32768.0000 | micro time: 1.907 | step time: 0.000
train | epoch   1 | Iter:   1910/ 13000 | global iter:   1910/ 13000 | loss: 1.8026 | ds_loss: 0.0000 | lr: 4.7437e-05 | scale: 32768.0000 | micro time: 1.801 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1910/ 13000 | global iter:   1910/ 13000 | loss: 2.2851 | ds_loss: 0.0000 | lr: 4.7437e-05 | scale: 32768.0000 | micro time: 1.801 | step time: 1.833
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1911/ 13000 | global iter:   1911/ 13000 | loss: 2.0655 | ds_loss: 0.0000 | lr: 4.7435e-05 | scale: 32768.0000 | micro time: 1.833 | step time: 0.000
train | epoch   1 | Iter:   1912/ 13000 | global iter:   1912/ 13000 | loss: 2.4261 | ds_loss: 0.0000 | lr: 4.7432e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   1 | Iter:   1913/ 13000 | global iter:   1913/ 13000 | loss: 2.2686 | ds_loss: 0.0000 | lr: 4.7429e-05 | scale: 32768.0000 | micro time: 1.903 | step time: 0.000
train | epoch   1 | Iter:   1914/ 13000 | global iter:   1914/ 13000 | loss: 1.9333 | ds_loss: 0.0000 | lr: 4.7427e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   1 | Iter:   1915/ 13000 | global iter:   1915/ 13000 | loss: 2.0506 | ds_loss: 0.0000 | lr: 4.7424e-05 | scale: 32768.0000 | micro time: 1.751 | step time: 0.000
train | epoch   1 | Iter:   1916/ 13000 | global iter:   1916/ 13000 | loss: 2.0568 | ds_loss: 0.0000 | lr: 4.7421e-05 | scale: 32768.0000 | micro time: 1.902 | step time: 0.000
train | epoch   1 | Iter:   1917/ 13000 | global iter:   1917/ 13000 | loss: 2.4270 | ds_loss: 0.0000 | lr: 4.7419e-05 | scale: 32768.0000 | micro time: 1.792 | step time: 0.000
train | epoch   1 | Iter:   1918/ 13000 | global iter:   1918/ 13000 | loss: 2.2216 | ds_loss: 0.0000 | lr: 4.7416e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   1 | Iter:   1919/ 13000 | global iter:   1919/ 13000 | loss: 2.4932 | ds_loss: 0.0000 | lr: 4.7413e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   1 | Iter:   1920/ 13000 | global iter:   1920/ 13000 | loss: 1.5083 | ds_loss: 0.0000 | lr: 4.7411e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1920/ 13000 | global iter:   1920/ 13000 | loss: 2.1451 | ds_loss: 0.0000 | lr: 4.7411e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 1.824
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1921/ 13000 | global iter:   1921/ 13000 | loss: 1.8570 | ds_loss: 0.0000 | lr: 4.7408e-05 | scale: 32768.0000 | micro time: 1.766 | step time: 0.000
train | epoch   1 | Iter:   1922/ 13000 | global iter:   1922/ 13000 | loss: 2.2552 | ds_loss: 0.0000 | lr: 4.7405e-05 | scale: 32768.0000 | micro time: 1.806 | step time: 0.000
train | epoch   1 | Iter:   1923/ 13000 | global iter:   1923/ 13000 | loss: 2.0535 | ds_loss: 0.0000 | lr: 4.7402e-05 | scale: 32768.0000 | micro time: 1.850 | step time: 0.000
train | epoch   1 | Iter:   1924/ 13000 | global iter:   1924/ 13000 | loss: 2.1084 | ds_loss: 0.0000 | lr: 4.7400e-05 | scale: 32768.0000 | micro time: 1.690 | step time: 0.000
train | epoch   1 | Iter:   1925/ 13000 | global iter:   1925/ 13000 | loss: 1.3631 | ds_loss: 0.0000 | lr: 4.7397e-05 | scale: 32768.0000 | micro time: 1.748 | step time: 0.000
train | epoch   1 | Iter:   1926/ 13000 | global iter:   1926/ 13000 | loss: 2.3012 | ds_loss: 0.0000 | lr: 4.7394e-05 | scale: 32768.0000 | micro time: 1.920 | step time: 0.000
train | epoch   1 | Iter:   1927/ 13000 | global iter:   1927/ 13000 | loss: 2.0381 | ds_loss: 0.0000 | lr: 4.7392e-05 | scale: 32768.0000 | micro time: 1.870 | step time: 0.000
train | epoch   1 | Iter:   1928/ 13000 | global iter:   1928/ 13000 | loss: 2.1266 | ds_loss: 0.0000 | lr: 4.7389e-05 | scale: 32768.0000 | micro time: 1.794 | step time: 0.000
train | epoch   1 | Iter:   1929/ 13000 | global iter:   1929/ 13000 | loss: 2.4143 | ds_loss: 0.0000 | lr: 4.7386e-05 | scale: 32768.0000 | micro time: 1.736 | step time: 0.000
train | epoch   1 | Iter:   1930/ 13000 | global iter:   1930/ 13000 | loss: 1.7191 | ds_loss: 0.0000 | lr: 4.7384e-05 | scale: 32768.0000 | micro time: 1.766 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1930/ 13000 | global iter:   1930/ 13000 | loss: 2.0237 | ds_loss: 0.0000 | lr: 4.7384e-05 | scale: 32768.0000 | micro time: 1.766 | step time: 1.795
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1931/ 13000 | global iter:   1931/ 13000 | loss: 2.0875 | ds_loss: 0.0000 | lr: 4.7381e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 0.000
train | epoch   1 | Iter:   1932/ 13000 | global iter:   1932/ 13000 | loss: 2.1540 | ds_loss: 0.0000 | lr: 4.7378e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 0.000
train | epoch   1 | Iter:   1933/ 13000 | global iter:   1933/ 13000 | loss: 2.5128 | ds_loss: 0.0000 | lr: 4.7376e-05 | scale: 32768.0000 | micro time: 1.822 | step time: 0.000
train | epoch   1 | Iter:   1934/ 13000 | global iter:   1934/ 13000 | loss: 2.3253 | ds_loss: 0.0000 | lr: 4.7373e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   1 | Iter:   1935/ 13000 | global iter:   1935/ 13000 | loss: 1.7334 | ds_loss: 0.0000 | lr: 4.7370e-05 | scale: 32768.0000 | micro time: 1.785 | step time: 0.000
train | epoch   1 | Iter:   1936/ 13000 | global iter:   1936/ 13000 | loss: 2.5813 | ds_loss: 0.0000 | lr: 4.7368e-05 | scale: 32768.0000 | micro time: 1.761 | step time: 0.000
train | epoch   1 | Iter:   1937/ 13000 | global iter:   1937/ 13000 | loss: 2.3655 | ds_loss: 0.0000 | lr: 4.7365e-05 | scale: 32768.0000 | micro time: 1.795 | step time: 0.000
train | epoch   1 | Iter:   1938/ 13000 | global iter:   1938/ 13000 | loss: 1.8925 | ds_loss: 0.0000 | lr: 4.7362e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   1 | Iter:   1939/ 13000 | global iter:   1939/ 13000 | loss: 2.4325 | ds_loss: 0.0000 | lr: 4.7359e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   1 | Iter:   1940/ 13000 | global iter:   1940/ 13000 | loss: 1.9893 | ds_loss: 0.0000 | lr: 4.7357e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1940/ 13000 | global iter:   1940/ 13000 | loss: 2.2074 | ds_loss: 0.0000 | lr: 4.7357e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 1.811
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1941/ 13000 | global iter:   1941/ 13000 | loss: 1.6619 | ds_loss: 0.0000 | lr: 4.7354e-05 | scale: 32768.0000 | micro time: 1.790 | step time: 0.000
train | epoch   1 | Iter:   1942/ 13000 | global iter:   1942/ 13000 | loss: 1.8490 | ds_loss: 0.0000 | lr: 4.7351e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   1 | Iter:   1943/ 13000 | global iter:   1943/ 13000 | loss: 2.6225 | ds_loss: 0.0000 | lr: 4.7349e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   1 | Iter:   1944/ 13000 | global iter:   1944/ 13000 | loss: 1.8984 | ds_loss: 0.0000 | lr: 4.7346e-05 | scale: 32768.0000 | micro time: 1.862 | step time: 0.000
train | epoch   1 | Iter:   1945/ 13000 | global iter:   1945/ 13000 | loss: 2.2865 | ds_loss: 0.0000 | lr: 4.7343e-05 | scale: 32768.0000 | micro time: 1.798 | step time: 0.000
train | epoch   1 | Iter:   1946/ 13000 | global iter:   1946/ 13000 | loss: 2.6590 | ds_loss: 0.0000 | lr: 4.7341e-05 | scale: 32768.0000 | micro time: 1.744 | step time: 0.000
train | epoch   1 | Iter:   1947/ 13000 | global iter:   1947/ 13000 | loss: 2.6990 | ds_loss: 0.0000 | lr: 4.7338e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
train | epoch   1 | Iter:   1948/ 13000 | global iter:   1948/ 13000 | loss: 2.0390 | ds_loss: 0.0000 | lr: 4.7335e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   1 | Iter:   1949/ 13000 | global iter:   1949/ 13000 | loss: 2.6342 | ds_loss: 0.0000 | lr: 4.7332e-05 | scale: 32768.0000 | micro time: 1.795 | step time: 0.000
train | epoch   1 | Iter:   1950/ 13000 | global iter:   1950/ 13000 | loss: 2.1518 | ds_loss: 0.0000 | lr: 4.7330e-05 | scale: 32768.0000 | micro time: 1.810 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1950/ 13000 | global iter:   1950/ 13000 | loss: 2.2501 | ds_loss: 0.0000 | lr: 4.7330e-05 | scale: 32768.0000 | micro time: 1.810 | step time: 1.799
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1951/ 13000 | global iter:   1951/ 13000 | loss: 1.4897 | ds_loss: 0.0000 | lr: 4.7327e-05 | scale: 32768.0000 | micro time: 1.628 | step time: 0.000
train | epoch   1 | Iter:   1952/ 13000 | global iter:   1952/ 13000 | loss: 1.5240 | ds_loss: 0.0000 | lr: 4.7324e-05 | scale: 32768.0000 | micro time: 1.854 | step time: 0.000
train | epoch   1 | Iter:   1953/ 13000 | global iter:   1953/ 13000 | loss: 1.7022 | ds_loss: 0.0000 | lr: 4.7322e-05 | scale: 32768.0000 | micro time: 1.810 | step time: 0.000
train | epoch   1 | Iter:   1954/ 13000 | global iter:   1954/ 13000 | loss: 2.2452 | ds_loss: 0.0000 | lr: 4.7319e-05 | scale: 32768.0000 | micro time: 1.708 | step time: 0.000
train | epoch   1 | Iter:   1955/ 13000 | global iter:   1955/ 13000 | loss: 1.1991 | ds_loss: 0.0000 | lr: 4.7316e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   1 | Iter:   1956/ 13000 | global iter:   1956/ 13000 | loss: 2.1017 | ds_loss: 0.0000 | lr: 4.7313e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 0.000
train | epoch   1 | Iter:   1957/ 13000 | global iter:   1957/ 13000 | loss: 2.1702 | ds_loss: 0.0000 | lr: 4.7311e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   1 | Iter:   1958/ 13000 | global iter:   1958/ 13000 | loss: 2.0932 | ds_loss: 0.0000 | lr: 4.7308e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 0.000
train | epoch   1 | Iter:   1959/ 13000 | global iter:   1959/ 13000 | loss: 2.2636 | ds_loss: 0.0000 | lr: 4.7305e-05 | scale: 32768.0000 | micro time: 1.943 | step time: 0.000
train | epoch   1 | Iter:   1960/ 13000 | global iter:   1960/ 13000 | loss: 2.1748 | ds_loss: 0.0000 | lr: 4.7302e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1960/ 13000 | global iter:   1960/ 13000 | loss: 1.8964 | ds_loss: 0.0000 | lr: 4.7302e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 1.816
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1961/ 13000 | global iter:   1961/ 13000 | loss: 2.0464 | ds_loss: 0.0000 | lr: 4.7300e-05 | scale: 32768.0000 | micro time: 1.701 | step time: 0.000
train | epoch   1 | Iter:   1962/ 13000 | global iter:   1962/ 13000 | loss: 1.8720 | ds_loss: 0.0000 | lr: 4.7297e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   1 | Iter:   1963/ 13000 | global iter:   1963/ 13000 | loss: 2.0810 | ds_loss: 0.0000 | lr: 4.7294e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   1 | Iter:   1964/ 13000 | global iter:   1964/ 13000 | loss: 2.0529 | ds_loss: 0.0000 | lr: 4.7292e-05 | scale: 32768.0000 | micro time: 1.731 | step time: 0.000
train | epoch   1 | Iter:   1965/ 13000 | global iter:   1965/ 13000 | loss: 2.2842 | ds_loss: 0.0000 | lr: 4.7289e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   1 | Iter:   1966/ 13000 | global iter:   1966/ 13000 | loss: 2.4197 | ds_loss: 0.0000 | lr: 4.7286e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   1 | Iter:   1967/ 13000 | global iter:   1967/ 13000 | loss: 2.5023 | ds_loss: 0.0000 | lr: 4.7283e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   1 | Iter:   1968/ 13000 | global iter:   1968/ 13000 | loss: 1.9295 | ds_loss: 0.0000 | lr: 4.7281e-05 | scale: 32768.0000 | micro time: 1.824 | step time: 0.000
train | epoch   1 | Iter:   1969/ 13000 | global iter:   1969/ 13000 | loss: 2.3414 | ds_loss: 0.0000 | lr: 4.7278e-05 | scale: 32768.0000 | micro time: 1.710 | step time: 0.000
train | epoch   1 | Iter:   1970/ 13000 | global iter:   1970/ 13000 | loss: 2.2246 | ds_loss: 0.0000 | lr: 4.7275e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1970/ 13000 | global iter:   1970/ 13000 | loss: 2.1754 | ds_loss: 0.0000 | lr: 4.7275e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 1.776
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1971/ 13000 | global iter:   1971/ 13000 | loss: 1.7286 | ds_loss: 0.0000 | lr: 4.7272e-05 | scale: 32768.0000 | micro time: 1.838 | step time: 0.000
train | epoch   1 | Iter:   1972/ 13000 | global iter:   1972/ 13000 | loss: 1.7309 | ds_loss: 0.0000 | lr: 4.7270e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   1 | Iter:   1973/ 13000 | global iter:   1973/ 13000 | loss: 2.1575 | ds_loss: 0.0000 | lr: 4.7267e-05 | scale: 32768.0000 | micro time: 1.897 | step time: 0.000
train | epoch   1 | Iter:   1974/ 13000 | global iter:   1974/ 13000 | loss: 2.0893 | ds_loss: 0.0000 | lr: 4.7264e-05 | scale: 32768.0000 | micro time: 1.804 | step time: 0.000
train | epoch   1 | Iter:   1975/ 13000 | global iter:   1975/ 13000 | loss: 2.5701 | ds_loss: 0.0000 | lr: 4.7261e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   1 | Iter:   1976/ 13000 | global iter:   1976/ 13000 | loss: 2.1974 | ds_loss: 0.0000 | lr: 4.7259e-05 | scale: 32768.0000 | micro time: 1.806 | step time: 0.000
train | epoch   1 | Iter:   1977/ 13000 | global iter:   1977/ 13000 | loss: 2.2476 | ds_loss: 0.0000 | lr: 4.7256e-05 | scale: 32768.0000 | micro time: 1.740 | step time: 0.000
train | epoch   1 | Iter:   1978/ 13000 | global iter:   1978/ 13000 | loss: 2.1543 | ds_loss: 0.0000 | lr: 4.7253e-05 | scale: 32768.0000 | micro time: 1.750 | step time: 0.000
train | epoch   1 | Iter:   1979/ 13000 | global iter:   1979/ 13000 | loss: 1.9368 | ds_loss: 0.0000 | lr: 4.7250e-05 | scale: 32768.0000 | micro time: 1.861 | step time: 0.000
train | epoch   1 | Iter:   1980/ 13000 | global iter:   1980/ 13000 | loss: 2.3450 | ds_loss: 0.0000 | lr: 4.7248e-05 | scale: 32768.0000 | micro time: 1.723 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1980/ 13000 | global iter:   1980/ 13000 | loss: 2.1157 | ds_loss: 0.0000 | lr: 4.7248e-05 | scale: 32768.0000 | micro time: 1.723 | step time: 1.807
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1981/ 13000 | global iter:   1981/ 13000 | loss: 1.7184 | ds_loss: 0.0000 | lr: 4.7245e-05 | scale: 32768.0000 | micro time: 1.816 | step time: 0.000
train | epoch   1 | Iter:   1982/ 13000 | global iter:   1982/ 13000 | loss: 1.9895 | ds_loss: 0.0000 | lr: 4.7242e-05 | scale: 32768.0000 | micro time: 1.768 | step time: 0.000
train | epoch   1 | Iter:   1983/ 13000 | global iter:   1983/ 13000 | loss: 2.4042 | ds_loss: 0.0000 | lr: 4.7239e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   1 | Iter:   1984/ 13000 | global iter:   1984/ 13000 | loss: 1.9104 | ds_loss: 0.0000 | lr: 4.7237e-05 | scale: 32768.0000 | micro time: 1.735 | step time: 0.000
train | epoch   1 | Iter:   1985/ 13000 | global iter:   1985/ 13000 | loss: 1.7405 | ds_loss: 0.0000 | lr: 4.7234e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   1 | Iter:   1986/ 13000 | global iter:   1986/ 13000 | loss: 2.1969 | ds_loss: 0.0000 | lr: 4.7231e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   1 | Iter:   1987/ 13000 | global iter:   1987/ 13000 | loss: 2.4497 | ds_loss: 0.0000 | lr: 4.7228e-05 | scale: 32768.0000 | micro time: 2.143 | step time: 0.000
train | epoch   1 | Iter:   1988/ 13000 | global iter:   1988/ 13000 | loss: 2.4431 | ds_loss: 0.0000 | lr: 4.7226e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   1 | Iter:   1989/ 13000 | global iter:   1989/ 13000 | loss: 2.1572 | ds_loss: 0.0000 | lr: 4.7223e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 0.000
train | epoch   1 | Iter:   1990/ 13000 | global iter:   1990/ 13000 | loss: 2.1259 | ds_loss: 0.0000 | lr: 4.7220e-05 | scale: 32768.0000 | micro time: 1.866 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   1990/ 13000 | global iter:   1990/ 13000 | loss: 2.1136 | ds_loss: 0.0000 | lr: 4.7220e-05 | scale: 32768.0000 | micro time: 1.866 | step time: 1.845
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   1991/ 13000 | global iter:   1991/ 13000 | loss: 1.7241 | ds_loss: 0.0000 | lr: 4.7217e-05 | scale: 32768.0000 | micro time: 1.798 | step time: 0.000
train | epoch   1 | Iter:   1992/ 13000 | global iter:   1992/ 13000 | loss: 2.1560 | ds_loss: 0.0000 | lr: 4.7215e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   1 | Iter:   1993/ 13000 | global iter:   1993/ 13000 | loss: 1.3001 | ds_loss: 0.0000 | lr: 4.7212e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   1 | Iter:   1994/ 13000 | global iter:   1994/ 13000 | loss: 2.3188 | ds_loss: 0.0000 | lr: 4.7209e-05 | scale: 32768.0000 | micro time: 1.795 | step time: 0.000
train | epoch   1 | Iter:   1995/ 13000 | global iter:   1995/ 13000 | loss: 2.1462 | ds_loss: 0.0000 | lr: 4.7206e-05 | scale: 32768.0000 | micro time: 1.762 | step time: 0.000
train | epoch   1 | Iter:   1996/ 13000 | global iter:   1996/ 13000 | loss: 2.2884 | ds_loss: 0.0000 | lr: 4.7203e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   1 | Iter:   1997/ 13000 | global iter:   1997/ 13000 | loss: 1.5758 | ds_loss: 0.0000 | lr: 4.7201e-05 | scale: 32768.0000 | micro time: 1.746 | step time: 0.000
train | epoch   1 | Iter:   1998/ 13000 | global iter:   1998/ 13000 | loss: 2.2223 | ds_loss: 0.0000 | lr: 4.7198e-05 | scale: 32768.0000 | micro time: 1.845 | step time: 0.000
train | epoch   1 | Iter:   1999/ 13000 | global iter:   1999/ 13000 | loss: 1.1178 | ds_loss: 0.0000 | lr: 4.7195e-05 | scale: 32768.0000 | micro time: 1.740 | step time: 0.000
train | epoch   1 | Iter:   2000/ 13000 | global iter:   2000/ 13000 | loss: 1.6567 | ds_loss: 0.0000 | lr: 4.7192e-05 | scale: 32768.0000 | micro time: 1.876 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2000/ 13000 | global iter:   2000/ 13000 | loss: 1.8506 | ds_loss: 0.0000 | lr: 4.7192e-05 | scale: 32768.0000 | micro time: 1.876 | step time: 1.805
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2001/ 13000 | global iter:   2001/ 13000 | loss: 2.4770 | ds_loss: 0.0000 | lr: 4.7190e-05 | scale: 32768.0000 | micro time: 1.828 | step time: 0.000
train | epoch   1 | Iter:   2002/ 13000 | global iter:   2002/ 13000 | loss: 2.0514 | ds_loss: 0.0000 | lr: 4.7187e-05 | scale: 32768.0000 | micro time: 1.677 | step time: 0.000
train | epoch   1 | Iter:   2003/ 13000 | global iter:   2003/ 13000 | loss: 2.3558 | ds_loss: 0.0000 | lr: 4.7184e-05 | scale: 32768.0000 | micro time: 1.820 | step time: 0.000
train | epoch   1 | Iter:   2004/ 13000 | global iter:   2004/ 13000 | loss: 1.9604 | ds_loss: 0.0000 | lr: 4.7181e-05 | scale: 32768.0000 | micro time: 1.882 | step time: 0.000
train | epoch   1 | Iter:   2005/ 13000 | global iter:   2005/ 13000 | loss: 2.4364 | ds_loss: 0.0000 | lr: 4.7178e-05 | scale: 32768.0000 | micro time: 1.764 | step time: 0.000
train | epoch   1 | Iter:   2006/ 13000 | global iter:   2006/ 13000 | loss: 1.7468 | ds_loss: 0.0000 | lr: 4.7176e-05 | scale: 32768.0000 | micro time: 1.774 | step time: 0.000
train | epoch   1 | Iter:   2007/ 13000 | global iter:   2007/ 13000 | loss: 1.9435 | ds_loss: 0.0000 | lr: 4.7173e-05 | scale: 32768.0000 | micro time: 1.655 | step time: 0.000
train | epoch   1 | Iter:   2008/ 13000 | global iter:   2008/ 13000 | loss: 1.8471 | ds_loss: 0.0000 | lr: 4.7170e-05 | scale: 32768.0000 | micro time: 1.879 | step time: 0.000
train | epoch   1 | Iter:   2009/ 13000 | global iter:   2009/ 13000 | loss: 2.0226 | ds_loss: 0.0000 | lr: 4.7167e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
train | epoch   1 | Iter:   2010/ 13000 | global iter:   2010/ 13000 | loss: 2.3136 | ds_loss: 0.0000 | lr: 4.7165e-05 | scale: 32768.0000 | micro time: 1.711 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2010/ 13000 | global iter:   2010/ 13000 | loss: 2.1155 | ds_loss: 0.0000 | lr: 4.7165e-05 | scale: 32768.0000 | micro time: 1.711 | step time: 1.776
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2011/ 13000 | global iter:   2011/ 13000 | loss: 2.4976 | ds_loss: 0.0000 | lr: 4.7162e-05 | scale: 32768.0000 | micro time: 1.717 | step time: 0.000
train | epoch   1 | Iter:   2012/ 13000 | global iter:   2012/ 13000 | loss: 2.2538 | ds_loss: 0.0000 | lr: 4.7159e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
train | epoch   1 | Iter:   2013/ 13000 | global iter:   2013/ 13000 | loss: 2.5565 | ds_loss: 0.0000 | lr: 4.7156e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   1 | Iter:   2014/ 13000 | global iter:   2014/ 13000 | loss: 2.4517 | ds_loss: 0.0000 | lr: 4.7153e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   1 | Iter:   2015/ 13000 | global iter:   2015/ 13000 | loss: 1.7003 | ds_loss: 0.0000 | lr: 4.7151e-05 | scale: 32768.0000 | micro time: 1.719 | step time: 0.000
train | epoch   1 | Iter:   2016/ 13000 | global iter:   2016/ 13000 | loss: 2.1358 | ds_loss: 0.0000 | lr: 4.7148e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   1 | Iter:   2017/ 13000 | global iter:   2017/ 13000 | loss: 2.6032 | ds_loss: 0.0000 | lr: 4.7145e-05 | scale: 32768.0000 | micro time: 1.761 | step time: 0.000
train | epoch   1 | Iter:   2018/ 13000 | global iter:   2018/ 13000 | loss: 2.6031 | ds_loss: 0.0000 | lr: 4.7142e-05 | scale: 32768.0000 | micro time: 1.817 | step time: 0.000
train | epoch   1 | Iter:   2019/ 13000 | global iter:   2019/ 13000 | loss: 2.0249 | ds_loss: 0.0000 | lr: 4.7139e-05 | scale: 32768.0000 | micro time: 1.896 | step time: 0.000
train | epoch   1 | Iter:   2020/ 13000 | global iter:   2020/ 13000 | loss: 2.6845 | ds_loss: 0.0000 | lr: 4.7137e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2020/ 13000 | global iter:   2020/ 13000 | loss: 2.3511 | ds_loss: 0.0000 | lr: 4.7137e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 1.787
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2021/ 13000 | global iter:   2021/ 13000 | loss: 2.1057 | ds_loss: 0.0000 | lr: 4.7134e-05 | scale: 32768.0000 | micro time: 1.816 | step time: 0.000
train | epoch   1 | Iter:   2022/ 13000 | global iter:   2022/ 13000 | loss: 2.1766 | ds_loss: 0.0000 | lr: 4.7131e-05 | scale: 32768.0000 | micro time: 1.820 | step time: 0.000
train | epoch   1 | Iter:   2023/ 13000 | global iter:   2023/ 13000 | loss: 2.4591 | ds_loss: 0.0000 | lr: 4.7128e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   1 | Iter:   2024/ 13000 | global iter:   2024/ 13000 | loss: 2.0333 | ds_loss: 0.0000 | lr: 4.7125e-05 | scale: 32768.0000 | micro time: 1.797 | step time: 0.000
train | epoch   1 | Iter:   2025/ 13000 | global iter:   2025/ 13000 | loss: 2.4554 | ds_loss: 0.0000 | lr: 4.7122e-05 | scale: 32768.0000 | micro time: 1.850 | step time: 0.000
train | epoch   1 | Iter:   2026/ 13000 | global iter:   2026/ 13000 | loss: 2.4984 | ds_loss: 0.0000 | lr: 4.7120e-05 | scale: 32768.0000 | micro time: 1.852 | step time: 0.000
train | epoch   1 | Iter:   2027/ 13000 | global iter:   2027/ 13000 | loss: 1.8867 | ds_loss: 0.0000 | lr: 4.7117e-05 | scale: 32768.0000 | micro time: 1.740 | step time: 0.000
train | epoch   1 | Iter:   2028/ 13000 | global iter:   2028/ 13000 | loss: 2.3546 | ds_loss: 0.0000 | lr: 4.7114e-05 | scale: 32768.0000 | micro time: 2.166 | step time: 0.000
train | epoch   1 | Iter:   2029/ 13000 | global iter:   2029/ 13000 | loss: 1.5093 | ds_loss: 0.0000 | lr: 4.7111e-05 | scale: 32768.0000 | micro time: 1.887 | step time: 0.000
train | epoch   1 | Iter:   2030/ 13000 | global iter:   2030/ 13000 | loss: 1.7859 | ds_loss: 0.0000 | lr: 4.7108e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2030/ 13000 | global iter:   2030/ 13000 | loss: 2.1265 | ds_loss: 0.0000 | lr: 4.7108e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 1.859
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2031/ 13000 | global iter:   2031/ 13000 | loss: 2.1118 | ds_loss: 0.0000 | lr: 4.7106e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   1 | Iter:   2032/ 13000 | global iter:   2032/ 13000 | loss: 1.9018 | ds_loss: 0.0000 | lr: 4.7103e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   1 | Iter:   2033/ 13000 | global iter:   2033/ 13000 | loss: 2.4846 | ds_loss: 0.0000 | lr: 4.7100e-05 | scale: 32768.0000 | micro time: 1.785 | step time: 0.000
train | epoch   1 | Iter:   2034/ 13000 | global iter:   2034/ 13000 | loss: 1.3737 | ds_loss: 0.0000 | lr: 4.7097e-05 | scale: 32768.0000 | micro time: 1.717 | step time: 0.000
train | epoch   1 | Iter:   2035/ 13000 | global iter:   2035/ 13000 | loss: 2.7060 | ds_loss: 0.0000 | lr: 4.7094e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   1 | Iter:   2036/ 13000 | global iter:   2036/ 13000 | loss: 2.0160 | ds_loss: 0.0000 | lr: 4.7091e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   1 | Iter:   2037/ 13000 | global iter:   2037/ 13000 | loss: 1.8270 | ds_loss: 0.0000 | lr: 4.7089e-05 | scale: 32768.0000 | micro time: 1.798 | step time: 0.000
train | epoch   1 | Iter:   2038/ 13000 | global iter:   2038/ 13000 | loss: 2.4307 | ds_loss: 0.0000 | lr: 4.7086e-05 | scale: 32768.0000 | micro time: 1.940 | step time: 0.000
train | epoch   1 | Iter:   2039/ 13000 | global iter:   2039/ 13000 | loss: 2.3011 | ds_loss: 0.0000 | lr: 4.7083e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   1 | Iter:   2040/ 13000 | global iter:   2040/ 13000 | loss: 2.5298 | ds_loss: 0.0000 | lr: 4.7080e-05 | scale: 32768.0000 | micro time: 1.903 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2040/ 13000 | global iter:   2040/ 13000 | loss: 2.1682 | ds_loss: 0.0000 | lr: 4.7080e-05 | scale: 32768.0000 | micro time: 1.903 | step time: 1.811
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2041/ 13000 | global iter:   2041/ 13000 | loss: 2.3518 | ds_loss: 0.0000 | lr: 4.7077e-05 | scale: 32768.0000 | micro time: 1.792 | step time: 0.000
train | epoch   1 | Iter:   2042/ 13000 | global iter:   2042/ 13000 | loss: 2.2689 | ds_loss: 0.0000 | lr: 4.7075e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   1 | Iter:   2043/ 13000 | global iter:   2043/ 13000 | loss: 2.3792 | ds_loss: 0.0000 | lr: 4.7072e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   1 | Iter:   2044/ 13000 | global iter:   2044/ 13000 | loss: 2.2497 | ds_loss: 0.0000 | lr: 4.7069e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   1 | Iter:   2045/ 13000 | global iter:   2045/ 13000 | loss: 2.3620 | ds_loss: 0.0000 | lr: 4.7066e-05 | scale: 32768.0000 | micro time: 1.687 | step time: 0.000
train | epoch   1 | Iter:   2046/ 13000 | global iter:   2046/ 13000 | loss: 2.3692 | ds_loss: 0.0000 | lr: 4.7063e-05 | scale: 32768.0000 | micro time: 1.776 | step time: 0.000
train | epoch   1 | Iter:   2047/ 13000 | global iter:   2047/ 13000 | loss: 2.0996 | ds_loss: 0.0000 | lr: 4.7060e-05 | scale: 32768.0000 | micro time: 1.777 | step time: 0.000
train | epoch   1 | Iter:   2048/ 13000 | global iter:   2048/ 13000 | loss: 2.5349 | ds_loss: 0.0000 | lr: 4.7057e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   1 | Iter:   2049/ 13000 | global iter:   2049/ 13000 | loss: 2.2012 | ds_loss: 0.0000 | lr: 4.7055e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
train | epoch   1 | Iter:   2050/ 13000 | global iter:   2050/ 13000 | loss: 2.3907 | ds_loss: 0.0000 | lr: 4.7052e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2050/ 13000 | global iter:   2050/ 13000 | loss: 2.3207 | ds_loss: 0.0000 | lr: 4.7052e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 1.788
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2051/ 13000 | global iter:   2051/ 13000 | loss: 2.0720 | ds_loss: 0.0000 | lr: 4.7049e-05 | scale: 32768.0000 | micro time: 1.790 | step time: 0.000
train | epoch   1 | Iter:   2052/ 13000 | global iter:   2052/ 13000 | loss: 2.4638 | ds_loss: 0.0000 | lr: 4.7046e-05 | scale: 32768.0000 | micro time: 1.755 | step time: 0.000
train | epoch   1 | Iter:   2053/ 13000 | global iter:   2053/ 13000 | loss: 1.7457 | ds_loss: 0.0000 | lr: 4.7043e-05 | scale: 32768.0000 | micro time: 1.826 | step time: 0.000
train | epoch   1 | Iter:   2054/ 13000 | global iter:   2054/ 13000 | loss: 2.1719 | ds_loss: 0.0000 | lr: 4.7040e-05 | scale: 32768.0000 | micro time: 1.842 | step time: 0.000
train | epoch   1 | Iter:   2055/ 13000 | global iter:   2055/ 13000 | loss: 1.6677 | ds_loss: 0.0000 | lr: 4.7038e-05 | scale: 32768.0000 | micro time: 1.865 | step time: 0.000
train | epoch   1 | Iter:   2056/ 13000 | global iter:   2056/ 13000 | loss: 1.9370 | ds_loss: 0.0000 | lr: 4.7035e-05 | scale: 32768.0000 | micro time: 1.832 | step time: 0.000
train | epoch   1 | Iter:   2057/ 13000 | global iter:   2057/ 13000 | loss: 1.9540 | ds_loss: 0.0000 | lr: 4.7032e-05 | scale: 32768.0000 | micro time: 1.821 | step time: 0.000
train | epoch   1 | Iter:   2058/ 13000 | global iter:   2058/ 13000 | loss: 1.8907 | ds_loss: 0.0000 | lr: 4.7029e-05 | scale: 32768.0000 | micro time: 1.830 | step time: 0.000
train | epoch   1 | Iter:   2059/ 13000 | global iter:   2059/ 13000 | loss: 1.7448 | ds_loss: 0.0000 | lr: 4.7026e-05 | scale: 32768.0000 | micro time: 1.768 | step time: 0.000
train | epoch   1 | Iter:   2060/ 13000 | global iter:   2060/ 13000 | loss: 2.0862 | ds_loss: 0.0000 | lr: 4.7023e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2060/ 13000 | global iter:   2060/ 13000 | loss: 1.9734 | ds_loss: 0.0000 | lr: 4.7023e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 1.816
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2061/ 13000 | global iter:   2061/ 13000 | loss: 2.4188 | ds_loss: 0.0000 | lr: 4.7020e-05 | scale: 32768.0000 | micro time: 1.818 | step time: 0.000
train | epoch   1 | Iter:   2062/ 13000 | global iter:   2062/ 13000 | loss: 1.6056 | ds_loss: 0.0000 | lr: 4.7018e-05 | scale: 32768.0000 | micro time: 1.786 | step time: 0.000
train | epoch   1 | Iter:   2063/ 13000 | global iter:   2063/ 13000 | loss: 1.7534 | ds_loss: 0.0000 | lr: 4.7015e-05 | scale: 32768.0000 | micro time: 1.884 | step time: 0.000
train | epoch   1 | Iter:   2064/ 13000 | global iter:   2064/ 13000 | loss: 2.3306 | ds_loss: 0.0000 | lr: 4.7012e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   1 | Iter:   2065/ 13000 | global iter:   2065/ 13000 | loss: 1.9441 | ds_loss: 0.0000 | lr: 4.7009e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   1 | Iter:   2066/ 13000 | global iter:   2066/ 13000 | loss: 1.9394 | ds_loss: 0.0000 | lr: 4.7006e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
train | epoch   1 | Iter:   2067/ 13000 | global iter:   2067/ 13000 | loss: 1.6595 | ds_loss: 0.0000 | lr: 4.7003e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   1 | Iter:   2068/ 13000 | global iter:   2068/ 13000 | loss: 1.8205 | ds_loss: 0.0000 | lr: 4.7000e-05 | scale: 32768.0000 | micro time: 1.794 | step time: 0.000
train | epoch   1 | Iter:   2069/ 13000 | global iter:   2069/ 13000 | loss: 2.1622 | ds_loss: 0.0000 | lr: 4.6998e-05 | scale: 32768.0000 | micro time: 1.808 | step time: 0.000
train | epoch   1 | Iter:   2070/ 13000 | global iter:   2070/ 13000 | loss: 1.5755 | ds_loss: 0.0000 | lr: 4.6995e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2070/ 13000 | global iter:   2070/ 13000 | loss: 1.9210 | ds_loss: 0.0000 | lr: 4.6995e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 1.798
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2071/ 13000 | global iter:   2071/ 13000 | loss: 1.8821 | ds_loss: 0.0000 | lr: 4.6992e-05 | scale: 32768.0000 | micro time: 1.829 | step time: 0.000
train | epoch   1 | Iter:   2072/ 13000 | global iter:   2072/ 13000 | loss: 2.0491 | ds_loss: 0.0000 | lr: 4.6989e-05 | scale: 32768.0000 | micro time: 1.895 | step time: 0.000
train | epoch   1 | Iter:   2073/ 13000 | global iter:   2073/ 13000 | loss: 2.0088 | ds_loss: 0.0000 | lr: 4.6986e-05 | scale: 32768.0000 | micro time: 1.718 | step time: 0.000
train | epoch   1 | Iter:   2074/ 13000 | global iter:   2074/ 13000 | loss: 2.6035 | ds_loss: 0.0000 | lr: 4.6983e-05 | scale: 32768.0000 | micro time: 1.744 | step time: 0.000
train | epoch   1 | Iter:   2075/ 13000 | global iter:   2075/ 13000 | loss: 2.0277 | ds_loss: 0.0000 | lr: 4.6980e-05 | scale: 32768.0000 | micro time: 1.711 | step time: 0.000
train | epoch   1 | Iter:   2076/ 13000 | global iter:   2076/ 13000 | loss: 1.8981 | ds_loss: 0.0000 | lr: 4.6977e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   1 | Iter:   2077/ 13000 | global iter:   2077/ 13000 | loss: 1.8679 | ds_loss: 0.0000 | lr: 4.6975e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   1 | Iter:   2078/ 13000 | global iter:   2078/ 13000 | loss: 1.6040 | ds_loss: 0.0000 | lr: 4.6972e-05 | scale: 32768.0000 | micro time: 1.865 | step time: 0.000
train | epoch   1 | Iter:   2079/ 13000 | global iter:   2079/ 13000 | loss: 2.0854 | ds_loss: 0.0000 | lr: 4.6969e-05 | scale: 32768.0000 | micro time: 1.792 | step time: 0.000
train | epoch   1 | Iter:   2080/ 13000 | global iter:   2080/ 13000 | loss: 1.5681 | ds_loss: 0.0000 | lr: 4.6966e-05 | scale: 32768.0000 | micro time: 1.884 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2080/ 13000 | global iter:   2080/ 13000 | loss: 1.9595 | ds_loss: 0.0000 | lr: 4.6966e-05 | scale: 32768.0000 | micro time: 1.884 | step time: 1.802
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2081/ 13000 | global iter:   2081/ 13000 | loss: 2.1814 | ds_loss: 0.0000 | lr: 4.6963e-05 | scale: 32768.0000 | micro time: 1.841 | step time: 0.000
train | epoch   1 | Iter:   2082/ 13000 | global iter:   2082/ 13000 | loss: 1.7245 | ds_loss: 0.0000 | lr: 4.6960e-05 | scale: 32768.0000 | micro time: 1.777 | step time: 0.000
train | epoch   1 | Iter:   2083/ 13000 | global iter:   2083/ 13000 | loss: 1.9537 | ds_loss: 0.0000 | lr: 4.6957e-05 | scale: 32768.0000 | micro time: 1.800 | step time: 0.000
train | epoch   1 | Iter:   2084/ 13000 | global iter:   2084/ 13000 | loss: 1.8807 | ds_loss: 0.0000 | lr: 4.6954e-05 | scale: 32768.0000 | micro time: 1.880 | step time: 0.000
train | epoch   1 | Iter:   2085/ 13000 | global iter:   2085/ 13000 | loss: 2.4535 | ds_loss: 0.0000 | lr: 4.6952e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   1 | Iter:   2086/ 13000 | global iter:   2086/ 13000 | loss: 2.2289 | ds_loss: 0.0000 | lr: 4.6949e-05 | scale: 32768.0000 | micro time: 1.837 | step time: 0.000
train | epoch   1 | Iter:   2087/ 13000 | global iter:   2087/ 13000 | loss: 2.1500 | ds_loss: 0.0000 | lr: 4.6946e-05 | scale: 32768.0000 | micro time: 1.785 | step time: 0.000
train | epoch   1 | Iter:   2088/ 13000 | global iter:   2088/ 13000 | loss: 2.3993 | ds_loss: 0.0000 | lr: 4.6943e-05 | scale: 32768.0000 | micro time: 1.727 | step time: 0.000
train | epoch   1 | Iter:   2089/ 13000 | global iter:   2089/ 13000 | loss: 2.2772 | ds_loss: 0.0000 | lr: 4.6940e-05 | scale: 32768.0000 | micro time: 1.810 | step time: 0.000
train | epoch   1 | Iter:   2090/ 13000 | global iter:   2090/ 13000 | loss: 2.4145 | ds_loss: 0.0000 | lr: 4.6937e-05 | scale: 32768.0000 | micro time: 1.817 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2090/ 13000 | global iter:   2090/ 13000 | loss: 2.1664 | ds_loss: 0.0000 | lr: 4.6937e-05 | scale: 32768.0000 | micro time: 1.817 | step time: 1.804
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2091/ 13000 | global iter:   2091/ 13000 | loss: 2.1392 | ds_loss: 0.0000 | lr: 4.6934e-05 | scale: 32768.0000 | micro time: 1.850 | step time: 0.000
train | epoch   1 | Iter:   2092/ 13000 | global iter:   2092/ 13000 | loss: 2.3686 | ds_loss: 0.0000 | lr: 4.6931e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   1 | Iter:   2093/ 13000 | global iter:   2093/ 13000 | loss: 2.5539 | ds_loss: 0.0000 | lr: 4.6928e-05 | scale: 32768.0000 | micro time: 1.849 | step time: 0.000
train | epoch   1 | Iter:   2094/ 13000 | global iter:   2094/ 13000 | loss: 2.3335 | ds_loss: 0.0000 | lr: 4.6925e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   1 | Iter:   2095/ 13000 | global iter:   2095/ 13000 | loss: 2.1518 | ds_loss: 0.0000 | lr: 4.6923e-05 | scale: 32768.0000 | micro time: 1.781 | step time: 0.000
train | epoch   1 | Iter:   2096/ 13000 | global iter:   2096/ 13000 | loss: 2.2876 | ds_loss: 0.0000 | lr: 4.6920e-05 | scale: 32768.0000 | micro time: 1.903 | step time: 0.000
train | epoch   1 | Iter:   2097/ 13000 | global iter:   2097/ 13000 | loss: 2.4042 | ds_loss: 0.0000 | lr: 4.6917e-05 | scale: 32768.0000 | micro time: 1.820 | step time: 0.000
train | epoch   1 | Iter:   2098/ 13000 | global iter:   2098/ 13000 | loss: 2.0378 | ds_loss: 0.0000 | lr: 4.6914e-05 | scale: 32768.0000 | micro time: 1.805 | step time: 0.000
train | epoch   1 | Iter:   2099/ 13000 | global iter:   2099/ 13000 | loss: 1.8564 | ds_loss: 0.0000 | lr: 4.6911e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   1 | Iter:   2100/ 13000 | global iter:   2100/ 13000 | loss: 2.2151 | ds_loss: 0.0000 | lr: 4.6908e-05 | scale: 32768.0000 | micro time: 1.896 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2100/ 13000 | global iter:   2100/ 13000 | loss: 2.2348 | ds_loss: 0.0000 | lr: 4.6908e-05 | scale: 32768.0000 | micro time: 1.896 | step time: 1.829
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2101/ 13000 | global iter:   2101/ 13000 | loss: 2.6301 | ds_loss: 0.0000 | lr: 4.6905e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   1 | Iter:   2102/ 13000 | global iter:   2102/ 13000 | loss: 1.8698 | ds_loss: 0.0000 | lr: 4.6902e-05 | scale: 32768.0000 | micro time: 1.750 | step time: 0.000
train | epoch   1 | Iter:   2103/ 13000 | global iter:   2103/ 13000 | loss: 2.3559 | ds_loss: 0.0000 | lr: 4.6899e-05 | scale: 32768.0000 | micro time: 1.852 | step time: 0.000
train | epoch   1 | Iter:   2104/ 13000 | global iter:   2104/ 13000 | loss: 2.4298 | ds_loss: 0.0000 | lr: 4.6896e-05 | scale: 32768.0000 | micro time: 1.769 | step time: 0.000
train | epoch   1 | Iter:   2105/ 13000 | global iter:   2105/ 13000 | loss: 1.4952 | ds_loss: 0.0000 | lr: 4.6893e-05 | scale: 32768.0000 | micro time: 1.861 | step time: 0.000
train | epoch   1 | Iter:   2106/ 13000 | global iter:   2106/ 13000 | loss: 2.3325 | ds_loss: 0.0000 | lr: 4.6891e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   1 | Iter:   2107/ 13000 | global iter:   2107/ 13000 | loss: 2.2857 | ds_loss: 0.0000 | lr: 4.6888e-05 | scale: 32768.0000 | micro time: 1.899 | step time: 0.000
train | epoch   1 | Iter:   2108/ 13000 | global iter:   2108/ 13000 | loss: 2.3742 | ds_loss: 0.0000 | lr: 4.6885e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
train | epoch   1 | Iter:   2109/ 13000 | global iter:   2109/ 13000 | loss: 2.7037 | ds_loss: 0.0000 | lr: 4.6882e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   1 | Iter:   2110/ 13000 | global iter:   2110/ 13000 | loss: 2.2034 | ds_loss: 0.0000 | lr: 4.6879e-05 | scale: 32768.0000 | micro time: 1.857 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2110/ 13000 | global iter:   2110/ 13000 | loss: 2.2680 | ds_loss: 0.0000 | lr: 4.6879e-05 | scale: 32768.0000 | micro time: 1.857 | step time: 1.820
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2111/ 13000 | global iter:   2111/ 13000 | loss: 1.9404 | ds_loss: 0.0000 | lr: 4.6876e-05 | scale: 32768.0000 | micro time: 1.796 | step time: 0.000
train | epoch   1 | Iter:   2112/ 13000 | global iter:   2112/ 13000 | loss: 2.3551 | ds_loss: 0.0000 | lr: 4.6873e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   1 | Iter:   2113/ 13000 | global iter:   2113/ 13000 | loss: 1.9153 | ds_loss: 0.0000 | lr: 4.6870e-05 | scale: 32768.0000 | micro time: 1.812 | step time: 0.000
train | epoch   1 | Iter:   2114/ 13000 | global iter:   2114/ 13000 | loss: 2.6002 | ds_loss: 0.0000 | lr: 4.6867e-05 | scale: 32768.0000 | micro time: 1.642 | step time: 0.000
train | epoch   1 | Iter:   2115/ 13000 | global iter:   2115/ 13000 | loss: 2.1853 | ds_loss: 0.0000 | lr: 4.6864e-05 | scale: 32768.0000 | micro time: 1.826 | step time: 0.000
train | epoch   1 | Iter:   2116/ 13000 | global iter:   2116/ 13000 | loss: 2.1290 | ds_loss: 0.0000 | lr: 4.6861e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   1 | Iter:   2117/ 13000 | global iter:   2117/ 13000 | loss: 1.8212 | ds_loss: 0.0000 | lr: 4.6858e-05 | scale: 32768.0000 | micro time: 1.832 | step time: 0.000
train | epoch   1 | Iter:   2118/ 13000 | global iter:   2118/ 13000 | loss: 2.2823 | ds_loss: 0.0000 | lr: 4.6856e-05 | scale: 32768.0000 | micro time: 1.770 | step time: 0.000
train | epoch   1 | Iter:   2119/ 13000 | global iter:   2119/ 13000 | loss: 2.5350 | ds_loss: 0.0000 | lr: 4.6853e-05 | scale: 32768.0000 | micro time: 1.727 | step time: 0.000
train | epoch   1 | Iter:   2120/ 13000 | global iter:   2120/ 13000 | loss: 2.3320 | ds_loss: 0.0000 | lr: 4.6850e-05 | scale: 32768.0000 | micro time: 1.760 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2120/ 13000 | global iter:   2120/ 13000 | loss: 2.2096 | ds_loss: 0.0000 | lr: 4.6850e-05 | scale: 32768.0000 | micro time: 1.760 | step time: 1.778
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2121/ 13000 | global iter:   2121/ 13000 | loss: 2.2437 | ds_loss: 0.0000 | lr: 4.6847e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   1 | Iter:   2122/ 13000 | global iter:   2122/ 13000 | loss: 1.7216 | ds_loss: 0.0000 | lr: 4.6844e-05 | scale: 32768.0000 | micro time: 1.849 | step time: 0.000
train | epoch   1 | Iter:   2123/ 13000 | global iter:   2123/ 13000 | loss: 1.9438 | ds_loss: 0.0000 | lr: 4.6841e-05 | scale: 32768.0000 | micro time: 1.777 | step time: 0.000
train | epoch   1 | Iter:   2124/ 13000 | global iter:   2124/ 13000 | loss: 1.8937 | ds_loss: 0.0000 | lr: 4.6838e-05 | scale: 32768.0000 | micro time: 1.891 | step time: 0.000
train | epoch   1 | Iter:   2125/ 13000 | global iter:   2125/ 13000 | loss: 2.0388 | ds_loss: 0.0000 | lr: 4.6835e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   1 | Iter:   2126/ 13000 | global iter:   2126/ 13000 | loss: 1.5064 | ds_loss: 0.0000 | lr: 4.6832e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   1 | Iter:   2127/ 13000 | global iter:   2127/ 13000 | loss: 2.4862 | ds_loss: 0.0000 | lr: 4.6829e-05 | scale: 32768.0000 | micro time: 1.723 | step time: 0.000
train | epoch   1 | Iter:   2128/ 13000 | global iter:   2128/ 13000 | loss: 2.2693 | ds_loss: 0.0000 | lr: 4.6826e-05 | scale: 32768.0000 | micro time: 1.824 | step time: 0.000
train | epoch   1 | Iter:   2129/ 13000 | global iter:   2129/ 13000 | loss: 1.8467 | ds_loss: 0.0000 | lr: 4.6823e-05 | scale: 32768.0000 | micro time: 1.800 | step time: 0.000
train | epoch   1 | Iter:   2130/ 13000 | global iter:   2130/ 13000 | loss: 2.5141 | ds_loss: 0.0000 | lr: 4.6820e-05 | scale: 32768.0000 | micro time: 1.777 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2130/ 13000 | global iter:   2130/ 13000 | loss: 2.0464 | ds_loss: 0.0000 | lr: 4.6820e-05 | scale: 32768.0000 | micro time: 1.777 | step time: 1.809
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2131/ 13000 | global iter:   2131/ 13000 | loss: 1.9021 | ds_loss: 0.0000 | lr: 4.6817e-05 | scale: 32768.0000 | micro time: 1.834 | step time: 0.000
train | epoch   1 | Iter:   2132/ 13000 | global iter:   2132/ 13000 | loss: 2.3841 | ds_loss: 0.0000 | lr: 4.6814e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   1 | Iter:   2133/ 13000 | global iter:   2133/ 13000 | loss: 1.8948 | ds_loss: 0.0000 | lr: 4.6811e-05 | scale: 32768.0000 | micro time: 1.785 | step time: 0.000
train | epoch   1 | Iter:   2134/ 13000 | global iter:   2134/ 13000 | loss: 2.5045 | ds_loss: 0.0000 | lr: 4.6808e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
train | epoch   1 | Iter:   2135/ 13000 | global iter:   2135/ 13000 | loss: 2.1221 | ds_loss: 0.0000 | lr: 4.6806e-05 | scale: 32768.0000 | micro time: 1.852 | step time: 0.000
train | epoch   1 | Iter:   2136/ 13000 | global iter:   2136/ 13000 | loss: 1.9246 | ds_loss: 0.0000 | lr: 4.6803e-05 | scale: 32768.0000 | micro time: 1.715 | step time: 0.000
train | epoch   1 | Iter:   2137/ 13000 | global iter:   2137/ 13000 | loss: 2.1633 | ds_loss: 0.0000 | lr: 4.6800e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   1 | Iter:   2138/ 13000 | global iter:   2138/ 13000 | loss: 2.1708 | ds_loss: 0.0000 | lr: 4.6797e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   1 | Iter:   2139/ 13000 | global iter:   2139/ 13000 | loss: 1.5648 | ds_loss: 0.0000 | lr: 4.6794e-05 | scale: 32768.0000 | micro time: 1.915 | step time: 0.000
train | epoch   1 | Iter:   2140/ 13000 | global iter:   2140/ 13000 | loss: 1.8045 | ds_loss: 0.0000 | lr: 4.6791e-05 | scale: 32768.0000 | micro time: 1.755 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2140/ 13000 | global iter:   2140/ 13000 | loss: 2.0436 | ds_loss: 0.0000 | lr: 4.6791e-05 | scale: 32768.0000 | micro time: 1.755 | step time: 1.809
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2141/ 13000 | global iter:   2141/ 13000 | loss: 2.1314 | ds_loss: 0.0000 | lr: 4.6788e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   1 | Iter:   2142/ 13000 | global iter:   2142/ 13000 | loss: 1.9411 | ds_loss: 0.0000 | lr: 4.6785e-05 | scale: 32768.0000 | micro time: 1.832 | step time: 0.000
train | epoch   1 | Iter:   2143/ 13000 | global iter:   2143/ 13000 | loss: 2.8054 | ds_loss: 0.0000 | lr: 4.6782e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   1 | Iter:   2144/ 13000 | global iter:   2144/ 13000 | loss: 2.0109 | ds_loss: 0.0000 | lr: 4.6779e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   1 | Iter:   2145/ 13000 | global iter:   2145/ 13000 | loss: 2.0926 | ds_loss: 0.0000 | lr: 4.6776e-05 | scale: 32768.0000 | micro time: 1.844 | step time: 0.000
train | epoch   1 | Iter:   2146/ 13000 | global iter:   2146/ 13000 | loss: 2.2339 | ds_loss: 0.0000 | lr: 4.6773e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   1 | Iter:   2147/ 13000 | global iter:   2147/ 13000 | loss: 2.3313 | ds_loss: 0.0000 | lr: 4.6770e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   1 | Iter:   2148/ 13000 | global iter:   2148/ 13000 | loss: 2.1390 | ds_loss: 0.0000 | lr: 4.6767e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   1 | Iter:   2149/ 13000 | global iter:   2149/ 13000 | loss: 2.6992 | ds_loss: 0.0000 | lr: 4.6764e-05 | scale: 32768.0000 | micro time: 1.736 | step time: 0.000
train | epoch   1 | Iter:   2150/ 13000 | global iter:   2150/ 13000 | loss: 2.0871 | ds_loss: 0.0000 | lr: 4.6761e-05 | scale: 32768.0000 | micro time: 1.788 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2150/ 13000 | global iter:   2150/ 13000 | loss: 2.2472 | ds_loss: 0.0000 | lr: 4.6761e-05 | scale: 32768.0000 | micro time: 1.788 | step time: 1.804
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
[2025-04-19 12:13:08,904] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
train | epoch   1 | Iter:   2151/ 13000 | global iter:   2151/ 13000 | loss: 2.7764 | ds_loss: 0.0000 | lr: 4.6761e-05 | scale: 16384.0000 | micro time: 1.403 | step time: 0.000
train | epoch   1 | Iter:   2152/ 13000 | global iter:   2152/ 13000 | loss: 2.0771 | ds_loss: 0.0000 | lr: 4.6758e-05 | scale: 16384.0000 | micro time: 1.842 | step time: 0.000
train | epoch   1 | Iter:   2153/ 13000 | global iter:   2153/ 13000 | loss: 1.8904 | ds_loss: 0.0000 | lr: 4.6755e-05 | scale: 16384.0000 | micro time: 1.877 | step time: 0.000
train | epoch   1 | Iter:   2154/ 13000 | global iter:   2154/ 13000 | loss: 2.3416 | ds_loss: 0.0000 | lr: 4.6752e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   1 | Iter:   2155/ 13000 | global iter:   2155/ 13000 | loss: 1.9704 | ds_loss: 0.0000 | lr: 4.6749e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   1 | Iter:   2156/ 13000 | global iter:   2156/ 13000 | loss: 1.5041 | ds_loss: 0.0000 | lr: 4.6746e-05 | scale: 16384.0000 | micro time: 1.721 | step time: 0.000
train | epoch   1 | Iter:   2157/ 13000 | global iter:   2157/ 13000 | loss: 2.2049 | ds_loss: 0.0000 | lr: 4.6743e-05 | scale: 16384.0000 | micro time: 1.717 | step time: 0.000
train | epoch   1 | Iter:   2158/ 13000 | global iter:   2158/ 13000 | loss: 1.9874 | ds_loss: 0.0000 | lr: 4.6740e-05 | scale: 16384.0000 | micro time: 1.895 | step time: 0.000
train | epoch   1 | Iter:   2159/ 13000 | global iter:   2159/ 13000 | loss: 2.0548 | ds_loss: 0.0000 | lr: 4.6737e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   1 | Iter:   2160/ 13000 | global iter:   2160/ 13000 | loss: 2.1233 | ds_loss: 0.0000 | lr: 4.6734e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2160/ 13000 | global iter:   2160/ 13000 | loss: 2.0930 | ds_loss: 0.0000 | lr: 4.6734e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 1.777
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2161/ 13000 | global iter:   2161/ 13000 | loss: 1.6584 | ds_loss: 0.0000 | lr: 4.6731e-05 | scale: 16384.0000 | micro time: 1.724 | step time: 0.000
train | epoch   1 | Iter:   2162/ 13000 | global iter:   2162/ 13000 | loss: 2.1935 | ds_loss: 0.0000 | lr: 4.6728e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   1 | Iter:   2163/ 13000 | global iter:   2163/ 13000 | loss: 2.3331 | ds_loss: 0.0000 | lr: 4.6725e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   1 | Iter:   2164/ 13000 | global iter:   2164/ 13000 | loss: 1.8563 | ds_loss: 0.0000 | lr: 4.6722e-05 | scale: 16384.0000 | micro time: 1.808 | step time: 0.000
train | epoch   1 | Iter:   2165/ 13000 | global iter:   2165/ 13000 | loss: 2.2833 | ds_loss: 0.0000 | lr: 4.6719e-05 | scale: 16384.0000 | micro time: 1.794 | step time: 0.000
train | epoch   1 | Iter:   2166/ 13000 | global iter:   2166/ 13000 | loss: 1.6789 | ds_loss: 0.0000 | lr: 4.6716e-05 | scale: 16384.0000 | micro time: 1.899 | step time: 0.000
train | epoch   1 | Iter:   2167/ 13000 | global iter:   2167/ 13000 | loss: 1.7647 | ds_loss: 0.0000 | lr: 4.6713e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 0.000
train | epoch   1 | Iter:   2168/ 13000 | global iter:   2168/ 13000 | loss: 2.1906 | ds_loss: 0.0000 | lr: 4.6710e-05 | scale: 16384.0000 | micro time: 1.825 | step time: 0.000
train | epoch   1 | Iter:   2169/ 13000 | global iter:   2169/ 13000 | loss: 2.3689 | ds_loss: 0.0000 | lr: 4.6707e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   1 | Iter:   2170/ 13000 | global iter:   2170/ 13000 | loss: 2.0539 | ds_loss: 0.0000 | lr: 4.6704e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2170/ 13000 | global iter:   2170/ 13000 | loss: 2.0382 | ds_loss: 0.0000 | lr: 4.6704e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 1.831
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2171/ 13000 | global iter:   2171/ 13000 | loss: 2.5054 | ds_loss: 0.0000 | lr: 4.6701e-05 | scale: 16384.0000 | micro time: 1.622 | step time: 0.000
train | epoch   1 | Iter:   2172/ 13000 | global iter:   2172/ 13000 | loss: 2.0315 | ds_loss: 0.0000 | lr: 4.6698e-05 | scale: 16384.0000 | micro time: 1.820 | step time: 0.000
train | epoch   1 | Iter:   2173/ 13000 | global iter:   2173/ 13000 | loss: 2.3111 | ds_loss: 0.0000 | lr: 4.6695e-05 | scale: 16384.0000 | micro time: 1.915 | step time: 0.000
train | epoch   1 | Iter:   2174/ 13000 | global iter:   2174/ 13000 | loss: 1.9511 | ds_loss: 0.0000 | lr: 4.6692e-05 | scale: 16384.0000 | micro time: 1.700 | step time: 0.000
train | epoch   1 | Iter:   2175/ 13000 | global iter:   2175/ 13000 | loss: 2.1175 | ds_loss: 0.0000 | lr: 4.6689e-05 | scale: 16384.0000 | micro time: 1.719 | step time: 0.000
train | epoch   1 | Iter:   2176/ 13000 | global iter:   2176/ 13000 | loss: 2.1706 | ds_loss: 0.0000 | lr: 4.6686e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   1 | Iter:   2177/ 13000 | global iter:   2177/ 13000 | loss: 2.1618 | ds_loss: 0.0000 | lr: 4.6683e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   1 | Iter:   2178/ 13000 | global iter:   2178/ 13000 | loss: 1.9579 | ds_loss: 0.0000 | lr: 4.6680e-05 | scale: 16384.0000 | micro time: 1.862 | step time: 0.000
train | epoch   1 | Iter:   2179/ 13000 | global iter:   2179/ 13000 | loss: 2.4178 | ds_loss: 0.0000 | lr: 4.6677e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   1 | Iter:   2180/ 13000 | global iter:   2180/ 13000 | loss: 2.2967 | ds_loss: 0.0000 | lr: 4.6674e-05 | scale: 16384.0000 | micro time: 1.778 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2180/ 13000 | global iter:   2180/ 13000 | loss: 2.1921 | ds_loss: 0.0000 | lr: 4.6674e-05 | scale: 16384.0000 | micro time: 1.778 | step time: 1.782
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2181/ 13000 | global iter:   2181/ 13000 | loss: 2.6177 | ds_loss: 0.0000 | lr: 4.6671e-05 | scale: 16384.0000 | micro time: 1.770 | step time: 0.000
train | epoch   1 | Iter:   2182/ 13000 | global iter:   2182/ 13000 | loss: 1.9339 | ds_loss: 0.0000 | lr: 4.6668e-05 | scale: 16384.0000 | micro time: 1.849 | step time: 0.000
train | epoch   1 | Iter:   2183/ 13000 | global iter:   2183/ 13000 | loss: 2.3879 | ds_loss: 0.0000 | lr: 4.6665e-05 | scale: 16384.0000 | micro time: 1.768 | step time: 0.000
train | epoch   1 | Iter:   2184/ 13000 | global iter:   2184/ 13000 | loss: 2.3900 | ds_loss: 0.0000 | lr: 4.6662e-05 | scale: 16384.0000 | micro time: 1.709 | step time: 0.000
train | epoch   1 | Iter:   2185/ 13000 | global iter:   2185/ 13000 | loss: 2.4845 | ds_loss: 0.0000 | lr: 4.6659e-05 | scale: 16384.0000 | micro time: 1.797 | step time: 0.000
train | epoch   1 | Iter:   2186/ 13000 | global iter:   2186/ 13000 | loss: 1.9845 | ds_loss: 0.0000 | lr: 4.6656e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   1 | Iter:   2187/ 13000 | global iter:   2187/ 13000 | loss: 2.3821 | ds_loss: 0.0000 | lr: 4.6653e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   1 | Iter:   2188/ 13000 | global iter:   2188/ 13000 | loss: 2.1323 | ds_loss: 0.0000 | lr: 4.6650e-05 | scale: 16384.0000 | micro time: 1.931 | step time: 0.000
train | epoch   1 | Iter:   2189/ 13000 | global iter:   2189/ 13000 | loss: 1.6943 | ds_loss: 0.0000 | lr: 4.6647e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
train | epoch   1 | Iter:   2190/ 13000 | global iter:   2190/ 13000 | loss: 1.7517 | ds_loss: 0.0000 | lr: 4.6644e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2190/ 13000 | global iter:   2190/ 13000 | loss: 2.1759 | ds_loss: 0.0000 | lr: 4.6644e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 1.820
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2191/ 13000 | global iter:   2191/ 13000 | loss: 2.3557 | ds_loss: 0.0000 | lr: 4.6641e-05 | scale: 16384.0000 | micro time: 1.804 | step time: 0.000
train | epoch   1 | Iter:   2192/ 13000 | global iter:   2192/ 13000 | loss: 1.8590 | ds_loss: 0.0000 | lr: 4.6638e-05 | scale: 16384.0000 | micro time: 1.760 | step time: 0.000
train | epoch   1 | Iter:   2193/ 13000 | global iter:   2193/ 13000 | loss: 1.9036 | ds_loss: 0.0000 | lr: 4.6635e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 0.000
train | epoch   1 | Iter:   2194/ 13000 | global iter:   2194/ 13000 | loss: 2.2079 | ds_loss: 0.0000 | lr: 4.6632e-05 | scale: 16384.0000 | micro time: 1.825 | step time: 0.000
train | epoch   1 | Iter:   2195/ 13000 | global iter:   2195/ 13000 | loss: 2.5324 | ds_loss: 0.0000 | lr: 4.6629e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   1 | Iter:   2196/ 13000 | global iter:   2196/ 13000 | loss: 2.4566 | ds_loss: 0.0000 | lr: 4.6626e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   1 | Iter:   2197/ 13000 | global iter:   2197/ 13000 | loss: 2.3521 | ds_loss: 0.0000 | lr: 4.6623e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   1 | Iter:   2198/ 13000 | global iter:   2198/ 13000 | loss: 2.0136 | ds_loss: 0.0000 | lr: 4.6620e-05 | scale: 16384.0000 | micro time: 1.703 | step time: 0.000
train | epoch   1 | Iter:   2199/ 13000 | global iter:   2199/ 13000 | loss: 2.3733 | ds_loss: 0.0000 | lr: 4.6617e-05 | scale: 16384.0000 | micro time: 1.759 | step time: 0.000
train | epoch   1 | Iter:   2200/ 13000 | global iter:   2200/ 13000 | loss: 2.0559 | ds_loss: 0.0000 | lr: 4.6614e-05 | scale: 16384.0000 | micro time: 1.706 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2200/ 13000 | global iter:   2200/ 13000 | loss: 2.2110 | ds_loss: 0.0000 | lr: 4.6614e-05 | scale: 16384.0000 | micro time: 1.706 | step time: 1.796
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2201/ 13000 | global iter:   2201/ 13000 | loss: 2.0438 | ds_loss: 0.0000 | lr: 4.6611e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 0.000
train | epoch   1 | Iter:   2202/ 13000 | global iter:   2202/ 13000 | loss: 2.1211 | ds_loss: 0.0000 | lr: 4.6608e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   1 | Iter:   2203/ 13000 | global iter:   2203/ 13000 | loss: 1.6737 | ds_loss: 0.0000 | lr: 4.6605e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   1 | Iter:   2204/ 13000 | global iter:   2204/ 13000 | loss: 2.8472 | ds_loss: 0.0000 | lr: 4.6602e-05 | scale: 16384.0000 | micro time: 1.743 | step time: 0.000
train | epoch   1 | Iter:   2205/ 13000 | global iter:   2205/ 13000 | loss: 1.8317 | ds_loss: 0.0000 | lr: 4.6599e-05 | scale: 16384.0000 | micro time: 1.916 | step time: 0.000
train | epoch   1 | Iter:   2206/ 13000 | global iter:   2206/ 13000 | loss: 2.0871 | ds_loss: 0.0000 | lr: 4.6596e-05 | scale: 16384.0000 | micro time: 1.754 | step time: 0.000
train | epoch   1 | Iter:   2207/ 13000 | global iter:   2207/ 13000 | loss: 2.1528 | ds_loss: 0.0000 | lr: 4.6593e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   1 | Iter:   2208/ 13000 | global iter:   2208/ 13000 | loss: 1.9003 | ds_loss: 0.0000 | lr: 4.6590e-05 | scale: 16384.0000 | micro time: 1.736 | step time: 0.000
train | epoch   1 | Iter:   2209/ 13000 | global iter:   2209/ 13000 | loss: 1.5222 | ds_loss: 0.0000 | lr: 4.6587e-05 | scale: 16384.0000 | micro time: 1.738 | step time: 0.000
train | epoch   1 | Iter:   2210/ 13000 | global iter:   2210/ 13000 | loss: 2.6155 | ds_loss: 0.0000 | lr: 4.6584e-05 | scale: 16384.0000 | micro time: 1.864 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2210/ 13000 | global iter:   2210/ 13000 | loss: 2.0795 | ds_loss: 0.0000 | lr: 4.6584e-05 | scale: 16384.0000 | micro time: 1.864 | step time: 1.801
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2211/ 13000 | global iter:   2211/ 13000 | loss: 2.5358 | ds_loss: 0.0000 | lr: 4.6581e-05 | scale: 16384.0000 | micro time: 1.742 | step time: 0.000
train | epoch   1 | Iter:   2212/ 13000 | global iter:   2212/ 13000 | loss: 2.1921 | ds_loss: 0.0000 | lr: 4.6578e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   1 | Iter:   2213/ 13000 | global iter:   2213/ 13000 | loss: 2.0378 | ds_loss: 0.0000 | lr: 4.6574e-05 | scale: 16384.0000 | micro time: 1.776 | step time: 0.000
train | epoch   1 | Iter:   2214/ 13000 | global iter:   2214/ 13000 | loss: 2.6531 | ds_loss: 0.0000 | lr: 4.6571e-05 | scale: 16384.0000 | micro time: 1.886 | step time: 0.000
train | epoch   1 | Iter:   2215/ 13000 | global iter:   2215/ 13000 | loss: 1.8789 | ds_loss: 0.0000 | lr: 4.6568e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   1 | Iter:   2216/ 13000 | global iter:   2216/ 13000 | loss: 2.1980 | ds_loss: 0.0000 | lr: 4.6565e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   1 | Iter:   2217/ 13000 | global iter:   2217/ 13000 | loss: 2.4860 | ds_loss: 0.0000 | lr: 4.6562e-05 | scale: 16384.0000 | micro time: 1.727 | step time: 0.000
train | epoch   1 | Iter:   2218/ 13000 | global iter:   2218/ 13000 | loss: 1.8215 | ds_loss: 0.0000 | lr: 4.6559e-05 | scale: 16384.0000 | micro time: 1.915 | step time: 0.000
train | epoch   1 | Iter:   2219/ 13000 | global iter:   2219/ 13000 | loss: 1.9628 | ds_loss: 0.0000 | lr: 4.6556e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   1 | Iter:   2220/ 13000 | global iter:   2220/ 13000 | loss: 2.4147 | ds_loss: 0.0000 | lr: 4.6553e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2220/ 13000 | global iter:   2220/ 13000 | loss: 2.2181 | ds_loss: 0.0000 | lr: 4.6553e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 1.812
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2221/ 13000 | global iter:   2221/ 13000 | loss: 1.8063 | ds_loss: 0.0000 | lr: 4.6550e-05 | scale: 16384.0000 | micro time: 1.842 | step time: 0.000
train | epoch   1 | Iter:   2222/ 13000 | global iter:   2222/ 13000 | loss: 2.1926 | ds_loss: 0.0000 | lr: 4.6547e-05 | scale: 16384.0000 | micro time: 1.778 | step time: 0.000
train | epoch   1 | Iter:   2223/ 13000 | global iter:   2223/ 13000 | loss: 2.3420 | ds_loss: 0.0000 | lr: 4.6544e-05 | scale: 16384.0000 | micro time: 1.720 | step time: 0.000
train | epoch   1 | Iter:   2224/ 13000 | global iter:   2224/ 13000 | loss: 2.2892 | ds_loss: 0.0000 | lr: 4.6541e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   1 | Iter:   2225/ 13000 | global iter:   2225/ 13000 | loss: 2.4128 | ds_loss: 0.0000 | lr: 4.6538e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   1 | Iter:   2226/ 13000 | global iter:   2226/ 13000 | loss: 1.2280 | ds_loss: 0.0000 | lr: 4.6535e-05 | scale: 16384.0000 | micro time: 1.818 | step time: 0.000
train | epoch   1 | Iter:   2227/ 13000 | global iter:   2227/ 13000 | loss: 2.5057 | ds_loss: 0.0000 | lr: 4.6532e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   1 | Iter:   2228/ 13000 | global iter:   2228/ 13000 | loss: 1.8304 | ds_loss: 0.0000 | lr: 4.6529e-05 | scale: 16384.0000 | micro time: 1.720 | step time: 0.000
train | epoch   1 | Iter:   2229/ 13000 | global iter:   2229/ 13000 | loss: 1.7810 | ds_loss: 0.0000 | lr: 4.6526e-05 | scale: 16384.0000 | micro time: 1.809 | step time: 0.000
train | epoch   1 | Iter:   2230/ 13000 | global iter:   2230/ 13000 | loss: 1.8544 | ds_loss: 0.0000 | lr: 4.6522e-05 | scale: 16384.0000 | micro time: 1.901 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2230/ 13000 | global iter:   2230/ 13000 | loss: 2.0242 | ds_loss: 0.0000 | lr: 4.6522e-05 | scale: 16384.0000 | micro time: 1.901 | step time: 1.795
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2231/ 13000 | global iter:   2231/ 13000 | loss: 1.6198 | ds_loss: 0.0000 | lr: 4.6519e-05 | scale: 16384.0000 | micro time: 1.770 | step time: 0.000
train | epoch   1 | Iter:   2232/ 13000 | global iter:   2232/ 13000 | loss: 2.1211 | ds_loss: 0.0000 | lr: 4.6516e-05 | scale: 16384.0000 | micro time: 1.817 | step time: 0.000
train | epoch   1 | Iter:   2233/ 13000 | global iter:   2233/ 13000 | loss: 1.9398 | ds_loss: 0.0000 | lr: 4.6513e-05 | scale: 16384.0000 | micro time: 1.945 | step time: 0.000
train | epoch   1 | Iter:   2234/ 13000 | global iter:   2234/ 13000 | loss: 1.7849 | ds_loss: 0.0000 | lr: 4.6510e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   1 | Iter:   2235/ 13000 | global iter:   2235/ 13000 | loss: 1.9224 | ds_loss: 0.0000 | lr: 4.6507e-05 | scale: 16384.0000 | micro time: 1.733 | step time: 0.000
train | epoch   1 | Iter:   2236/ 13000 | global iter:   2236/ 13000 | loss: 2.5046 | ds_loss: 0.0000 | lr: 4.6504e-05 | scale: 16384.0000 | micro time: 1.745 | step time: 0.000
train | epoch   1 | Iter:   2237/ 13000 | global iter:   2237/ 13000 | loss: 2.0177 | ds_loss: 0.0000 | lr: 4.6501e-05 | scale: 16384.0000 | micro time: 1.876 | step time: 0.000
train | epoch   1 | Iter:   2238/ 13000 | global iter:   2238/ 13000 | loss: 1.8328 | ds_loss: 0.0000 | lr: 4.6498e-05 | scale: 16384.0000 | micro time: 1.870 | step time: 0.000
train | epoch   1 | Iter:   2239/ 13000 | global iter:   2239/ 13000 | loss: 2.5290 | ds_loss: 0.0000 | lr: 4.6495e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   1 | Iter:   2240/ 13000 | global iter:   2240/ 13000 | loss: 2.4762 | ds_loss: 0.0000 | lr: 4.6492e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2240/ 13000 | global iter:   2240/ 13000 | loss: 2.0748 | ds_loss: 0.0000 | lr: 4.6492e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 1.818
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2241/ 13000 | global iter:   2241/ 13000 | loss: 1.4549 | ds_loss: 0.0000 | lr: 4.6489e-05 | scale: 16384.0000 | micro time: 1.901 | step time: 0.000
train | epoch   1 | Iter:   2242/ 13000 | global iter:   2242/ 13000 | loss: 2.1577 | ds_loss: 0.0000 | lr: 4.6486e-05 | scale: 16384.0000 | micro time: 1.774 | step time: 0.000
train | epoch   1 | Iter:   2243/ 13000 | global iter:   2243/ 13000 | loss: 2.2801 | ds_loss: 0.0000 | lr: 4.6482e-05 | scale: 16384.0000 | micro time: 1.927 | step time: 0.000
train | epoch   1 | Iter:   2244/ 13000 | global iter:   2244/ 13000 | loss: 1.5425 | ds_loss: 0.0000 | lr: 4.6479e-05 | scale: 16384.0000 | micro time: 1.778 | step time: 0.000
train | epoch   1 | Iter:   2245/ 13000 | global iter:   2245/ 13000 | loss: 2.3441 | ds_loss: 0.0000 | lr: 4.6476e-05 | scale: 16384.0000 | micro time: 1.735 | step time: 0.000
train | epoch   1 | Iter:   2246/ 13000 | global iter:   2246/ 13000 | loss: 1.7056 | ds_loss: 0.0000 | lr: 4.6473e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   1 | Iter:   2247/ 13000 | global iter:   2247/ 13000 | loss: 2.6167 | ds_loss: 0.0000 | lr: 4.6470e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   1 | Iter:   2248/ 13000 | global iter:   2248/ 13000 | loss: 1.8816 | ds_loss: 0.0000 | lr: 4.6467e-05 | scale: 16384.0000 | micro time: 1.782 | step time: 0.000
train | epoch   1 | Iter:   2249/ 13000 | global iter:   2249/ 13000 | loss: 1.9045 | ds_loss: 0.0000 | lr: 4.6464e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   1 | Iter:   2250/ 13000 | global iter:   2250/ 13000 | loss: 1.7360 | ds_loss: 0.0000 | lr: 4.6461e-05 | scale: 16384.0000 | micro time: 1.753 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2250/ 13000 | global iter:   2250/ 13000 | loss: 1.9624 | ds_loss: 0.0000 | lr: 4.6461e-05 | scale: 16384.0000 | micro time: 1.753 | step time: 1.806
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2251/ 13000 | global iter:   2251/ 13000 | loss: 1.9280 | ds_loss: 0.0000 | lr: 4.6458e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
train | epoch   1 | Iter:   2252/ 13000 | global iter:   2252/ 13000 | loss: 2.2341 | ds_loss: 0.0000 | lr: 4.6455e-05 | scale: 16384.0000 | micro time: 1.790 | step time: 0.000
train | epoch   1 | Iter:   2253/ 13000 | global iter:   2253/ 13000 | loss: 1.7382 | ds_loss: 0.0000 | lr: 4.6451e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   1 | Iter:   2254/ 13000 | global iter:   2254/ 13000 | loss: 1.5088 | ds_loss: 0.0000 | lr: 4.6448e-05 | scale: 16384.0000 | micro time: 1.759 | step time: 0.000
train | epoch   1 | Iter:   2255/ 13000 | global iter:   2255/ 13000 | loss: 2.1788 | ds_loss: 0.0000 | lr: 4.6445e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   1 | Iter:   2256/ 13000 | global iter:   2256/ 13000 | loss: 2.0451 | ds_loss: 0.0000 | lr: 4.6442e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   1 | Iter:   2257/ 13000 | global iter:   2257/ 13000 | loss: 2.6195 | ds_loss: 0.0000 | lr: 4.6439e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   1 | Iter:   2258/ 13000 | global iter:   2258/ 13000 | loss: 2.4255 | ds_loss: 0.0000 | lr: 4.6436e-05 | scale: 16384.0000 | micro time: 1.800 | step time: 0.000
train | epoch   1 | Iter:   2259/ 13000 | global iter:   2259/ 13000 | loss: 2.2265 | ds_loss: 0.0000 | lr: 4.6433e-05 | scale: 16384.0000 | micro time: 1.766 | step time: 0.000
train | epoch   1 | Iter:   2260/ 13000 | global iter:   2260/ 13000 | loss: 2.0107 | ds_loss: 0.0000 | lr: 4.6430e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2260/ 13000 | global iter:   2260/ 13000 | loss: 2.0915 | ds_loss: 0.0000 | lr: 4.6430e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 1.803
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2261/ 13000 | global iter:   2261/ 13000 | loss: 2.2616 | ds_loss: 0.0000 | lr: 4.6427e-05 | scale: 16384.0000 | micro time: 1.825 | step time: 0.000
train | epoch   1 | Iter:   2262/ 13000 | global iter:   2262/ 13000 | loss: 2.6021 | ds_loss: 0.0000 | lr: 4.6424e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   1 | Iter:   2263/ 13000 | global iter:   2263/ 13000 | loss: 1.6844 | ds_loss: 0.0000 | lr: 4.6420e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   1 | Iter:   2264/ 13000 | global iter:   2264/ 13000 | loss: 2.7563 | ds_loss: 0.0000 | lr: 4.6417e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   1 | Iter:   2265/ 13000 | global iter:   2265/ 13000 | loss: 2.6085 | ds_loss: 0.0000 | lr: 4.6414e-05 | scale: 16384.0000 | micro time: 1.760 | step time: 0.000
train | epoch   1 | Iter:   2266/ 13000 | global iter:   2266/ 13000 | loss: 2.4354 | ds_loss: 0.0000 | lr: 4.6411e-05 | scale: 16384.0000 | micro time: 1.810 | step time: 0.000
train | epoch   1 | Iter:   2267/ 13000 | global iter:   2267/ 13000 | loss: 2.2280 | ds_loss: 0.0000 | lr: 4.6408e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   1 | Iter:   2268/ 13000 | global iter:   2268/ 13000 | loss: 2.2629 | ds_loss: 0.0000 | lr: 4.6405e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   1 | Iter:   2269/ 13000 | global iter:   2269/ 13000 | loss: 2.2652 | ds_loss: 0.0000 | lr: 4.6402e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   1 | Iter:   2270/ 13000 | global iter:   2270/ 13000 | loss: 1.6102 | ds_loss: 0.0000 | lr: 4.6399e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2270/ 13000 | global iter:   2270/ 13000 | loss: 2.2715 | ds_loss: 0.0000 | lr: 4.6399e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 1.800
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2271/ 13000 | global iter:   2271/ 13000 | loss: 1.7111 | ds_loss: 0.0000 | lr: 4.6396e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   1 | Iter:   2272/ 13000 | global iter:   2272/ 13000 | loss: 2.2384 | ds_loss: 0.0000 | lr: 4.6392e-05 | scale: 16384.0000 | micro time: 1.821 | step time: 0.000
train | epoch   1 | Iter:   2273/ 13000 | global iter:   2273/ 13000 | loss: 2.4088 | ds_loss: 0.0000 | lr: 4.6389e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   1 | Iter:   2274/ 13000 | global iter:   2274/ 13000 | loss: 2.0921 | ds_loss: 0.0000 | lr: 4.6386e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   1 | Iter:   2275/ 13000 | global iter:   2275/ 13000 | loss: 2.0755 | ds_loss: 0.0000 | lr: 4.6383e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 0.000
train | epoch   1 | Iter:   2276/ 13000 | global iter:   2276/ 13000 | loss: 2.3465 | ds_loss: 0.0000 | lr: 4.6380e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   1 | Iter:   2277/ 13000 | global iter:   2277/ 13000 | loss: 2.3779 | ds_loss: 0.0000 | lr: 4.6377e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   1 | Iter:   2278/ 13000 | global iter:   2278/ 13000 | loss: 2.0399 | ds_loss: 0.0000 | lr: 4.6374e-05 | scale: 16384.0000 | micro time: 1.915 | step time: 0.000
train | epoch   1 | Iter:   2279/ 13000 | global iter:   2279/ 13000 | loss: 1.8268 | ds_loss: 0.0000 | lr: 4.6370e-05 | scale: 16384.0000 | micro time: 1.858 | step time: 0.000
train | epoch   1 | Iter:   2280/ 13000 | global iter:   2280/ 13000 | loss: 2.3509 | ds_loss: 0.0000 | lr: 4.6367e-05 | scale: 16384.0000 | micro time: 1.911 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2280/ 13000 | global iter:   2280/ 13000 | loss: 2.1468 | ds_loss: 0.0000 | lr: 4.6367e-05 | scale: 16384.0000 | micro time: 1.911 | step time: 1.823
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2281/ 13000 | global iter:   2281/ 13000 | loss: 1.7432 | ds_loss: 0.0000 | lr: 4.6364e-05 | scale: 16384.0000 | micro time: 1.865 | step time: 0.000
train | epoch   1 | Iter:   2282/ 13000 | global iter:   2282/ 13000 | loss: 1.8412 | ds_loss: 0.0000 | lr: 4.6361e-05 | scale: 16384.0000 | micro time: 1.798 | step time: 0.000
train | epoch   1 | Iter:   2283/ 13000 | global iter:   2283/ 13000 | loss: 1.8906 | ds_loss: 0.0000 | lr: 4.6358e-05 | scale: 16384.0000 | micro time: 1.812 | step time: 0.000
train | epoch   1 | Iter:   2284/ 13000 | global iter:   2284/ 13000 | loss: 2.6264 | ds_loss: 0.0000 | lr: 4.6355e-05 | scale: 16384.0000 | micro time: 1.808 | step time: 0.000
train | epoch   1 | Iter:   2285/ 13000 | global iter:   2285/ 13000 | loss: 2.3662 | ds_loss: 0.0000 | lr: 4.6352e-05 | scale: 16384.0000 | micro time: 1.797 | step time: 0.000
train | epoch   1 | Iter:   2286/ 13000 | global iter:   2286/ 13000 | loss: 1.8888 | ds_loss: 0.0000 | lr: 4.6349e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 0.000
train | epoch   1 | Iter:   2287/ 13000 | global iter:   2287/ 13000 | loss: 2.6816 | ds_loss: 0.0000 | lr: 4.6345e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   1 | Iter:   2288/ 13000 | global iter:   2288/ 13000 | loss: 2.3888 | ds_loss: 0.0000 | lr: 4.6342e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   1 | Iter:   2289/ 13000 | global iter:   2289/ 13000 | loss: 1.9672 | ds_loss: 0.0000 | lr: 4.6339e-05 | scale: 16384.0000 | micro time: 1.702 | step time: 0.000
train | epoch   1 | Iter:   2290/ 13000 | global iter:   2290/ 13000 | loss: 1.8450 | ds_loss: 0.0000 | lr: 4.6336e-05 | scale: 16384.0000 | micro time: 1.868 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2290/ 13000 | global iter:   2290/ 13000 | loss: 2.1239 | ds_loss: 0.0000 | lr: 4.6336e-05 | scale: 16384.0000 | micro time: 1.868 | step time: 1.806
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2291/ 13000 | global iter:   2291/ 13000 | loss: 2.1048 | ds_loss: 0.0000 | lr: 4.6333e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   1 | Iter:   2292/ 13000 | global iter:   2292/ 13000 | loss: 2.4052 | ds_loss: 0.0000 | lr: 4.6330e-05 | scale: 16384.0000 | micro time: 1.886 | step time: 0.000
train | epoch   1 | Iter:   2293/ 13000 | global iter:   2293/ 13000 | loss: 2.2095 | ds_loss: 0.0000 | lr: 4.6327e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   1 | Iter:   2294/ 13000 | global iter:   2294/ 13000 | loss: 2.4127 | ds_loss: 0.0000 | lr: 4.6323e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   1 | Iter:   2295/ 13000 | global iter:   2295/ 13000 | loss: 2.5046 | ds_loss: 0.0000 | lr: 4.6320e-05 | scale: 16384.0000 | micro time: 1.911 | step time: 0.000
train | epoch   1 | Iter:   2296/ 13000 | global iter:   2296/ 13000 | loss: 1.8855 | ds_loss: 0.0000 | lr: 4.6317e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   1 | Iter:   2297/ 13000 | global iter:   2297/ 13000 | loss: 2.7707 | ds_loss: 0.0000 | lr: 4.6314e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   1 | Iter:   2298/ 13000 | global iter:   2298/ 13000 | loss: 2.1877 | ds_loss: 0.0000 | lr: 4.6311e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   1 | Iter:   2299/ 13000 | global iter:   2299/ 13000 | loss: 1.8475 | ds_loss: 0.0000 | lr: 4.6308e-05 | scale: 16384.0000 | micro time: 1.907 | step time: 0.000
train | epoch   1 | Iter:   2300/ 13000 | global iter:   2300/ 13000 | loss: 2.2474 | ds_loss: 0.0000 | lr: 4.6304e-05 | scale: 16384.0000 | micro time: 1.845 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2300/ 13000 | global iter:   2300/ 13000 | loss: 2.2576 | ds_loss: 0.0000 | lr: 4.6304e-05 | scale: 16384.0000 | micro time: 1.845 | step time: 1.837
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2301/ 13000 | global iter:   2301/ 13000 | loss: 2.3028 | ds_loss: 0.0000 | lr: 4.6301e-05 | scale: 16384.0000 | micro time: 1.782 | step time: 0.000
train | epoch   1 | Iter:   2302/ 13000 | global iter:   2302/ 13000 | loss: 2.2883 | ds_loss: 0.0000 | lr: 4.6298e-05 | scale: 16384.0000 | micro time: 1.693 | step time: 0.000
train | epoch   1 | Iter:   2303/ 13000 | global iter:   2303/ 13000 | loss: 2.5741 | ds_loss: 0.0000 | lr: 4.6295e-05 | scale: 16384.0000 | micro time: 1.824 | step time: 0.000
train | epoch   1 | Iter:   2304/ 13000 | global iter:   2304/ 13000 | loss: 1.6766 | ds_loss: 0.0000 | lr: 4.6292e-05 | scale: 16384.0000 | micro time: 1.854 | step time: 0.000
train | epoch   1 | Iter:   2305/ 13000 | global iter:   2305/ 13000 | loss: 2.3384 | ds_loss: 0.0000 | lr: 4.6289e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   1 | Iter:   2306/ 13000 | global iter:   2306/ 13000 | loss: 2.4540 | ds_loss: 0.0000 | lr: 4.6285e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   1 | Iter:   2307/ 13000 | global iter:   2307/ 13000 | loss: 1.8622 | ds_loss: 0.0000 | lr: 4.6282e-05 | scale: 16384.0000 | micro time: 1.769 | step time: 0.000
train | epoch   1 | Iter:   2308/ 13000 | global iter:   2308/ 13000 | loss: 2.1628 | ds_loss: 0.0000 | lr: 4.6279e-05 | scale: 16384.0000 | micro time: 1.705 | step time: 0.000
train | epoch   1 | Iter:   2309/ 13000 | global iter:   2309/ 13000 | loss: 2.3875 | ds_loss: 0.0000 | lr: 4.6276e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   1 | Iter:   2310/ 13000 | global iter:   2310/ 13000 | loss: 2.4780 | ds_loss: 0.0000 | lr: 4.6273e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2310/ 13000 | global iter:   2310/ 13000 | loss: 2.2525 | ds_loss: 0.0000 | lr: 4.6273e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 1.794
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2311/ 13000 | global iter:   2311/ 13000 | loss: 2.2994 | ds_loss: 0.0000 | lr: 4.6270e-05 | scale: 16384.0000 | micro time: 1.832 | step time: 0.000
train | epoch   1 | Iter:   2312/ 13000 | global iter:   2312/ 13000 | loss: 1.8686 | ds_loss: 0.0000 | lr: 4.6266e-05 | scale: 16384.0000 | micro time: 1.702 | step time: 0.000
train | epoch   1 | Iter:   2313/ 13000 | global iter:   2313/ 13000 | loss: 2.8388 | ds_loss: 0.0000 | lr: 4.6263e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   1 | Iter:   2314/ 13000 | global iter:   2314/ 13000 | loss: 2.5990 | ds_loss: 0.0000 | lr: 4.6260e-05 | scale: 16384.0000 | micro time: 1.731 | step time: 0.000
train | epoch   1 | Iter:   2315/ 13000 | global iter:   2315/ 13000 | loss: 2.3934 | ds_loss: 0.0000 | lr: 4.6257e-05 | scale: 16384.0000 | micro time: 1.717 | step time: 0.000
train | epoch   1 | Iter:   2316/ 13000 | global iter:   2316/ 13000 | loss: 1.9635 | ds_loss: 0.0000 | lr: 4.6254e-05 | scale: 16384.0000 | micro time: 1.897 | step time: 0.000
train | epoch   1 | Iter:   2317/ 13000 | global iter:   2317/ 13000 | loss: 2.4045 | ds_loss: 0.0000 | lr: 4.6251e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   1 | Iter:   2318/ 13000 | global iter:   2318/ 13000 | loss: 1.6456 | ds_loss: 0.0000 | lr: 4.6247e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   1 | Iter:   2319/ 13000 | global iter:   2319/ 13000 | loss: 2.1233 | ds_loss: 0.0000 | lr: 4.6244e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   1 | Iter:   2320/ 13000 | global iter:   2320/ 13000 | loss: 2.3634 | ds_loss: 0.0000 | lr: 4.6241e-05 | scale: 16384.0000 | micro time: 1.842 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2320/ 13000 | global iter:   2320/ 13000 | loss: 2.2500 | ds_loss: 0.0000 | lr: 4.6241e-05 | scale: 16384.0000 | micro time: 1.842 | step time: 1.811
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2321/ 13000 | global iter:   2321/ 13000 | loss: 2.4582 | ds_loss: 0.0000 | lr: 4.6238e-05 | scale: 16384.0000 | micro time: 1.925 | step time: 0.000
train | epoch   1 | Iter:   2322/ 13000 | global iter:   2322/ 13000 | loss: 2.3562 | ds_loss: 0.0000 | lr: 4.6235e-05 | scale: 16384.0000 | micro time: 1.888 | step time: 0.000
train | epoch   1 | Iter:   2323/ 13000 | global iter:   2323/ 13000 | loss: 2.0027 | ds_loss: 0.0000 | lr: 4.6231e-05 | scale: 16384.0000 | micro time: 1.760 | step time: 0.000
train | epoch   1 | Iter:   2324/ 13000 | global iter:   2324/ 13000 | loss: 1.9996 | ds_loss: 0.0000 | lr: 4.6228e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   1 | Iter:   2325/ 13000 | global iter:   2325/ 13000 | loss: 2.3904 | ds_loss: 0.0000 | lr: 4.6225e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   1 | Iter:   2326/ 13000 | global iter:   2326/ 13000 | loss: 2.0978 | ds_loss: 0.0000 | lr: 4.6222e-05 | scale: 16384.0000 | micro time: 1.809 | step time: 0.000
train | epoch   1 | Iter:   2327/ 13000 | global iter:   2327/ 13000 | loss: 2.6018 | ds_loss: 0.0000 | lr: 4.6219e-05 | scale: 16384.0000 | micro time: 1.911 | step time: 0.000
train | epoch   1 | Iter:   2328/ 13000 | global iter:   2328/ 13000 | loss: 1.5523 | ds_loss: 0.0000 | lr: 4.6216e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   1 | Iter:   2329/ 13000 | global iter:   2329/ 13000 | loss: 1.7003 | ds_loss: 0.0000 | lr: 4.6212e-05 | scale: 16384.0000 | micro time: 1.733 | step time: 0.000
train | epoch   1 | Iter:   2330/ 13000 | global iter:   2330/ 13000 | loss: 2.5053 | ds_loss: 0.0000 | lr: 4.6209e-05 | scale: 16384.0000 | micro time: 1.877 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2330/ 13000 | global iter:   2330/ 13000 | loss: 2.1664 | ds_loss: 0.0000 | lr: 4.6209e-05 | scale: 16384.0000 | micro time: 1.877 | step time: 1.819
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2331/ 13000 | global iter:   2331/ 13000 | loss: 2.2185 | ds_loss: 0.0000 | lr: 4.6206e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   1 | Iter:   2332/ 13000 | global iter:   2332/ 13000 | loss: 2.4393 | ds_loss: 0.0000 | lr: 4.6203e-05 | scale: 16384.0000 | micro time: 1.879 | step time: 0.000
train | epoch   1 | Iter:   2333/ 13000 | global iter:   2333/ 13000 | loss: 2.5656 | ds_loss: 0.0000 | lr: 4.6200e-05 | scale: 16384.0000 | micro time: 1.816 | step time: 0.000
train | epoch   1 | Iter:   2334/ 13000 | global iter:   2334/ 13000 | loss: 2.3565 | ds_loss: 0.0000 | lr: 4.6196e-05 | scale: 16384.0000 | micro time: 1.838 | step time: 0.000
train | epoch   1 | Iter:   2335/ 13000 | global iter:   2335/ 13000 | loss: 2.1637 | ds_loss: 0.0000 | lr: 4.6193e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   1 | Iter:   2336/ 13000 | global iter:   2336/ 13000 | loss: 1.9283 | ds_loss: 0.0000 | lr: 4.6190e-05 | scale: 16384.0000 | micro time: 1.911 | step time: 0.000
train | epoch   1 | Iter:   2337/ 13000 | global iter:   2337/ 13000 | loss: 2.4360 | ds_loss: 0.0000 | lr: 4.6187e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   1 | Iter:   2338/ 13000 | global iter:   2338/ 13000 | loss: 2.1858 | ds_loss: 0.0000 | lr: 4.6184e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   1 | Iter:   2339/ 13000 | global iter:   2339/ 13000 | loss: 2.5020 | ds_loss: 0.0000 | lr: 4.6180e-05 | scale: 16384.0000 | micro time: 1.872 | step time: 0.000
train | epoch   1 | Iter:   2340/ 13000 | global iter:   2340/ 13000 | loss: 2.1823 | ds_loss: 0.0000 | lr: 4.6177e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2340/ 13000 | global iter:   2340/ 13000 | loss: 2.2978 | ds_loss: 0.0000 | lr: 4.6177e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 1.829
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2341/ 13000 | global iter:   2341/ 13000 | loss: 2.4498 | ds_loss: 0.0000 | lr: 4.6174e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   1 | Iter:   2342/ 13000 | global iter:   2342/ 13000 | loss: 1.9497 | ds_loss: 0.0000 | lr: 4.6171e-05 | scale: 16384.0000 | micro time: 1.788 | step time: 0.000
train | epoch   1 | Iter:   2343/ 13000 | global iter:   2343/ 13000 | loss: 2.1699 | ds_loss: 0.0000 | lr: 4.6168e-05 | scale: 16384.0000 | micro time: 1.789 | step time: 0.000
train | epoch   1 | Iter:   2344/ 13000 | global iter:   2344/ 13000 | loss: 1.8827 | ds_loss: 0.0000 | lr: 4.6164e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   1 | Iter:   2345/ 13000 | global iter:   2345/ 13000 | loss: 2.7256 | ds_loss: 0.0000 | lr: 4.6161e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   1 | Iter:   2346/ 13000 | global iter:   2346/ 13000 | loss: 1.7183 | ds_loss: 0.0000 | lr: 4.6158e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   1 | Iter:   2347/ 13000 | global iter:   2347/ 13000 | loss: 2.2555 | ds_loss: 0.0000 | lr: 4.6155e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   1 | Iter:   2348/ 13000 | global iter:   2348/ 13000 | loss: 1.7899 | ds_loss: 0.0000 | lr: 4.6151e-05 | scale: 16384.0000 | micro time: 1.826 | step time: 0.000
train | epoch   1 | Iter:   2349/ 13000 | global iter:   2349/ 13000 | loss: 2.1383 | ds_loss: 0.0000 | lr: 4.6148e-05 | scale: 16384.0000 | micro time: 1.873 | step time: 0.000
train | epoch   1 | Iter:   2350/ 13000 | global iter:   2350/ 13000 | loss: 1.4368 | ds_loss: 0.0000 | lr: 4.6145e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2350/ 13000 | global iter:   2350/ 13000 | loss: 2.0516 | ds_loss: 0.0000 | lr: 4.6145e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 1.829
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2351/ 13000 | global iter:   2351/ 13000 | loss: 2.1421 | ds_loss: 0.0000 | lr: 4.6142e-05 | scale: 16384.0000 | micro time: 1.810 | step time: 0.000
train | epoch   1 | Iter:   2352/ 13000 | global iter:   2352/ 13000 | loss: 1.6259 | ds_loss: 0.0000 | lr: 4.6139e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   1 | Iter:   2353/ 13000 | global iter:   2353/ 13000 | loss: 2.2015 | ds_loss: 0.0000 | lr: 4.6135e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   1 | Iter:   2354/ 13000 | global iter:   2354/ 13000 | loss: 2.2074 | ds_loss: 0.0000 | lr: 4.6132e-05 | scale: 16384.0000 | micro time: 1.853 | step time: 0.000
train | epoch   1 | Iter:   2355/ 13000 | global iter:   2355/ 13000 | loss: 2.0480 | ds_loss: 0.0000 | lr: 4.6129e-05 | scale: 16384.0000 | micro time: 1.889 | step time: 0.000
train | epoch   1 | Iter:   2356/ 13000 | global iter:   2356/ 13000 | loss: 2.4530 | ds_loss: 0.0000 | lr: 4.6126e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   1 | Iter:   2357/ 13000 | global iter:   2357/ 13000 | loss: 2.4097 | ds_loss: 0.0000 | lr: 4.6122e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   1 | Iter:   2358/ 13000 | global iter:   2358/ 13000 | loss: 1.6780 | ds_loss: 0.0000 | lr: 4.6119e-05 | scale: 16384.0000 | micro time: 1.715 | step time: 0.000
train | epoch   1 | Iter:   2359/ 13000 | global iter:   2359/ 13000 | loss: 2.0998 | ds_loss: 0.0000 | lr: 4.6116e-05 | scale: 16384.0000 | micro time: 1.774 | step time: 0.000
train | epoch   1 | Iter:   2360/ 13000 | global iter:   2360/ 13000 | loss: 1.6486 | ds_loss: 0.0000 | lr: 4.6113e-05 | scale: 16384.0000 | micro time: 1.740 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2360/ 13000 | global iter:   2360/ 13000 | loss: 2.0514 | ds_loss: 0.0000 | lr: 4.6113e-05 | scale: 16384.0000 | micro time: 1.740 | step time: 1.810
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2361/ 13000 | global iter:   2361/ 13000 | loss: 2.2774 | ds_loss: 0.0000 | lr: 4.6110e-05 | scale: 16384.0000 | micro time: 1.888 | step time: 0.000
train | epoch   1 | Iter:   2362/ 13000 | global iter:   2362/ 13000 | loss: 2.2207 | ds_loss: 0.0000 | lr: 4.6106e-05 | scale: 16384.0000 | micro time: 1.788 | step time: 0.000
train | epoch   1 | Iter:   2363/ 13000 | global iter:   2363/ 13000 | loss: 2.0384 | ds_loss: 0.0000 | lr: 4.6103e-05 | scale: 16384.0000 | micro time: 1.794 | step time: 0.000
train | epoch   1 | Iter:   2364/ 13000 | global iter:   2364/ 13000 | loss: 1.3099 | ds_loss: 0.0000 | lr: 4.6100e-05 | scale: 16384.0000 | micro time: 1.874 | step time: 0.000
train | epoch   1 | Iter:   2365/ 13000 | global iter:   2365/ 13000 | loss: 1.7933 | ds_loss: 0.0000 | lr: 4.6097e-05 | scale: 16384.0000 | micro time: 1.800 | step time: 0.000
train | epoch   1 | Iter:   2366/ 13000 | global iter:   2366/ 13000 | loss: 2.0041 | ds_loss: 0.0000 | lr: 4.6093e-05 | scale: 16384.0000 | micro time: 1.928 | step time: 0.000
train | epoch   1 | Iter:   2367/ 13000 | global iter:   2367/ 13000 | loss: 2.2520 | ds_loss: 0.0000 | lr: 4.6090e-05 | scale: 16384.0000 | micro time: 1.798 | step time: 0.000
train | epoch   1 | Iter:   2368/ 13000 | global iter:   2368/ 13000 | loss: 1.7582 | ds_loss: 0.0000 | lr: 4.6087e-05 | scale: 16384.0000 | micro time: 1.723 | step time: 0.000
train | epoch   1 | Iter:   2369/ 13000 | global iter:   2369/ 13000 | loss: 2.0048 | ds_loss: 0.0000 | lr: 4.6084e-05 | scale: 16384.0000 | micro time: 1.879 | step time: 0.000
train | epoch   1 | Iter:   2370/ 13000 | global iter:   2370/ 13000 | loss: 2.1596 | ds_loss: 0.0000 | lr: 4.6080e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2370/ 13000 | global iter:   2370/ 13000 | loss: 1.9819 | ds_loss: 0.0000 | lr: 4.6080e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 1.831
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2371/ 13000 | global iter:   2371/ 13000 | loss: 1.7729 | ds_loss: 0.0000 | lr: 4.6077e-05 | scale: 16384.0000 | micro time: 1.770 | step time: 0.000
train | epoch   1 | Iter:   2372/ 13000 | global iter:   2372/ 13000 | loss: 2.2015 | ds_loss: 0.0000 | lr: 4.6074e-05 | scale: 16384.0000 | micro time: 1.824 | step time: 0.000
train | epoch   1 | Iter:   2373/ 13000 | global iter:   2373/ 13000 | loss: 2.0487 | ds_loss: 0.0000 | lr: 4.6071e-05 | scale: 16384.0000 | micro time: 1.881 | step time: 0.000
train | epoch   1 | Iter:   2374/ 13000 | global iter:   2374/ 13000 | loss: 2.4329 | ds_loss: 0.0000 | lr: 4.6067e-05 | scale: 16384.0000 | micro time: 1.816 | step time: 0.000
train | epoch   1 | Iter:   2375/ 13000 | global iter:   2375/ 13000 | loss: 2.7515 | ds_loss: 0.0000 | lr: 4.6064e-05 | scale: 16384.0000 | micro time: 1.834 | step time: 0.000
train | epoch   1 | Iter:   2376/ 13000 | global iter:   2376/ 13000 | loss: 2.1170 | ds_loss: 0.0000 | lr: 4.6061e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   1 | Iter:   2377/ 13000 | global iter:   2377/ 13000 | loss: 2.3072 | ds_loss: 0.0000 | lr: 4.6058e-05 | scale: 16384.0000 | micro time: 1.745 | step time: 0.000
train | epoch   1 | Iter:   2378/ 13000 | global iter:   2378/ 13000 | loss: 2.5163 | ds_loss: 0.0000 | lr: 4.6054e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 0.000
train | epoch   1 | Iter:   2379/ 13000 | global iter:   2379/ 13000 | loss: 2.2459 | ds_loss: 0.0000 | lr: 4.6051e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   1 | Iter:   2380/ 13000 | global iter:   2380/ 13000 | loss: 2.2972 | ds_loss: 0.0000 | lr: 4.6048e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2380/ 13000 | global iter:   2380/ 13000 | loss: 2.2691 | ds_loss: 0.0000 | lr: 4.6048e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 1.823
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2381/ 13000 | global iter:   2381/ 13000 | loss: 2.4478 | ds_loss: 0.0000 | lr: 4.6045e-05 | scale: 16384.0000 | micro time: 1.788 | step time: 0.000
train | epoch   1 | Iter:   2382/ 13000 | global iter:   2382/ 13000 | loss: 2.3552 | ds_loss: 0.0000 | lr: 4.6041e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   1 | Iter:   2383/ 13000 | global iter:   2383/ 13000 | loss: 2.0944 | ds_loss: 0.0000 | lr: 4.6038e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   1 | Iter:   2384/ 13000 | global iter:   2384/ 13000 | loss: 2.5055 | ds_loss: 0.0000 | lr: 4.6035e-05 | scale: 16384.0000 | micro time: 1.800 | step time: 0.000
train | epoch   1 | Iter:   2385/ 13000 | global iter:   2385/ 13000 | loss: 2.2425 | ds_loss: 0.0000 | lr: 4.6032e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   1 | Iter:   2386/ 13000 | global iter:   2386/ 13000 | loss: 2.1823 | ds_loss: 0.0000 | lr: 4.6028e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   1 | Iter:   2387/ 13000 | global iter:   2387/ 13000 | loss: 2.2615 | ds_loss: 0.0000 | lr: 4.6025e-05 | scale: 16384.0000 | micro time: 1.738 | step time: 0.000
train | epoch   1 | Iter:   2388/ 13000 | global iter:   2388/ 13000 | loss: 1.9255 | ds_loss: 0.0000 | lr: 4.6022e-05 | scale: 16384.0000 | micro time: 1.834 | step time: 0.000
train | epoch   1 | Iter:   2389/ 13000 | global iter:   2389/ 13000 | loss: 2.3017 | ds_loss: 0.0000 | lr: 4.6019e-05 | scale: 16384.0000 | micro time: 1.872 | step time: 0.000
train | epoch   1 | Iter:   2390/ 13000 | global iter:   2390/ 13000 | loss: 1.8164 | ds_loss: 0.0000 | lr: 4.6015e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2390/ 13000 | global iter:   2390/ 13000 | loss: 2.2133 | ds_loss: 0.0000 | lr: 4.6015e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 1.787
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2391/ 13000 | global iter:   2391/ 13000 | loss: 2.4601 | ds_loss: 0.0000 | lr: 4.6012e-05 | scale: 16384.0000 | micro time: 1.834 | step time: 0.000
train | epoch   1 | Iter:   2392/ 13000 | global iter:   2392/ 13000 | loss: 2.3289 | ds_loss: 0.0000 | lr: 4.6009e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   1 | Iter:   2393/ 13000 | global iter:   2393/ 13000 | loss: 2.4286 | ds_loss: 0.0000 | lr: 4.6005e-05 | scale: 16384.0000 | micro time: 1.774 | step time: 0.000
train | epoch   1 | Iter:   2394/ 13000 | global iter:   2394/ 13000 | loss: 2.0767 | ds_loss: 0.0000 | lr: 4.6002e-05 | scale: 16384.0000 | micro time: 1.688 | step time: 0.000
train | epoch   1 | Iter:   2395/ 13000 | global iter:   2395/ 13000 | loss: 1.6842 | ds_loss: 0.0000 | lr: 4.5999e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   1 | Iter:   2396/ 13000 | global iter:   2396/ 13000 | loss: 1.9160 | ds_loss: 0.0000 | lr: 4.5996e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   1 | Iter:   2397/ 13000 | global iter:   2397/ 13000 | loss: 2.0884 | ds_loss: 0.0000 | lr: 4.5992e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   1 | Iter:   2398/ 13000 | global iter:   2398/ 13000 | loss: 1.5283 | ds_loss: 0.0000 | lr: 4.5989e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   1 | Iter:   2399/ 13000 | global iter:   2399/ 13000 | loss: 2.2470 | ds_loss: 0.0000 | lr: 4.5986e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   1 | Iter:   2400/ 13000 | global iter:   2400/ 13000 | loss: 2.2387 | ds_loss: 0.0000 | lr: 4.5982e-05 | scale: 16384.0000 | micro time: 1.862 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2400/ 13000 | global iter:   2400/ 13000 | loss: 2.0997 | ds_loss: 0.0000 | lr: 4.5982e-05 | scale: 16384.0000 | micro time: 1.862 | step time: 1.802
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2401/ 13000 | global iter:   2401/ 13000 | loss: 2.3077 | ds_loss: 0.0000 | lr: 4.5979e-05 | scale: 16384.0000 | micro time: 1.862 | step time: 0.000
train | epoch   1 | Iter:   2402/ 13000 | global iter:   2402/ 13000 | loss: 1.8486 | ds_loss: 0.0000 | lr: 4.5976e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 0.000
train | epoch   1 | Iter:   2403/ 13000 | global iter:   2403/ 13000 | loss: 1.9864 | ds_loss: 0.0000 | lr: 4.5973e-05 | scale: 16384.0000 | micro time: 1.872 | step time: 0.000
train | epoch   1 | Iter:   2404/ 13000 | global iter:   2404/ 13000 | loss: 2.4999 | ds_loss: 0.0000 | lr: 4.5969e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   1 | Iter:   2405/ 13000 | global iter:   2405/ 13000 | loss: 2.2721 | ds_loss: 0.0000 | lr: 4.5966e-05 | scale: 16384.0000 | micro time: 1.649 | step time: 0.000
train | epoch   1 | Iter:   2406/ 13000 | global iter:   2406/ 13000 | loss: 2.0924 | ds_loss: 0.0000 | lr: 4.5963e-05 | scale: 16384.0000 | micro time: 1.825 | step time: 0.000
train | epoch   1 | Iter:   2407/ 13000 | global iter:   2407/ 13000 | loss: 1.8442 | ds_loss: 0.0000 | lr: 4.5959e-05 | scale: 16384.0000 | micro time: 1.717 | step time: 0.000
train | epoch   1 | Iter:   2408/ 13000 | global iter:   2408/ 13000 | loss: 2.4683 | ds_loss: 0.0000 | lr: 4.5956e-05 | scale: 16384.0000 | micro time: 1.838 | step time: 0.000
train | epoch   1 | Iter:   2409/ 13000 | global iter:   2409/ 13000 | loss: 2.4578 | ds_loss: 0.0000 | lr: 4.5953e-05 | scale: 16384.0000 | micro time: 1.845 | step time: 0.000
train | epoch   1 | Iter:   2410/ 13000 | global iter:   2410/ 13000 | loss: 1.8409 | ds_loss: 0.0000 | lr: 4.5950e-05 | scale: 16384.0000 | micro time: 1.675 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2410/ 13000 | global iter:   2410/ 13000 | loss: 2.1618 | ds_loss: 0.0000 | lr: 4.5950e-05 | scale: 16384.0000 | micro time: 1.675 | step time: 1.786
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2411/ 13000 | global iter:   2411/ 13000 | loss: 2.5089 | ds_loss: 0.0000 | lr: 4.5946e-05 | scale: 16384.0000 | micro time: 1.798 | step time: 0.000
train | epoch   1 | Iter:   2412/ 13000 | global iter:   2412/ 13000 | loss: 1.7370 | ds_loss: 0.0000 | lr: 4.5943e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   1 | Iter:   2413/ 13000 | global iter:   2413/ 13000 | loss: 2.1896 | ds_loss: 0.0000 | lr: 4.5940e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   1 | Iter:   2414/ 13000 | global iter:   2414/ 13000 | loss: 2.2847 | ds_loss: 0.0000 | lr: 4.5936e-05 | scale: 16384.0000 | micro time: 1.723 | step time: 0.000
train | epoch   1 | Iter:   2415/ 13000 | global iter:   2415/ 13000 | loss: 2.5989 | ds_loss: 0.0000 | lr: 4.5933e-05 | scale: 16384.0000 | micro time: 1.879 | step time: 0.000
train | epoch   1 | Iter:   2416/ 13000 | global iter:   2416/ 13000 | loss: 2.2670 | ds_loss: 0.0000 | lr: 4.5930e-05 | scale: 16384.0000 | micro time: 1.794 | step time: 0.000
train | epoch   1 | Iter:   2417/ 13000 | global iter:   2417/ 13000 | loss: 1.7765 | ds_loss: 0.0000 | lr: 4.5927e-05 | scale: 16384.0000 | micro time: 1.869 | step time: 0.000
train | epoch   1 | Iter:   2418/ 13000 | global iter:   2418/ 13000 | loss: 2.0400 | ds_loss: 0.0000 | lr: 4.5923e-05 | scale: 16384.0000 | micro time: 1.887 | step time: 0.000
train | epoch   1 | Iter:   2419/ 13000 | global iter:   2419/ 13000 | loss: 1.8670 | ds_loss: 0.0000 | lr: 4.5920e-05 | scale: 16384.0000 | micro time: 1.714 | step time: 0.000
train | epoch   1 | Iter:   2420/ 13000 | global iter:   2420/ 13000 | loss: 2.5792 | ds_loss: 0.0000 | lr: 4.5917e-05 | scale: 16384.0000 | micro time: 1.951 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2420/ 13000 | global iter:   2420/ 13000 | loss: 2.1849 | ds_loss: 0.0000 | lr: 4.5917e-05 | scale: 16384.0000 | micro time: 1.951 | step time: 1.815
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2421/ 13000 | global iter:   2421/ 13000 | loss: 2.2549 | ds_loss: 0.0000 | lr: 4.5913e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   1 | Iter:   2422/ 13000 | global iter:   2422/ 13000 | loss: 2.4250 | ds_loss: 0.0000 | lr: 4.5910e-05 | scale: 16384.0000 | micro time: 1.706 | step time: 0.000
train | epoch   1 | Iter:   2423/ 13000 | global iter:   2423/ 13000 | loss: 2.7497 | ds_loss: 0.0000 | lr: 4.5907e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   1 | Iter:   2424/ 13000 | global iter:   2424/ 13000 | loss: 1.5804 | ds_loss: 0.0000 | lr: 4.5903e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   1 | Iter:   2425/ 13000 | global iter:   2425/ 13000 | loss: 2.8695 | ds_loss: 0.0000 | lr: 4.5900e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   1 | Iter:   2426/ 13000 | global iter:   2426/ 13000 | loss: 1.9966 | ds_loss: 0.0000 | lr: 4.5897e-05 | scale: 16384.0000 | micro time: 1.836 | step time: 0.000
train | epoch   1 | Iter:   2427/ 13000 | global iter:   2427/ 13000 | loss: 1.6759 | ds_loss: 0.0000 | lr: 4.5893e-05 | scale: 16384.0000 | micro time: 1.804 | step time: 0.000
train | epoch   1 | Iter:   2428/ 13000 | global iter:   2428/ 13000 | loss: 1.6064 | ds_loss: 0.0000 | lr: 4.5890e-05 | scale: 16384.0000 | micro time: 1.812 | step time: 0.000
train | epoch   1 | Iter:   2429/ 13000 | global iter:   2429/ 13000 | loss: 2.3481 | ds_loss: 0.0000 | lr: 4.5887e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   1 | Iter:   2430/ 13000 | global iter:   2430/ 13000 | loss: 2.1879 | ds_loss: 0.0000 | lr: 4.5884e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2430/ 13000 | global iter:   2430/ 13000 | loss: 2.1694 | ds_loss: 0.0000 | lr: 4.5884e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 1.802
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2431/ 13000 | global iter:   2431/ 13000 | loss: 2.5499 | ds_loss: 0.0000 | lr: 4.5880e-05 | scale: 16384.0000 | micro time: 1.806 | step time: 0.000
train | epoch   1 | Iter:   2432/ 13000 | global iter:   2432/ 13000 | loss: 1.8981 | ds_loss: 0.0000 | lr: 4.5877e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   1 | Iter:   2433/ 13000 | global iter:   2433/ 13000 | loss: 2.5203 | ds_loss: 0.0000 | lr: 4.5874e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   1 | Iter:   2434/ 13000 | global iter:   2434/ 13000 | loss: 1.7337 | ds_loss: 0.0000 | lr: 4.5870e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   1 | Iter:   2435/ 13000 | global iter:   2435/ 13000 | loss: 2.2289 | ds_loss: 0.0000 | lr: 4.5867e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   1 | Iter:   2436/ 13000 | global iter:   2436/ 13000 | loss: 2.1473 | ds_loss: 0.0000 | lr: 4.5864e-05 | scale: 16384.0000 | micro time: 1.907 | step time: 0.000
train | epoch   1 | Iter:   2437/ 13000 | global iter:   2437/ 13000 | loss: 2.3397 | ds_loss: 0.0000 | lr: 4.5860e-05 | scale: 16384.0000 | micro time: 1.915 | step time: 0.000
train | epoch   1 | Iter:   2438/ 13000 | global iter:   2438/ 13000 | loss: 1.7425 | ds_loss: 0.0000 | lr: 4.5857e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   1 | Iter:   2439/ 13000 | global iter:   2439/ 13000 | loss: 1.8215 | ds_loss: 0.0000 | lr: 4.5854e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   1 | Iter:   2440/ 13000 | global iter:   2440/ 13000 | loss: 2.4020 | ds_loss: 0.0000 | lr: 4.5850e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2440/ 13000 | global iter:   2440/ 13000 | loss: 2.1384 | ds_loss: 0.0000 | lr: 4.5850e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 1.816
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2441/ 13000 | global iter:   2441/ 13000 | loss: 2.4451 | ds_loss: 0.0000 | lr: 4.5847e-05 | scale: 16384.0000 | micro time: 1.778 | step time: 0.000
train | epoch   1 | Iter:   2442/ 13000 | global iter:   2442/ 13000 | loss: 1.7105 | ds_loss: 0.0000 | lr: 4.5844e-05 | scale: 16384.0000 | micro time: 1.848 | step time: 0.000
train | epoch   1 | Iter:   2443/ 13000 | global iter:   2443/ 13000 | loss: 2.0276 | ds_loss: 0.0000 | lr: 4.5840e-05 | scale: 16384.0000 | micro time: 1.753 | step time: 0.000
train | epoch   1 | Iter:   2444/ 13000 | global iter:   2444/ 13000 | loss: 1.3503 | ds_loss: 0.0000 | lr: 4.5837e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   1 | Iter:   2445/ 13000 | global iter:   2445/ 13000 | loss: 2.4678 | ds_loss: 0.0000 | lr: 4.5834e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   1 | Iter:   2446/ 13000 | global iter:   2446/ 13000 | loss: 2.3650 | ds_loss: 0.0000 | lr: 4.5830e-05 | scale: 16384.0000 | micro time: 1.738 | step time: 0.000
train | epoch   1 | Iter:   2447/ 13000 | global iter:   2447/ 13000 | loss: 1.2686 | ds_loss: 0.0000 | lr: 4.5827e-05 | scale: 16384.0000 | micro time: 1.844 | step time: 0.000
train | epoch   1 | Iter:   2448/ 13000 | global iter:   2448/ 13000 | loss: 1.9845 | ds_loss: 0.0000 | lr: 4.5824e-05 | scale: 16384.0000 | micro time: 1.824 | step time: 0.000
train | epoch   1 | Iter:   2449/ 13000 | global iter:   2449/ 13000 | loss: 2.6426 | ds_loss: 0.0000 | lr: 4.5820e-05 | scale: 16384.0000 | micro time: 1.732 | step time: 0.000
train | epoch   1 | Iter:   2450/ 13000 | global iter:   2450/ 13000 | loss: 2.3744 | ds_loss: 0.0000 | lr: 4.5817e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2450/ 13000 | global iter:   2450/ 13000 | loss: 2.0637 | ds_loss: 0.0000 | lr: 4.5817e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 1.803
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2451/ 13000 | global iter:   2451/ 13000 | loss: 2.3357 | ds_loss: 0.0000 | lr: 4.5814e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   1 | Iter:   2452/ 13000 | global iter:   2452/ 13000 | loss: 2.0903 | ds_loss: 0.0000 | lr: 4.5810e-05 | scale: 16384.0000 | micro time: 1.860 | step time: 0.000
train | epoch   1 | Iter:   2453/ 13000 | global iter:   2453/ 13000 | loss: 2.4151 | ds_loss: 0.0000 | lr: 4.5807e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   1 | Iter:   2454/ 13000 | global iter:   2454/ 13000 | loss: 1.9966 | ds_loss: 0.0000 | lr: 4.5804e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   1 | Iter:   2455/ 13000 | global iter:   2455/ 13000 | loss: 2.2057 | ds_loss: 0.0000 | lr: 4.5800e-05 | scale: 16384.0000 | micro time: 1.743 | step time: 0.000
train | epoch   1 | Iter:   2456/ 13000 | global iter:   2456/ 13000 | loss: 2.3289 | ds_loss: 0.0000 | lr: 4.5797e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   1 | Iter:   2457/ 13000 | global iter:   2457/ 13000 | loss: 2.0762 | ds_loss: 0.0000 | lr: 4.5793e-05 | scale: 16384.0000 | micro time: 1.742 | step time: 0.000
train | epoch   1 | Iter:   2458/ 13000 | global iter:   2458/ 13000 | loss: 2.4709 | ds_loss: 0.0000 | lr: 4.5790e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   1 | Iter:   2459/ 13000 | global iter:   2459/ 13000 | loss: 1.7511 | ds_loss: 0.0000 | lr: 4.5787e-05 | scale: 16384.0000 | micro time: 1.927 | step time: 0.000
train | epoch   1 | Iter:   2460/ 13000 | global iter:   2460/ 13000 | loss: 2.1682 | ds_loss: 0.0000 | lr: 4.5783e-05 | scale: 16384.0000 | micro time: 1.895 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2460/ 13000 | global iter:   2460/ 13000 | loss: 2.1839 | ds_loss: 0.0000 | lr: 4.5783e-05 | scale: 16384.0000 | micro time: 1.895 | step time: 1.835
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2461/ 13000 | global iter:   2461/ 13000 | loss: 1.8409 | ds_loss: 0.0000 | lr: 4.5780e-05 | scale: 16384.0000 | micro time: 1.754 | step time: 0.000
train | epoch   1 | Iter:   2462/ 13000 | global iter:   2462/ 13000 | loss: 2.2943 | ds_loss: 0.0000 | lr: 4.5777e-05 | scale: 16384.0000 | micro time: 1.765 | step time: 0.000
train | epoch   1 | Iter:   2463/ 13000 | global iter:   2463/ 13000 | loss: 2.1538 | ds_loss: 0.0000 | lr: 4.5773e-05 | scale: 16384.0000 | micro time: 1.886 | step time: 0.000
train | epoch   1 | Iter:   2464/ 13000 | global iter:   2464/ 13000 | loss: 2.2821 | ds_loss: 0.0000 | lr: 4.5770e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   1 | Iter:   2465/ 13000 | global iter:   2465/ 13000 | loss: 1.9036 | ds_loss: 0.0000 | lr: 4.5767e-05 | scale: 16384.0000 | micro time: 1.804 | step time: 0.000
train | epoch   1 | Iter:   2466/ 13000 | global iter:   2466/ 13000 | loss: 2.6263 | ds_loss: 0.0000 | lr: 4.5763e-05 | scale: 16384.0000 | micro time: 1.844 | step time: 0.000
train | epoch   1 | Iter:   2467/ 13000 | global iter:   2467/ 13000 | loss: 2.1413 | ds_loss: 0.0000 | lr: 4.5760e-05 | scale: 16384.0000 | micro time: 1.936 | step time: 0.000
train | epoch   1 | Iter:   2468/ 13000 | global iter:   2468/ 13000 | loss: 2.0591 | ds_loss: 0.0000 | lr: 4.5757e-05 | scale: 16384.0000 | micro time: 1.908 | step time: 0.000
train | epoch   1 | Iter:   2469/ 13000 | global iter:   2469/ 13000 | loss: 2.2573 | ds_loss: 0.0000 | lr: 4.5753e-05 | scale: 16384.0000 | micro time: 1.943 | step time: 0.000
train | epoch   1 | Iter:   2470/ 13000 | global iter:   2470/ 13000 | loss: 2.6366 | ds_loss: 0.0000 | lr: 4.5750e-05 | scale: 16384.0000 | micro time: 1.927 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2470/ 13000 | global iter:   2470/ 13000 | loss: 2.2195 | ds_loss: 0.0000 | lr: 4.5750e-05 | scale: 16384.0000 | micro time: 1.927 | step time: 1.857
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2471/ 13000 | global iter:   2471/ 13000 | loss: 2.3214 | ds_loss: 0.0000 | lr: 4.5746e-05 | scale: 16384.0000 | micro time: 1.748 | step time: 0.000
train | epoch   1 | Iter:   2472/ 13000 | global iter:   2472/ 13000 | loss: 2.3394 | ds_loss: 0.0000 | lr: 4.5743e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 0.000
train | epoch   1 | Iter:   2473/ 13000 | global iter:   2473/ 13000 | loss: 1.7730 | ds_loss: 0.0000 | lr: 4.5740e-05 | scale: 16384.0000 | micro time: 1.740 | step time: 0.000
train | epoch   1 | Iter:   2474/ 13000 | global iter:   2474/ 13000 | loss: 2.0159 | ds_loss: 0.0000 | lr: 4.5736e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   1 | Iter:   2475/ 13000 | global iter:   2475/ 13000 | loss: 2.6083 | ds_loss: 0.0000 | lr: 4.5733e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   1 | Iter:   2476/ 13000 | global iter:   2476/ 13000 | loss: 2.0082 | ds_loss: 0.0000 | lr: 4.5730e-05 | scale: 16384.0000 | micro time: 1.921 | step time: 0.000
train | epoch   1 | Iter:   2477/ 13000 | global iter:   2477/ 13000 | loss: 2.1859 | ds_loss: 0.0000 | lr: 4.5726e-05 | scale: 16384.0000 | micro time: 1.929 | step time: 0.000
train | epoch   1 | Iter:   2478/ 13000 | global iter:   2478/ 13000 | loss: 1.7462 | ds_loss: 0.0000 | lr: 4.5723e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   1 | Iter:   2479/ 13000 | global iter:   2479/ 13000 | loss: 1.3099 | ds_loss: 0.0000 | lr: 4.5719e-05 | scale: 16384.0000 | micro time: 1.757 | step time: 0.000
train | epoch   1 | Iter:   2480/ 13000 | global iter:   2480/ 13000 | loss: 2.1242 | ds_loss: 0.0000 | lr: 4.5716e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2480/ 13000 | global iter:   2480/ 13000 | loss: 2.0432 | ds_loss: 0.0000 | lr: 4.5716e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 1.827
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2481/ 13000 | global iter:   2481/ 13000 | loss: 1.7994 | ds_loss: 0.0000 | lr: 4.5713e-05 | scale: 16384.0000 | micro time: 1.733 | step time: 0.000
train | epoch   1 | Iter:   2482/ 13000 | global iter:   2482/ 13000 | loss: 2.0878 | ds_loss: 0.0000 | lr: 4.5709e-05 | scale: 16384.0000 | micro time: 1.715 | step time: 0.000
train | epoch   1 | Iter:   2483/ 13000 | global iter:   2483/ 13000 | loss: 2.1512 | ds_loss: 0.0000 | lr: 4.5706e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   1 | Iter:   2484/ 13000 | global iter:   2484/ 13000 | loss: 1.5651 | ds_loss: 0.0000 | lr: 4.5703e-05 | scale: 16384.0000 | micro time: 1.824 | step time: 0.000
train | epoch   1 | Iter:   2485/ 13000 | global iter:   2485/ 13000 | loss: 2.1066 | ds_loss: 0.0000 | lr: 4.5699e-05 | scale: 16384.0000 | micro time: 1.881 | step time: 0.000
train | epoch   1 | Iter:   2486/ 13000 | global iter:   2486/ 13000 | loss: 2.4989 | ds_loss: 0.0000 | lr: 4.5696e-05 | scale: 16384.0000 | micro time: 1.872 | step time: 0.000
train | epoch   1 | Iter:   2487/ 13000 | global iter:   2487/ 13000 | loss: 2.0724 | ds_loss: 0.0000 | lr: 4.5692e-05 | scale: 16384.0000 | micro time: 1.934 | step time: 0.000
train | epoch   1 | Iter:   2488/ 13000 | global iter:   2488/ 13000 | loss: 1.7499 | ds_loss: 0.0000 | lr: 4.5689e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   1 | Iter:   2489/ 13000 | global iter:   2489/ 13000 | loss: 2.2222 | ds_loss: 0.0000 | lr: 4.5686e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   1 | Iter:   2490/ 13000 | global iter:   2490/ 13000 | loss: 2.2145 | ds_loss: 0.0000 | lr: 4.5682e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2490/ 13000 | global iter:   2490/ 13000 | loss: 2.0468 | ds_loss: 0.0000 | lr: 4.5682e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 1.814
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2491/ 13000 | global iter:   2491/ 13000 | loss: 1.8878 | ds_loss: 0.0000 | lr: 4.5679e-05 | scale: 16384.0000 | micro time: 1.804 | step time: 0.000
train | epoch   1 | Iter:   2492/ 13000 | global iter:   2492/ 13000 | loss: 1.6735 | ds_loss: 0.0000 | lr: 4.5675e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   1 | Iter:   2493/ 13000 | global iter:   2493/ 13000 | loss: 1.3697 | ds_loss: 0.0000 | lr: 4.5672e-05 | scale: 16384.0000 | micro time: 1.820 | step time: 0.000
train | epoch   1 | Iter:   2494/ 13000 | global iter:   2494/ 13000 | loss: 2.2283 | ds_loss: 0.0000 | lr: 4.5669e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   1 | Iter:   2495/ 13000 | global iter:   2495/ 13000 | loss: 2.4739 | ds_loss: 0.0000 | lr: 4.5665e-05 | scale: 16384.0000 | micro time: 1.802 | step time: 0.000
train | epoch   1 | Iter:   2496/ 13000 | global iter:   2496/ 13000 | loss: 1.9840 | ds_loss: 0.0000 | lr: 4.5662e-05 | scale: 16384.0000 | micro time: 1.821 | step time: 0.000
train | epoch   1 | Iter:   2497/ 13000 | global iter:   2497/ 13000 | loss: 1.4445 | ds_loss: 0.0000 | lr: 4.5659e-05 | scale: 16384.0000 | micro time: 1.745 | step time: 0.000
train | epoch   1 | Iter:   2498/ 13000 | global iter:   2498/ 13000 | loss: 2.3260 | ds_loss: 0.0000 | lr: 4.5655e-05 | scale: 16384.0000 | micro time: 1.715 | step time: 0.000
train | epoch   1 | Iter:   2499/ 13000 | global iter:   2499/ 13000 | loss: 1.8496 | ds_loss: 0.0000 | lr: 4.5652e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   1 | Iter:   2500/ 13000 | global iter:   2500/ 13000 | loss: 1.7673 | ds_loss: 0.0000 | lr: 4.5648e-05 | scale: 16384.0000 | micro time: 1.820 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2500/ 13000 | global iter:   2500/ 13000 | loss: 1.9005 | ds_loss: 0.0000 | lr: 4.5648e-05 | scale: 16384.0000 | micro time: 1.820 | step time: 1.796
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2501/ 13000 | global iter:   2501/ 13000 | loss: 2.2035 | ds_loss: 0.0000 | lr: 4.5645e-05 | scale: 16384.0000 | micro time: 1.845 | step time: 0.000
train | epoch   1 | Iter:   2502/ 13000 | global iter:   2502/ 13000 | loss: 1.9307 | ds_loss: 0.0000 | lr: 4.5642e-05 | scale: 16384.0000 | micro time: 1.941 | step time: 0.000
train | epoch   1 | Iter:   2503/ 13000 | global iter:   2503/ 13000 | loss: 1.8877 | ds_loss: 0.0000 | lr: 4.5638e-05 | scale: 16384.0000 | micro time: 1.896 | step time: 0.000
train | epoch   1 | Iter:   2504/ 13000 | global iter:   2504/ 13000 | loss: 2.0691 | ds_loss: 0.0000 | lr: 4.5635e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   1 | Iter:   2505/ 13000 | global iter:   2505/ 13000 | loss: 2.4589 | ds_loss: 0.0000 | lr: 4.5631e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   1 | Iter:   2506/ 13000 | global iter:   2506/ 13000 | loss: 2.2140 | ds_loss: 0.0000 | lr: 4.5628e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   1 | Iter:   2507/ 13000 | global iter:   2507/ 13000 | loss: 1.7711 | ds_loss: 0.0000 | lr: 4.5624e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   1 | Iter:   2508/ 13000 | global iter:   2508/ 13000 | loss: 2.4440 | ds_loss: 0.0000 | lr: 4.5621e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   1 | Iter:   2509/ 13000 | global iter:   2509/ 13000 | loss: 2.0552 | ds_loss: 0.0000 | lr: 4.5618e-05 | scale: 16384.0000 | micro time: 1.743 | step time: 0.000
train | epoch   1 | Iter:   2510/ 13000 | global iter:   2510/ 13000 | loss: 2.3618 | ds_loss: 0.0000 | lr: 4.5614e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2510/ 13000 | global iter:   2510/ 13000 | loss: 2.1396 | ds_loss: 0.0000 | lr: 4.5614e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 1.826
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2511/ 13000 | global iter:   2511/ 13000 | loss: 2.4621 | ds_loss: 0.0000 | lr: 4.5611e-05 | scale: 16384.0000 | micro time: 1.842 | step time: 0.000
train | epoch   1 | Iter:   2512/ 13000 | global iter:   2512/ 13000 | loss: 1.5123 | ds_loss: 0.0000 | lr: 4.5607e-05 | scale: 16384.0000 | micro time: 1.786 | step time: 0.000
train | epoch   1 | Iter:   2513/ 13000 | global iter:   2513/ 13000 | loss: 2.1756 | ds_loss: 0.0000 | lr: 4.5604e-05 | scale: 16384.0000 | micro time: 1.808 | step time: 0.000
train | epoch   1 | Iter:   2514/ 13000 | global iter:   2514/ 13000 | loss: 2.1200 | ds_loss: 0.0000 | lr: 4.5601e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   1 | Iter:   2515/ 13000 | global iter:   2515/ 13000 | loss: 2.1207 | ds_loss: 0.0000 | lr: 4.5597e-05 | scale: 16384.0000 | micro time: 1.822 | step time: 0.000
train | epoch   1 | Iter:   2516/ 13000 | global iter:   2516/ 13000 | loss: 2.0513 | ds_loss: 0.0000 | lr: 4.5594e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   1 | Iter:   2517/ 13000 | global iter:   2517/ 13000 | loss: 1.3294 | ds_loss: 0.0000 | lr: 4.5590e-05 | scale: 16384.0000 | micro time: 1.850 | step time: 0.000
train | epoch   1 | Iter:   2518/ 13000 | global iter:   2518/ 13000 | loss: 2.1754 | ds_loss: 0.0000 | lr: 4.5587e-05 | scale: 16384.0000 | micro time: 1.806 | step time: 0.000
train | epoch   1 | Iter:   2519/ 13000 | global iter:   2519/ 13000 | loss: 2.3281 | ds_loss: 0.0000 | lr: 4.5583e-05 | scale: 16384.0000 | micro time: 1.824 | step time: 0.000
train | epoch   1 | Iter:   2520/ 13000 | global iter:   2520/ 13000 | loss: 2.5745 | ds_loss: 0.0000 | lr: 4.5580e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2520/ 13000 | global iter:   2520/ 13000 | loss: 2.0849 | ds_loss: 0.0000 | lr: 4.5580e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 1.811
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2521/ 13000 | global iter:   2521/ 13000 | loss: 2.5180 | ds_loss: 0.0000 | lr: 4.5577e-05 | scale: 16384.0000 | micro time: 1.870 | step time: 0.000
train | epoch   1 | Iter:   2522/ 13000 | global iter:   2522/ 13000 | loss: 2.1369 | ds_loss: 0.0000 | lr: 4.5573e-05 | scale: 16384.0000 | micro time: 1.881 | step time: 0.000
train | epoch   1 | Iter:   2523/ 13000 | global iter:   2523/ 13000 | loss: 1.9370 | ds_loss: 0.0000 | lr: 4.5570e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   1 | Iter:   2524/ 13000 | global iter:   2524/ 13000 | loss: 1.9422 | ds_loss: 0.0000 | lr: 4.5566e-05 | scale: 16384.0000 | micro time: 1.696 | step time: 0.000
train | epoch   1 | Iter:   2525/ 13000 | global iter:   2525/ 13000 | loss: 2.3465 | ds_loss: 0.0000 | lr: 4.5563e-05 | scale: 16384.0000 | micro time: 1.858 | step time: 0.000
train | epoch   1 | Iter:   2526/ 13000 | global iter:   2526/ 13000 | loss: 2.3372 | ds_loss: 0.0000 | lr: 4.5559e-05 | scale: 16384.0000 | micro time: 1.882 | step time: 0.000
train | epoch   1 | Iter:   2527/ 13000 | global iter:   2527/ 13000 | loss: 2.1871 | ds_loss: 0.0000 | lr: 4.5556e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   1 | Iter:   2528/ 13000 | global iter:   2528/ 13000 | loss: 2.1954 | ds_loss: 0.0000 | lr: 4.5553e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   1 | Iter:   2529/ 13000 | global iter:   2529/ 13000 | loss: 2.5667 | ds_loss: 0.0000 | lr: 4.5549e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   1 | Iter:   2530/ 13000 | global iter:   2530/ 13000 | loss: 2.1652 | ds_loss: 0.0000 | lr: 4.5546e-05 | scale: 16384.0000 | micro time: 1.801 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2530/ 13000 | global iter:   2530/ 13000 | loss: 2.2332 | ds_loss: 0.0000 | lr: 4.5546e-05 | scale: 16384.0000 | micro time: 1.801 | step time: 1.818
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2531/ 13000 | global iter:   2531/ 13000 | loss: 2.1111 | ds_loss: 0.0000 | lr: 4.5542e-05 | scale: 16384.0000 | micro time: 1.667 | step time: 0.000
train | epoch   1 | Iter:   2532/ 13000 | global iter:   2532/ 13000 | loss: 2.3780 | ds_loss: 0.0000 | lr: 4.5539e-05 | scale: 16384.0000 | micro time: 1.721 | step time: 0.000
train | epoch   1 | Iter:   2533/ 13000 | global iter:   2533/ 13000 | loss: 2.4671 | ds_loss: 0.0000 | lr: 4.5535e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   1 | Iter:   2534/ 13000 | global iter:   2534/ 13000 | loss: 1.9341 | ds_loss: 0.0000 | lr: 4.5532e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   1 | Iter:   2535/ 13000 | global iter:   2535/ 13000 | loss: 2.1385 | ds_loss: 0.0000 | lr: 4.5528e-05 | scale: 16384.0000 | micro time: 1.942 | step time: 0.000
train | epoch   1 | Iter:   2536/ 13000 | global iter:   2536/ 13000 | loss: 1.9179 | ds_loss: 0.0000 | lr: 4.5525e-05 | scale: 16384.0000 | micro time: 1.852 | step time: 0.000
train | epoch   1 | Iter:   2537/ 13000 | global iter:   2537/ 13000 | loss: 2.3940 | ds_loss: 0.0000 | lr: 4.5522e-05 | scale: 16384.0000 | micro time: 1.782 | step time: 0.000
train | epoch   1 | Iter:   2538/ 13000 | global iter:   2538/ 13000 | loss: 2.2221 | ds_loss: 0.0000 | lr: 4.5518e-05 | scale: 16384.0000 | micro time: 1.824 | step time: 0.000
train | epoch   1 | Iter:   2539/ 13000 | global iter:   2539/ 13000 | loss: 1.3445 | ds_loss: 0.0000 | lr: 4.5515e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   1 | Iter:   2540/ 13000 | global iter:   2540/ 13000 | loss: 1.9671 | ds_loss: 0.0000 | lr: 4.5511e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2540/ 13000 | global iter:   2540/ 13000 | loss: 2.0874 | ds_loss: 0.0000 | lr: 4.5511e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 1.805
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2541/ 13000 | global iter:   2541/ 13000 | loss: 2.1488 | ds_loss: 0.0000 | lr: 4.5508e-05 | scale: 16384.0000 | micro time: 1.881 | step time: 0.000
train | epoch   1 | Iter:   2542/ 13000 | global iter:   2542/ 13000 | loss: 1.7657 | ds_loss: 0.0000 | lr: 4.5504e-05 | scale: 16384.0000 | micro time: 1.876 | step time: 0.000
train | epoch   1 | Iter:   2543/ 13000 | global iter:   2543/ 13000 | loss: 2.3728 | ds_loss: 0.0000 | lr: 4.5501e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   1 | Iter:   2544/ 13000 | global iter:   2544/ 13000 | loss: 2.0581 | ds_loss: 0.0000 | lr: 4.5497e-05 | scale: 16384.0000 | micro time: 1.923 | step time: 0.000
train | epoch   1 | Iter:   2545/ 13000 | global iter:   2545/ 13000 | loss: 2.4537 | ds_loss: 0.0000 | lr: 4.5494e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   1 | Iter:   2546/ 13000 | global iter:   2546/ 13000 | loss: 1.9040 | ds_loss: 0.0000 | lr: 4.5491e-05 | scale: 16384.0000 | micro time: 1.836 | step time: 0.000
train | epoch   1 | Iter:   2547/ 13000 | global iter:   2547/ 13000 | loss: 1.8939 | ds_loss: 0.0000 | lr: 4.5487e-05 | scale: 16384.0000 | micro time: 1.832 | step time: 0.000
train | epoch   1 | Iter:   2548/ 13000 | global iter:   2548/ 13000 | loss: 2.5955 | ds_loss: 0.0000 | lr: 4.5484e-05 | scale: 16384.0000 | micro time: 1.765 | step time: 0.000
train | epoch   1 | Iter:   2549/ 13000 | global iter:   2549/ 13000 | loss: 2.0669 | ds_loss: 0.0000 | lr: 4.5480e-05 | scale: 16384.0000 | micro time: 1.676 | step time: 0.000
train | epoch   1 | Iter:   2550/ 13000 | global iter:   2550/ 13000 | loss: 1.6956 | ds_loss: 0.0000 | lr: 4.5477e-05 | scale: 16384.0000 | micro time: 1.777 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2550/ 13000 | global iter:   2550/ 13000 | loss: 2.0955 | ds_loss: 0.0000 | lr: 4.5477e-05 | scale: 16384.0000 | micro time: 1.777 | step time: 1.819
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2551/ 13000 | global iter:   2551/ 13000 | loss: 1.8389 | ds_loss: 0.0000 | lr: 4.5473e-05 | scale: 16384.0000 | micro time: 1.723 | step time: 0.000
train | epoch   1 | Iter:   2552/ 13000 | global iter:   2552/ 13000 | loss: 2.3308 | ds_loss: 0.0000 | lr: 4.5470e-05 | scale: 16384.0000 | micro time: 1.702 | step time: 0.000
train | epoch   1 | Iter:   2553/ 13000 | global iter:   2553/ 13000 | loss: 2.3470 | ds_loss: 0.0000 | lr: 4.5466e-05 | scale: 16384.0000 | micro time: 1.818 | step time: 0.000
train | epoch   1 | Iter:   2554/ 13000 | global iter:   2554/ 13000 | loss: 2.0939 | ds_loss: 0.0000 | lr: 4.5463e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   1 | Iter:   2555/ 13000 | global iter:   2555/ 13000 | loss: 1.9612 | ds_loss: 0.0000 | lr: 4.5459e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   1 | Iter:   2556/ 13000 | global iter:   2556/ 13000 | loss: 1.7360 | ds_loss: 0.0000 | lr: 4.5456e-05 | scale: 16384.0000 | micro time: 1.830 | step time: 0.000
train | epoch   1 | Iter:   2557/ 13000 | global iter:   2557/ 13000 | loss: 2.1971 | ds_loss: 0.0000 | lr: 4.5452e-05 | scale: 16384.0000 | micro time: 1.685 | step time: 0.000
train | epoch   1 | Iter:   2558/ 13000 | global iter:   2558/ 13000 | loss: 1.6936 | ds_loss: 0.0000 | lr: 4.5449e-05 | scale: 16384.0000 | micro time: 1.773 | step time: 0.000
train | epoch   1 | Iter:   2559/ 13000 | global iter:   2559/ 13000 | loss: 2.3005 | ds_loss: 0.0000 | lr: 4.5445e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   1 | Iter:   2560/ 13000 | global iter:   2560/ 13000 | loss: 2.0560 | ds_loss: 0.0000 | lr: 4.5442e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2560/ 13000 | global iter:   2560/ 13000 | loss: 2.0555 | ds_loss: 0.0000 | lr: 4.5442e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 1.779
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2561/ 13000 | global iter:   2561/ 13000 | loss: 2.0905 | ds_loss: 0.0000 | lr: 4.5439e-05 | scale: 16384.0000 | micro time: 1.905 | step time: 0.000
train | epoch   1 | Iter:   2562/ 13000 | global iter:   2562/ 13000 | loss: 1.7139 | ds_loss: 0.0000 | lr: 4.5435e-05 | scale: 16384.0000 | micro time: 1.719 | step time: 0.000
train | epoch   1 | Iter:   2563/ 13000 | global iter:   2563/ 13000 | loss: 2.0407 | ds_loss: 0.0000 | lr: 4.5432e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   1 | Iter:   2564/ 13000 | global iter:   2564/ 13000 | loss: 2.1983 | ds_loss: 0.0000 | lr: 4.5428e-05 | scale: 16384.0000 | micro time: 1.726 | step time: 0.000
train | epoch   1 | Iter:   2565/ 13000 | global iter:   2565/ 13000 | loss: 2.4617 | ds_loss: 0.0000 | lr: 4.5425e-05 | scale: 16384.0000 | micro time: 1.712 | step time: 0.000
train | epoch   1 | Iter:   2566/ 13000 | global iter:   2566/ 13000 | loss: 2.0787 | ds_loss: 0.0000 | lr: 4.5421e-05 | scale: 16384.0000 | micro time: 1.840 | step time: 0.000
train | epoch   1 | Iter:   2567/ 13000 | global iter:   2567/ 13000 | loss: 1.8684 | ds_loss: 0.0000 | lr: 4.5418e-05 | scale: 16384.0000 | micro time: 1.675 | step time: 0.000
train | epoch   1 | Iter:   2568/ 13000 | global iter:   2568/ 13000 | loss: 2.0755 | ds_loss: 0.0000 | lr: 4.5414e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   1 | Iter:   2569/ 13000 | global iter:   2569/ 13000 | loss: 2.0344 | ds_loss: 0.0000 | lr: 4.5411e-05 | scale: 16384.0000 | micro time: 1.857 | step time: 0.000
train | epoch   1 | Iter:   2570/ 13000 | global iter:   2570/ 13000 | loss: 1.9894 | ds_loss: 0.0000 | lr: 4.5407e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2570/ 13000 | global iter:   2570/ 13000 | loss: 2.0552 | ds_loss: 0.0000 | lr: 4.5407e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 1.786
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2571/ 13000 | global iter:   2571/ 13000 | loss: 1.8494 | ds_loss: 0.0000 | lr: 4.5404e-05 | scale: 16384.0000 | micro time: 1.822 | step time: 0.000
train | epoch   1 | Iter:   2572/ 13000 | global iter:   2572/ 13000 | loss: 2.2270 | ds_loss: 0.0000 | lr: 4.5400e-05 | scale: 16384.0000 | micro time: 1.832 | step time: 0.000
train | epoch   1 | Iter:   2573/ 13000 | global iter:   2573/ 13000 | loss: 2.0032 | ds_loss: 0.0000 | lr: 4.5397e-05 | scale: 16384.0000 | micro time: 1.757 | step time: 0.000
train | epoch   1 | Iter:   2574/ 13000 | global iter:   2574/ 13000 | loss: 1.9446 | ds_loss: 0.0000 | lr: 4.5393e-05 | scale: 16384.0000 | micro time: 1.780 | step time: 0.000
train | epoch   1 | Iter:   2575/ 13000 | global iter:   2575/ 13000 | loss: 2.0838 | ds_loss: 0.0000 | lr: 4.5390e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   1 | Iter:   2576/ 13000 | global iter:   2576/ 13000 | loss: 1.8608 | ds_loss: 0.0000 | lr: 4.5386e-05 | scale: 16384.0000 | micro time: 1.854 | step time: 0.000
train | epoch   1 | Iter:   2577/ 13000 | global iter:   2577/ 13000 | loss: 2.3831 | ds_loss: 0.0000 | lr: 4.5383e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   1 | Iter:   2578/ 13000 | global iter:   2578/ 13000 | loss: 2.3262 | ds_loss: 0.0000 | lr: 4.5379e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 0.000
train | epoch   1 | Iter:   2579/ 13000 | global iter:   2579/ 13000 | loss: 1.6473 | ds_loss: 0.0000 | lr: 4.5376e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   1 | Iter:   2580/ 13000 | global iter:   2580/ 13000 | loss: 1.4888 | ds_loss: 0.0000 | lr: 4.5372e-05 | scale: 16384.0000 | micro time: 1.731 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2580/ 13000 | global iter:   2580/ 13000 | loss: 1.9814 | ds_loss: 0.0000 | lr: 4.5372e-05 | scale: 16384.0000 | micro time: 1.731 | step time: 1.785
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2581/ 13000 | global iter:   2581/ 13000 | loss: 1.9988 | ds_loss: 0.0000 | lr: 4.5369e-05 | scale: 16384.0000 | micro time: 1.848 | step time: 0.000
train | epoch   1 | Iter:   2582/ 13000 | global iter:   2582/ 13000 | loss: 2.7988 | ds_loss: 0.0000 | lr: 4.5365e-05 | scale: 16384.0000 | micro time: 1.792 | step time: 0.000
train | epoch   1 | Iter:   2583/ 13000 | global iter:   2583/ 13000 | loss: 1.7564 | ds_loss: 0.0000 | lr: 4.5362e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   1 | Iter:   2584/ 13000 | global iter:   2584/ 13000 | loss: 1.9217 | ds_loss: 0.0000 | lr: 4.5358e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   1 | Iter:   2585/ 13000 | global iter:   2585/ 13000 | loss: 2.6106 | ds_loss: 0.0000 | lr: 4.5355e-05 | scale: 16384.0000 | micro time: 1.887 | step time: 0.000
train | epoch   1 | Iter:   2586/ 13000 | global iter:   2586/ 13000 | loss: 2.1248 | ds_loss: 0.0000 | lr: 4.5351e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   1 | Iter:   2587/ 13000 | global iter:   2587/ 13000 | loss: 2.0184 | ds_loss: 0.0000 | lr: 4.5348e-05 | scale: 16384.0000 | micro time: 1.841 | step time: 0.000
train | epoch   1 | Iter:   2588/ 13000 | global iter:   2588/ 13000 | loss: 2.4650 | ds_loss: 0.0000 | lr: 4.5344e-05 | scale: 16384.0000 | micro time: 1.802 | step time: 0.000
train | epoch   1 | Iter:   2589/ 13000 | global iter:   2589/ 13000 | loss: 1.8576 | ds_loss: 0.0000 | lr: 4.5341e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   1 | Iter:   2590/ 13000 | global iter:   2590/ 13000 | loss: 2.3974 | ds_loss: 0.0000 | lr: 4.5337e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2590/ 13000 | global iter:   2590/ 13000 | loss: 2.1949 | ds_loss: 0.0000 | lr: 4.5337e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 1.820
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2591/ 13000 | global iter:   2591/ 13000 | loss: 2.1372 | ds_loss: 0.0000 | lr: 4.5334e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   1 | Iter:   2592/ 13000 | global iter:   2592/ 13000 | loss: 2.4664 | ds_loss: 0.0000 | lr: 4.5330e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   1 | Iter:   2593/ 13000 | global iter:   2593/ 13000 | loss: 2.4741 | ds_loss: 0.0000 | lr: 4.5327e-05 | scale: 16384.0000 | micro time: 1.715 | step time: 0.000
train | epoch   1 | Iter:   2594/ 13000 | global iter:   2594/ 13000 | loss: 2.1189 | ds_loss: 0.0000 | lr: 4.5323e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   1 | Iter:   2595/ 13000 | global iter:   2595/ 13000 | loss: 2.2954 | ds_loss: 0.0000 | lr: 4.5320e-05 | scale: 16384.0000 | micro time: 1.881 | step time: 0.000
train | epoch   1 | Iter:   2596/ 13000 | global iter:   2596/ 13000 | loss: 1.9359 | ds_loss: 0.0000 | lr: 4.5316e-05 | scale: 16384.0000 | micro time: 1.877 | step time: 0.000
train | epoch   1 | Iter:   2597/ 13000 | global iter:   2597/ 13000 | loss: 2.1776 | ds_loss: 0.0000 | lr: 4.5313e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   1 | Iter:   2598/ 13000 | global iter:   2598/ 13000 | loss: 2.3497 | ds_loss: 0.0000 | lr: 4.5309e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   1 | Iter:   2599/ 13000 | global iter:   2599/ 13000 | loss: 2.3190 | ds_loss: 0.0000 | lr: 4.5306e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   1 | Iter:   2600/ 13000 | global iter:   2600/ 13000 | loss: 1.8603 | ds_loss: 0.0000 | lr: 4.5302e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
****************************************************************************************************
train | epoch   1 | Iter:   2600/ 13000 | global iter:   2600/ 13000 | loss: 2.2135 | ds_loss: 0.0000 | lr: 4.5302e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 1.817
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   1 | Iter:   2601/ 13000 | global iter:   2601/ 13000 | loss: 1.7021 | ds_loss: 0.0000 | lr: 4.5299e-05 | scale: 16384.0000 | micro time: 1.849 | step time: 0.000
train | epoch   1 | Iter:   2602/ 13000 | global iter:   2602/ 13000 | loss: 2.6106 | ds_loss: 0.0000 | lr: 4.5295e-05 | scale: 16384.0000 | micro time: 1.721 | step time: 0.000
Sat Apr 19 12:26:46 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   40C    P2            104W /  230W |   21731MiB /  23028MiB |     73%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   44C    P2            124W /  230W |   22429MiB /  23028MiB |     90%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   42C    P2            117W /  230W |   21691MiB /  23028MiB |     87%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   40C    P2            123W /  230W |   21059MiB /  23028MiB |     65%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3      21724MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3      22422MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3      21684MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3      21052MiB |
+-----------------------------------------------------------------------------------------+

Sat Apr 19 12:26:46 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   40C    P2            104W /  230W |   21731MiB /  23028MiB |     73%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   44C    P2            124W /  230W |   22429MiB /  23028MiB |     90%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   42C    P2            117W /  230W |   21691MiB /  23028MiB |     87%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   40C    P2            114W /  230W |   21059MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3      21724MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3      22422MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3      21684MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3      21052MiB |
+-----------------------------------------------------------------------------------------+

Sat Apr 19 12:26:46 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   40C    P2             97W /  230W |   21731MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   43C    P2            114W /  230W |   22429MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   42C    P2            106W /  230W |   21691MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   42C    P2            115W /  230W |   21059MiB /  23028MiB |      4%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3      21724MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3      22422MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3      21684MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3      21052MiB |
+-----------------------------------------------------------------------------------------+

Sat Apr 19 12:26:46 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   40C    P2            104W /  230W |   21731MiB /  23028MiB |     73%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   43C    P2            114W /  230W |   22429MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   42C    P2            106W /  230W |   21691MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   42C    P2            115W /  230W |   21059MiB /  23028MiB |      4%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3      21724MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3      22422MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3      21684MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3      21052MiB |
+-----------------------------------------------------------------------------------------+

train | epoch   2 | Iter:   2603/ 13000 | global iter:   2603/ 13000 | loss: 1.7454 | ds_loss: 0.0000 | lr: 4.5292e-05 | scale: 16384.0000 | micro time: 1.738 | step time: 0.000
train | epoch   2 | Iter:   2604/ 13000 | global iter:   2604/ 13000 | loss: 1.1488 | ds_loss: 0.0000 | lr: 4.5288e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   2 | Iter:   2605/ 13000 | global iter:   2605/ 13000 | loss: 2.3952 | ds_loss: 0.0000 | lr: 4.5284e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   2 | Iter:   2606/ 13000 | global iter:   2606/ 13000 | loss: 1.8142 | ds_loss: 0.0000 | lr: 4.5281e-05 | scale: 16384.0000 | micro time: 1.939 | step time: 0.000
train | epoch   2 | Iter:   2607/ 13000 | global iter:   2607/ 13000 | loss: 2.0921 | ds_loss: 0.0000 | lr: 4.5277e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   2 | Iter:   2608/ 13000 | global iter:   2608/ 13000 | loss: 2.5037 | ds_loss: 0.0000 | lr: 4.5274e-05 | scale: 16384.0000 | micro time: 1.857 | step time: 0.000
train | epoch   2 | Iter:   2609/ 13000 | global iter:   2609/ 13000 | loss: 1.8512 | ds_loss: 0.0000 | lr: 4.5270e-05 | scale: 16384.0000 | micro time: 1.773 | step time: 0.000
train | epoch   2 | Iter:   2610/ 13000 | global iter:   2610/ 13000 | loss: 1.2090 | ds_loss: 0.0000 | lr: 4.5267e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2610/ 13000 | global iter:   2610/ 13000 | loss: 1.9072 | ds_loss: 0.0000 | lr: 4.5267e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 1.810
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2611/ 13000 | global iter:   2611/ 13000 | loss: 1.8649 | ds_loss: 0.0000 | lr: 4.5263e-05 | scale: 16384.0000 | micro time: 1.731 | step time: 0.000
train | epoch   2 | Iter:   2612/ 13000 | global iter:   2612/ 13000 | loss: 1.7818 | ds_loss: 0.0000 | lr: 4.5260e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   2 | Iter:   2613/ 13000 | global iter:   2613/ 13000 | loss: 1.9113 | ds_loss: 0.0000 | lr: 4.5256e-05 | scale: 16384.0000 | micro time: 1.703 | step time: 0.000
train | epoch   2 | Iter:   2614/ 13000 | global iter:   2614/ 13000 | loss: 2.0308 | ds_loss: 0.0000 | lr: 4.5253e-05 | scale: 16384.0000 | micro time: 1.818 | step time: 0.000
train | epoch   2 | Iter:   2615/ 13000 | global iter:   2615/ 13000 | loss: 1.9755 | ds_loss: 0.0000 | lr: 4.5249e-05 | scale: 16384.0000 | micro time: 1.756 | step time: 0.000
train | epoch   2 | Iter:   2616/ 13000 | global iter:   2616/ 13000 | loss: 1.5803 | ds_loss: 0.0000 | lr: 4.5246e-05 | scale: 16384.0000 | micro time: 1.846 | step time: 0.000
train | epoch   2 | Iter:   2617/ 13000 | global iter:   2617/ 13000 | loss: 1.8597 | ds_loss: 0.0000 | lr: 4.5242e-05 | scale: 16384.0000 | micro time: 1.895 | step time: 0.000
train | epoch   2 | Iter:   2618/ 13000 | global iter:   2618/ 13000 | loss: 2.2373 | ds_loss: 0.0000 | lr: 4.5239e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   2 | Iter:   2619/ 13000 | global iter:   2619/ 13000 | loss: 1.9009 | ds_loss: 0.0000 | lr: 4.5235e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   2 | Iter:   2620/ 13000 | global iter:   2620/ 13000 | loss: 2.1334 | ds_loss: 0.0000 | lr: 4.5231e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2620/ 13000 | global iter:   2620/ 13000 | loss: 1.9276 | ds_loss: 0.0000 | lr: 4.5231e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 1.809
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2621/ 13000 | global iter:   2621/ 13000 | loss: 1.6755 | ds_loss: 0.0000 | lr: 4.5228e-05 | scale: 16384.0000 | micro time: 1.870 | step time: 0.000
train | epoch   2 | Iter:   2622/ 13000 | global iter:   2622/ 13000 | loss: 2.1503 | ds_loss: 0.0000 | lr: 4.5224e-05 | scale: 16384.0000 | micro time: 1.789 | step time: 0.000
train | epoch   2 | Iter:   2623/ 13000 | global iter:   2623/ 13000 | loss: 1.5626 | ds_loss: 0.0000 | lr: 4.5221e-05 | scale: 16384.0000 | micro time: 1.873 | step time: 0.000
train | epoch   2 | Iter:   2624/ 13000 | global iter:   2624/ 13000 | loss: 2.4485 | ds_loss: 0.0000 | lr: 4.5217e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   2 | Iter:   2625/ 13000 | global iter:   2625/ 13000 | loss: 1.9466 | ds_loss: 0.0000 | lr: 4.5214e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   2 | Iter:   2626/ 13000 | global iter:   2626/ 13000 | loss: 1.5855 | ds_loss: 0.0000 | lr: 4.5210e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   2 | Iter:   2627/ 13000 | global iter:   2627/ 13000 | loss: 2.3162 | ds_loss: 0.0000 | lr: 4.5207e-05 | scale: 16384.0000 | micro time: 1.899 | step time: 0.000
train | epoch   2 | Iter:   2628/ 13000 | global iter:   2628/ 13000 | loss: 1.6516 | ds_loss: 0.0000 | lr: 4.5203e-05 | scale: 16384.0000 | micro time: 1.789 | step time: 0.000
train | epoch   2 | Iter:   2629/ 13000 | global iter:   2629/ 13000 | loss: 1.2222 | ds_loss: 0.0000 | lr: 4.5199e-05 | scale: 16384.0000 | micro time: 1.765 | step time: 0.000
train | epoch   2 | Iter:   2630/ 13000 | global iter:   2630/ 13000 | loss: 1.9810 | ds_loss: 0.0000 | lr: 4.5196e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2630/ 13000 | global iter:   2630/ 13000 | loss: 1.8540 | ds_loss: 0.0000 | lr: 4.5196e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 1.817
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2631/ 13000 | global iter:   2631/ 13000 | loss: 2.1731 | ds_loss: 0.0000 | lr: 4.5192e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   2 | Iter:   2632/ 13000 | global iter:   2632/ 13000 | loss: 2.1135 | ds_loss: 0.0000 | lr: 4.5189e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   2 | Iter:   2633/ 13000 | global iter:   2633/ 13000 | loss: 1.8462 | ds_loss: 0.0000 | lr: 4.5185e-05 | scale: 16384.0000 | micro time: 1.893 | step time: 0.000
train | epoch   2 | Iter:   2634/ 13000 | global iter:   2634/ 13000 | loss: 1.3463 | ds_loss: 0.0000 | lr: 4.5182e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   2 | Iter:   2635/ 13000 | global iter:   2635/ 13000 | loss: 2.2167 | ds_loss: 0.0000 | lr: 4.5178e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   2 | Iter:   2636/ 13000 | global iter:   2636/ 13000 | loss: 1.8235 | ds_loss: 0.0000 | lr: 4.5175e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   2 | Iter:   2637/ 13000 | global iter:   2637/ 13000 | loss: 1.8101 | ds_loss: 0.0000 | lr: 4.5171e-05 | scale: 16384.0000 | micro time: 1.899 | step time: 0.000
train | epoch   2 | Iter:   2638/ 13000 | global iter:   2638/ 13000 | loss: 1.8494 | ds_loss: 0.0000 | lr: 4.5167e-05 | scale: 16384.0000 | micro time: 1.834 | step time: 0.000
train | epoch   2 | Iter:   2639/ 13000 | global iter:   2639/ 13000 | loss: 1.8202 | ds_loss: 0.0000 | lr: 4.5164e-05 | scale: 16384.0000 | micro time: 1.895 | step time: 0.000
train | epoch   2 | Iter:   2640/ 13000 | global iter:   2640/ 13000 | loss: 1.9677 | ds_loss: 0.0000 | lr: 4.5160e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2640/ 13000 | global iter:   2640/ 13000 | loss: 1.8967 | ds_loss: 0.0000 | lr: 4.5160e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 1.837
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2641/ 13000 | global iter:   2641/ 13000 | loss: 2.1877 | ds_loss: 0.0000 | lr: 4.5157e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   2 | Iter:   2642/ 13000 | global iter:   2642/ 13000 | loss: 1.8469 | ds_loss: 0.0000 | lr: 4.5153e-05 | scale: 16384.0000 | micro time: 1.759 | step time: 0.000
train | epoch   2 | Iter:   2643/ 13000 | global iter:   2643/ 13000 | loss: 1.9550 | ds_loss: 0.0000 | lr: 4.5150e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   2 | Iter:   2644/ 13000 | global iter:   2644/ 13000 | loss: 2.0690 | ds_loss: 0.0000 | lr: 4.5146e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   2 | Iter:   2645/ 13000 | global iter:   2645/ 13000 | loss: 1.7072 | ds_loss: 0.0000 | lr: 4.5142e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   2 | Iter:   2646/ 13000 | global iter:   2646/ 13000 | loss: 1.7503 | ds_loss: 0.0000 | lr: 4.5139e-05 | scale: 16384.0000 | micro time: 1.727 | step time: 0.000
train | epoch   2 | Iter:   2647/ 13000 | global iter:   2647/ 13000 | loss: 2.0148 | ds_loss: 0.0000 | lr: 4.5135e-05 | scale: 16384.0000 | micro time: 1.784 | step time: 0.000
train | epoch   2 | Iter:   2648/ 13000 | global iter:   2648/ 13000 | loss: 1.8976 | ds_loss: 0.0000 | lr: 4.5132e-05 | scale: 16384.0000 | micro time: 1.798 | step time: 0.000
train | epoch   2 | Iter:   2649/ 13000 | global iter:   2649/ 13000 | loss: 1.9923 | ds_loss: 0.0000 | lr: 4.5128e-05 | scale: 16384.0000 | micro time: 1.793 | step time: 0.000
train | epoch   2 | Iter:   2650/ 13000 | global iter:   2650/ 13000 | loss: 1.5272 | ds_loss: 0.0000 | lr: 4.5125e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2650/ 13000 | global iter:   2650/ 13000 | loss: 1.8948 | ds_loss: 0.0000 | lr: 4.5125e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 1.787
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2651/ 13000 | global iter:   2651/ 13000 | loss: 1.7857 | ds_loss: 0.0000 | lr: 4.5121e-05 | scale: 16384.0000 | micro time: 1.736 | step time: 0.000
train | epoch   2 | Iter:   2652/ 13000 | global iter:   2652/ 13000 | loss: 2.1711 | ds_loss: 0.0000 | lr: 4.5117e-05 | scale: 16384.0000 | micro time: 1.888 | step time: 0.000
train | epoch   2 | Iter:   2653/ 13000 | global iter:   2653/ 13000 | loss: 1.6748 | ds_loss: 0.0000 | lr: 4.5114e-05 | scale: 16384.0000 | micro time: 1.798 | step time: 0.000
train | epoch   2 | Iter:   2654/ 13000 | global iter:   2654/ 13000 | loss: 1.6905 | ds_loss: 0.0000 | lr: 4.5110e-05 | scale: 16384.0000 | micro time: 1.856 | step time: 0.000
train | epoch   2 | Iter:   2655/ 13000 | global iter:   2655/ 13000 | loss: 1.5392 | ds_loss: 0.0000 | lr: 4.5107e-05 | scale: 16384.0000 | micro time: 1.862 | step time: 0.000
train | epoch   2 | Iter:   2656/ 13000 | global iter:   2656/ 13000 | loss: 1.3699 | ds_loss: 0.0000 | lr: 4.5103e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   2 | Iter:   2657/ 13000 | global iter:   2657/ 13000 | loss: 1.4637 | ds_loss: 0.0000 | lr: 4.5099e-05 | scale: 16384.0000 | micro time: 1.672 | step time: 0.000
train | epoch   2 | Iter:   2658/ 13000 | global iter:   2658/ 13000 | loss: 2.1336 | ds_loss: 0.0000 | lr: 4.5096e-05 | scale: 16384.0000 | micro time: 1.766 | step time: 0.000
train | epoch   2 | Iter:   2659/ 13000 | global iter:   2659/ 13000 | loss: 1.8030 | ds_loss: 0.0000 | lr: 4.5092e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   2 | Iter:   2660/ 13000 | global iter:   2660/ 13000 | loss: 1.5443 | ds_loss: 0.0000 | lr: 4.5089e-05 | scale: 16384.0000 | micro time: 1.759 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2660/ 13000 | global iter:   2660/ 13000 | loss: 1.7176 | ds_loss: 0.0000 | lr: 4.5089e-05 | scale: 16384.0000 | micro time: 1.759 | step time: 1.799
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2661/ 13000 | global iter:   2661/ 13000 | loss: 1.1401 | ds_loss: 0.0000 | lr: 4.5085e-05 | scale: 16384.0000 | micro time: 1.924 | step time: 0.000
train | epoch   2 | Iter:   2662/ 13000 | global iter:   2662/ 13000 | loss: 2.0603 | ds_loss: 0.0000 | lr: 4.5081e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   2 | Iter:   2663/ 13000 | global iter:   2663/ 13000 | loss: 1.9927 | ds_loss: 0.0000 | lr: 4.5078e-05 | scale: 16384.0000 | micro time: 1.887 | step time: 0.000
train | epoch   2 | Iter:   2664/ 13000 | global iter:   2664/ 13000 | loss: 1.4610 | ds_loss: 0.0000 | lr: 4.5074e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   2 | Iter:   2665/ 13000 | global iter:   2665/ 13000 | loss: 2.1401 | ds_loss: 0.0000 | lr: 4.5071e-05 | scale: 16384.0000 | micro time: 1.665 | step time: 0.000
train | epoch   2 | Iter:   2666/ 13000 | global iter:   2666/ 13000 | loss: 1.6759 | ds_loss: 0.0000 | lr: 4.5067e-05 | scale: 16384.0000 | micro time: 1.686 | step time: 0.000
train | epoch   2 | Iter:   2667/ 13000 | global iter:   2667/ 13000 | loss: 2.2266 | ds_loss: 0.0000 | lr: 4.5064e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   2 | Iter:   2668/ 13000 | global iter:   2668/ 13000 | loss: 2.0609 | ds_loss: 0.0000 | lr: 4.5060e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   2 | Iter:   2669/ 13000 | global iter:   2669/ 13000 | loss: 2.2999 | ds_loss: 0.0000 | lr: 4.5056e-05 | scale: 16384.0000 | micro time: 1.744 | step time: 0.000
train | epoch   2 | Iter:   2670/ 13000 | global iter:   2670/ 13000 | loss: 1.9540 | ds_loss: 0.0000 | lr: 4.5053e-05 | scale: 16384.0000 | micro time: 1.894 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2670/ 13000 | global iter:   2670/ 13000 | loss: 1.9012 | ds_loss: 0.0000 | lr: 4.5053e-05 | scale: 16384.0000 | micro time: 1.894 | step time: 1.806
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2671/ 13000 | global iter:   2671/ 13000 | loss: 1.9708 | ds_loss: 0.0000 | lr: 4.5049e-05 | scale: 16384.0000 | micro time: 1.829 | step time: 0.000
train | epoch   2 | Iter:   2672/ 13000 | global iter:   2672/ 13000 | loss: 1.6885 | ds_loss: 0.0000 | lr: 4.5045e-05 | scale: 16384.0000 | micro time: 1.876 | step time: 0.000
train | epoch   2 | Iter:   2673/ 13000 | global iter:   2673/ 13000 | loss: 1.3872 | ds_loss: 0.0000 | lr: 4.5042e-05 | scale: 16384.0000 | micro time: 1.866 | step time: 0.000
train | epoch   2 | Iter:   2674/ 13000 | global iter:   2674/ 13000 | loss: 1.4116 | ds_loss: 0.0000 | lr: 4.5038e-05 | scale: 16384.0000 | micro time: 1.917 | step time: 0.000
train | epoch   2 | Iter:   2675/ 13000 | global iter:   2675/ 13000 | loss: 1.3146 | ds_loss: 0.0000 | lr: 4.5035e-05 | scale: 16384.0000 | micro time: 1.889 | step time: 0.000
train | epoch   2 | Iter:   2676/ 13000 | global iter:   2676/ 13000 | loss: 1.2949 | ds_loss: 0.0000 | lr: 4.5031e-05 | scale: 16384.0000 | micro time: 1.711 | step time: 0.000
train | epoch   2 | Iter:   2677/ 13000 | global iter:   2677/ 13000 | loss: 1.6800 | ds_loss: 0.0000 | lr: 4.5027e-05 | scale: 16384.0000 | micro time: 1.833 | step time: 0.000
train | epoch   2 | Iter:   2678/ 13000 | global iter:   2678/ 13000 | loss: 1.5877 | ds_loss: 0.0000 | lr: 4.5024e-05 | scale: 16384.0000 | micro time: 1.777 | step time: 0.000
train | epoch   2 | Iter:   2679/ 13000 | global iter:   2679/ 13000 | loss: 2.2492 | ds_loss: 0.0000 | lr: 4.5020e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   2 | Iter:   2680/ 13000 | global iter:   2680/ 13000 | loss: 1.9027 | ds_loss: 0.0000 | lr: 4.5017e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2680/ 13000 | global iter:   2680/ 13000 | loss: 1.6487 | ds_loss: 0.0000 | lr: 4.5017e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 1.827
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2681/ 13000 | global iter:   2681/ 13000 | loss: 1.4035 | ds_loss: 0.0000 | lr: 4.5013e-05 | scale: 16384.0000 | micro time: 1.738 | step time: 0.000
train | epoch   2 | Iter:   2682/ 13000 | global iter:   2682/ 13000 | loss: 1.2615 | ds_loss: 0.0000 | lr: 4.5009e-05 | scale: 16384.0000 | micro time: 1.886 | step time: 0.000
train | epoch   2 | Iter:   2683/ 13000 | global iter:   2683/ 13000 | loss: 1.7416 | ds_loss: 0.0000 | lr: 4.5006e-05 | scale: 16384.0000 | micro time: 1.688 | step time: 0.000
train | epoch   2 | Iter:   2684/ 13000 | global iter:   2684/ 13000 | loss: 1.7808 | ds_loss: 0.0000 | lr: 4.5002e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   2 | Iter:   2685/ 13000 | global iter:   2685/ 13000 | loss: 1.5894 | ds_loss: 0.0000 | lr: 4.4999e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   2 | Iter:   2686/ 13000 | global iter:   2686/ 13000 | loss: 1.2877 | ds_loss: 0.0000 | lr: 4.4995e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 0.000
train | epoch   2 | Iter:   2687/ 13000 | global iter:   2687/ 13000 | loss: 1.8492 | ds_loss: 0.0000 | lr: 4.4991e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 0.000
train | epoch   2 | Iter:   2688/ 13000 | global iter:   2688/ 13000 | loss: 1.6531 | ds_loss: 0.0000 | lr: 4.4988e-05 | scale: 16384.0000 | micro time: 1.792 | step time: 0.000
train | epoch   2 | Iter:   2689/ 13000 | global iter:   2689/ 13000 | loss: 1.8919 | ds_loss: 0.0000 | lr: 4.4984e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   2 | Iter:   2690/ 13000 | global iter:   2690/ 13000 | loss: 2.1052 | ds_loss: 0.0000 | lr: 4.4980e-05 | scale: 16384.0000 | micro time: 1.760 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2690/ 13000 | global iter:   2690/ 13000 | loss: 1.6564 | ds_loss: 0.0000 | lr: 4.4980e-05 | scale: 16384.0000 | micro time: 1.760 | step time: 1.804
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2691/ 13000 | global iter:   2691/ 13000 | loss: 1.4623 | ds_loss: 0.0000 | lr: 4.4977e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 0.000
train | epoch   2 | Iter:   2692/ 13000 | global iter:   2692/ 13000 | loss: 1.7655 | ds_loss: 0.0000 | lr: 4.4973e-05 | scale: 16384.0000 | micro time: 1.806 | step time: 0.000
train | epoch   2 | Iter:   2693/ 13000 | global iter:   2693/ 13000 | loss: 1.5415 | ds_loss: 0.0000 | lr: 4.4970e-05 | scale: 16384.0000 | micro time: 1.672 | step time: 0.000
train | epoch   2 | Iter:   2694/ 13000 | global iter:   2694/ 13000 | loss: 1.7384 | ds_loss: 0.0000 | lr: 4.4966e-05 | scale: 16384.0000 | micro time: 1.825 | step time: 0.000
train | epoch   2 | Iter:   2695/ 13000 | global iter:   2695/ 13000 | loss: 1.6146 | ds_loss: 0.0000 | lr: 4.4962e-05 | scale: 16384.0000 | micro time: 1.796 | step time: 0.000
train | epoch   2 | Iter:   2696/ 13000 | global iter:   2696/ 13000 | loss: 1.9034 | ds_loss: 0.0000 | lr: 4.4959e-05 | scale: 16384.0000 | micro time: 1.848 | step time: 0.000
train | epoch   2 | Iter:   2697/ 13000 | global iter:   2697/ 13000 | loss: 2.0182 | ds_loss: 0.0000 | lr: 4.4955e-05 | scale: 16384.0000 | micro time: 1.679 | step time: 0.000
train | epoch   2 | Iter:   2698/ 13000 | global iter:   2698/ 13000 | loss: 1.5156 | ds_loss: 0.0000 | lr: 4.4951e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   2 | Iter:   2699/ 13000 | global iter:   2699/ 13000 | loss: 1.8926 | ds_loss: 0.0000 | lr: 4.4948e-05 | scale: 16384.0000 | micro time: 1.842 | step time: 0.000
train | epoch   2 | Iter:   2700/ 13000 | global iter:   2700/ 13000 | loss: 1.7446 | ds_loss: 0.0000 | lr: 4.4944e-05 | scale: 16384.0000 | micro time: 1.860 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2700/ 13000 | global iter:   2700/ 13000 | loss: 1.7197 | ds_loss: 0.0000 | lr: 4.4944e-05 | scale: 16384.0000 | micro time: 1.860 | step time: 1.792
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2701/ 13000 | global iter:   2701/ 13000 | loss: 1.9575 | ds_loss: 0.0000 | lr: 4.4940e-05 | scale: 16384.0000 | micro time: 1.802 | step time: 0.000
train | epoch   2 | Iter:   2702/ 13000 | global iter:   2702/ 13000 | loss: 1.5003 | ds_loss: 0.0000 | lr: 4.4937e-05 | scale: 16384.0000 | micro time: 1.743 | step time: 0.000
train | epoch   2 | Iter:   2703/ 13000 | global iter:   2703/ 13000 | loss: 2.0142 | ds_loss: 0.0000 | lr: 4.4933e-05 | scale: 16384.0000 | micro time: 1.911 | step time: 0.000
train | epoch   2 | Iter:   2704/ 13000 | global iter:   2704/ 13000 | loss: 2.0913 | ds_loss: 0.0000 | lr: 4.4929e-05 | scale: 16384.0000 | micro time: 1.750 | step time: 0.000
train | epoch   2 | Iter:   2705/ 13000 | global iter:   2705/ 13000 | loss: 1.8636 | ds_loss: 0.0000 | lr: 4.4926e-05 | scale: 16384.0000 | micro time: 1.668 | step time: 0.000
train | epoch   2 | Iter:   2706/ 13000 | global iter:   2706/ 13000 | loss: 1.6964 | ds_loss: 0.0000 | lr: 4.4922e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   2 | Iter:   2707/ 13000 | global iter:   2707/ 13000 | loss: 1.5267 | ds_loss: 0.0000 | lr: 4.4919e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   2 | Iter:   2708/ 13000 | global iter:   2708/ 13000 | loss: 1.7626 | ds_loss: 0.0000 | lr: 4.4915e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   2 | Iter:   2709/ 13000 | global iter:   2709/ 13000 | loss: 1.9516 | ds_loss: 0.0000 | lr: 4.4911e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   2 | Iter:   2710/ 13000 | global iter:   2710/ 13000 | loss: 1.3450 | ds_loss: 0.0000 | lr: 4.4908e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2710/ 13000 | global iter:   2710/ 13000 | loss: 1.7709 | ds_loss: 0.0000 | lr: 4.4908e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 1.800
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2711/ 13000 | global iter:   2711/ 13000 | loss: 1.6003 | ds_loss: 0.0000 | lr: 4.4904e-05 | scale: 16384.0000 | micro time: 1.861 | step time: 0.000
train | epoch   2 | Iter:   2712/ 13000 | global iter:   2712/ 13000 | loss: 2.0299 | ds_loss: 0.0000 | lr: 4.4900e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   2 | Iter:   2713/ 13000 | global iter:   2713/ 13000 | loss: 1.3909 | ds_loss: 0.0000 | lr: 4.4897e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   2 | Iter:   2714/ 13000 | global iter:   2714/ 13000 | loss: 2.1033 | ds_loss: 0.0000 | lr: 4.4893e-05 | scale: 16384.0000 | micro time: 1.846 | step time: 0.000
train | epoch   2 | Iter:   2715/ 13000 | global iter:   2715/ 13000 | loss: 1.2497 | ds_loss: 0.0000 | lr: 4.4889e-05 | scale: 16384.0000 | micro time: 1.792 | step time: 0.000
train | epoch   2 | Iter:   2716/ 13000 | global iter:   2716/ 13000 | loss: 1.8520 | ds_loss: 0.0000 | lr: 4.4886e-05 | scale: 16384.0000 | micro time: 1.833 | step time: 0.000
train | epoch   2 | Iter:   2717/ 13000 | global iter:   2717/ 13000 | loss: 1.5493 | ds_loss: 0.0000 | lr: 4.4882e-05 | scale: 16384.0000 | micro time: 1.845 | step time: 0.000
train | epoch   2 | Iter:   2718/ 13000 | global iter:   2718/ 13000 | loss: 2.1436 | ds_loss: 0.0000 | lr: 4.4878e-05 | scale: 16384.0000 | micro time: 1.870 | step time: 0.000
train | epoch   2 | Iter:   2719/ 13000 | global iter:   2719/ 13000 | loss: 1.7562 | ds_loss: 0.0000 | lr: 4.4875e-05 | scale: 16384.0000 | micro time: 1.695 | step time: 0.000
train | epoch   2 | Iter:   2720/ 13000 | global iter:   2720/ 13000 | loss: 1.8985 | ds_loss: 0.0000 | lr: 4.4871e-05 | scale: 16384.0000 | micro time: 1.719 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2720/ 13000 | global iter:   2720/ 13000 | loss: 1.7574 | ds_loss: 0.0000 | lr: 4.4871e-05 | scale: 16384.0000 | micro time: 1.719 | step time: 1.804
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2721/ 13000 | global iter:   2721/ 13000 | loss: 1.7217 | ds_loss: 0.0000 | lr: 4.4867e-05 | scale: 16384.0000 | micro time: 1.766 | step time: 0.000
train | epoch   2 | Iter:   2722/ 13000 | global iter:   2722/ 13000 | loss: 1.5035 | ds_loss: 0.0000 | lr: 4.4864e-05 | scale: 16384.0000 | micro time: 1.872 | step time: 0.000
train | epoch   2 | Iter:   2723/ 13000 | global iter:   2723/ 13000 | loss: 1.8734 | ds_loss: 0.0000 | lr: 4.4860e-05 | scale: 16384.0000 | micro time: 1.848 | step time: 0.000
train | epoch   2 | Iter:   2724/ 13000 | global iter:   2724/ 13000 | loss: 2.0462 | ds_loss: 0.0000 | lr: 4.4856e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   2 | Iter:   2725/ 13000 | global iter:   2725/ 13000 | loss: 2.2338 | ds_loss: 0.0000 | lr: 4.4853e-05 | scale: 16384.0000 | micro time: 1.886 | step time: 0.000
train | epoch   2 | Iter:   2726/ 13000 | global iter:   2726/ 13000 | loss: 1.8981 | ds_loss: 0.0000 | lr: 4.4849e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   2 | Iter:   2727/ 13000 | global iter:   2727/ 13000 | loss: 2.1931 | ds_loss: 0.0000 | lr: 4.4845e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   2 | Iter:   2728/ 13000 | global iter:   2728/ 13000 | loss: 1.8185 | ds_loss: 0.0000 | lr: 4.4842e-05 | scale: 16384.0000 | micro time: 1.931 | step time: 0.000
train | epoch   2 | Iter:   2729/ 13000 | global iter:   2729/ 13000 | loss: 1.7989 | ds_loss: 0.0000 | lr: 4.4838e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   2 | Iter:   2730/ 13000 | global iter:   2730/ 13000 | loss: 2.2267 | ds_loss: 0.0000 | lr: 4.4834e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2730/ 13000 | global iter:   2730/ 13000 | loss: 1.9314 | ds_loss: 0.0000 | lr: 4.4834e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 1.822
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2731/ 13000 | global iter:   2731/ 13000 | loss: 1.9121 | ds_loss: 0.0000 | lr: 4.4831e-05 | scale: 16384.0000 | micro time: 1.786 | step time: 0.000
train | epoch   2 | Iter:   2732/ 13000 | global iter:   2732/ 13000 | loss: 1.7321 | ds_loss: 0.0000 | lr: 4.4827e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   2 | Iter:   2733/ 13000 | global iter:   2733/ 13000 | loss: 1.6639 | ds_loss: 0.0000 | lr: 4.4823e-05 | scale: 16384.0000 | micro time: 1.712 | step time: 0.000
train | epoch   2 | Iter:   2734/ 13000 | global iter:   2734/ 13000 | loss: 2.3211 | ds_loss: 0.0000 | lr: 4.4820e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   2 | Iter:   2735/ 13000 | global iter:   2735/ 13000 | loss: 2.2173 | ds_loss: 0.0000 | lr: 4.4816e-05 | scale: 16384.0000 | micro time: 1.770 | step time: 0.000
train | epoch   2 | Iter:   2736/ 13000 | global iter:   2736/ 13000 | loss: 2.4044 | ds_loss: 0.0000 | lr: 4.4812e-05 | scale: 16384.0000 | micro time: 1.734 | step time: 0.000
train | epoch   2 | Iter:   2737/ 13000 | global iter:   2737/ 13000 | loss: 1.8475 | ds_loss: 0.0000 | lr: 4.4809e-05 | scale: 16384.0000 | micro time: 1.756 | step time: 0.000
train | epoch   2 | Iter:   2738/ 13000 | global iter:   2738/ 13000 | loss: 1.7370 | ds_loss: 0.0000 | lr: 4.4805e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   2 | Iter:   2739/ 13000 | global iter:   2739/ 13000 | loss: 2.0737 | ds_loss: 0.0000 | lr: 4.4801e-05 | scale: 16384.0000 | micro time: 1.683 | step time: 0.000
train | epoch   2 | Iter:   2740/ 13000 | global iter:   2740/ 13000 | loss: 2.1875 | ds_loss: 0.0000 | lr: 4.4798e-05 | scale: 16384.0000 | micro time: 1.733 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2740/ 13000 | global iter:   2740/ 13000 | loss: 2.0097 | ds_loss: 0.0000 | lr: 4.4798e-05 | scale: 16384.0000 | micro time: 1.733 | step time: 1.761
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2741/ 13000 | global iter:   2741/ 13000 | loss: 1.7415 | ds_loss: 0.0000 | lr: 4.4794e-05 | scale: 16384.0000 | micro time: 1.804 | step time: 0.000
train | epoch   2 | Iter:   2742/ 13000 | global iter:   2742/ 13000 | loss: 1.4679 | ds_loss: 0.0000 | lr: 4.4790e-05 | scale: 16384.0000 | micro time: 1.785 | step time: 0.000
train | epoch   2 | Iter:   2743/ 13000 | global iter:   2743/ 13000 | loss: 1.6133 | ds_loss: 0.0000 | lr: 4.4787e-05 | scale: 16384.0000 | micro time: 1.800 | step time: 0.000
train | epoch   2 | Iter:   2744/ 13000 | global iter:   2744/ 13000 | loss: 1.5949 | ds_loss: 0.0000 | lr: 4.4783e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   2 | Iter:   2745/ 13000 | global iter:   2745/ 13000 | loss: 1.9421 | ds_loss: 0.0000 | lr: 4.4779e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   2 | Iter:   2746/ 13000 | global iter:   2746/ 13000 | loss: 1.4888 | ds_loss: 0.0000 | lr: 4.4775e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   2 | Iter:   2747/ 13000 | global iter:   2747/ 13000 | loss: 1.5709 | ds_loss: 0.0000 | lr: 4.4772e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   2 | Iter:   2748/ 13000 | global iter:   2748/ 13000 | loss: 1.8227 | ds_loss: 0.0000 | lr: 4.4768e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   2 | Iter:   2749/ 13000 | global iter:   2749/ 13000 | loss: 2.2158 | ds_loss: 0.0000 | lr: 4.4764e-05 | scale: 16384.0000 | micro time: 1.756 | step time: 0.000
train | epoch   2 | Iter:   2750/ 13000 | global iter:   2750/ 13000 | loss: 1.5467 | ds_loss: 0.0000 | lr: 4.4761e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2750/ 13000 | global iter:   2750/ 13000 | loss: 1.7005 | ds_loss: 0.0000 | lr: 4.4761e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 1.810
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2751/ 13000 | global iter:   2751/ 13000 | loss: 2.1630 | ds_loss: 0.0000 | lr: 4.4757e-05 | scale: 16384.0000 | micro time: 1.754 | step time: 0.000
train | epoch   2 | Iter:   2752/ 13000 | global iter:   2752/ 13000 | loss: 1.5938 | ds_loss: 0.0000 | lr: 4.4753e-05 | scale: 16384.0000 | micro time: 1.849 | step time: 0.000
train | epoch   2 | Iter:   2753/ 13000 | global iter:   2753/ 13000 | loss: 1.6559 | ds_loss: 0.0000 | lr: 4.4750e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   2 | Iter:   2754/ 13000 | global iter:   2754/ 13000 | loss: 2.1318 | ds_loss: 0.0000 | lr: 4.4746e-05 | scale: 16384.0000 | micro time: 1.719 | step time: 0.000
train | epoch   2 | Iter:   2755/ 13000 | global iter:   2755/ 13000 | loss: 2.1951 | ds_loss: 0.0000 | lr: 4.4742e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 0.000
train | epoch   2 | Iter:   2756/ 13000 | global iter:   2756/ 13000 | loss: 1.5129 | ds_loss: 0.0000 | lr: 4.4738e-05 | scale: 16384.0000 | micro time: 1.860 | step time: 0.000
train | epoch   2 | Iter:   2757/ 13000 | global iter:   2757/ 13000 | loss: 1.8316 | ds_loss: 0.0000 | lr: 4.4735e-05 | scale: 16384.0000 | micro time: 1.919 | step time: 0.000
train | epoch   2 | Iter:   2758/ 13000 | global iter:   2758/ 13000 | loss: 1.8287 | ds_loss: 0.0000 | lr: 4.4731e-05 | scale: 16384.0000 | micro time: 1.836 | step time: 0.000
train | epoch   2 | Iter:   2759/ 13000 | global iter:   2759/ 13000 | loss: 2.0415 | ds_loss: 0.0000 | lr: 4.4727e-05 | scale: 16384.0000 | micro time: 1.885 | step time: 0.000
train | epoch   2 | Iter:   2760/ 13000 | global iter:   2760/ 13000 | loss: 1.8959 | ds_loss: 0.0000 | lr: 4.4724e-05 | scale: 16384.0000 | micro time: 1.731 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2760/ 13000 | global iter:   2760/ 13000 | loss: 1.8850 | ds_loss: 0.0000 | lr: 4.4724e-05 | scale: 16384.0000 | micro time: 1.731 | step time: 1.826
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2761/ 13000 | global iter:   2761/ 13000 | loss: 1.4979 | ds_loss: 0.0000 | lr: 4.4720e-05 | scale: 16384.0000 | micro time: 1.846 | step time: 0.000
train | epoch   2 | Iter:   2762/ 13000 | global iter:   2762/ 13000 | loss: 1.3973 | ds_loss: 0.0000 | lr: 4.4716e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   2 | Iter:   2763/ 13000 | global iter:   2763/ 13000 | loss: 1.2070 | ds_loss: 0.0000 | lr: 4.4713e-05 | scale: 16384.0000 | micro time: 1.719 | step time: 0.000
train | epoch   2 | Iter:   2764/ 13000 | global iter:   2764/ 13000 | loss: 2.2103 | ds_loss: 0.0000 | lr: 4.4709e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   2 | Iter:   2765/ 13000 | global iter:   2765/ 13000 | loss: 1.9205 | ds_loss: 0.0000 | lr: 4.4705e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   2 | Iter:   2766/ 13000 | global iter:   2766/ 13000 | loss: 1.1437 | ds_loss: 0.0000 | lr: 4.4701e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   2 | Iter:   2767/ 13000 | global iter:   2767/ 13000 | loss: 1.7040 | ds_loss: 0.0000 | lr: 4.4698e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
train | epoch   2 | Iter:   2768/ 13000 | global iter:   2768/ 13000 | loss: 2.2358 | ds_loss: 0.0000 | lr: 4.4694e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   2 | Iter:   2769/ 13000 | global iter:   2769/ 13000 | loss: 1.9362 | ds_loss: 0.0000 | lr: 4.4690e-05 | scale: 16384.0000 | micro time: 1.927 | step time: 0.000
train | epoch   2 | Iter:   2770/ 13000 | global iter:   2770/ 13000 | loss: 1.3011 | ds_loss: 0.0000 | lr: 4.4687e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2770/ 13000 | global iter:   2770/ 13000 | loss: 1.6554 | ds_loss: 0.0000 | lr: 4.4687e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 1.824
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2771/ 13000 | global iter:   2771/ 13000 | loss: 1.8126 | ds_loss: 0.0000 | lr: 4.4683e-05 | scale: 16384.0000 | micro time: 1.730 | step time: 0.000
train | epoch   2 | Iter:   2772/ 13000 | global iter:   2772/ 13000 | loss: 1.8719 | ds_loss: 0.0000 | lr: 4.4679e-05 | scale: 16384.0000 | micro time: 1.793 | step time: 0.000
train | epoch   2 | Iter:   2773/ 13000 | global iter:   2773/ 13000 | loss: 1.7352 | ds_loss: 0.0000 | lr: 4.4675e-05 | scale: 16384.0000 | micro time: 1.826 | step time: 0.000
train | epoch   2 | Iter:   2774/ 13000 | global iter:   2774/ 13000 | loss: 1.7879 | ds_loss: 0.0000 | lr: 4.4672e-05 | scale: 16384.0000 | micro time: 1.738 | step time: 0.000
train | epoch   2 | Iter:   2775/ 13000 | global iter:   2775/ 13000 | loss: 1.2228 | ds_loss: 0.0000 | lr: 4.4668e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   2 | Iter:   2776/ 13000 | global iter:   2776/ 13000 | loss: 1.6427 | ds_loss: 0.0000 | lr: 4.4664e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   2 | Iter:   2777/ 13000 | global iter:   2777/ 13000 | loss: 1.6744 | ds_loss: 0.0000 | lr: 4.4660e-05 | scale: 16384.0000 | micro time: 1.741 | step time: 0.000
train | epoch   2 | Iter:   2778/ 13000 | global iter:   2778/ 13000 | loss: 1.5791 | ds_loss: 0.0000 | lr: 4.4657e-05 | scale: 16384.0000 | micro time: 1.789 | step time: 0.000
train | epoch   2 | Iter:   2779/ 13000 | global iter:   2779/ 13000 | loss: 1.9324 | ds_loss: 0.0000 | lr: 4.4653e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   2 | Iter:   2780/ 13000 | global iter:   2780/ 13000 | loss: 1.3175 | ds_loss: 0.0000 | lr: 4.4649e-05 | scale: 16384.0000 | micro time: 1.743 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2780/ 13000 | global iter:   2780/ 13000 | loss: 1.6577 | ds_loss: 0.0000 | lr: 4.4649e-05 | scale: 16384.0000 | micro time: 1.743 | step time: 1.777
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2781/ 13000 | global iter:   2781/ 13000 | loss: 2.0688 | ds_loss: 0.0000 | lr: 4.4646e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   2 | Iter:   2782/ 13000 | global iter:   2782/ 13000 | loss: 1.4938 | ds_loss: 0.0000 | lr: 4.4642e-05 | scale: 16384.0000 | micro time: 1.734 | step time: 0.000
train | epoch   2 | Iter:   2783/ 13000 | global iter:   2783/ 13000 | loss: 1.7609 | ds_loss: 0.0000 | lr: 4.4638e-05 | scale: 16384.0000 | micro time: 1.635 | step time: 0.000
train | epoch   2 | Iter:   2784/ 13000 | global iter:   2784/ 13000 | loss: 1.7924 | ds_loss: 0.0000 | lr: 4.4634e-05 | scale: 16384.0000 | micro time: 1.782 | step time: 0.000
train | epoch   2 | Iter:   2785/ 13000 | global iter:   2785/ 13000 | loss: 1.6289 | ds_loss: 0.0000 | lr: 4.4631e-05 | scale: 16384.0000 | micro time: 1.636 | step time: 0.000
train | epoch   2 | Iter:   2786/ 13000 | global iter:   2786/ 13000 | loss: 2.2913 | ds_loss: 0.0000 | lr: 4.4627e-05 | scale: 16384.0000 | micro time: 1.715 | step time: 0.000
train | epoch   2 | Iter:   2787/ 13000 | global iter:   2787/ 13000 | loss: 1.8363 | ds_loss: 0.0000 | lr: 4.4623e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   2 | Iter:   2788/ 13000 | global iter:   2788/ 13000 | loss: 1.3099 | ds_loss: 0.0000 | lr: 4.4619e-05 | scale: 16384.0000 | micro time: 1.840 | step time: 0.000
train | epoch   2 | Iter:   2789/ 13000 | global iter:   2789/ 13000 | loss: 1.9207 | ds_loss: 0.0000 | lr: 4.4616e-05 | scale: 16384.0000 | micro time: 1.898 | step time: 0.000
train | epoch   2 | Iter:   2790/ 13000 | global iter:   2790/ 13000 | loss: 1.5495 | ds_loss: 0.0000 | lr: 4.4612e-05 | scale: 16384.0000 | micro time: 1.861 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2790/ 13000 | global iter:   2790/ 13000 | loss: 1.7652 | ds_loss: 0.0000 | lr: 4.4612e-05 | scale: 16384.0000 | micro time: 1.861 | step time: 1.773
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2791/ 13000 | global iter:   2791/ 13000 | loss: 1.8066 | ds_loss: 0.0000 | lr: 4.4608e-05 | scale: 16384.0000 | micro time: 1.830 | step time: 0.000
train | epoch   2 | Iter:   2792/ 13000 | global iter:   2792/ 13000 | loss: 2.1548 | ds_loss: 0.0000 | lr: 4.4604e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   2 | Iter:   2793/ 13000 | global iter:   2793/ 13000 | loss: 2.0999 | ds_loss: 0.0000 | lr: 4.4601e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   2 | Iter:   2794/ 13000 | global iter:   2794/ 13000 | loss: 1.5703 | ds_loss: 0.0000 | lr: 4.4597e-05 | scale: 16384.0000 | micro time: 1.894 | step time: 0.000
train | epoch   2 | Iter:   2795/ 13000 | global iter:   2795/ 13000 | loss: 1.9457 | ds_loss: 0.0000 | lr: 4.4593e-05 | scale: 16384.0000 | micro time: 1.902 | step time: 0.000
train | epoch   2 | Iter:   2796/ 13000 | global iter:   2796/ 13000 | loss: 1.9287 | ds_loss: 0.0000 | lr: 4.4589e-05 | scale: 16384.0000 | micro time: 1.770 | step time: 0.000
train | epoch   2 | Iter:   2797/ 13000 | global iter:   2797/ 13000 | loss: 1.9912 | ds_loss: 0.0000 | lr: 4.4586e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   2 | Iter:   2798/ 13000 | global iter:   2798/ 13000 | loss: 1.8671 | ds_loss: 0.0000 | lr: 4.4582e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   2 | Iter:   2799/ 13000 | global iter:   2799/ 13000 | loss: 2.3331 | ds_loss: 0.0000 | lr: 4.4578e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   2 | Iter:   2800/ 13000 | global iter:   2800/ 13000 | loss: 1.1269 | ds_loss: 0.0000 | lr: 4.4574e-05 | scale: 16384.0000 | micro time: 1.927 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2800/ 13000 | global iter:   2800/ 13000 | loss: 1.8824 | ds_loss: 0.0000 | lr: 4.4574e-05 | scale: 16384.0000 | micro time: 1.927 | step time: 1.833
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2801/ 13000 | global iter:   2801/ 13000 | loss: 1.8431 | ds_loss: 0.0000 | lr: 4.4571e-05 | scale: 16384.0000 | micro time: 1.794 | step time: 0.000
train | epoch   2 | Iter:   2802/ 13000 | global iter:   2802/ 13000 | loss: 1.7570 | ds_loss: 0.0000 | lr: 4.4567e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   2 | Iter:   2803/ 13000 | global iter:   2803/ 13000 | loss: 1.4912 | ds_loss: 0.0000 | lr: 4.4563e-05 | scale: 16384.0000 | micro time: 1.708 | step time: 0.000
train | epoch   2 | Iter:   2804/ 13000 | global iter:   2804/ 13000 | loss: 1.8887 | ds_loss: 0.0000 | lr: 4.4559e-05 | scale: 16384.0000 | micro time: 1.730 | step time: 0.000
train | epoch   2 | Iter:   2805/ 13000 | global iter:   2805/ 13000 | loss: 2.2959 | ds_loss: 0.0000 | lr: 4.4556e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   2 | Iter:   2806/ 13000 | global iter:   2806/ 13000 | loss: 1.7712 | ds_loss: 0.0000 | lr: 4.4552e-05 | scale: 16384.0000 | micro time: 1.838 | step time: 0.000
train | epoch   2 | Iter:   2807/ 13000 | global iter:   2807/ 13000 | loss: 1.9519 | ds_loss: 0.0000 | lr: 4.4548e-05 | scale: 16384.0000 | micro time: 1.727 | step time: 0.000
train | epoch   2 | Iter:   2808/ 13000 | global iter:   2808/ 13000 | loss: 1.4199 | ds_loss: 0.0000 | lr: 4.4544e-05 | scale: 16384.0000 | micro time: 1.732 | step time: 0.000
train | epoch   2 | Iter:   2809/ 13000 | global iter:   2809/ 13000 | loss: 1.3081 | ds_loss: 0.0000 | lr: 4.4541e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   2 | Iter:   2810/ 13000 | global iter:   2810/ 13000 | loss: 1.7834 | ds_loss: 0.0000 | lr: 4.4537e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2810/ 13000 | global iter:   2810/ 13000 | loss: 1.7510 | ds_loss: 0.0000 | lr: 4.4537e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 1.768
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2811/ 13000 | global iter:   2811/ 13000 | loss: 1.8107 | ds_loss: 0.0000 | lr: 4.4533e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   2 | Iter:   2812/ 13000 | global iter:   2812/ 13000 | loss: 1.7623 | ds_loss: 0.0000 | lr: 4.4529e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
train | epoch   2 | Iter:   2813/ 13000 | global iter:   2813/ 13000 | loss: 1.3947 | ds_loss: 0.0000 | lr: 4.4526e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   2 | Iter:   2814/ 13000 | global iter:   2814/ 13000 | loss: 2.1084 | ds_loss: 0.0000 | lr: 4.4522e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   2 | Iter:   2815/ 13000 | global iter:   2815/ 13000 | loss: 2.2578 | ds_loss: 0.0000 | lr: 4.4518e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   2 | Iter:   2816/ 13000 | global iter:   2816/ 13000 | loss: 2.2103 | ds_loss: 0.0000 | lr: 4.4514e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   2 | Iter:   2817/ 13000 | global iter:   2817/ 13000 | loss: 1.8509 | ds_loss: 0.0000 | lr: 4.4510e-05 | scale: 16384.0000 | micro time: 1.903 | step time: 0.000
train | epoch   2 | Iter:   2818/ 13000 | global iter:   2818/ 13000 | loss: 2.4301 | ds_loss: 0.0000 | lr: 4.4507e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   2 | Iter:   2819/ 13000 | global iter:   2819/ 13000 | loss: 1.1783 | ds_loss: 0.0000 | lr: 4.4503e-05 | scale: 16384.0000 | micro time: 1.689 | step time: 0.000
train | epoch   2 | Iter:   2820/ 13000 | global iter:   2820/ 13000 | loss: 2.0952 | ds_loss: 0.0000 | lr: 4.4499e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2820/ 13000 | global iter:   2820/ 13000 | loss: 1.9099 | ds_loss: 0.0000 | lr: 4.4499e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 1.832
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2821/ 13000 | global iter:   2821/ 13000 | loss: 1.8215 | ds_loss: 0.0000 | lr: 4.4495e-05 | scale: 16384.0000 | micro time: 1.734 | step time: 0.000
train | epoch   2 | Iter:   2822/ 13000 | global iter:   2822/ 13000 | loss: 1.0475 | ds_loss: 0.0000 | lr: 4.4492e-05 | scale: 16384.0000 | micro time: 1.840 | step time: 0.000
train | epoch   2 | Iter:   2823/ 13000 | global iter:   2823/ 13000 | loss: 1.7001 | ds_loss: 0.0000 | lr: 4.4488e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   2 | Iter:   2824/ 13000 | global iter:   2824/ 13000 | loss: 1.6669 | ds_loss: 0.0000 | lr: 4.4484e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   2 | Iter:   2825/ 13000 | global iter:   2825/ 13000 | loss: 2.2239 | ds_loss: 0.0000 | lr: 4.4480e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   2 | Iter:   2826/ 13000 | global iter:   2826/ 13000 | loss: 1.7295 | ds_loss: 0.0000 | lr: 4.4476e-05 | scale: 16384.0000 | micro time: 1.742 | step time: 0.000
train | epoch   2 | Iter:   2827/ 13000 | global iter:   2827/ 13000 | loss: 2.3038 | ds_loss: 0.0000 | lr: 4.4473e-05 | scale: 16384.0000 | micro time: 1.764 | step time: 0.000
train | epoch   2 | Iter:   2828/ 13000 | global iter:   2828/ 13000 | loss: 1.7410 | ds_loss: 0.0000 | lr: 4.4469e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   2 | Iter:   2829/ 13000 | global iter:   2829/ 13000 | loss: 1.9681 | ds_loss: 0.0000 | lr: 4.4465e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   2 | Iter:   2830/ 13000 | global iter:   2830/ 13000 | loss: 1.6040 | ds_loss: 0.0000 | lr: 4.4461e-05 | scale: 16384.0000 | micro time: 1.793 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2830/ 13000 | global iter:   2830/ 13000 | loss: 1.7807 | ds_loss: 0.0000 | lr: 4.4461e-05 | scale: 16384.0000 | micro time: 1.793 | step time: 1.796
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2831/ 13000 | global iter:   2831/ 13000 | loss: 1.8779 | ds_loss: 0.0000 | lr: 4.4457e-05 | scale: 16384.0000 | micro time: 1.856 | step time: 0.000
train | epoch   2 | Iter:   2832/ 13000 | global iter:   2832/ 13000 | loss: 1.8188 | ds_loss: 0.0000 | lr: 4.4454e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   2 | Iter:   2833/ 13000 | global iter:   2833/ 13000 | loss: 1.9288 | ds_loss: 0.0000 | lr: 4.4450e-05 | scale: 16384.0000 | micro time: 1.899 | step time: 0.000
train | epoch   2 | Iter:   2834/ 13000 | global iter:   2834/ 13000 | loss: 1.8361 | ds_loss: 0.0000 | lr: 4.4446e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   2 | Iter:   2835/ 13000 | global iter:   2835/ 13000 | loss: 2.0529 | ds_loss: 0.0000 | lr: 4.4442e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   2 | Iter:   2836/ 13000 | global iter:   2836/ 13000 | loss: 1.3928 | ds_loss: 0.0000 | lr: 4.4439e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   2 | Iter:   2837/ 13000 | global iter:   2837/ 13000 | loss: 1.6947 | ds_loss: 0.0000 | lr: 4.4435e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   2 | Iter:   2838/ 13000 | global iter:   2838/ 13000 | loss: 1.7432 | ds_loss: 0.0000 | lr: 4.4431e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 0.000
train | epoch   2 | Iter:   2839/ 13000 | global iter:   2839/ 13000 | loss: 1.9270 | ds_loss: 0.0000 | lr: 4.4427e-05 | scale: 16384.0000 | micro time: 1.923 | step time: 0.000
train | epoch   2 | Iter:   2840/ 13000 | global iter:   2840/ 13000 | loss: 2.0198 | ds_loss: 0.0000 | lr: 4.4423e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2840/ 13000 | global iter:   2840/ 13000 | loss: 1.8292 | ds_loss: 0.0000 | lr: 4.4423e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 1.827
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2841/ 13000 | global iter:   2841/ 13000 | loss: 1.9365 | ds_loss: 0.0000 | lr: 4.4420e-05 | scale: 16384.0000 | micro time: 1.757 | step time: 0.000
train | epoch   2 | Iter:   2842/ 13000 | global iter:   2842/ 13000 | loss: 1.5160 | ds_loss: 0.0000 | lr: 4.4416e-05 | scale: 16384.0000 | micro time: 1.671 | step time: 0.000
train | epoch   2 | Iter:   2843/ 13000 | global iter:   2843/ 13000 | loss: 2.0963 | ds_loss: 0.0000 | lr: 4.4412e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 0.000
train | epoch   2 | Iter:   2844/ 13000 | global iter:   2844/ 13000 | loss: 2.1318 | ds_loss: 0.0000 | lr: 4.4408e-05 | scale: 16384.0000 | micro time: 1.810 | step time: 0.000
train | epoch   2 | Iter:   2845/ 13000 | global iter:   2845/ 13000 | loss: 2.1985 | ds_loss: 0.0000 | lr: 4.4404e-05 | scale: 16384.0000 | micro time: 1.802 | step time: 0.000
train | epoch   2 | Iter:   2846/ 13000 | global iter:   2846/ 13000 | loss: 1.2783 | ds_loss: 0.0000 | lr: 4.4401e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 0.000
train | epoch   2 | Iter:   2847/ 13000 | global iter:   2847/ 13000 | loss: 1.6612 | ds_loss: 0.0000 | lr: 4.4397e-05 | scale: 16384.0000 | micro time: 1.759 | step time: 0.000
train | epoch   2 | Iter:   2848/ 13000 | global iter:   2848/ 13000 | loss: 2.4280 | ds_loss: 0.0000 | lr: 4.4393e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   2 | Iter:   2849/ 13000 | global iter:   2849/ 13000 | loss: 1.2391 | ds_loss: 0.0000 | lr: 4.4389e-05 | scale: 16384.0000 | micro time: 1.762 | step time: 0.000
train | epoch   2 | Iter:   2850/ 13000 | global iter:   2850/ 13000 | loss: 1.9433 | ds_loss: 0.0000 | lr: 4.4385e-05 | scale: 16384.0000 | micro time: 1.853 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2850/ 13000 | global iter:   2850/ 13000 | loss: 1.8429 | ds_loss: 0.0000 | lr: 4.4385e-05 | scale: 16384.0000 | micro time: 1.853 | step time: 1.772
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2851/ 13000 | global iter:   2851/ 13000 | loss: 1.4811 | ds_loss: 0.0000 | lr: 4.4381e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   2 | Iter:   2852/ 13000 | global iter:   2852/ 13000 | loss: 2.0040 | ds_loss: 0.0000 | lr: 4.4378e-05 | scale: 16384.0000 | micro time: 1.792 | step time: 0.000
train | epoch   2 | Iter:   2853/ 13000 | global iter:   2853/ 13000 | loss: 1.9626 | ds_loss: 0.0000 | lr: 4.4374e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   2 | Iter:   2854/ 13000 | global iter:   2854/ 13000 | loss: 1.3595 | ds_loss: 0.0000 | lr: 4.4370e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   2 | Iter:   2855/ 13000 | global iter:   2855/ 13000 | loss: 1.7886 | ds_loss: 0.0000 | lr: 4.4366e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   2 | Iter:   2856/ 13000 | global iter:   2856/ 13000 | loss: 1.4521 | ds_loss: 0.0000 | lr: 4.4362e-05 | scale: 16384.0000 | micro time: 1.707 | step time: 0.000
train | epoch   2 | Iter:   2857/ 13000 | global iter:   2857/ 13000 | loss: 1.9332 | ds_loss: 0.0000 | lr: 4.4359e-05 | scale: 16384.0000 | micro time: 1.879 | step time: 0.000
train | epoch   2 | Iter:   2858/ 13000 | global iter:   2858/ 13000 | loss: 2.3055 | ds_loss: 0.0000 | lr: 4.4355e-05 | scale: 16384.0000 | micro time: 1.840 | step time: 0.000
train | epoch   2 | Iter:   2859/ 13000 | global iter:   2859/ 13000 | loss: 2.1143 | ds_loss: 0.0000 | lr: 4.4351e-05 | scale: 16384.0000 | micro time: 1.816 | step time: 0.000
train | epoch   2 | Iter:   2860/ 13000 | global iter:   2860/ 13000 | loss: 2.0709 | ds_loss: 0.0000 | lr: 4.4347e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2860/ 13000 | global iter:   2860/ 13000 | loss: 1.8472 | ds_loss: 0.0000 | lr: 4.4347e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 1.791
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2861/ 13000 | global iter:   2861/ 13000 | loss: 2.1448 | ds_loss: 0.0000 | lr: 4.4343e-05 | scale: 16384.0000 | micro time: 1.738 | step time: 0.000
train | epoch   2 | Iter:   2862/ 13000 | global iter:   2862/ 13000 | loss: 1.8133 | ds_loss: 0.0000 | lr: 4.4339e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   2 | Iter:   2863/ 13000 | global iter:   2863/ 13000 | loss: 1.7967 | ds_loss: 0.0000 | lr: 4.4336e-05 | scale: 16384.0000 | micro time: 1.781 | step time: 0.000
train | epoch   2 | Iter:   2864/ 13000 | global iter:   2864/ 13000 | loss: 1.9216 | ds_loss: 0.0000 | lr: 4.4332e-05 | scale: 16384.0000 | micro time: 1.820 | step time: 0.000
train | epoch   2 | Iter:   2865/ 13000 | global iter:   2865/ 13000 | loss: 2.6569 | ds_loss: 0.0000 | lr: 4.4328e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   2 | Iter:   2866/ 13000 | global iter:   2866/ 13000 | loss: 1.7897 | ds_loss: 0.0000 | lr: 4.4324e-05 | scale: 16384.0000 | micro time: 1.758 | step time: 0.000
train | epoch   2 | Iter:   2867/ 13000 | global iter:   2867/ 13000 | loss: 1.9081 | ds_loss: 0.0000 | lr: 4.4320e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   2 | Iter:   2868/ 13000 | global iter:   2868/ 13000 | loss: 1.5679 | ds_loss: 0.0000 | lr: 4.4317e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   2 | Iter:   2869/ 13000 | global iter:   2869/ 13000 | loss: 1.4282 | ds_loss: 0.0000 | lr: 4.4313e-05 | scale: 16384.0000 | micro time: 1.828 | step time: 0.000
train | epoch   2 | Iter:   2870/ 13000 | global iter:   2870/ 13000 | loss: 2.0658 | ds_loss: 0.0000 | lr: 4.4309e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2870/ 13000 | global iter:   2870/ 13000 | loss: 1.9093 | ds_loss: 0.0000 | lr: 4.4309e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 1.816
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2871/ 13000 | global iter:   2871/ 13000 | loss: 1.9699 | ds_loss: 0.0000 | lr: 4.4305e-05 | scale: 16384.0000 | micro time: 1.678 | step time: 0.000
train | epoch   2 | Iter:   2872/ 13000 | global iter:   2872/ 13000 | loss: 2.1421 | ds_loss: 0.0000 | lr: 4.4301e-05 | scale: 16384.0000 | micro time: 1.695 | step time: 0.000
train | epoch   2 | Iter:   2873/ 13000 | global iter:   2873/ 13000 | loss: 2.0443 | ds_loss: 0.0000 | lr: 4.4297e-05 | scale: 16384.0000 | micro time: 1.830 | step time: 0.000
train | epoch   2 | Iter:   2874/ 13000 | global iter:   2874/ 13000 | loss: 2.2339 | ds_loss: 0.0000 | lr: 4.4294e-05 | scale: 16384.0000 | micro time: 1.800 | step time: 0.000
train | epoch   2 | Iter:   2875/ 13000 | global iter:   2875/ 13000 | loss: 2.1080 | ds_loss: 0.0000 | lr: 4.4290e-05 | scale: 16384.0000 | micro time: 1.731 | step time: 0.000
train | epoch   2 | Iter:   2876/ 13000 | global iter:   2876/ 13000 | loss: 1.7471 | ds_loss: 0.0000 | lr: 4.4286e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   2 | Iter:   2877/ 13000 | global iter:   2877/ 13000 | loss: 2.0230 | ds_loss: 0.0000 | lr: 4.4282e-05 | scale: 16384.0000 | micro time: 1.906 | step time: 0.000
train | epoch   2 | Iter:   2878/ 13000 | global iter:   2878/ 13000 | loss: 2.1613 | ds_loss: 0.0000 | lr: 4.4278e-05 | scale: 16384.0000 | micro time: 1.924 | step time: 0.000
train | epoch   2 | Iter:   2879/ 13000 | global iter:   2879/ 13000 | loss: 1.9309 | ds_loss: 0.0000 | lr: 4.4274e-05 | scale: 16384.0000 | micro time: 1.884 | step time: 0.000
train | epoch   2 | Iter:   2880/ 13000 | global iter:   2880/ 13000 | loss: 1.6504 | ds_loss: 0.0000 | lr: 4.4270e-05 | scale: 16384.0000 | micro time: 1.900 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2880/ 13000 | global iter:   2880/ 13000 | loss: 2.0011 | ds_loss: 0.0000 | lr: 4.4270e-05 | scale: 16384.0000 | micro time: 1.900 | step time: 1.813
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2881/ 13000 | global iter:   2881/ 13000 | loss: 1.1774 | ds_loss: 0.0000 | lr: 4.4267e-05 | scale: 16384.0000 | micro time: 1.830 | step time: 0.000
train | epoch   2 | Iter:   2882/ 13000 | global iter:   2882/ 13000 | loss: 2.1442 | ds_loss: 0.0000 | lr: 4.4263e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   2 | Iter:   2883/ 13000 | global iter:   2883/ 13000 | loss: 1.5617 | ds_loss: 0.0000 | lr: 4.4259e-05 | scale: 16384.0000 | micro time: 1.768 | step time: 0.000
train | epoch   2 | Iter:   2884/ 13000 | global iter:   2884/ 13000 | loss: 1.6919 | ds_loss: 0.0000 | lr: 4.4255e-05 | scale: 16384.0000 | micro time: 1.671 | step time: 0.000
train | epoch   2 | Iter:   2885/ 13000 | global iter:   2885/ 13000 | loss: 1.6441 | ds_loss: 0.0000 | lr: 4.4251e-05 | scale: 16384.0000 | micro time: 1.842 | step time: 0.000
train | epoch   2 | Iter:   2886/ 13000 | global iter:   2886/ 13000 | loss: 1.7400 | ds_loss: 0.0000 | lr: 4.4247e-05 | scale: 16384.0000 | micro time: 1.814 | step time: 0.000
train | epoch   2 | Iter:   2887/ 13000 | global iter:   2887/ 13000 | loss: 1.5230 | ds_loss: 0.0000 | lr: 4.4244e-05 | scale: 16384.0000 | micro time: 1.928 | step time: 0.000
train | epoch   2 | Iter:   2888/ 13000 | global iter:   2888/ 13000 | loss: 1.5477 | ds_loss: 0.0000 | lr: 4.4240e-05 | scale: 16384.0000 | micro time: 1.785 | step time: 0.000
train | epoch   2 | Iter:   2889/ 13000 | global iter:   2889/ 13000 | loss: 2.1113 | ds_loss: 0.0000 | lr: 4.4236e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   2 | Iter:   2890/ 13000 | global iter:   2890/ 13000 | loss: 2.2058 | ds_loss: 0.0000 | lr: 4.4232e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2890/ 13000 | global iter:   2890/ 13000 | loss: 1.7347 | ds_loss: 0.0000 | lr: 4.4232e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 1.800
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2891/ 13000 | global iter:   2891/ 13000 | loss: 2.0579 | ds_loss: 0.0000 | lr: 4.4228e-05 | scale: 16384.0000 | micro time: 1.840 | step time: 0.000
train | epoch   2 | Iter:   2892/ 13000 | global iter:   2892/ 13000 | loss: 1.5674 | ds_loss: 0.0000 | lr: 4.4224e-05 | scale: 16384.0000 | micro time: 1.683 | step time: 0.000
train | epoch   2 | Iter:   2893/ 13000 | global iter:   2893/ 13000 | loss: 1.7125 | ds_loss: 0.0000 | lr: 4.4220e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   2 | Iter:   2894/ 13000 | global iter:   2894/ 13000 | loss: 2.1279 | ds_loss: 0.0000 | lr: 4.4217e-05 | scale: 16384.0000 | micro time: 1.711 | step time: 0.000
train | epoch   2 | Iter:   2895/ 13000 | global iter:   2895/ 13000 | loss: 1.3539 | ds_loss: 0.0000 | lr: 4.4213e-05 | scale: 16384.0000 | micro time: 1.738 | step time: 0.000
train | epoch   2 | Iter:   2896/ 13000 | global iter:   2896/ 13000 | loss: 1.5938 | ds_loss: 0.0000 | lr: 4.4209e-05 | scale: 16384.0000 | micro time: 1.824 | step time: 0.000
train | epoch   2 | Iter:   2897/ 13000 | global iter:   2897/ 13000 | loss: 2.2774 | ds_loss: 0.0000 | lr: 4.4205e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   2 | Iter:   2898/ 13000 | global iter:   2898/ 13000 | loss: 2.0961 | ds_loss: 0.0000 | lr: 4.4201e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   2 | Iter:   2899/ 13000 | global iter:   2899/ 13000 | loss: 1.7189 | ds_loss: 0.0000 | lr: 4.4197e-05 | scale: 16384.0000 | micro time: 1.830 | step time: 0.000
train | epoch   2 | Iter:   2900/ 13000 | global iter:   2900/ 13000 | loss: 1.7688 | ds_loss: 0.0000 | lr: 4.4193e-05 | scale: 16384.0000 | micro time: 1.856 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2900/ 13000 | global iter:   2900/ 13000 | loss: 1.8275 | ds_loss: 0.0000 | lr: 4.4193e-05 | scale: 16384.0000 | micro time: 1.856 | step time: 1.781
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2901/ 13000 | global iter:   2901/ 13000 | loss: 2.0786 | ds_loss: 0.0000 | lr: 4.4189e-05 | scale: 16384.0000 | micro time: 1.892 | step time: 0.000
train | epoch   2 | Iter:   2902/ 13000 | global iter:   2902/ 13000 | loss: 1.5644 | ds_loss: 0.0000 | lr: 4.4186e-05 | scale: 16384.0000 | micro time: 1.832 | step time: 0.000
train | epoch   2 | Iter:   2903/ 13000 | global iter:   2903/ 13000 | loss: 2.2737 | ds_loss: 0.0000 | lr: 4.4182e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   2 | Iter:   2904/ 13000 | global iter:   2904/ 13000 | loss: 1.9216 | ds_loss: 0.0000 | lr: 4.4178e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   2 | Iter:   2905/ 13000 | global iter:   2905/ 13000 | loss: 1.3986 | ds_loss: 0.0000 | lr: 4.4174e-05 | scale: 16384.0000 | micro time: 1.826 | step time: 0.000
train | epoch   2 | Iter:   2906/ 13000 | global iter:   2906/ 13000 | loss: 1.9385 | ds_loss: 0.0000 | lr: 4.4170e-05 | scale: 16384.0000 | micro time: 1.887 | step time: 0.000
train | epoch   2 | Iter:   2907/ 13000 | global iter:   2907/ 13000 | loss: 1.2932 | ds_loss: 0.0000 | lr: 4.4166e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   2 | Iter:   2908/ 13000 | global iter:   2908/ 13000 | loss: 2.3308 | ds_loss: 0.0000 | lr: 4.4162e-05 | scale: 16384.0000 | micro time: 1.692 | step time: 0.000
train | epoch   2 | Iter:   2909/ 13000 | global iter:   2909/ 13000 | loss: 2.0439 | ds_loss: 0.0000 | lr: 4.4159e-05 | scale: 16384.0000 | micro time: 1.812 | step time: 0.000
train | epoch   2 | Iter:   2910/ 13000 | global iter:   2910/ 13000 | loss: 1.8456 | ds_loss: 0.0000 | lr: 4.4155e-05 | scale: 16384.0000 | micro time: 1.702 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2910/ 13000 | global iter:   2910/ 13000 | loss: 1.8689 | ds_loss: 0.0000 | lr: 4.4155e-05 | scale: 16384.0000 | micro time: 1.702 | step time: 1.809
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2911/ 13000 | global iter:   2911/ 13000 | loss: 2.2614 | ds_loss: 0.0000 | lr: 4.4151e-05 | scale: 16384.0000 | micro time: 1.798 | step time: 0.000
train | epoch   2 | Iter:   2912/ 13000 | global iter:   2912/ 13000 | loss: 2.2065 | ds_loss: 0.0000 | lr: 4.4147e-05 | scale: 16384.0000 | micro time: 1.768 | step time: 0.000
train | epoch   2 | Iter:   2913/ 13000 | global iter:   2913/ 13000 | loss: 2.0253 | ds_loss: 0.0000 | lr: 4.4143e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   2 | Iter:   2914/ 13000 | global iter:   2914/ 13000 | loss: 1.9816 | ds_loss: 0.0000 | lr: 4.4139e-05 | scale: 16384.0000 | micro time: 1.711 | step time: 0.000
train | epoch   2 | Iter:   2915/ 13000 | global iter:   2915/ 13000 | loss: 1.6232 | ds_loss: 0.0000 | lr: 4.4135e-05 | scale: 16384.0000 | micro time: 1.712 | step time: 0.000
train | epoch   2 | Iter:   2916/ 13000 | global iter:   2916/ 13000 | loss: 1.2891 | ds_loss: 0.0000 | lr: 4.4131e-05 | scale: 16384.0000 | micro time: 1.830 | step time: 0.000
train | epoch   2 | Iter:   2917/ 13000 | global iter:   2917/ 13000 | loss: 2.3635 | ds_loss: 0.0000 | lr: 4.4127e-05 | scale: 16384.0000 | micro time: 1.740 | step time: 0.000
train | epoch   2 | Iter:   2918/ 13000 | global iter:   2918/ 13000 | loss: 1.7979 | ds_loss: 0.0000 | lr: 4.4124e-05 | scale: 16384.0000 | micro time: 1.885 | step time: 0.000
train | epoch   2 | Iter:   2919/ 13000 | global iter:   2919/ 13000 | loss: 1.3393 | ds_loss: 0.0000 | lr: 4.4120e-05 | scale: 16384.0000 | micro time: 1.834 | step time: 0.000
train | epoch   2 | Iter:   2920/ 13000 | global iter:   2920/ 13000 | loss: 1.1909 | ds_loss: 0.0000 | lr: 4.4116e-05 | scale: 16384.0000 | micro time: 1.856 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2920/ 13000 | global iter:   2920/ 13000 | loss: 1.8079 | ds_loss: 0.0000 | lr: 4.4116e-05 | scale: 16384.0000 | micro time: 1.856 | step time: 1.799
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2921/ 13000 | global iter:   2921/ 13000 | loss: 1.6814 | ds_loss: 0.0000 | lr: 4.4112e-05 | scale: 16384.0000 | micro time: 1.724 | step time: 0.000
train | epoch   2 | Iter:   2922/ 13000 | global iter:   2922/ 13000 | loss: 1.9268 | ds_loss: 0.0000 | lr: 4.4108e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   2 | Iter:   2923/ 13000 | global iter:   2923/ 13000 | loss: 1.5035 | ds_loss: 0.0000 | lr: 4.4104e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   2 | Iter:   2924/ 13000 | global iter:   2924/ 13000 | loss: 1.7790 | ds_loss: 0.0000 | lr: 4.4100e-05 | scale: 16384.0000 | micro time: 1.784 | step time: 0.000
train | epoch   2 | Iter:   2925/ 13000 | global iter:   2925/ 13000 | loss: 1.4271 | ds_loss: 0.0000 | lr: 4.4096e-05 | scale: 16384.0000 | micro time: 1.786 | step time: 0.000
train | epoch   2 | Iter:   2926/ 13000 | global iter:   2926/ 13000 | loss: 2.2245 | ds_loss: 0.0000 | lr: 4.4092e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   2 | Iter:   2927/ 13000 | global iter:   2927/ 13000 | loss: 1.5981 | ds_loss: 0.0000 | lr: 4.4089e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   2 | Iter:   2928/ 13000 | global iter:   2928/ 13000 | loss: 1.9994 | ds_loss: 0.0000 | lr: 4.4085e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   2 | Iter:   2929/ 13000 | global iter:   2929/ 13000 | loss: 1.7696 | ds_loss: 0.0000 | lr: 4.4081e-05 | scale: 16384.0000 | micro time: 1.786 | step time: 0.000
train | epoch   2 | Iter:   2930/ 13000 | global iter:   2930/ 13000 | loss: 2.0823 | ds_loss: 0.0000 | lr: 4.4077e-05 | scale: 16384.0000 | micro time: 1.862 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2930/ 13000 | global iter:   2930/ 13000 | loss: 1.7992 | ds_loss: 0.0000 | lr: 4.4077e-05 | scale: 16384.0000 | micro time: 1.862 | step time: 1.801
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2931/ 13000 | global iter:   2931/ 13000 | loss: 1.6566 | ds_loss: 0.0000 | lr: 4.4073e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   2 | Iter:   2932/ 13000 | global iter:   2932/ 13000 | loss: 2.2228 | ds_loss: 0.0000 | lr: 4.4069e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   2 | Iter:   2933/ 13000 | global iter:   2933/ 13000 | loss: 2.3502 | ds_loss: 0.0000 | lr: 4.4065e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
train | epoch   2 | Iter:   2934/ 13000 | global iter:   2934/ 13000 | loss: 2.0716 | ds_loss: 0.0000 | lr: 4.4061e-05 | scale: 16384.0000 | micro time: 1.885 | step time: 0.000
train | epoch   2 | Iter:   2935/ 13000 | global iter:   2935/ 13000 | loss: 1.5364 | ds_loss: 0.0000 | lr: 4.4057e-05 | scale: 16384.0000 | micro time: 1.889 | step time: 0.000
train | epoch   2 | Iter:   2936/ 13000 | global iter:   2936/ 13000 | loss: 1.2635 | ds_loss: 0.0000 | lr: 4.4053e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   2 | Iter:   2937/ 13000 | global iter:   2937/ 13000 | loss: 1.8228 | ds_loss: 0.0000 | lr: 4.4050e-05 | scale: 16384.0000 | micro time: 1.818 | step time: 0.000
train | epoch   2 | Iter:   2938/ 13000 | global iter:   2938/ 13000 | loss: 2.1793 | ds_loss: 0.0000 | lr: 4.4046e-05 | scale: 16384.0000 | micro time: 1.768 | step time: 0.000
train | epoch   2 | Iter:   2939/ 13000 | global iter:   2939/ 13000 | loss: 1.5777 | ds_loss: 0.0000 | lr: 4.4042e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   2 | Iter:   2940/ 13000 | global iter:   2940/ 13000 | loss: 2.0533 | ds_loss: 0.0000 | lr: 4.4038e-05 | scale: 16384.0000 | micro time: 1.653 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2940/ 13000 | global iter:   2940/ 13000 | loss: 1.8734 | ds_loss: 0.0000 | lr: 4.4038e-05 | scale: 16384.0000 | micro time: 1.653 | step time: 1.820
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2941/ 13000 | global iter:   2941/ 13000 | loss: 2.1509 | ds_loss: 0.0000 | lr: 4.4034e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   2 | Iter:   2942/ 13000 | global iter:   2942/ 13000 | loss: 1.8173 | ds_loss: 0.0000 | lr: 4.4030e-05 | scale: 16384.0000 | micro time: 1.865 | step time: 0.000
train | epoch   2 | Iter:   2943/ 13000 | global iter:   2943/ 13000 | loss: 1.9042 | ds_loss: 0.0000 | lr: 4.4026e-05 | scale: 16384.0000 | micro time: 1.956 | step time: 0.000
train | epoch   2 | Iter:   2944/ 13000 | global iter:   2944/ 13000 | loss: 1.5466 | ds_loss: 0.0000 | lr: 4.4022e-05 | scale: 16384.0000 | micro time: 1.777 | step time: 0.000
train | epoch   2 | Iter:   2945/ 13000 | global iter:   2945/ 13000 | loss: 1.9119 | ds_loss: 0.0000 | lr: 4.4018e-05 | scale: 16384.0000 | micro time: 1.841 | step time: 0.000
train | epoch   2 | Iter:   2946/ 13000 | global iter:   2946/ 13000 | loss: 1.7069 | ds_loss: 0.0000 | lr: 4.4014e-05 | scale: 16384.0000 | micro time: 1.716 | step time: 0.000
train | epoch   2 | Iter:   2947/ 13000 | global iter:   2947/ 13000 | loss: 1.4160 | ds_loss: 0.0000 | lr: 4.4010e-05 | scale: 16384.0000 | micro time: 1.853 | step time: 0.000
train | epoch   2 | Iter:   2948/ 13000 | global iter:   2948/ 13000 | loss: 2.3155 | ds_loss: 0.0000 | lr: 4.4006e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   2 | Iter:   2949/ 13000 | global iter:   2949/ 13000 | loss: 1.8932 | ds_loss: 0.0000 | lr: 4.4003e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   2 | Iter:   2950/ 13000 | global iter:   2950/ 13000 | loss: 1.4973 | ds_loss: 0.0000 | lr: 4.3999e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2950/ 13000 | global iter:   2950/ 13000 | loss: 1.8160 | ds_loss: 0.0000 | lr: 4.3999e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 1.816
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2951/ 13000 | global iter:   2951/ 13000 | loss: 1.4701 | ds_loss: 0.0000 | lr: 4.3995e-05 | scale: 16384.0000 | micro time: 1.866 | step time: 0.000
train | epoch   2 | Iter:   2952/ 13000 | global iter:   2952/ 13000 | loss: 2.0407 | ds_loss: 0.0000 | lr: 4.3991e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   2 | Iter:   2953/ 13000 | global iter:   2953/ 13000 | loss: 2.0298 | ds_loss: 0.0000 | lr: 4.3987e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   2 | Iter:   2954/ 13000 | global iter:   2954/ 13000 | loss: 1.7631 | ds_loss: 0.0000 | lr: 4.3983e-05 | scale: 16384.0000 | micro time: 1.911 | step time: 0.000
train | epoch   2 | Iter:   2955/ 13000 | global iter:   2955/ 13000 | loss: 1.1496 | ds_loss: 0.0000 | lr: 4.3979e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   2 | Iter:   2956/ 13000 | global iter:   2956/ 13000 | loss: 2.4642 | ds_loss: 0.0000 | lr: 4.3975e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 0.000
train | epoch   2 | Iter:   2957/ 13000 | global iter:   2957/ 13000 | loss: 1.5164 | ds_loss: 0.0000 | lr: 4.3971e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   2 | Iter:   2958/ 13000 | global iter:   2958/ 13000 | loss: 1.7016 | ds_loss: 0.0000 | lr: 4.3967e-05 | scale: 16384.0000 | micro time: 1.758 | step time: 0.000
train | epoch   2 | Iter:   2959/ 13000 | global iter:   2959/ 13000 | loss: 1.5739 | ds_loss: 0.0000 | lr: 4.3963e-05 | scale: 16384.0000 | micro time: 1.784 | step time: 0.000
train | epoch   2 | Iter:   2960/ 13000 | global iter:   2960/ 13000 | loss: 1.1067 | ds_loss: 0.0000 | lr: 4.3959e-05 | scale: 16384.0000 | micro time: 1.808 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2960/ 13000 | global iter:   2960/ 13000 | loss: 1.6816 | ds_loss: 0.0000 | lr: 4.3959e-05 | scale: 16384.0000 | micro time: 1.808 | step time: 1.816
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2961/ 13000 | global iter:   2961/ 13000 | loss: 1.2935 | ds_loss: 0.0000 | lr: 4.3955e-05 | scale: 16384.0000 | micro time: 1.740 | step time: 0.000
train | epoch   2 | Iter:   2962/ 13000 | global iter:   2962/ 13000 | loss: 2.0795 | ds_loss: 0.0000 | lr: 4.3951e-05 | scale: 16384.0000 | micro time: 1.808 | step time: 0.000
train | epoch   2 | Iter:   2963/ 13000 | global iter:   2963/ 13000 | loss: 1.3811 | ds_loss: 0.0000 | lr: 4.3948e-05 | scale: 16384.0000 | micro time: 1.838 | step time: 0.000
train | epoch   2 | Iter:   2964/ 13000 | global iter:   2964/ 13000 | loss: 2.2978 | ds_loss: 0.0000 | lr: 4.3944e-05 | scale: 16384.0000 | micro time: 1.753 | step time: 0.000
train | epoch   2 | Iter:   2965/ 13000 | global iter:   2965/ 13000 | loss: 1.6285 | ds_loss: 0.0000 | lr: 4.3940e-05 | scale: 16384.0000 | micro time: 1.773 | step time: 0.000
train | epoch   2 | Iter:   2966/ 13000 | global iter:   2966/ 13000 | loss: 1.7873 | ds_loss: 0.0000 | lr: 4.3936e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   2 | Iter:   2967/ 13000 | global iter:   2967/ 13000 | loss: 1.9170 | ds_loss: 0.0000 | lr: 4.3932e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   2 | Iter:   2968/ 13000 | global iter:   2968/ 13000 | loss: 1.7807 | ds_loss: 0.0000 | lr: 4.3928e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   2 | Iter:   2969/ 13000 | global iter:   2969/ 13000 | loss: 1.7604 | ds_loss: 0.0000 | lr: 4.3924e-05 | scale: 16384.0000 | micro time: 1.829 | step time: 0.000
train | epoch   2 | Iter:   2970/ 13000 | global iter:   2970/ 13000 | loss: 1.6354 | ds_loss: 0.0000 | lr: 4.3920e-05 | scale: 16384.0000 | micro time: 1.821 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2970/ 13000 | global iter:   2970/ 13000 | loss: 1.7561 | ds_loss: 0.0000 | lr: 4.3920e-05 | scale: 16384.0000 | micro time: 1.821 | step time: 1.793
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2971/ 13000 | global iter:   2971/ 13000 | loss: 2.3490 | ds_loss: 0.0000 | lr: 4.3916e-05 | scale: 16384.0000 | micro time: 1.862 | step time: 0.000
train | epoch   2 | Iter:   2972/ 13000 | global iter:   2972/ 13000 | loss: 2.0593 | ds_loss: 0.0000 | lr: 4.3912e-05 | scale: 16384.0000 | micro time: 1.711 | step time: 0.000
train | epoch   2 | Iter:   2973/ 13000 | global iter:   2973/ 13000 | loss: 1.2025 | ds_loss: 0.0000 | lr: 4.3908e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   2 | Iter:   2974/ 13000 | global iter:   2974/ 13000 | loss: 2.1744 | ds_loss: 0.0000 | lr: 4.3904e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   2 | Iter:   2975/ 13000 | global iter:   2975/ 13000 | loss: 1.5082 | ds_loss: 0.0000 | lr: 4.3900e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   2 | Iter:   2976/ 13000 | global iter:   2976/ 13000 | loss: 1.6731 | ds_loss: 0.0000 | lr: 4.3896e-05 | scale: 16384.0000 | micro time: 1.729 | step time: 0.000
train | epoch   2 | Iter:   2977/ 13000 | global iter:   2977/ 13000 | loss: 1.6572 | ds_loss: 0.0000 | lr: 4.3892e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   2 | Iter:   2978/ 13000 | global iter:   2978/ 13000 | loss: 1.8768 | ds_loss: 0.0000 | lr: 4.3888e-05 | scale: 16384.0000 | micro time: 1.872 | step time: 0.000
train | epoch   2 | Iter:   2979/ 13000 | global iter:   2979/ 13000 | loss: 1.9804 | ds_loss: 0.0000 | lr: 4.3884e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   2 | Iter:   2980/ 13000 | global iter:   2980/ 13000 | loss: 1.4834 | ds_loss: 0.0000 | lr: 4.3880e-05 | scale: 16384.0000 | micro time: 1.735 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2980/ 13000 | global iter:   2980/ 13000 | loss: 1.7964 | ds_loss: 0.0000 | lr: 4.3880e-05 | scale: 16384.0000 | micro time: 1.735 | step time: 1.784
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2981/ 13000 | global iter:   2981/ 13000 | loss: 1.9725 | ds_loss: 0.0000 | lr: 4.3876e-05 | scale: 16384.0000 | micro time: 1.818 | step time: 0.000
train | epoch   2 | Iter:   2982/ 13000 | global iter:   2982/ 13000 | loss: 1.9101 | ds_loss: 0.0000 | lr: 4.3873e-05 | scale: 16384.0000 | micro time: 1.842 | step time: 0.000
train | epoch   2 | Iter:   2983/ 13000 | global iter:   2983/ 13000 | loss: 1.6229 | ds_loss: 0.0000 | lr: 4.3869e-05 | scale: 16384.0000 | micro time: 1.935 | step time: 0.000
train | epoch   2 | Iter:   2984/ 13000 | global iter:   2984/ 13000 | loss: 2.2563 | ds_loss: 0.0000 | lr: 4.3865e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
train | epoch   2 | Iter:   2985/ 13000 | global iter:   2985/ 13000 | loss: 2.1767 | ds_loss: 0.0000 | lr: 4.3861e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   2 | Iter:   2986/ 13000 | global iter:   2986/ 13000 | loss: 2.1896 | ds_loss: 0.0000 | lr: 4.3857e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   2 | Iter:   2987/ 13000 | global iter:   2987/ 13000 | loss: 2.1121 | ds_loss: 0.0000 | lr: 4.3853e-05 | scale: 16384.0000 | micro time: 1.710 | step time: 0.000
train | epoch   2 | Iter:   2988/ 13000 | global iter:   2988/ 13000 | loss: 1.8888 | ds_loss: 0.0000 | lr: 4.3849e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   2 | Iter:   2989/ 13000 | global iter:   2989/ 13000 | loss: 2.0893 | ds_loss: 0.0000 | lr: 4.3845e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   2 | Iter:   2990/ 13000 | global iter:   2990/ 13000 | loss: 1.9748 | ds_loss: 0.0000 | lr: 4.3841e-05 | scale: 16384.0000 | micro time: 1.903 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   2990/ 13000 | global iter:   2990/ 13000 | loss: 2.0193 | ds_loss: 0.0000 | lr: 4.3841e-05 | scale: 16384.0000 | micro time: 1.903 | step time: 1.840
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   2991/ 13000 | global iter:   2991/ 13000 | loss: 1.0452 | ds_loss: 0.0000 | lr: 4.3837e-05 | scale: 16384.0000 | micro time: 1.758 | step time: 0.000
train | epoch   2 | Iter:   2992/ 13000 | global iter:   2992/ 13000 | loss: 1.9570 | ds_loss: 0.0000 | lr: 4.3833e-05 | scale: 16384.0000 | micro time: 1.820 | step time: 0.000
train | epoch   2 | Iter:   2993/ 13000 | global iter:   2993/ 13000 | loss: 2.3273 | ds_loss: 0.0000 | lr: 4.3829e-05 | scale: 16384.0000 | micro time: 1.889 | step time: 0.000
train | epoch   2 | Iter:   2994/ 13000 | global iter:   2994/ 13000 | loss: 1.8491 | ds_loss: 0.0000 | lr: 4.3825e-05 | scale: 16384.0000 | micro time: 1.816 | step time: 0.000
train | epoch   2 | Iter:   2995/ 13000 | global iter:   2995/ 13000 | loss: 2.1375 | ds_loss: 0.0000 | lr: 4.3821e-05 | scale: 16384.0000 | micro time: 1.801 | step time: 0.000
train | epoch   2 | Iter:   2996/ 13000 | global iter:   2996/ 13000 | loss: 1.5835 | ds_loss: 0.0000 | lr: 4.3817e-05 | scale: 16384.0000 | micro time: 1.696 | step time: 0.000
train | epoch   2 | Iter:   2997/ 13000 | global iter:   2997/ 13000 | loss: 2.0844 | ds_loss: 0.0000 | lr: 4.3813e-05 | scale: 16384.0000 | micro time: 1.818 | step time: 0.000
train | epoch   2 | Iter:   2998/ 13000 | global iter:   2998/ 13000 | loss: 2.0609 | ds_loss: 0.0000 | lr: 4.3809e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   2 | Iter:   2999/ 13000 | global iter:   2999/ 13000 | loss: 1.8991 | ds_loss: 0.0000 | lr: 4.3805e-05 | scale: 16384.0000 | micro time: 1.886 | step time: 0.000
train | epoch   2 | Iter:   3000/ 13000 | global iter:   3000/ 13000 | loss: 1.3318 | ds_loss: 0.0000 | lr: 4.3801e-05 | scale: 16384.0000 | micro time: 1.780 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3000/ 13000 | global iter:   3000/ 13000 | loss: 1.8276 | ds_loss: 0.0000 | lr: 4.3801e-05 | scale: 16384.0000 | micro time: 1.780 | step time: 1.806
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3001/ 13000 | global iter:   3001/ 13000 | loss: 2.2743 | ds_loss: 0.0000 | lr: 4.3797e-05 | scale: 16384.0000 | micro time: 1.844 | step time: 0.000
train | epoch   2 | Iter:   3002/ 13000 | global iter:   3002/ 13000 | loss: 2.2908 | ds_loss: 0.0000 | lr: 4.3793e-05 | scale: 16384.0000 | micro time: 1.750 | step time: 0.000
train | epoch   2 | Iter:   3003/ 13000 | global iter:   3003/ 13000 | loss: 2.0919 | ds_loss: 0.0000 | lr: 4.3789e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   2 | Iter:   3004/ 13000 | global iter:   3004/ 13000 | loss: 1.8632 | ds_loss: 0.0000 | lr: 4.3785e-05 | scale: 16384.0000 | micro time: 1.852 | step time: 0.000
train | epoch   2 | Iter:   3005/ 13000 | global iter:   3005/ 13000 | loss: 2.0533 | ds_loss: 0.0000 | lr: 4.3781e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   2 | Iter:   3006/ 13000 | global iter:   3006/ 13000 | loss: 1.7959 | ds_loss: 0.0000 | lr: 4.3777e-05 | scale: 16384.0000 | micro time: 1.778 | step time: 0.000
train | epoch   2 | Iter:   3007/ 13000 | global iter:   3007/ 13000 | loss: 1.6584 | ds_loss: 0.0000 | lr: 4.3773e-05 | scale: 16384.0000 | micro time: 1.850 | step time: 0.000
train | epoch   2 | Iter:   3008/ 13000 | global iter:   3008/ 13000 | loss: 1.5880 | ds_loss: 0.0000 | lr: 4.3769e-05 | scale: 16384.0000 | micro time: 1.789 | step time: 0.000
train | epoch   2 | Iter:   3009/ 13000 | global iter:   3009/ 13000 | loss: 2.5073 | ds_loss: 0.0000 | lr: 4.3765e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   2 | Iter:   3010/ 13000 | global iter:   3010/ 13000 | loss: 1.9017 | ds_loss: 0.0000 | lr: 4.3761e-05 | scale: 16384.0000 | micro time: 1.776 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3010/ 13000 | global iter:   3010/ 13000 | loss: 2.0025 | ds_loss: 0.0000 | lr: 4.3761e-05 | scale: 16384.0000 | micro time: 1.776 | step time: 1.810
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3011/ 13000 | global iter:   3011/ 13000 | loss: 1.8966 | ds_loss: 0.0000 | lr: 4.3757e-05 | scale: 16384.0000 | micro time: 1.897 | step time: 0.000
train | epoch   2 | Iter:   3012/ 13000 | global iter:   3012/ 13000 | loss: 1.6727 | ds_loss: 0.0000 | lr: 4.3753e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   2 | Iter:   3013/ 13000 | global iter:   3013/ 13000 | loss: 1.7694 | ds_loss: 0.0000 | lr: 4.3749e-05 | scale: 16384.0000 | micro time: 1.711 | step time: 0.000
train | epoch   2 | Iter:   3014/ 13000 | global iter:   3014/ 13000 | loss: 0.9148 | ds_loss: 0.0000 | lr: 4.3745e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   2 | Iter:   3015/ 13000 | global iter:   3015/ 13000 | loss: 2.0196 | ds_loss: 0.0000 | lr: 4.3741e-05 | scale: 16384.0000 | micro time: 1.774 | step time: 0.000
train | epoch   2 | Iter:   3016/ 13000 | global iter:   3016/ 13000 | loss: 1.7813 | ds_loss: 0.0000 | lr: 4.3737e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 0.000
train | epoch   2 | Iter:   3017/ 13000 | global iter:   3017/ 13000 | loss: 1.7356 | ds_loss: 0.0000 | lr: 4.3733e-05 | scale: 16384.0000 | micro time: 1.825 | step time: 0.000
train | epoch   2 | Iter:   3018/ 13000 | global iter:   3018/ 13000 | loss: 1.9393 | ds_loss: 0.0000 | lr: 4.3729e-05 | scale: 16384.0000 | micro time: 1.721 | step time: 0.000
train | epoch   2 | Iter:   3019/ 13000 | global iter:   3019/ 13000 | loss: 2.3784 | ds_loss: 0.0000 | lr: 4.3725e-05 | scale: 16384.0000 | micro time: 1.749 | step time: 0.000
train | epoch   2 | Iter:   3020/ 13000 | global iter:   3020/ 13000 | loss: 1.8417 | ds_loss: 0.0000 | lr: 4.3721e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3020/ 13000 | global iter:   3020/ 13000 | loss: 1.7949 | ds_loss: 0.0000 | lr: 4.3721e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 1.796
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3021/ 13000 | global iter:   3021/ 13000 | loss: 1.5817 | ds_loss: 0.0000 | lr: 4.3717e-05 | scale: 16384.0000 | micro time: 1.726 | step time: 0.000
train | epoch   2 | Iter:   3022/ 13000 | global iter:   3022/ 13000 | loss: 1.7791 | ds_loss: 0.0000 | lr: 4.3713e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   2 | Iter:   3023/ 13000 | global iter:   3023/ 13000 | loss: 1.7315 | ds_loss: 0.0000 | lr: 4.3709e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   2 | Iter:   3024/ 13000 | global iter:   3024/ 13000 | loss: 2.1436 | ds_loss: 0.0000 | lr: 4.3705e-05 | scale: 16384.0000 | micro time: 1.784 | step time: 0.000
train | epoch   2 | Iter:   3025/ 13000 | global iter:   3025/ 13000 | loss: 1.3958 | ds_loss: 0.0000 | lr: 4.3701e-05 | scale: 16384.0000 | micro time: 1.810 | step time: 0.000
train | epoch   2 | Iter:   3026/ 13000 | global iter:   3026/ 13000 | loss: 2.0872 | ds_loss: 0.0000 | lr: 4.3697e-05 | scale: 16384.0000 | micro time: 1.718 | step time: 0.000
train | epoch   2 | Iter:   3027/ 13000 | global iter:   3027/ 13000 | loss: 1.3765 | ds_loss: 0.0000 | lr: 4.3693e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   2 | Iter:   3028/ 13000 | global iter:   3028/ 13000 | loss: 2.1995 | ds_loss: 0.0000 | lr: 4.3689e-05 | scale: 16384.0000 | micro time: 1.675 | step time: 0.000
train | epoch   2 | Iter:   3029/ 13000 | global iter:   3029/ 13000 | loss: 1.1864 | ds_loss: 0.0000 | lr: 4.3685e-05 | scale: 16384.0000 | micro time: 1.870 | step time: 0.000
train | epoch   2 | Iter:   3030/ 13000 | global iter:   3030/ 13000 | loss: 2.1173 | ds_loss: 0.0000 | lr: 4.3681e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3030/ 13000 | global iter:   3030/ 13000 | loss: 1.7599 | ds_loss: 0.0000 | lr: 4.3681e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 1.785
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3031/ 13000 | global iter:   3031/ 13000 | loss: 1.7998 | ds_loss: 0.0000 | lr: 4.3677e-05 | scale: 16384.0000 | micro time: 1.878 | step time: 0.000
train | epoch   2 | Iter:   3032/ 13000 | global iter:   3032/ 13000 | loss: 2.0195 | ds_loss: 0.0000 | lr: 4.3673e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   2 | Iter:   3033/ 13000 | global iter:   3033/ 13000 | loss: 2.2250 | ds_loss: 0.0000 | lr: 4.3669e-05 | scale: 16384.0000 | micro time: 1.860 | step time: 0.000
train | epoch   2 | Iter:   3034/ 13000 | global iter:   3034/ 13000 | loss: 2.1406 | ds_loss: 0.0000 | lr: 4.3665e-05 | scale: 16384.0000 | micro time: 1.656 | step time: 0.000
train | epoch   2 | Iter:   3035/ 13000 | global iter:   3035/ 13000 | loss: 1.5294 | ds_loss: 0.0000 | lr: 4.3661e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   2 | Iter:   3036/ 13000 | global iter:   3036/ 13000 | loss: 1.6640 | ds_loss: 0.0000 | lr: 4.3657e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 0.000
train | epoch   2 | Iter:   3037/ 13000 | global iter:   3037/ 13000 | loss: 2.1332 | ds_loss: 0.0000 | lr: 4.3653e-05 | scale: 16384.0000 | micro time: 2.135 | step time: 0.000
train | epoch   2 | Iter:   3038/ 13000 | global iter:   3038/ 13000 | loss: 2.0822 | ds_loss: 0.0000 | lr: 4.3649e-05 | scale: 16384.0000 | micro time: 1.891 | step time: 0.000
train | epoch   2 | Iter:   3039/ 13000 | global iter:   3039/ 13000 | loss: 1.8499 | ds_loss: 0.0000 | lr: 4.3645e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   2 | Iter:   3040/ 13000 | global iter:   3040/ 13000 | loss: 1.7644 | ds_loss: 0.0000 | lr: 4.3641e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3040/ 13000 | global iter:   3040/ 13000 | loss: 1.9208 | ds_loss: 0.0000 | lr: 4.3641e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 1.841
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3041/ 13000 | global iter:   3041/ 13000 | loss: 2.0490 | ds_loss: 0.0000 | lr: 4.3637e-05 | scale: 16384.0000 | micro time: 1.818 | step time: 0.000
train | epoch   2 | Iter:   3042/ 13000 | global iter:   3042/ 13000 | loss: 1.6549 | ds_loss: 0.0000 | lr: 4.3633e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   2 | Iter:   3043/ 13000 | global iter:   3043/ 13000 | loss: 1.7972 | ds_loss: 0.0000 | lr: 4.3629e-05 | scale: 16384.0000 | micro time: 1.825 | step time: 0.000
train | epoch   2 | Iter:   3044/ 13000 | global iter:   3044/ 13000 | loss: 1.5219 | ds_loss: 0.0000 | lr: 4.3625e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   2 | Iter:   3045/ 13000 | global iter:   3045/ 13000 | loss: 2.3032 | ds_loss: 0.0000 | lr: 4.3621e-05 | scale: 16384.0000 | micro time: 1.849 | step time: 0.000
train | epoch   2 | Iter:   3046/ 13000 | global iter:   3046/ 13000 | loss: 2.1128 | ds_loss: 0.0000 | lr: 4.3617e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   2 | Iter:   3047/ 13000 | global iter:   3047/ 13000 | loss: 1.7813 | ds_loss: 0.0000 | lr: 4.3613e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   2 | Iter:   3048/ 13000 | global iter:   3048/ 13000 | loss: 1.6619 | ds_loss: 0.0000 | lr: 4.3609e-05 | scale: 16384.0000 | micro time: 1.832 | step time: 0.000
train | epoch   2 | Iter:   3049/ 13000 | global iter:   3049/ 13000 | loss: 1.7646 | ds_loss: 0.0000 | lr: 4.3605e-05 | scale: 16384.0000 | micro time: 1.896 | step time: 0.000
train | epoch   2 | Iter:   3050/ 13000 | global iter:   3050/ 13000 | loss: 1.9252 | ds_loss: 0.0000 | lr: 4.3601e-05 | scale: 16384.0000 | micro time: 1.668 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3050/ 13000 | global iter:   3050/ 13000 | loss: 1.8572 | ds_loss: 0.0000 | lr: 4.3601e-05 | scale: 16384.0000 | micro time: 1.668 | step time: 1.813
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3051/ 13000 | global iter:   3051/ 13000 | loss: 1.6105 | ds_loss: 0.0000 | lr: 4.3597e-05 | scale: 16384.0000 | micro time: 1.782 | step time: 0.000
train | epoch   2 | Iter:   3052/ 13000 | global iter:   3052/ 13000 | loss: 1.9780 | ds_loss: 0.0000 | lr: 4.3593e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   2 | Iter:   3053/ 13000 | global iter:   3053/ 13000 | loss: 2.0363 | ds_loss: 0.0000 | lr: 4.3589e-05 | scale: 16384.0000 | micro time: 1.818 | step time: 0.000
train | epoch   2 | Iter:   3054/ 13000 | global iter:   3054/ 13000 | loss: 2.0934 | ds_loss: 0.0000 | lr: 4.3585e-05 | scale: 16384.0000 | micro time: 1.801 | step time: 0.000
train | epoch   2 | Iter:   3055/ 13000 | global iter:   3055/ 13000 | loss: 1.8798 | ds_loss: 0.0000 | lr: 4.3581e-05 | scale: 16384.0000 | micro time: 1.683 | step time: 0.000
train | epoch   2 | Iter:   3056/ 13000 | global iter:   3056/ 13000 | loss: 1.7104 | ds_loss: 0.0000 | lr: 4.3577e-05 | scale: 16384.0000 | micro time: 1.794 | step time: 0.000
train | epoch   2 | Iter:   3057/ 13000 | global iter:   3057/ 13000 | loss: 1.6801 | ds_loss: 0.0000 | lr: 4.3573e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   2 | Iter:   3058/ 13000 | global iter:   3058/ 13000 | loss: 1.8698 | ds_loss: 0.0000 | lr: 4.3569e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   2 | Iter:   3059/ 13000 | global iter:   3059/ 13000 | loss: 2.0987 | ds_loss: 0.0000 | lr: 4.3565e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   2 | Iter:   3060/ 13000 | global iter:   3060/ 13000 | loss: 1.9641 | ds_loss: 0.0000 | lr: 4.3561e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3060/ 13000 | global iter:   3060/ 13000 | loss: 1.8921 | ds_loss: 0.0000 | lr: 4.3561e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 1.786
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3061/ 13000 | global iter:   3061/ 13000 | loss: 2.0195 | ds_loss: 0.0000 | lr: 4.3556e-05 | scale: 16384.0000 | micro time: 1.814 | step time: 0.000
train | epoch   2 | Iter:   3062/ 13000 | global iter:   3062/ 13000 | loss: 1.8388 | ds_loss: 0.0000 | lr: 4.3552e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   2 | Iter:   3063/ 13000 | global iter:   3063/ 13000 | loss: 1.4068 | ds_loss: 0.0000 | lr: 4.3548e-05 | scale: 16384.0000 | micro time: 1.743 | step time: 0.000
train | epoch   2 | Iter:   3064/ 13000 | global iter:   3064/ 13000 | loss: 1.7926 | ds_loss: 0.0000 | lr: 4.3544e-05 | scale: 16384.0000 | micro time: 1.735 | step time: 0.000
train | epoch   2 | Iter:   3065/ 13000 | global iter:   3065/ 13000 | loss: 1.8808 | ds_loss: 0.0000 | lr: 4.3540e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   2 | Iter:   3066/ 13000 | global iter:   3066/ 13000 | loss: 1.4806 | ds_loss: 0.0000 | lr: 4.3536e-05 | scale: 16384.0000 | micro time: 1.879 | step time: 0.000
train | epoch   2 | Iter:   3067/ 13000 | global iter:   3067/ 13000 | loss: 2.2772 | ds_loss: 0.0000 | lr: 4.3532e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   2 | Iter:   3068/ 13000 | global iter:   3068/ 13000 | loss: 1.8551 | ds_loss: 0.0000 | lr: 4.3528e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   2 | Iter:   3069/ 13000 | global iter:   3069/ 13000 | loss: 2.1658 | ds_loss: 0.0000 | lr: 4.3524e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   2 | Iter:   3070/ 13000 | global iter:   3070/ 13000 | loss: 1.8540 | ds_loss: 0.0000 | lr: 4.3520e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3070/ 13000 | global iter:   3070/ 13000 | loss: 1.8571 | ds_loss: 0.0000 | lr: 4.3520e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 1.828
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3071/ 13000 | global iter:   3071/ 13000 | loss: 1.9980 | ds_loss: 0.0000 | lr: 4.3516e-05 | scale: 16384.0000 | micro time: 1.801 | step time: 0.000
train | epoch   2 | Iter:   3072/ 13000 | global iter:   3072/ 13000 | loss: 1.8461 | ds_loss: 0.0000 | lr: 4.3512e-05 | scale: 16384.0000 | micro time: 1.869 | step time: 0.000
train | epoch   2 | Iter:   3073/ 13000 | global iter:   3073/ 13000 | loss: 1.9114 | ds_loss: 0.0000 | lr: 4.3508e-05 | scale: 16384.0000 | micro time: 1.772 | step time: 0.000
train | epoch   2 | Iter:   3074/ 13000 | global iter:   3074/ 13000 | loss: 1.4754 | ds_loss: 0.0000 | lr: 4.3504e-05 | scale: 16384.0000 | micro time: 1.689 | step time: 0.000
train | epoch   2 | Iter:   3075/ 13000 | global iter:   3075/ 13000 | loss: 1.7654 | ds_loss: 0.0000 | lr: 4.3500e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   2 | Iter:   3076/ 13000 | global iter:   3076/ 13000 | loss: 1.1201 | ds_loss: 0.0000 | lr: 4.3496e-05 | scale: 16384.0000 | micro time: 1.742 | step time: 0.000
train | epoch   2 | Iter:   3077/ 13000 | global iter:   3077/ 13000 | loss: 1.8447 | ds_loss: 0.0000 | lr: 4.3492e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   2 | Iter:   3078/ 13000 | global iter:   3078/ 13000 | loss: 1.3402 | ds_loss: 0.0000 | lr: 4.3488e-05 | scale: 16384.0000 | micro time: 2.163 | step time: 0.000
train | epoch   2 | Iter:   3079/ 13000 | global iter:   3079/ 13000 | loss: 2.3002 | ds_loss: 0.0000 | lr: 4.3483e-05 | scale: 16384.0000 | micro time: 1.844 | step time: 0.000
train | epoch   2 | Iter:   3080/ 13000 | global iter:   3080/ 13000 | loss: 1.8996 | ds_loss: 0.0000 | lr: 4.3479e-05 | scale: 16384.0000 | micro time: 1.745 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3080/ 13000 | global iter:   3080/ 13000 | loss: 1.7501 | ds_loss: 0.0000 | lr: 4.3479e-05 | scale: 16384.0000 | micro time: 1.745 | step time: 1.829
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3081/ 13000 | global iter:   3081/ 13000 | loss: 1.7526 | ds_loss: 0.0000 | lr: 4.3475e-05 | scale: 16384.0000 | micro time: 1.828 | step time: 0.000
train | epoch   2 | Iter:   3082/ 13000 | global iter:   3082/ 13000 | loss: 1.9311 | ds_loss: 0.0000 | lr: 4.3471e-05 | scale: 16384.0000 | micro time: 1.829 | step time: 0.000
train | epoch   2 | Iter:   3083/ 13000 | global iter:   3083/ 13000 | loss: 1.8401 | ds_loss: 0.0000 | lr: 4.3467e-05 | scale: 16384.0000 | micro time: 1.786 | step time: 0.000
train | epoch   2 | Iter:   3084/ 13000 | global iter:   3084/ 13000 | loss: 1.6476 | ds_loss: 0.0000 | lr: 4.3463e-05 | scale: 16384.0000 | micro time: 1.796 | step time: 0.000
train | epoch   2 | Iter:   3085/ 13000 | global iter:   3085/ 13000 | loss: 2.0011 | ds_loss: 0.0000 | lr: 4.3459e-05 | scale: 16384.0000 | micro time: 1.886 | step time: 0.000
train | epoch   2 | Iter:   3086/ 13000 | global iter:   3086/ 13000 | loss: 2.4242 | ds_loss: 0.0000 | lr: 4.3455e-05 | scale: 16384.0000 | micro time: 1.813 | step time: 0.000
train | epoch   2 | Iter:   3087/ 13000 | global iter:   3087/ 13000 | loss: 1.7852 | ds_loss: 0.0000 | lr: 4.3451e-05 | scale: 16384.0000 | micro time: 1.796 | step time: 0.000
train | epoch   2 | Iter:   3088/ 13000 | global iter:   3088/ 13000 | loss: 1.9174 | ds_loss: 0.0000 | lr: 4.3447e-05 | scale: 16384.0000 | micro time: 1.750 | step time: 0.000
train | epoch   2 | Iter:   3089/ 13000 | global iter:   3089/ 13000 | loss: 2.2187 | ds_loss: 0.0000 | lr: 4.3443e-05 | scale: 16384.0000 | micro time: 1.740 | step time: 0.000
train | epoch   2 | Iter:   3090/ 13000 | global iter:   3090/ 13000 | loss: 1.8494 | ds_loss: 0.0000 | lr: 4.3439e-05 | scale: 16384.0000 | micro time: 1.745 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3090/ 13000 | global iter:   3090/ 13000 | loss: 1.9367 | ds_loss: 0.0000 | lr: 4.3439e-05 | scale: 16384.0000 | micro time: 1.745 | step time: 1.797
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3091/ 13000 | global iter:   3091/ 13000 | loss: 2.3080 | ds_loss: 0.0000 | lr: 4.3435e-05 | scale: 16384.0000 | micro time: 1.802 | step time: 0.000
train | epoch   2 | Iter:   3092/ 13000 | global iter:   3092/ 13000 | loss: 1.5160 | ds_loss: 0.0000 | lr: 4.3431e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   2 | Iter:   3093/ 13000 | global iter:   3093/ 13000 | loss: 1.8888 | ds_loss: 0.0000 | lr: 4.3427e-05 | scale: 16384.0000 | micro time: 1.708 | step time: 0.000
train | epoch   2 | Iter:   3094/ 13000 | global iter:   3094/ 13000 | loss: 2.1431 | ds_loss: 0.0000 | lr: 4.3422e-05 | scale: 16384.0000 | micro time: 1.887 | step time: 0.000
train | epoch   2 | Iter:   3095/ 13000 | global iter:   3095/ 13000 | loss: 1.7047 | ds_loss: 0.0000 | lr: 4.3418e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   2 | Iter:   3096/ 13000 | global iter:   3096/ 13000 | loss: 1.4762 | ds_loss: 0.0000 | lr: 4.3414e-05 | scale: 16384.0000 | micro time: 1.738 | step time: 0.000
train | epoch   2 | Iter:   3097/ 13000 | global iter:   3097/ 13000 | loss: 1.4373 | ds_loss: 0.0000 | lr: 4.3410e-05 | scale: 16384.0000 | micro time: 1.766 | step time: 0.000
train | epoch   2 | Iter:   3098/ 13000 | global iter:   3098/ 13000 | loss: 1.9976 | ds_loss: 0.0000 | lr: 4.3406e-05 | scale: 16384.0000 | micro time: 1.829 | step time: 0.000
train | epoch   2 | Iter:   3099/ 13000 | global iter:   3099/ 13000 | loss: 1.4948 | ds_loss: 0.0000 | lr: 4.3402e-05 | scale: 16384.0000 | micro time: 1.798 | step time: 0.000
train | epoch   2 | Iter:   3100/ 13000 | global iter:   3100/ 13000 | loss: 1.8472 | ds_loss: 0.0000 | lr: 4.3398e-05 | scale: 16384.0000 | micro time: 1.821 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3100/ 13000 | global iter:   3100/ 13000 | loss: 1.7814 | ds_loss: 0.0000 | lr: 4.3398e-05 | scale: 16384.0000 | micro time: 1.821 | step time: 1.798
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3101/ 13000 | global iter:   3101/ 13000 | loss: 1.3957 | ds_loss: 0.0000 | lr: 4.3394e-05 | scale: 16384.0000 | micro time: 1.782 | step time: 0.000
train | epoch   2 | Iter:   3102/ 13000 | global iter:   3102/ 13000 | loss: 2.0802 | ds_loss: 0.0000 | lr: 4.3390e-05 | scale: 16384.0000 | micro time: 1.857 | step time: 0.000
train | epoch   2 | Iter:   3103/ 13000 | global iter:   3103/ 13000 | loss: 1.2301 | ds_loss: 0.0000 | lr: 4.3386e-05 | scale: 16384.0000 | micro time: 1.813 | step time: 0.000
train | epoch   2 | Iter:   3104/ 13000 | global iter:   3104/ 13000 | loss: 2.2238 | ds_loss: 0.0000 | lr: 4.3382e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   2 | Iter:   3105/ 13000 | global iter:   3105/ 13000 | loss: 2.2890 | ds_loss: 0.0000 | lr: 4.3377e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   2 | Iter:   3106/ 13000 | global iter:   3106/ 13000 | loss: 2.3887 | ds_loss: 0.0000 | lr: 4.3373e-05 | scale: 16384.0000 | micro time: 1.707 | step time: 0.000
train | epoch   2 | Iter:   3107/ 13000 | global iter:   3107/ 13000 | loss: 1.9452 | ds_loss: 0.0000 | lr: 4.3369e-05 | scale: 16384.0000 | micro time: 1.784 | step time: 0.000
train | epoch   2 | Iter:   3108/ 13000 | global iter:   3108/ 13000 | loss: 1.9852 | ds_loss: 0.0000 | lr: 4.3365e-05 | scale: 16384.0000 | micro time: 1.850 | step time: 0.000
train | epoch   2 | Iter:   3109/ 13000 | global iter:   3109/ 13000 | loss: 2.1318 | ds_loss: 0.0000 | lr: 4.3361e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   2 | Iter:   3110/ 13000 | global iter:   3110/ 13000 | loss: 2.4296 | ds_loss: 0.0000 | lr: 4.3357e-05 | scale: 16384.0000 | micro time: 1.893 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3110/ 13000 | global iter:   3110/ 13000 | loss: 2.0099 | ds_loss: 0.0000 | lr: 4.3357e-05 | scale: 16384.0000 | micro time: 1.893 | step time: 1.815
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3111/ 13000 | global iter:   3111/ 13000 | loss: 1.8518 | ds_loss: 0.0000 | lr: 4.3353e-05 | scale: 16384.0000 | micro time: 1.832 | step time: 0.000
train | epoch   2 | Iter:   3112/ 13000 | global iter:   3112/ 13000 | loss: 1.6327 | ds_loss: 0.0000 | lr: 4.3349e-05 | scale: 16384.0000 | micro time: 1.798 | step time: 0.000
train | epoch   2 | Iter:   3113/ 13000 | global iter:   3113/ 13000 | loss: 2.1228 | ds_loss: 0.0000 | lr: 4.3345e-05 | scale: 16384.0000 | micro time: 1.793 | step time: 0.000
train | epoch   2 | Iter:   3114/ 13000 | global iter:   3114/ 13000 | loss: 1.8299 | ds_loss: 0.0000 | lr: 4.3341e-05 | scale: 16384.0000 | micro time: 1.862 | step time: 0.000
train | epoch   2 | Iter:   3115/ 13000 | global iter:   3115/ 13000 | loss: 1.9882 | ds_loss: 0.0000 | lr: 4.3337e-05 | scale: 16384.0000 | micro time: 1.879 | step time: 0.000
train | epoch   2 | Iter:   3116/ 13000 | global iter:   3116/ 13000 | loss: 2.4255 | ds_loss: 0.0000 | lr: 4.3332e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   2 | Iter:   3117/ 13000 | global iter:   3117/ 13000 | loss: 2.2743 | ds_loss: 0.0000 | lr: 4.3328e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   2 | Iter:   3118/ 13000 | global iter:   3118/ 13000 | loss: 1.8607 | ds_loss: 0.0000 | lr: 4.3324e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   2 | Iter:   3119/ 13000 | global iter:   3119/ 13000 | loss: 1.8831 | ds_loss: 0.0000 | lr: 4.3320e-05 | scale: 16384.0000 | micro time: 1.725 | step time: 0.000
train | epoch   2 | Iter:   3120/ 13000 | global iter:   3120/ 13000 | loss: 1.8094 | ds_loss: 0.0000 | lr: 4.3316e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3120/ 13000 | global iter:   3120/ 13000 | loss: 1.9678 | ds_loss: 0.0000 | lr: 4.3316e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 1.819
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3121/ 13000 | global iter:   3121/ 13000 | loss: 1.7899 | ds_loss: 0.0000 | lr: 4.3312e-05 | scale: 16384.0000 | micro time: 1.785 | step time: 0.000
train | epoch   2 | Iter:   3122/ 13000 | global iter:   3122/ 13000 | loss: 1.5609 | ds_loss: 0.0000 | lr: 4.3308e-05 | scale: 16384.0000 | micro time: 1.625 | step time: 0.000
train | epoch   2 | Iter:   3123/ 13000 | global iter:   3123/ 13000 | loss: 2.3806 | ds_loss: 0.0000 | lr: 4.3304e-05 | scale: 16384.0000 | micro time: 1.769 | step time: 0.000
train | epoch   2 | Iter:   3124/ 13000 | global iter:   3124/ 13000 | loss: 2.1219 | ds_loss: 0.0000 | lr: 4.3300e-05 | scale: 16384.0000 | micro time: 1.846 | step time: 0.000
train | epoch   2 | Iter:   3125/ 13000 | global iter:   3125/ 13000 | loss: 1.9531 | ds_loss: 0.0000 | lr: 4.3295e-05 | scale: 16384.0000 | micro time: 1.764 | step time: 0.000
train | epoch   2 | Iter:   3126/ 13000 | global iter:   3126/ 13000 | loss: 1.9115 | ds_loss: 0.0000 | lr: 4.3291e-05 | scale: 16384.0000 | micro time: 1.820 | step time: 0.000
train | epoch   2 | Iter:   3127/ 13000 | global iter:   3127/ 13000 | loss: 2.0371 | ds_loss: 0.0000 | lr: 4.3287e-05 | scale: 16384.0000 | micro time: 1.746 | step time: 0.000
train | epoch   2 | Iter:   3128/ 13000 | global iter:   3128/ 13000 | loss: 1.7364 | ds_loss: 0.0000 | lr: 4.3283e-05 | scale: 16384.0000 | micro time: 1.854 | step time: 0.000
train | epoch   2 | Iter:   3129/ 13000 | global iter:   3129/ 13000 | loss: 2.0020 | ds_loss: 0.0000 | lr: 4.3279e-05 | scale: 16384.0000 | micro time: 1.904 | step time: 0.000
train | epoch   2 | Iter:   3130/ 13000 | global iter:   3130/ 13000 | loss: 1.6188 | ds_loss: 0.0000 | lr: 4.3275e-05 | scale: 16384.0000 | micro time: 1.750 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3130/ 13000 | global iter:   3130/ 13000 | loss: 1.9112 | ds_loss: 0.0000 | lr: 4.3275e-05 | scale: 16384.0000 | micro time: 1.750 | step time: 1.786
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3131/ 13000 | global iter:   3131/ 13000 | loss: 1.6374 | ds_loss: 0.0000 | lr: 4.3271e-05 | scale: 16384.0000 | micro time: 1.735 | step time: 0.000
train | epoch   2 | Iter:   3132/ 13000 | global iter:   3132/ 13000 | loss: 2.0999 | ds_loss: 0.0000 | lr: 4.3267e-05 | scale: 16384.0000 | micro time: 1.959 | step time: 0.000
train | epoch   2 | Iter:   3133/ 13000 | global iter:   3133/ 13000 | loss: 1.8919 | ds_loss: 0.0000 | lr: 4.3263e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   2 | Iter:   3134/ 13000 | global iter:   3134/ 13000 | loss: 2.2371 | ds_loss: 0.0000 | lr: 4.3258e-05 | scale: 16384.0000 | micro time: 1.785 | step time: 0.000
train | epoch   2 | Iter:   3135/ 13000 | global iter:   3135/ 13000 | loss: 1.5045 | ds_loss: 0.0000 | lr: 4.3254e-05 | scale: 16384.0000 | micro time: 1.793 | step time: 0.000
train | epoch   2 | Iter:   3136/ 13000 | global iter:   3136/ 13000 | loss: 1.3799 | ds_loss: 0.0000 | lr: 4.3250e-05 | scale: 16384.0000 | micro time: 1.879 | step time: 0.000
train | epoch   2 | Iter:   3137/ 13000 | global iter:   3137/ 13000 | loss: 1.9749 | ds_loss: 0.0000 | lr: 4.3246e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   2 | Iter:   3138/ 13000 | global iter:   3138/ 13000 | loss: 1.8857 | ds_loss: 0.0000 | lr: 4.3242e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   2 | Iter:   3139/ 13000 | global iter:   3139/ 13000 | loss: 1.7513 | ds_loss: 0.0000 | lr: 4.3238e-05 | scale: 16384.0000 | micro time: 1.880 | step time: 0.000
train | epoch   2 | Iter:   3140/ 13000 | global iter:   3140/ 13000 | loss: 2.2680 | ds_loss: 0.0000 | lr: 4.3234e-05 | scale: 16384.0000 | micro time: 1.898 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3140/ 13000 | global iter:   3140/ 13000 | loss: 1.8630 | ds_loss: 0.0000 | lr: 4.3234e-05 | scale: 16384.0000 | micro time: 1.898 | step time: 1.831
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3141/ 13000 | global iter:   3141/ 13000 | loss: 1.5465 | ds_loss: 0.0000 | lr: 4.3230e-05 | scale: 16384.0000 | micro time: 1.814 | step time: 0.000
train | epoch   2 | Iter:   3142/ 13000 | global iter:   3142/ 13000 | loss: 1.6670 | ds_loss: 0.0000 | lr: 4.3225e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   2 | Iter:   3143/ 13000 | global iter:   3143/ 13000 | loss: 1.8165 | ds_loss: 0.0000 | lr: 4.3221e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   2 | Iter:   3144/ 13000 | global iter:   3144/ 13000 | loss: 1.3946 | ds_loss: 0.0000 | lr: 4.3217e-05 | scale: 16384.0000 | micro time: 1.782 | step time: 0.000
train | epoch   2 | Iter:   3145/ 13000 | global iter:   3145/ 13000 | loss: 2.1572 | ds_loss: 0.0000 | lr: 4.3213e-05 | scale: 16384.0000 | micro time: 1.731 | step time: 0.000
train | epoch   2 | Iter:   3146/ 13000 | global iter:   3146/ 13000 | loss: 1.3063 | ds_loss: 0.0000 | lr: 4.3209e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   2 | Iter:   3147/ 13000 | global iter:   3147/ 13000 | loss: 1.6937 | ds_loss: 0.0000 | lr: 4.3205e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   2 | Iter:   3148/ 13000 | global iter:   3148/ 13000 | loss: 1.8609 | ds_loss: 0.0000 | lr: 4.3201e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   2 | Iter:   3149/ 13000 | global iter:   3149/ 13000 | loss: 2.2926 | ds_loss: 0.0000 | lr: 4.3196e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   2 | Iter:   3150/ 13000 | global iter:   3150/ 13000 | loss: 1.6540 | ds_loss: 0.0000 | lr: 4.3192e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3150/ 13000 | global iter:   3150/ 13000 | loss: 1.7389 | ds_loss: 0.0000 | lr: 4.3192e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 1.796
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3151/ 13000 | global iter:   3151/ 13000 | loss: 2.2466 | ds_loss: 0.0000 | lr: 4.3188e-05 | scale: 32768.0000 | micro time: 1.820 | step time: 0.000
train | epoch   2 | Iter:   3152/ 13000 | global iter:   3152/ 13000 | loss: 2.1598 | ds_loss: 0.0000 | lr: 4.3184e-05 | scale: 32768.0000 | micro time: 1.762 | step time: 0.000
train | epoch   2 | Iter:   3153/ 13000 | global iter:   3153/ 13000 | loss: 2.0618 | ds_loss: 0.0000 | lr: 4.3180e-05 | scale: 32768.0000 | micro time: 1.849 | step time: 0.000
train | epoch   2 | Iter:   3154/ 13000 | global iter:   3154/ 13000 | loss: 1.2060 | ds_loss: 0.0000 | lr: 4.3176e-05 | scale: 32768.0000 | micro time: 1.860 | step time: 0.000
train | epoch   2 | Iter:   3155/ 13000 | global iter:   3155/ 13000 | loss: 1.9225 | ds_loss: 0.0000 | lr: 4.3172e-05 | scale: 32768.0000 | micro time: 1.856 | step time: 0.000
train | epoch   2 | Iter:   3156/ 13000 | global iter:   3156/ 13000 | loss: 1.4690 | ds_loss: 0.0000 | lr: 4.3167e-05 | scale: 32768.0000 | micro time: 1.853 | step time: 0.000
train | epoch   2 | Iter:   3157/ 13000 | global iter:   3157/ 13000 | loss: 2.1806 | ds_loss: 0.0000 | lr: 4.3163e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   2 | Iter:   3158/ 13000 | global iter:   3158/ 13000 | loss: 2.3726 | ds_loss: 0.0000 | lr: 4.3159e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   2 | Iter:   3159/ 13000 | global iter:   3159/ 13000 | loss: 1.8978 | ds_loss: 0.0000 | lr: 4.3155e-05 | scale: 32768.0000 | micro time: 1.795 | step time: 0.000
train | epoch   2 | Iter:   3160/ 13000 | global iter:   3160/ 13000 | loss: 1.7967 | ds_loss: 0.0000 | lr: 4.3151e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3160/ 13000 | global iter:   3160/ 13000 | loss: 1.9313 | ds_loss: 0.0000 | lr: 4.3151e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 1.820
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3161/ 13000 | global iter:   3161/ 13000 | loss: 1.8411 | ds_loss: 0.0000 | lr: 4.3147e-05 | scale: 32768.0000 | micro time: 1.785 | step time: 0.000
train | epoch   2 | Iter:   3162/ 13000 | global iter:   3162/ 13000 | loss: 1.7459 | ds_loss: 0.0000 | lr: 4.3143e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 0.000
train | epoch   2 | Iter:   3163/ 13000 | global iter:   3163/ 13000 | loss: 1.5431 | ds_loss: 0.0000 | lr: 4.3138e-05 | scale: 32768.0000 | micro time: 1.773 | step time: 0.000
train | epoch   2 | Iter:   3164/ 13000 | global iter:   3164/ 13000 | loss: 1.7582 | ds_loss: 0.0000 | lr: 4.3134e-05 | scale: 32768.0000 | micro time: 1.765 | step time: 0.000
train | epoch   2 | Iter:   3165/ 13000 | global iter:   3165/ 13000 | loss: 1.4411 | ds_loss: 0.0000 | lr: 4.3130e-05 | scale: 32768.0000 | micro time: 1.903 | step time: 0.000
train | epoch   2 | Iter:   3166/ 13000 | global iter:   3166/ 13000 | loss: 1.3449 | ds_loss: 0.0000 | lr: 4.3126e-05 | scale: 32768.0000 | micro time: 1.842 | step time: 0.000
train | epoch   2 | Iter:   3167/ 13000 | global iter:   3167/ 13000 | loss: 1.7923 | ds_loss: 0.0000 | lr: 4.3122e-05 | scale: 32768.0000 | micro time: 1.804 | step time: 0.000
train | epoch   2 | Iter:   3168/ 13000 | global iter:   3168/ 13000 | loss: 2.1210 | ds_loss: 0.0000 | lr: 4.3118e-05 | scale: 32768.0000 | micro time: 1.692 | step time: 0.000
train | epoch   2 | Iter:   3169/ 13000 | global iter:   3169/ 13000 | loss: 1.7343 | ds_loss: 0.0000 | lr: 4.3113e-05 | scale: 32768.0000 | micro time: 1.866 | step time: 0.000
train | epoch   2 | Iter:   3170/ 13000 | global iter:   3170/ 13000 | loss: 1.9620 | ds_loss: 0.0000 | lr: 4.3109e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3170/ 13000 | global iter:   3170/ 13000 | loss: 1.7284 | ds_loss: 0.0000 | lr: 4.3109e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 1.813
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3171/ 13000 | global iter:   3171/ 13000 | loss: 2.0762 | ds_loss: 0.0000 | lr: 4.3105e-05 | scale: 32768.0000 | micro time: 1.890 | step time: 0.000
train | epoch   2 | Iter:   3172/ 13000 | global iter:   3172/ 13000 | loss: 2.2280 | ds_loss: 0.0000 | lr: 4.3101e-05 | scale: 32768.0000 | micro time: 1.731 | step time: 0.000
train | epoch   2 | Iter:   3173/ 13000 | global iter:   3173/ 13000 | loss: 1.7492 | ds_loss: 0.0000 | lr: 4.3097e-05 | scale: 32768.0000 | micro time: 1.742 | step time: 0.000
train | epoch   2 | Iter:   3174/ 13000 | global iter:   3174/ 13000 | loss: 1.2205 | ds_loss: 0.0000 | lr: 4.3093e-05 | scale: 32768.0000 | micro time: 1.684 | step time: 0.000
train | epoch   2 | Iter:   3175/ 13000 | global iter:   3175/ 13000 | loss: 2.1507 | ds_loss: 0.0000 | lr: 4.3088e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   2 | Iter:   3176/ 13000 | global iter:   3176/ 13000 | loss: 1.1783 | ds_loss: 0.0000 | lr: 4.3084e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   2 | Iter:   3177/ 13000 | global iter:   3177/ 13000 | loss: 2.0467 | ds_loss: 0.0000 | lr: 4.3080e-05 | scale: 32768.0000 | micro time: 1.707 | step time: 0.000
train | epoch   2 | Iter:   3178/ 13000 | global iter:   3178/ 13000 | loss: 1.9218 | ds_loss: 0.0000 | lr: 4.3076e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   2 | Iter:   3179/ 13000 | global iter:   3179/ 13000 | loss: 1.5735 | ds_loss: 0.0000 | lr: 4.3072e-05 | scale: 32768.0000 | micro time: 1.756 | step time: 0.000
train | epoch   2 | Iter:   3180/ 13000 | global iter:   3180/ 13000 | loss: 1.9005 | ds_loss: 0.0000 | lr: 4.3068e-05 | scale: 32768.0000 | micro time: 1.834 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3180/ 13000 | global iter:   3180/ 13000 | loss: 1.8046 | ds_loss: 0.0000 | lr: 4.3068e-05 | scale: 32768.0000 | micro time: 1.834 | step time: 1.782
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3181/ 13000 | global iter:   3181/ 13000 | loss: 1.6657 | ds_loss: 0.0000 | lr: 4.3063e-05 | scale: 32768.0000 | micro time: 1.746 | step time: 0.000
train | epoch   2 | Iter:   3182/ 13000 | global iter:   3182/ 13000 | loss: 2.2362 | ds_loss: 0.0000 | lr: 4.3059e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   2 | Iter:   3183/ 13000 | global iter:   3183/ 13000 | loss: 2.1533 | ds_loss: 0.0000 | lr: 4.3055e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
train | epoch   2 | Iter:   3184/ 13000 | global iter:   3184/ 13000 | loss: 1.3373 | ds_loss: 0.0000 | lr: 4.3051e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   2 | Iter:   3185/ 13000 | global iter:   3185/ 13000 | loss: 1.8303 | ds_loss: 0.0000 | lr: 4.3047e-05 | scale: 32768.0000 | micro time: 1.805 | step time: 0.000
train | epoch   2 | Iter:   3186/ 13000 | global iter:   3186/ 13000 | loss: 2.1852 | ds_loss: 0.0000 | lr: 4.3043e-05 | scale: 32768.0000 | micro time: 1.901 | step time: 0.000
train | epoch   2 | Iter:   3187/ 13000 | global iter:   3187/ 13000 | loss: 1.6422 | ds_loss: 0.0000 | lr: 4.3038e-05 | scale: 32768.0000 | micro time: 1.901 | step time: 0.000
train | epoch   2 | Iter:   3188/ 13000 | global iter:   3188/ 13000 | loss: 1.1414 | ds_loss: 0.0000 | lr: 4.3034e-05 | scale: 32768.0000 | micro time: 1.777 | step time: 0.000
train | epoch   2 | Iter:   3189/ 13000 | global iter:   3189/ 13000 | loss: 2.1772 | ds_loss: 0.0000 | lr: 4.3030e-05 | scale: 32768.0000 | micro time: 1.725 | step time: 0.000
train | epoch   2 | Iter:   3190/ 13000 | global iter:   3190/ 13000 | loss: 1.2277 | ds_loss: 0.0000 | lr: 4.3026e-05 | scale: 32768.0000 | micro time: 1.846 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3190/ 13000 | global iter:   3190/ 13000 | loss: 1.7596 | ds_loss: 0.0000 | lr: 4.3026e-05 | scale: 32768.0000 | micro time: 1.846 | step time: 1.805
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3191/ 13000 | global iter:   3191/ 13000 | loss: 1.6485 | ds_loss: 0.0000 | lr: 4.3022e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   2 | Iter:   3192/ 13000 | global iter:   3192/ 13000 | loss: 1.6602 | ds_loss: 0.0000 | lr: 4.3018e-05 | scale: 32768.0000 | micro time: 1.822 | step time: 0.000
train | epoch   2 | Iter:   3193/ 13000 | global iter:   3193/ 13000 | loss: 1.9513 | ds_loss: 0.0000 | lr: 4.3013e-05 | scale: 32768.0000 | micro time: 1.744 | step time: 0.000
train | epoch   2 | Iter:   3194/ 13000 | global iter:   3194/ 13000 | loss: 1.6233 | ds_loss: 0.0000 | lr: 4.3009e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   2 | Iter:   3195/ 13000 | global iter:   3195/ 13000 | loss: 2.1827 | ds_loss: 0.0000 | lr: 4.3005e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   2 | Iter:   3196/ 13000 | global iter:   3196/ 13000 | loss: 1.8896 | ds_loss: 0.0000 | lr: 4.3001e-05 | scale: 32768.0000 | micro time: 1.785 | step time: 0.000
train | epoch   2 | Iter:   3197/ 13000 | global iter:   3197/ 13000 | loss: 1.8834 | ds_loss: 0.0000 | lr: 4.2997e-05 | scale: 32768.0000 | micro time: 1.793 | step time: 0.000
train | epoch   2 | Iter:   3198/ 13000 | global iter:   3198/ 13000 | loss: 1.2109 | ds_loss: 0.0000 | lr: 4.2992e-05 | scale: 32768.0000 | micro time: 1.812 | step time: 0.000
train | epoch   2 | Iter:   3199/ 13000 | global iter:   3199/ 13000 | loss: 1.7433 | ds_loss: 0.0000 | lr: 4.2988e-05 | scale: 32768.0000 | micro time: 1.770 | step time: 0.000
train | epoch   2 | Iter:   3200/ 13000 | global iter:   3200/ 13000 | loss: 2.1421 | ds_loss: 0.0000 | lr: 4.2984e-05 | scale: 32768.0000 | micro time: 1.687 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3200/ 13000 | global iter:   3200/ 13000 | loss: 1.7935 | ds_loss: 0.0000 | lr: 4.2984e-05 | scale: 32768.0000 | micro time: 1.687 | step time: 1.783
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3201/ 13000 | global iter:   3201/ 13000 | loss: 2.0045 | ds_loss: 0.0000 | lr: 4.2980e-05 | scale: 32768.0000 | micro time: 1.862 | step time: 0.000
train | epoch   2 | Iter:   3202/ 13000 | global iter:   3202/ 13000 | loss: 1.9953 | ds_loss: 0.0000 | lr: 4.2976e-05 | scale: 32768.0000 | micro time: 1.707 | step time: 0.000
train | epoch   2 | Iter:   3203/ 13000 | global iter:   3203/ 13000 | loss: 2.0815 | ds_loss: 0.0000 | lr: 4.2971e-05 | scale: 32768.0000 | micro time: 1.796 | step time: 0.000
train | epoch   2 | Iter:   3204/ 13000 | global iter:   3204/ 13000 | loss: 2.2313 | ds_loss: 0.0000 | lr: 4.2967e-05 | scale: 32768.0000 | micro time: 1.865 | step time: 0.000
train | epoch   2 | Iter:   3205/ 13000 | global iter:   3205/ 13000 | loss: 1.8761 | ds_loss: 0.0000 | lr: 4.2963e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   2 | Iter:   3206/ 13000 | global iter:   3206/ 13000 | loss: 2.1317 | ds_loss: 0.0000 | lr: 4.2959e-05 | scale: 32768.0000 | micro time: 1.895 | step time: 0.000
train | epoch   2 | Iter:   3207/ 13000 | global iter:   3207/ 13000 | loss: 2.0951 | ds_loss: 0.0000 | lr: 4.2955e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   2 | Iter:   3208/ 13000 | global iter:   3208/ 13000 | loss: 1.6889 | ds_loss: 0.0000 | lr: 4.2950e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   2 | Iter:   3209/ 13000 | global iter:   3209/ 13000 | loss: 2.0957 | ds_loss: 0.0000 | lr: 4.2946e-05 | scale: 32768.0000 | micro time: 1.687 | step time: 0.000
train | epoch   2 | Iter:   3210/ 13000 | global iter:   3210/ 13000 | loss: 1.9584 | ds_loss: 0.0000 | lr: 4.2942e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3210/ 13000 | global iter:   3210/ 13000 | loss: 2.0158 | ds_loss: 0.0000 | lr: 4.2942e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 1.807
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3211/ 13000 | global iter:   3211/ 13000 | loss: 2.0601 | ds_loss: 0.0000 | lr: 4.2938e-05 | scale: 32768.0000 | micro time: 1.742 | step time: 0.000
train | epoch   2 | Iter:   3212/ 13000 | global iter:   3212/ 13000 | loss: 1.9212 | ds_loss: 0.0000 | lr: 4.2934e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
train | epoch   2 | Iter:   3213/ 13000 | global iter:   3213/ 13000 | loss: 1.6205 | ds_loss: 0.0000 | lr: 4.2929e-05 | scale: 32768.0000 | micro time: 1.887 | step time: 0.000
train | epoch   2 | Iter:   3214/ 13000 | global iter:   3214/ 13000 | loss: 1.9218 | ds_loss: 0.0000 | lr: 4.2925e-05 | scale: 32768.0000 | micro time: 1.731 | step time: 0.000
train | epoch   2 | Iter:   3215/ 13000 | global iter:   3215/ 13000 | loss: 1.5723 | ds_loss: 0.0000 | lr: 4.2921e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   2 | Iter:   3216/ 13000 | global iter:   3216/ 13000 | loss: 2.1100 | ds_loss: 0.0000 | lr: 4.2917e-05 | scale: 32768.0000 | micro time: 1.870 | step time: 0.000
train | epoch   2 | Iter:   3217/ 13000 | global iter:   3217/ 13000 | loss: 1.8255 | ds_loss: 0.0000 | lr: 4.2913e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 0.000
train | epoch   2 | Iter:   3218/ 13000 | global iter:   3218/ 13000 | loss: 1.6575 | ds_loss: 0.0000 | lr: 4.2908e-05 | scale: 32768.0000 | micro time: 1.848 | step time: 0.000
train | epoch   2 | Iter:   3219/ 13000 | global iter:   3219/ 13000 | loss: 2.0588 | ds_loss: 0.0000 | lr: 4.2904e-05 | scale: 32768.0000 | micro time: 1.745 | step time: 0.000
train | epoch   2 | Iter:   3220/ 13000 | global iter:   3220/ 13000 | loss: 1.3039 | ds_loss: 0.0000 | lr: 4.2900e-05 | scale: 32768.0000 | micro time: 1.727 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3220/ 13000 | global iter:   3220/ 13000 | loss: 1.8052 | ds_loss: 0.0000 | lr: 4.2900e-05 | scale: 32768.0000 | micro time: 1.727 | step time: 1.802
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3221/ 13000 | global iter:   3221/ 13000 | loss: 1.8456 | ds_loss: 0.0000 | lr: 4.2896e-05 | scale: 32768.0000 | micro time: 1.794 | step time: 0.000
train | epoch   2 | Iter:   3222/ 13000 | global iter:   3222/ 13000 | loss: 1.8090 | ds_loss: 0.0000 | lr: 4.2892e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 0.000
train | epoch   2 | Iter:   3223/ 13000 | global iter:   3223/ 13000 | loss: 1.5598 | ds_loss: 0.0000 | lr: 4.2887e-05 | scale: 32768.0000 | micro time: 1.809 | step time: 0.000
train | epoch   2 | Iter:   3224/ 13000 | global iter:   3224/ 13000 | loss: 2.0571 | ds_loss: 0.0000 | lr: 4.2883e-05 | scale: 32768.0000 | micro time: 1.872 | step time: 0.000
train | epoch   2 | Iter:   3225/ 13000 | global iter:   3225/ 13000 | loss: 1.6926 | ds_loss: 0.0000 | lr: 4.2879e-05 | scale: 32768.0000 | micro time: 1.792 | step time: 0.000
train | epoch   2 | Iter:   3226/ 13000 | global iter:   3226/ 13000 | loss: 1.6087 | ds_loss: 0.0000 | lr: 4.2875e-05 | scale: 32768.0000 | micro time: 1.862 | step time: 0.000
train | epoch   2 | Iter:   3227/ 13000 | global iter:   3227/ 13000 | loss: 2.2597 | ds_loss: 0.0000 | lr: 4.2870e-05 | scale: 32768.0000 | micro time: 1.895 | step time: 0.000
train | epoch   2 | Iter:   3228/ 13000 | global iter:   3228/ 13000 | loss: 2.1233 | ds_loss: 0.0000 | lr: 4.2866e-05 | scale: 32768.0000 | micro time: 1.715 | step time: 0.000
train | epoch   2 | Iter:   3229/ 13000 | global iter:   3229/ 13000 | loss: 1.7545 | ds_loss: 0.0000 | lr: 4.2862e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   2 | Iter:   3230/ 13000 | global iter:   3230/ 13000 | loss: 1.9467 | ds_loss: 0.0000 | lr: 4.2858e-05 | scale: 32768.0000 | micro time: 1.712 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3230/ 13000 | global iter:   3230/ 13000 | loss: 1.8657 | ds_loss: 0.0000 | lr: 4.2858e-05 | scale: 32768.0000 | micro time: 1.712 | step time: 1.817
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3231/ 13000 | global iter:   3231/ 13000 | loss: 1.8113 | ds_loss: 0.0000 | lr: 4.2854e-05 | scale: 32768.0000 | micro time: 1.798 | step time: 0.000
train | epoch   2 | Iter:   3232/ 13000 | global iter:   3232/ 13000 | loss: 2.1318 | ds_loss: 0.0000 | lr: 4.2849e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   2 | Iter:   3233/ 13000 | global iter:   3233/ 13000 | loss: 1.9505 | ds_loss: 0.0000 | lr: 4.2845e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   2 | Iter:   3234/ 13000 | global iter:   3234/ 13000 | loss: 1.5429 | ds_loss: 0.0000 | lr: 4.2841e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   2 | Iter:   3235/ 13000 | global iter:   3235/ 13000 | loss: 2.2054 | ds_loss: 0.0000 | lr: 4.2837e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   2 | Iter:   3236/ 13000 | global iter:   3236/ 13000 | loss: 2.3343 | ds_loss: 0.0000 | lr: 4.2832e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   2 | Iter:   3237/ 13000 | global iter:   3237/ 13000 | loss: 1.8122 | ds_loss: 0.0000 | lr: 4.2828e-05 | scale: 32768.0000 | micro time: 1.795 | step time: 0.000
train | epoch   2 | Iter:   3238/ 13000 | global iter:   3238/ 13000 | loss: 2.0112 | ds_loss: 0.0000 | lr: 4.2824e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   2 | Iter:   3239/ 13000 | global iter:   3239/ 13000 | loss: 1.4233 | ds_loss: 0.0000 | lr: 4.2820e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   2 | Iter:   3240/ 13000 | global iter:   3240/ 13000 | loss: 1.9790 | ds_loss: 0.0000 | lr: 4.2816e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3240/ 13000 | global iter:   3240/ 13000 | loss: 1.9202 | ds_loss: 0.0000 | lr: 4.2816e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 1.797
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3241/ 13000 | global iter:   3241/ 13000 | loss: 1.9209 | ds_loss: 0.0000 | lr: 4.2811e-05 | scale: 32768.0000 | micro time: 1.693 | step time: 0.000
train | epoch   2 | Iter:   3242/ 13000 | global iter:   3242/ 13000 | loss: 1.9938 | ds_loss: 0.0000 | lr: 4.2807e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   2 | Iter:   3243/ 13000 | global iter:   3243/ 13000 | loss: 2.2105 | ds_loss: 0.0000 | lr: 4.2803e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   2 | Iter:   3244/ 13000 | global iter:   3244/ 13000 | loss: 1.8311 | ds_loss: 0.0000 | lr: 4.2799e-05 | scale: 32768.0000 | micro time: 1.768 | step time: 0.000
train | epoch   2 | Iter:   3245/ 13000 | global iter:   3245/ 13000 | loss: 2.0953 | ds_loss: 0.0000 | lr: 4.2794e-05 | scale: 32768.0000 | micro time: 1.874 | step time: 0.000
train | epoch   2 | Iter:   3246/ 13000 | global iter:   3246/ 13000 | loss: 2.2780 | ds_loss: 0.0000 | lr: 4.2790e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   2 | Iter:   3247/ 13000 | global iter:   3247/ 13000 | loss: 2.1980 | ds_loss: 0.0000 | lr: 4.2786e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   2 | Iter:   3248/ 13000 | global iter:   3248/ 13000 | loss: 1.1303 | ds_loss: 0.0000 | lr: 4.2782e-05 | scale: 32768.0000 | micro time: 1.947 | step time: 0.000
train | epoch   2 | Iter:   3249/ 13000 | global iter:   3249/ 13000 | loss: 2.2729 | ds_loss: 0.0000 | lr: 4.2777e-05 | scale: 32768.0000 | micro time: 1.788 | step time: 0.000
train | epoch   2 | Iter:   3250/ 13000 | global iter:   3250/ 13000 | loss: 1.9631 | ds_loss: 0.0000 | lr: 4.2773e-05 | scale: 32768.0000 | micro time: 1.740 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3250/ 13000 | global iter:   3250/ 13000 | loss: 1.9894 | ds_loss: 0.0000 | lr: 4.2773e-05 | scale: 32768.0000 | micro time: 1.740 | step time: 1.802
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3251/ 13000 | global iter:   3251/ 13000 | loss: 1.9905 | ds_loss: 0.0000 | lr: 4.2769e-05 | scale: 32768.0000 | micro time: 1.790 | step time: 0.000
train | epoch   2 | Iter:   3252/ 13000 | global iter:   3252/ 13000 | loss: 1.9283 | ds_loss: 0.0000 | lr: 4.2765e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   2 | Iter:   3253/ 13000 | global iter:   3253/ 13000 | loss: 1.7352 | ds_loss: 0.0000 | lr: 4.2760e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   2 | Iter:   3254/ 13000 | global iter:   3254/ 13000 | loss: 2.4927 | ds_loss: 0.0000 | lr: 4.2756e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   2 | Iter:   3255/ 13000 | global iter:   3255/ 13000 | loss: 2.3793 | ds_loss: 0.0000 | lr: 4.2752e-05 | scale: 32768.0000 | micro time: 1.769 | step time: 0.000
train | epoch   2 | Iter:   3256/ 13000 | global iter:   3256/ 13000 | loss: 1.6943 | ds_loss: 0.0000 | lr: 4.2748e-05 | scale: 32768.0000 | micro time: 1.857 | step time: 0.000
train | epoch   2 | Iter:   3257/ 13000 | global iter:   3257/ 13000 | loss: 2.2660 | ds_loss: 0.0000 | lr: 4.2743e-05 | scale: 32768.0000 | micro time: 1.903 | step time: 0.000
train | epoch   2 | Iter:   3258/ 13000 | global iter:   3258/ 13000 | loss: 1.6020 | ds_loss: 0.0000 | lr: 4.2739e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   2 | Iter:   3259/ 13000 | global iter:   3259/ 13000 | loss: 1.6640 | ds_loss: 0.0000 | lr: 4.2735e-05 | scale: 32768.0000 | micro time: 1.682 | step time: 0.000
train | epoch   2 | Iter:   3260/ 13000 | global iter:   3260/ 13000 | loss: 1.7868 | ds_loss: 0.0000 | lr: 4.2731e-05 | scale: 32768.0000 | micro time: 1.796 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3260/ 13000 | global iter:   3260/ 13000 | loss: 1.9539 | ds_loss: 0.0000 | lr: 4.2731e-05 | scale: 32768.0000 | micro time: 1.796 | step time: 1.810
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3261/ 13000 | global iter:   3261/ 13000 | loss: 1.1399 | ds_loss: 0.0000 | lr: 4.2726e-05 | scale: 32768.0000 | micro time: 1.866 | step time: 0.000
train | epoch   2 | Iter:   3262/ 13000 | global iter:   3262/ 13000 | loss: 2.0242 | ds_loss: 0.0000 | lr: 4.2722e-05 | scale: 32768.0000 | micro time: 1.821 | step time: 0.000
train | epoch   2 | Iter:   3263/ 13000 | global iter:   3263/ 13000 | loss: 2.2665 | ds_loss: 0.0000 | lr: 4.2718e-05 | scale: 32768.0000 | micro time: 1.760 | step time: 0.000
train | epoch   2 | Iter:   3264/ 13000 | global iter:   3264/ 13000 | loss: 1.7919 | ds_loss: 0.0000 | lr: 4.2714e-05 | scale: 32768.0000 | micro time: 1.850 | step time: 0.000
train | epoch   2 | Iter:   3265/ 13000 | global iter:   3265/ 13000 | loss: 1.7559 | ds_loss: 0.0000 | lr: 4.2709e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   2 | Iter:   3266/ 13000 | global iter:   3266/ 13000 | loss: 1.7183 | ds_loss: 0.0000 | lr: 4.2705e-05 | scale: 32768.0000 | micro time: 1.716 | step time: 0.000
train | epoch   2 | Iter:   3267/ 13000 | global iter:   3267/ 13000 | loss: 2.0316 | ds_loss: 0.0000 | lr: 4.2701e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   2 | Iter:   3268/ 13000 | global iter:   3268/ 13000 | loss: 1.5416 | ds_loss: 0.0000 | lr: 4.2697e-05 | scale: 32768.0000 | micro time: 1.731 | step time: 0.000
train | epoch   2 | Iter:   3269/ 13000 | global iter:   3269/ 13000 | loss: 1.6599 | ds_loss: 0.0000 | lr: 4.2692e-05 | scale: 32768.0000 | micro time: 1.796 | step time: 0.000
train | epoch   2 | Iter:   3270/ 13000 | global iter:   3270/ 13000 | loss: 2.1095 | ds_loss: 0.0000 | lr: 4.2688e-05 | scale: 32768.0000 | micro time: 1.814 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3270/ 13000 | global iter:   3270/ 13000 | loss: 1.8039 | ds_loss: 0.0000 | lr: 4.2688e-05 | scale: 32768.0000 | micro time: 1.814 | step time: 1.806
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3271/ 13000 | global iter:   3271/ 13000 | loss: 1.9101 | ds_loss: 0.0000 | lr: 4.2684e-05 | scale: 32768.0000 | micro time: 1.774 | step time: 0.000
train | epoch   2 | Iter:   3272/ 13000 | global iter:   3272/ 13000 | loss: 2.1997 | ds_loss: 0.0000 | lr: 4.2680e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 0.000
train | epoch   2 | Iter:   3273/ 13000 | global iter:   3273/ 13000 | loss: 1.6569 | ds_loss: 0.0000 | lr: 4.2675e-05 | scale: 32768.0000 | micro time: 1.757 | step time: 0.000
train | epoch   2 | Iter:   3274/ 13000 | global iter:   3274/ 13000 | loss: 1.8269 | ds_loss: 0.0000 | lr: 4.2671e-05 | scale: 32768.0000 | micro time: 1.845 | step time: 0.000
train | epoch   2 | Iter:   3275/ 13000 | global iter:   3275/ 13000 | loss: 1.8620 | ds_loss: 0.0000 | lr: 4.2667e-05 | scale: 32768.0000 | micro time: 1.856 | step time: 0.000
train | epoch   2 | Iter:   3276/ 13000 | global iter:   3276/ 13000 | loss: 1.8993 | ds_loss: 0.0000 | lr: 4.2662e-05 | scale: 32768.0000 | micro time: 1.900 | step time: 0.000
train | epoch   2 | Iter:   3277/ 13000 | global iter:   3277/ 13000 | loss: 1.4346 | ds_loss: 0.0000 | lr: 4.2658e-05 | scale: 32768.0000 | micro time: 1.909 | step time: 0.000
train | epoch   2 | Iter:   3278/ 13000 | global iter:   3278/ 13000 | loss: 2.1262 | ds_loss: 0.0000 | lr: 4.2654e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   2 | Iter:   3279/ 13000 | global iter:   3279/ 13000 | loss: 1.5979 | ds_loss: 0.0000 | lr: 4.2650e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   2 | Iter:   3280/ 13000 | global iter:   3280/ 13000 | loss: 2.0294 | ds_loss: 0.0000 | lr: 4.2645e-05 | scale: 32768.0000 | micro time: 1.971 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3280/ 13000 | global iter:   3280/ 13000 | loss: 1.8543 | ds_loss: 0.0000 | lr: 4.2645e-05 | scale: 32768.0000 | micro time: 1.971 | step time: 1.856
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3281/ 13000 | global iter:   3281/ 13000 | loss: 1.9030 | ds_loss: 0.0000 | lr: 4.2641e-05 | scale: 32768.0000 | micro time: 1.894 | step time: 0.000
train | epoch   2 | Iter:   3282/ 13000 | global iter:   3282/ 13000 | loss: 2.0683 | ds_loss: 0.0000 | lr: 4.2637e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   2 | Iter:   3283/ 13000 | global iter:   3283/ 13000 | loss: 2.1859 | ds_loss: 0.0000 | lr: 4.2633e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   2 | Iter:   3284/ 13000 | global iter:   3284/ 13000 | loss: 2.2103 | ds_loss: 0.0000 | lr: 4.2628e-05 | scale: 32768.0000 | micro time: 1.677 | step time: 0.000
train | epoch   2 | Iter:   3285/ 13000 | global iter:   3285/ 13000 | loss: 2.0751 | ds_loss: 0.0000 | lr: 4.2624e-05 | scale: 32768.0000 | micro time: 1.841 | step time: 0.000
train | epoch   2 | Iter:   3286/ 13000 | global iter:   3286/ 13000 | loss: 1.5316 | ds_loss: 0.0000 | lr: 4.2620e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   2 | Iter:   3287/ 13000 | global iter:   3287/ 13000 | loss: 1.6049 | ds_loss: 0.0000 | lr: 4.2615e-05 | scale: 32768.0000 | micro time: 1.899 | step time: 0.000
train | epoch   2 | Iter:   3288/ 13000 | global iter:   3288/ 13000 | loss: 1.1408 | ds_loss: 0.0000 | lr: 4.2611e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   2 | Iter:   3289/ 13000 | global iter:   3289/ 13000 | loss: 2.2228 | ds_loss: 0.0000 | lr: 4.2607e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   2 | Iter:   3290/ 13000 | global iter:   3290/ 13000 | loss: 1.7376 | ds_loss: 0.0000 | lr: 4.2603e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3290/ 13000 | global iter:   3290/ 13000 | loss: 1.8680 | ds_loss: 0.0000 | lr: 4.2603e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 1.828
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3291/ 13000 | global iter:   3291/ 13000 | loss: 1.8485 | ds_loss: 0.0000 | lr: 4.2598e-05 | scale: 32768.0000 | micro time: 1.814 | step time: 0.000
train | epoch   2 | Iter:   3292/ 13000 | global iter:   3292/ 13000 | loss: 2.1001 | ds_loss: 0.0000 | lr: 4.2594e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   2 | Iter:   3293/ 13000 | global iter:   3293/ 13000 | loss: 2.0315 | ds_loss: 0.0000 | lr: 4.2590e-05 | scale: 32768.0000 | micro time: 1.868 | step time: 0.000
train | epoch   2 | Iter:   3294/ 13000 | global iter:   3294/ 13000 | loss: 1.4435 | ds_loss: 0.0000 | lr: 4.2585e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   2 | Iter:   3295/ 13000 | global iter:   3295/ 13000 | loss: 1.8857 | ds_loss: 0.0000 | lr: 4.2581e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   2 | Iter:   3296/ 13000 | global iter:   3296/ 13000 | loss: 2.1783 | ds_loss: 0.0000 | lr: 4.2577e-05 | scale: 32768.0000 | micro time: 1.744 | step time: 0.000
train | epoch   2 | Iter:   3297/ 13000 | global iter:   3297/ 13000 | loss: 1.7993 | ds_loss: 0.0000 | lr: 4.2573e-05 | scale: 32768.0000 | micro time: 1.818 | step time: 0.000
train | epoch   2 | Iter:   3298/ 13000 | global iter:   3298/ 13000 | loss: 2.1862 | ds_loss: 0.0000 | lr: 4.2568e-05 | scale: 32768.0000 | micro time: 1.879 | step time: 0.000
train | epoch   2 | Iter:   3299/ 13000 | global iter:   3299/ 13000 | loss: 1.5810 | ds_loss: 0.0000 | lr: 4.2564e-05 | scale: 32768.0000 | micro time: 1.822 | step time: 0.000
train | epoch   2 | Iter:   3300/ 13000 | global iter:   3300/ 13000 | loss: 2.2967 | ds_loss: 0.0000 | lr: 4.2560e-05 | scale: 32768.0000 | micro time: 1.887 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3300/ 13000 | global iter:   3300/ 13000 | loss: 1.9351 | ds_loss: 0.0000 | lr: 4.2560e-05 | scale: 32768.0000 | micro time: 1.887 | step time: 1.827
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3301/ 13000 | global iter:   3301/ 13000 | loss: 2.2307 | ds_loss: 0.0000 | lr: 4.2555e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   2 | Iter:   3302/ 13000 | global iter:   3302/ 13000 | loss: 1.8668 | ds_loss: 0.0000 | lr: 4.2551e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   2 | Iter:   3303/ 13000 | global iter:   3303/ 13000 | loss: 1.8700 | ds_loss: 0.0000 | lr: 4.2547e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   2 | Iter:   3304/ 13000 | global iter:   3304/ 13000 | loss: 1.8134 | ds_loss: 0.0000 | lr: 4.2542e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   2 | Iter:   3305/ 13000 | global iter:   3305/ 13000 | loss: 1.9838 | ds_loss: 0.0000 | lr: 4.2538e-05 | scale: 32768.0000 | micro time: 1.761 | step time: 0.000
train | epoch   2 | Iter:   3306/ 13000 | global iter:   3306/ 13000 | loss: 1.7194 | ds_loss: 0.0000 | lr: 4.2534e-05 | scale: 32768.0000 | micro time: 1.814 | step time: 0.000
train | epoch   2 | Iter:   3307/ 13000 | global iter:   3307/ 13000 | loss: 2.2283 | ds_loss: 0.0000 | lr: 4.2530e-05 | scale: 32768.0000 | micro time: 1.800 | step time: 0.000
train | epoch   2 | Iter:   3308/ 13000 | global iter:   3308/ 13000 | loss: 1.6064 | ds_loss: 0.0000 | lr: 4.2525e-05 | scale: 32768.0000 | micro time: 1.817 | step time: 0.000
train | epoch   2 | Iter:   3309/ 13000 | global iter:   3309/ 13000 | loss: 1.9287 | ds_loss: 0.0000 | lr: 4.2521e-05 | scale: 32768.0000 | micro time: 1.777 | step time: 0.000
train | epoch   2 | Iter:   3310/ 13000 | global iter:   3310/ 13000 | loss: 1.8106 | ds_loss: 0.0000 | lr: 4.2517e-05 | scale: 32768.0000 | micro time: 1.821 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3310/ 13000 | global iter:   3310/ 13000 | loss: 1.9058 | ds_loss: 0.0000 | lr: 4.2517e-05 | scale: 32768.0000 | micro time: 1.821 | step time: 1.819
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3311/ 13000 | global iter:   3311/ 13000 | loss: 1.3397 | ds_loss: 0.0000 | lr: 4.2512e-05 | scale: 32768.0000 | micro time: 1.781 | step time: 0.000
train | epoch   2 | Iter:   3312/ 13000 | global iter:   3312/ 13000 | loss: 1.8726 | ds_loss: 0.0000 | lr: 4.2508e-05 | scale: 32768.0000 | micro time: 1.795 | step time: 0.000
train | epoch   2 | Iter:   3313/ 13000 | global iter:   3313/ 13000 | loss: 1.3823 | ds_loss: 0.0000 | lr: 4.2504e-05 | scale: 32768.0000 | micro time: 1.824 | step time: 0.000
train | epoch   2 | Iter:   3314/ 13000 | global iter:   3314/ 13000 | loss: 1.8600 | ds_loss: 0.0000 | lr: 4.2499e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 0.000
train | epoch   2 | Iter:   3315/ 13000 | global iter:   3315/ 13000 | loss: 2.0343 | ds_loss: 0.0000 | lr: 4.2495e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   2 | Iter:   3316/ 13000 | global iter:   3316/ 13000 | loss: 1.7197 | ds_loss: 0.0000 | lr: 4.2491e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   2 | Iter:   3317/ 13000 | global iter:   3317/ 13000 | loss: 1.6659 | ds_loss: 0.0000 | lr: 4.2486e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   2 | Iter:   3318/ 13000 | global iter:   3318/ 13000 | loss: 2.0684 | ds_loss: 0.0000 | lr: 4.2482e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   2 | Iter:   3319/ 13000 | global iter:   3319/ 13000 | loss: 1.4392 | ds_loss: 0.0000 | lr: 4.2478e-05 | scale: 32768.0000 | micro time: 1.826 | step time: 0.000
train | epoch   2 | Iter:   3320/ 13000 | global iter:   3320/ 13000 | loss: 1.4414 | ds_loss: 0.0000 | lr: 4.2474e-05 | scale: 32768.0000 | micro time: 1.788 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3320/ 13000 | global iter:   3320/ 13000 | loss: 1.6824 | ds_loss: 0.0000 | lr: 4.2474e-05 | scale: 32768.0000 | micro time: 1.788 | step time: 1.816
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3321/ 13000 | global iter:   3321/ 13000 | loss: 1.9207 | ds_loss: 0.0000 | lr: 4.2469e-05 | scale: 32768.0000 | micro time: 1.877 | step time: 0.000
train | epoch   2 | Iter:   3322/ 13000 | global iter:   3322/ 13000 | loss: 2.1131 | ds_loss: 0.0000 | lr: 4.2465e-05 | scale: 32768.0000 | micro time: 1.747 | step time: 0.000
train | epoch   2 | Iter:   3323/ 13000 | global iter:   3323/ 13000 | loss: 1.8039 | ds_loss: 0.0000 | lr: 4.2461e-05 | scale: 32768.0000 | micro time: 1.731 | step time: 0.000
train | epoch   2 | Iter:   3324/ 13000 | global iter:   3324/ 13000 | loss: 1.9646 | ds_loss: 0.0000 | lr: 4.2456e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   2 | Iter:   3325/ 13000 | global iter:   3325/ 13000 | loss: 1.8337 | ds_loss: 0.0000 | lr: 4.2452e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   2 | Iter:   3326/ 13000 | global iter:   3326/ 13000 | loss: 1.7791 | ds_loss: 0.0000 | lr: 4.2448e-05 | scale: 32768.0000 | micro time: 1.673 | step time: 0.000
train | epoch   2 | Iter:   3327/ 13000 | global iter:   3327/ 13000 | loss: 2.1521 | ds_loss: 0.0000 | lr: 4.2443e-05 | scale: 32768.0000 | micro time: 1.861 | step time: 0.000
train | epoch   2 | Iter:   3328/ 13000 | global iter:   3328/ 13000 | loss: 1.3883 | ds_loss: 0.0000 | lr: 4.2439e-05 | scale: 32768.0000 | micro time: 1.865 | step time: 0.000
train | epoch   2 | Iter:   3329/ 13000 | global iter:   3329/ 13000 | loss: 2.5404 | ds_loss: 0.0000 | lr: 4.2435e-05 | scale: 32768.0000 | micro time: 1.893 | step time: 0.000
train | epoch   2 | Iter:   3330/ 13000 | global iter:   3330/ 13000 | loss: 2.1776 | ds_loss: 0.0000 | lr: 4.2430e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3330/ 13000 | global iter:   3330/ 13000 | loss: 1.9673 | ds_loss: 0.0000 | lr: 4.2430e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 1.817
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3331/ 13000 | global iter:   3331/ 13000 | loss: 1.9606 | ds_loss: 0.0000 | lr: 4.2426e-05 | scale: 32768.0000 | micro time: 1.885 | step time: 0.000
train | epoch   2 | Iter:   3332/ 13000 | global iter:   3332/ 13000 | loss: 2.1900 | ds_loss: 0.0000 | lr: 4.2422e-05 | scale: 32768.0000 | micro time: 1.885 | step time: 0.000
train | epoch   2 | Iter:   3333/ 13000 | global iter:   3333/ 13000 | loss: 1.9958 | ds_loss: 0.0000 | lr: 4.2417e-05 | scale: 32768.0000 | micro time: 1.853 | step time: 0.000
train | epoch   2 | Iter:   3334/ 13000 | global iter:   3334/ 13000 | loss: 1.7801 | ds_loss: 0.0000 | lr: 4.2413e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   2 | Iter:   3335/ 13000 | global iter:   3335/ 13000 | loss: 1.4973 | ds_loss: 0.0000 | lr: 4.2409e-05 | scale: 32768.0000 | micro time: 1.801 | step time: 0.000
train | epoch   2 | Iter:   3336/ 13000 | global iter:   3336/ 13000 | loss: 1.8845 | ds_loss: 0.0000 | lr: 4.2404e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   2 | Iter:   3337/ 13000 | global iter:   3337/ 13000 | loss: 1.7370 | ds_loss: 0.0000 | lr: 4.2400e-05 | scale: 32768.0000 | micro time: 1.857 | step time: 0.000
train | epoch   2 | Iter:   3338/ 13000 | global iter:   3338/ 13000 | loss: 1.9780 | ds_loss: 0.0000 | lr: 4.2396e-05 | scale: 32768.0000 | micro time: 1.749 | step time: 0.000
train | epoch   2 | Iter:   3339/ 13000 | global iter:   3339/ 13000 | loss: 1.5111 | ds_loss: 0.0000 | lr: 4.2391e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   2 | Iter:   3340/ 13000 | global iter:   3340/ 13000 | loss: 1.5217 | ds_loss: 0.0000 | lr: 4.2387e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3340/ 13000 | global iter:   3340/ 13000 | loss: 1.8056 | ds_loss: 0.0000 | lr: 4.2387e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 1.815
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3341/ 13000 | global iter:   3341/ 13000 | loss: 1.9913 | ds_loss: 0.0000 | lr: 4.2383e-05 | scale: 32768.0000 | micro time: 1.797 | step time: 0.000
train | epoch   2 | Iter:   3342/ 13000 | global iter:   3342/ 13000 | loss: 1.3623 | ds_loss: 0.0000 | lr: 4.2378e-05 | scale: 32768.0000 | micro time: 1.891 | step time: 0.000
train | epoch   2 | Iter:   3343/ 13000 | global iter:   3343/ 13000 | loss: 2.2022 | ds_loss: 0.0000 | lr: 4.2374e-05 | scale: 32768.0000 | micro time: 1.813 | step time: 0.000
train | epoch   2 | Iter:   3344/ 13000 | global iter:   3344/ 13000 | loss: 2.0773 | ds_loss: 0.0000 | lr: 4.2370e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   2 | Iter:   3345/ 13000 | global iter:   3345/ 13000 | loss: 1.4181 | ds_loss: 0.0000 | lr: 4.2365e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   2 | Iter:   3346/ 13000 | global iter:   3346/ 13000 | loss: 2.2664 | ds_loss: 0.0000 | lr: 4.2361e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
train | epoch   2 | Iter:   3347/ 13000 | global iter:   3347/ 13000 | loss: 1.7521 | ds_loss: 0.0000 | lr: 4.2357e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   2 | Iter:   3348/ 13000 | global iter:   3348/ 13000 | loss: 2.1752 | ds_loss: 0.0000 | lr: 4.2352e-05 | scale: 32768.0000 | micro time: 1.731 | step time: 0.000
train | epoch   2 | Iter:   3349/ 13000 | global iter:   3349/ 13000 | loss: 1.9141 | ds_loss: 0.0000 | lr: 4.2348e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   2 | Iter:   3350/ 13000 | global iter:   3350/ 13000 | loss: 1.9190 | ds_loss: 0.0000 | lr: 4.2344e-05 | scale: 32768.0000 | micro time: 1.828 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3350/ 13000 | global iter:   3350/ 13000 | loss: 1.9078 | ds_loss: 0.0000 | lr: 4.2344e-05 | scale: 32768.0000 | micro time: 1.828 | step time: 1.812
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3351/ 13000 | global iter:   3351/ 13000 | loss: 1.8055 | ds_loss: 0.0000 | lr: 4.2339e-05 | scale: 32768.0000 | micro time: 1.774 | step time: 0.000
train | epoch   2 | Iter:   3352/ 13000 | global iter:   3352/ 13000 | loss: 2.0669 | ds_loss: 0.0000 | lr: 4.2335e-05 | scale: 32768.0000 | micro time: 1.801 | step time: 0.000
train | epoch   2 | Iter:   3353/ 13000 | global iter:   3353/ 13000 | loss: 1.7419 | ds_loss: 0.0000 | lr: 4.2331e-05 | scale: 32768.0000 | micro time: 1.781 | step time: 0.000
train | epoch   2 | Iter:   3354/ 13000 | global iter:   3354/ 13000 | loss: 1.8118 | ds_loss: 0.0000 | lr: 4.2326e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   2 | Iter:   3355/ 13000 | global iter:   3355/ 13000 | loss: 1.3776 | ds_loss: 0.0000 | lr: 4.2322e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   2 | Iter:   3356/ 13000 | global iter:   3356/ 13000 | loss: 1.7587 | ds_loss: 0.0000 | lr: 4.2318e-05 | scale: 32768.0000 | micro time: 1.751 | step time: 0.000
train | epoch   2 | Iter:   3357/ 13000 | global iter:   3357/ 13000 | loss: 1.3142 | ds_loss: 0.0000 | lr: 4.2313e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   2 | Iter:   3358/ 13000 | global iter:   3358/ 13000 | loss: 1.8093 | ds_loss: 0.0000 | lr: 4.2309e-05 | scale: 32768.0000 | micro time: 1.731 | step time: 0.000
train | epoch   2 | Iter:   3359/ 13000 | global iter:   3359/ 13000 | loss: 2.0957 | ds_loss: 0.0000 | lr: 4.2304e-05 | scale: 32768.0000 | micro time: 1.804 | step time: 0.000
train | epoch   2 | Iter:   3360/ 13000 | global iter:   3360/ 13000 | loss: 2.2491 | ds_loss: 0.0000 | lr: 4.2300e-05 | scale: 32768.0000 | micro time: 1.830 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3360/ 13000 | global iter:   3360/ 13000 | loss: 1.8031 | ds_loss: 0.0000 | lr: 4.2300e-05 | scale: 32768.0000 | micro time: 1.830 | step time: 1.798
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3361/ 13000 | global iter:   3361/ 13000 | loss: 1.7124 | ds_loss: 0.0000 | lr: 4.2296e-05 | scale: 32768.0000 | micro time: 1.881 | step time: 0.000
train | epoch   2 | Iter:   3362/ 13000 | global iter:   3362/ 13000 | loss: 1.7868 | ds_loss: 0.0000 | lr: 4.2291e-05 | scale: 32768.0000 | micro time: 1.820 | step time: 0.000
train | epoch   2 | Iter:   3363/ 13000 | global iter:   3363/ 13000 | loss: 1.8198 | ds_loss: 0.0000 | lr: 4.2287e-05 | scale: 32768.0000 | micro time: 1.730 | step time: 0.000
train | epoch   2 | Iter:   3364/ 13000 | global iter:   3364/ 13000 | loss: 2.3632 | ds_loss: 0.0000 | lr: 4.2283e-05 | scale: 32768.0000 | micro time: 1.801 | step time: 0.000
train | epoch   2 | Iter:   3365/ 13000 | global iter:   3365/ 13000 | loss: 1.7387 | ds_loss: 0.0000 | lr: 4.2278e-05 | scale: 32768.0000 | micro time: 1.769 | step time: 0.000
train | epoch   2 | Iter:   3366/ 13000 | global iter:   3366/ 13000 | loss: 2.0911 | ds_loss: 0.0000 | lr: 4.2274e-05 | scale: 32768.0000 | micro time: 1.809 | step time: 0.000
train | epoch   2 | Iter:   3367/ 13000 | global iter:   3367/ 13000 | loss: 1.9367 | ds_loss: 0.0000 | lr: 4.2270e-05 | scale: 32768.0000 | micro time: 1.762 | step time: 0.000
train | epoch   2 | Iter:   3368/ 13000 | global iter:   3368/ 13000 | loss: 1.9453 | ds_loss: 0.0000 | lr: 4.2265e-05 | scale: 32768.0000 | micro time: 1.895 | step time: 0.000
train | epoch   2 | Iter:   3369/ 13000 | global iter:   3369/ 13000 | loss: 1.9152 | ds_loss: 0.0000 | lr: 4.2261e-05 | scale: 32768.0000 | micro time: 1.743 | step time: 0.000
train | epoch   2 | Iter:   3370/ 13000 | global iter:   3370/ 13000 | loss: 1.0198 | ds_loss: 0.0000 | lr: 4.2256e-05 | scale: 32768.0000 | micro time: 1.817 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3370/ 13000 | global iter:   3370/ 13000 | loss: 1.8329 | ds_loss: 0.0000 | lr: 4.2256e-05 | scale: 32768.0000 | micro time: 1.817 | step time: 1.803
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3371/ 13000 | global iter:   3371/ 13000 | loss: 1.8162 | ds_loss: 0.0000 | lr: 4.2252e-05 | scale: 32768.0000 | micro time: 1.796 | step time: 0.000
train | epoch   2 | Iter:   3372/ 13000 | global iter:   3372/ 13000 | loss: 1.8489 | ds_loss: 0.0000 | lr: 4.2248e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   2 | Iter:   3373/ 13000 | global iter:   3373/ 13000 | loss: 1.3858 | ds_loss: 0.0000 | lr: 4.2243e-05 | scale: 32768.0000 | micro time: 1.879 | step time: 0.000
train | epoch   2 | Iter:   3374/ 13000 | global iter:   3374/ 13000 | loss: 2.0587 | ds_loss: 0.0000 | lr: 4.2239e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   2 | Iter:   3375/ 13000 | global iter:   3375/ 13000 | loss: 1.5978 | ds_loss: 0.0000 | lr: 4.2235e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   2 | Iter:   3376/ 13000 | global iter:   3376/ 13000 | loss: 2.0198 | ds_loss: 0.0000 | lr: 4.2230e-05 | scale: 32768.0000 | micro time: 1.918 | step time: 0.000
train | epoch   2 | Iter:   3377/ 13000 | global iter:   3377/ 13000 | loss: 1.6886 | ds_loss: 0.0000 | lr: 4.2226e-05 | scale: 32768.0000 | micro time: 1.869 | step time: 0.000
train | epoch   2 | Iter:   3378/ 13000 | global iter:   3378/ 13000 | loss: 1.7952 | ds_loss: 0.0000 | lr: 4.2222e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 0.000
train | epoch   2 | Iter:   3379/ 13000 | global iter:   3379/ 13000 | loss: 1.8122 | ds_loss: 0.0000 | lr: 4.2217e-05 | scale: 32768.0000 | micro time: 1.820 | step time: 0.000
train | epoch   2 | Iter:   3380/ 13000 | global iter:   3380/ 13000 | loss: 1.2948 | ds_loss: 0.0000 | lr: 4.2213e-05 | scale: 32768.0000 | micro time: 1.814 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3380/ 13000 | global iter:   3380/ 13000 | loss: 1.7318 | ds_loss: 0.0000 | lr: 4.2213e-05 | scale: 32768.0000 | micro time: 1.814 | step time: 1.835
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3381/ 13000 | global iter:   3381/ 13000 | loss: 1.5905 | ds_loss: 0.0000 | lr: 4.2208e-05 | scale: 32768.0000 | micro time: 1.766 | step time: 0.000
train | epoch   2 | Iter:   3382/ 13000 | global iter:   3382/ 13000 | loss: 1.7746 | ds_loss: 0.0000 | lr: 4.2204e-05 | scale: 32768.0000 | micro time: 1.764 | step time: 0.000
train | epoch   2 | Iter:   3383/ 13000 | global iter:   3383/ 13000 | loss: 1.5846 | ds_loss: 0.0000 | lr: 4.2200e-05 | scale: 32768.0000 | micro time: 1.854 | step time: 0.000
train | epoch   2 | Iter:   3384/ 13000 | global iter:   3384/ 13000 | loss: 2.0341 | ds_loss: 0.0000 | lr: 4.2195e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   2 | Iter:   3385/ 13000 | global iter:   3385/ 13000 | loss: 1.6837 | ds_loss: 0.0000 | lr: 4.2191e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
train | epoch   2 | Iter:   3386/ 13000 | global iter:   3386/ 13000 | loss: 1.9966 | ds_loss: 0.0000 | lr: 4.2187e-05 | scale: 32768.0000 | micro time: 1.667 | step time: 0.000
train | epoch   2 | Iter:   3387/ 13000 | global iter:   3387/ 13000 | loss: 1.5223 | ds_loss: 0.0000 | lr: 4.2182e-05 | scale: 32768.0000 | micro time: 1.699 | step time: 0.000
train | epoch   2 | Iter:   3388/ 13000 | global iter:   3388/ 13000 | loss: 1.7251 | ds_loss: 0.0000 | lr: 4.2178e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   2 | Iter:   3389/ 13000 | global iter:   3389/ 13000 | loss: 1.7520 | ds_loss: 0.0000 | lr: 4.2173e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   2 | Iter:   3390/ 13000 | global iter:   3390/ 13000 | loss: 1.9170 | ds_loss: 0.0000 | lr: 4.2169e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3390/ 13000 | global iter:   3390/ 13000 | loss: 1.7581 | ds_loss: 0.0000 | lr: 4.2169e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 1.781
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3391/ 13000 | global iter:   3391/ 13000 | loss: 2.4558 | ds_loss: 0.0000 | lr: 4.2165e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   2 | Iter:   3392/ 13000 | global iter:   3392/ 13000 | loss: 1.6975 | ds_loss: 0.0000 | lr: 4.2160e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   2 | Iter:   3393/ 13000 | global iter:   3393/ 13000 | loss: 1.8235 | ds_loss: 0.0000 | lr: 4.2156e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   2 | Iter:   3394/ 13000 | global iter:   3394/ 13000 | loss: 1.8129 | ds_loss: 0.0000 | lr: 4.2151e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   2 | Iter:   3395/ 13000 | global iter:   3395/ 13000 | loss: 2.2679 | ds_loss: 0.0000 | lr: 4.2147e-05 | scale: 32768.0000 | micro time: 1.782 | step time: 0.000
train | epoch   2 | Iter:   3396/ 13000 | global iter:   3396/ 13000 | loss: 1.7387 | ds_loss: 0.0000 | lr: 4.2143e-05 | scale: 32768.0000 | micro time: 1.708 | step time: 0.000
train | epoch   2 | Iter:   3397/ 13000 | global iter:   3397/ 13000 | loss: 1.7053 | ds_loss: 0.0000 | lr: 4.2138e-05 | scale: 32768.0000 | micro time: 1.854 | step time: 0.000
train | epoch   2 | Iter:   3398/ 13000 | global iter:   3398/ 13000 | loss: 1.7737 | ds_loss: 0.0000 | lr: 4.2134e-05 | scale: 32768.0000 | micro time: 1.743 | step time: 0.000
train | epoch   2 | Iter:   3399/ 13000 | global iter:   3399/ 13000 | loss: 1.6523 | ds_loss: 0.0000 | lr: 4.2129e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   2 | Iter:   3400/ 13000 | global iter:   3400/ 13000 | loss: 1.7716 | ds_loss: 0.0000 | lr: 4.2125e-05 | scale: 32768.0000 | micro time: 1.813 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3400/ 13000 | global iter:   3400/ 13000 | loss: 1.8699 | ds_loss: 0.0000 | lr: 4.2125e-05 | scale: 32768.0000 | micro time: 1.813 | step time: 1.786
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3401/ 13000 | global iter:   3401/ 13000 | loss: 1.2758 | ds_loss: 0.0000 | lr: 4.2121e-05 | scale: 32768.0000 | micro time: 1.879 | step time: 0.000
train | epoch   2 | Iter:   3402/ 13000 | global iter:   3402/ 13000 | loss: 1.3660 | ds_loss: 0.0000 | lr: 4.2116e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   2 | Iter:   3403/ 13000 | global iter:   3403/ 13000 | loss: 1.6418 | ds_loss: 0.0000 | lr: 4.2112e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   2 | Iter:   3404/ 13000 | global iter:   3404/ 13000 | loss: 1.8570 | ds_loss: 0.0000 | lr: 4.2107e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   2 | Iter:   3405/ 13000 | global iter:   3405/ 13000 | loss: 1.8391 | ds_loss: 0.0000 | lr: 4.2103e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   2 | Iter:   3406/ 13000 | global iter:   3406/ 13000 | loss: 1.3623 | ds_loss: 0.0000 | lr: 4.2099e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   2 | Iter:   3407/ 13000 | global iter:   3407/ 13000 | loss: 1.5873 | ds_loss: 0.0000 | lr: 4.2094e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   2 | Iter:   3408/ 13000 | global iter:   3408/ 13000 | loss: 1.6707 | ds_loss: 0.0000 | lr: 4.2090e-05 | scale: 32768.0000 | micro time: 1.833 | step time: 0.000
train | epoch   2 | Iter:   3409/ 13000 | global iter:   3409/ 13000 | loss: 2.3715 | ds_loss: 0.0000 | lr: 4.2085e-05 | scale: 32768.0000 | micro time: 1.738 | step time: 0.000
train | epoch   2 | Iter:   3410/ 13000 | global iter:   3410/ 13000 | loss: 1.9020 | ds_loss: 0.0000 | lr: 4.2081e-05 | scale: 32768.0000 | micro time: 1.792 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3410/ 13000 | global iter:   3410/ 13000 | loss: 1.6874 | ds_loss: 0.0000 | lr: 4.2081e-05 | scale: 32768.0000 | micro time: 1.792 | step time: 1.823
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3411/ 13000 | global iter:   3411/ 13000 | loss: 1.4014 | ds_loss: 0.0000 | lr: 4.2077e-05 | scale: 32768.0000 | micro time: 1.817 | step time: 0.000
train | epoch   2 | Iter:   3412/ 13000 | global iter:   3412/ 13000 | loss: 2.3004 | ds_loss: 0.0000 | lr: 4.2072e-05 | scale: 32768.0000 | micro time: 1.723 | step time: 0.000
train | epoch   2 | Iter:   3413/ 13000 | global iter:   3413/ 13000 | loss: 1.4770 | ds_loss: 0.0000 | lr: 4.2068e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   2 | Iter:   3414/ 13000 | global iter:   3414/ 13000 | loss: 1.1922 | ds_loss: 0.0000 | lr: 4.2063e-05 | scale: 32768.0000 | micro time: 1.795 | step time: 0.000
train | epoch   2 | Iter:   3415/ 13000 | global iter:   3415/ 13000 | loss: 1.7009 | ds_loss: 0.0000 | lr: 4.2059e-05 | scale: 32768.0000 | micro time: 1.675 | step time: 0.000
train | epoch   2 | Iter:   3416/ 13000 | global iter:   3416/ 13000 | loss: 1.7689 | ds_loss: 0.0000 | lr: 4.2055e-05 | scale: 32768.0000 | micro time: 1.784 | step time: 0.000
train | epoch   2 | Iter:   3417/ 13000 | global iter:   3417/ 13000 | loss: 1.7428 | ds_loss: 0.0000 | lr: 4.2050e-05 | scale: 32768.0000 | micro time: 1.818 | step time: 0.000
train | epoch   2 | Iter:   3418/ 13000 | global iter:   3418/ 13000 | loss: 2.0292 | ds_loss: 0.0000 | lr: 4.2046e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   2 | Iter:   3419/ 13000 | global iter:   3419/ 13000 | loss: 2.0179 | ds_loss: 0.0000 | lr: 4.2041e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   2 | Iter:   3420/ 13000 | global iter:   3420/ 13000 | loss: 2.1341 | ds_loss: 0.0000 | lr: 4.2037e-05 | scale: 32768.0000 | micro time: 1.813 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3420/ 13000 | global iter:   3420/ 13000 | loss: 1.7765 | ds_loss: 0.0000 | lr: 4.2037e-05 | scale: 32768.0000 | micro time: 1.813 | step time: 1.787
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3421/ 13000 | global iter:   3421/ 13000 | loss: 1.6112 | ds_loss: 0.0000 | lr: 4.2033e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   2 | Iter:   3422/ 13000 | global iter:   3422/ 13000 | loss: 2.1694 | ds_loss: 0.0000 | lr: 4.2028e-05 | scale: 32768.0000 | micro time: 1.858 | step time: 0.000
train | epoch   2 | Iter:   3423/ 13000 | global iter:   3423/ 13000 | loss: 0.8652 | ds_loss: 0.0000 | lr: 4.2024e-05 | scale: 32768.0000 | micro time: 1.833 | step time: 0.000
train | epoch   2 | Iter:   3424/ 13000 | global iter:   3424/ 13000 | loss: 1.4012 | ds_loss: 0.0000 | lr: 4.2019e-05 | scale: 32768.0000 | micro time: 1.721 | step time: 0.000
train | epoch   2 | Iter:   3425/ 13000 | global iter:   3425/ 13000 | loss: 2.2895 | ds_loss: 0.0000 | lr: 4.2015e-05 | scale: 32768.0000 | micro time: 1.669 | step time: 0.000
train | epoch   2 | Iter:   3426/ 13000 | global iter:   3426/ 13000 | loss: 1.1899 | ds_loss: 0.0000 | lr: 4.2010e-05 | scale: 32768.0000 | micro time: 1.754 | step time: 0.000
train | epoch   2 | Iter:   3427/ 13000 | global iter:   3427/ 13000 | loss: 1.3187 | ds_loss: 0.0000 | lr: 4.2006e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 0.000
train | epoch   2 | Iter:   3428/ 13000 | global iter:   3428/ 13000 | loss: 1.8070 | ds_loss: 0.0000 | lr: 4.2002e-05 | scale: 32768.0000 | micro time: 1.774 | step time: 0.000
train | epoch   2 | Iter:   3429/ 13000 | global iter:   3429/ 13000 | loss: 1.6234 | ds_loss: 0.0000 | lr: 4.1997e-05 | scale: 32768.0000 | micro time: 1.857 | step time: 0.000
train | epoch   2 | Iter:   3430/ 13000 | global iter:   3430/ 13000 | loss: 1.0299 | ds_loss: 0.0000 | lr: 4.1993e-05 | scale: 32768.0000 | micro time: 1.915 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3430/ 13000 | global iter:   3430/ 13000 | loss: 1.5305 | ds_loss: 0.0000 | lr: 4.1993e-05 | scale: 32768.0000 | micro time: 1.915 | step time: 1.802
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3431/ 13000 | global iter:   3431/ 13000 | loss: 1.9921 | ds_loss: 0.0000 | lr: 4.1988e-05 | scale: 32768.0000 | micro time: 1.896 | step time: 0.000
train | epoch   2 | Iter:   3432/ 13000 | global iter:   3432/ 13000 | loss: 1.6701 | ds_loss: 0.0000 | lr: 4.1984e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   2 | Iter:   3433/ 13000 | global iter:   3433/ 13000 | loss: 1.3590 | ds_loss: 0.0000 | lr: 4.1979e-05 | scale: 32768.0000 | micro time: 1.813 | step time: 0.000
train | epoch   2 | Iter:   3434/ 13000 | global iter:   3434/ 13000 | loss: 1.8096 | ds_loss: 0.0000 | lr: 4.1975e-05 | scale: 32768.0000 | micro time: 1.832 | step time: 0.000
train | epoch   2 | Iter:   3435/ 13000 | global iter:   3435/ 13000 | loss: 2.1809 | ds_loss: 0.0000 | lr: 4.1971e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
train | epoch   2 | Iter:   3436/ 13000 | global iter:   3436/ 13000 | loss: 1.7399 | ds_loss: 0.0000 | lr: 4.1966e-05 | scale: 32768.0000 | micro time: 1.874 | step time: 0.000
train | epoch   2 | Iter:   3437/ 13000 | global iter:   3437/ 13000 | loss: 1.9988 | ds_loss: 0.0000 | lr: 4.1962e-05 | scale: 32768.0000 | micro time: 1.798 | step time: 0.000
train | epoch   2 | Iter:   3438/ 13000 | global iter:   3438/ 13000 | loss: 2.1303 | ds_loss: 0.0000 | lr: 4.1957e-05 | scale: 32768.0000 | micro time: 1.881 | step time: 0.000
train | epoch   2 | Iter:   3439/ 13000 | global iter:   3439/ 13000 | loss: 2.0710 | ds_loss: 0.0000 | lr: 4.1953e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   2 | Iter:   3440/ 13000 | global iter:   3440/ 13000 | loss: 1.7712 | ds_loss: 0.0000 | lr: 4.1948e-05 | scale: 32768.0000 | micro time: 1.755 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3440/ 13000 | global iter:   3440/ 13000 | loss: 1.8723 | ds_loss: 0.0000 | lr: 4.1948e-05 | scale: 32768.0000 | micro time: 1.755 | step time: 1.834
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3441/ 13000 | global iter:   3441/ 13000 | loss: 2.2881 | ds_loss: 0.0000 | lr: 4.1944e-05 | scale: 32768.0000 | micro time: 1.821 | step time: 0.000
train | epoch   2 | Iter:   3442/ 13000 | global iter:   3442/ 13000 | loss: 1.9566 | ds_loss: 0.0000 | lr: 4.1940e-05 | scale: 32768.0000 | micro time: 1.743 | step time: 0.000
train | epoch   2 | Iter:   3443/ 13000 | global iter:   3443/ 13000 | loss: 2.0853 | ds_loss: 0.0000 | lr: 4.1935e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   2 | Iter:   3444/ 13000 | global iter:   3444/ 13000 | loss: 2.0587 | ds_loss: 0.0000 | lr: 4.1931e-05 | scale: 32768.0000 | micro time: 1.780 | step time: 0.000
train | epoch   2 | Iter:   3445/ 13000 | global iter:   3445/ 13000 | loss: 1.7959 | ds_loss: 0.0000 | lr: 4.1926e-05 | scale: 32768.0000 | micro time: 1.717 | step time: 0.000
train | epoch   2 | Iter:   3446/ 13000 | global iter:   3446/ 13000 | loss: 1.4605 | ds_loss: 0.0000 | lr: 4.1922e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   2 | Iter:   3447/ 13000 | global iter:   3447/ 13000 | loss: 1.7819 | ds_loss: 0.0000 | lr: 4.1917e-05 | scale: 32768.0000 | micro time: 1.765 | step time: 0.000
train | epoch   2 | Iter:   3448/ 13000 | global iter:   3448/ 13000 | loss: 2.1905 | ds_loss: 0.0000 | lr: 4.1913e-05 | scale: 32768.0000 | micro time: 1.873 | step time: 0.000
train | epoch   2 | Iter:   3449/ 13000 | global iter:   3449/ 13000 | loss: 1.9278 | ds_loss: 0.0000 | lr: 4.1908e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   2 | Iter:   3450/ 13000 | global iter:   3450/ 13000 | loss: 1.7156 | ds_loss: 0.0000 | lr: 4.1904e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3450/ 13000 | global iter:   3450/ 13000 | loss: 1.9261 | ds_loss: 0.0000 | lr: 4.1904e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 1.794
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3451/ 13000 | global iter:   3451/ 13000 | loss: 1.9442 | ds_loss: 0.0000 | lr: 4.1900e-05 | scale: 32768.0000 | micro time: 1.814 | step time: 0.000
train | epoch   2 | Iter:   3452/ 13000 | global iter:   3452/ 13000 | loss: 2.0386 | ds_loss: 0.0000 | lr: 4.1895e-05 | scale: 32768.0000 | micro time: 1.892 | step time: 0.000
train | epoch   2 | Iter:   3453/ 13000 | global iter:   3453/ 13000 | loss: 1.3523 | ds_loss: 0.0000 | lr: 4.1891e-05 | scale: 32768.0000 | micro time: 1.898 | step time: 0.000
train | epoch   2 | Iter:   3454/ 13000 | global iter:   3454/ 13000 | loss: 1.9912 | ds_loss: 0.0000 | lr: 4.1886e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   2 | Iter:   3455/ 13000 | global iter:   3455/ 13000 | loss: 1.6120 | ds_loss: 0.0000 | lr: 4.1882e-05 | scale: 32768.0000 | micro time: 1.795 | step time: 0.000
train | epoch   2 | Iter:   3456/ 13000 | global iter:   3456/ 13000 | loss: 2.2437 | ds_loss: 0.0000 | lr: 4.1877e-05 | scale: 32768.0000 | micro time: 1.866 | step time: 0.000
train | epoch   2 | Iter:   3457/ 13000 | global iter:   3457/ 13000 | loss: 2.1591 | ds_loss: 0.0000 | lr: 4.1873e-05 | scale: 32768.0000 | micro time: 1.852 | step time: 0.000
train | epoch   2 | Iter:   3458/ 13000 | global iter:   3458/ 13000 | loss: 1.3988 | ds_loss: 0.0000 | lr: 4.1868e-05 | scale: 32768.0000 | micro time: 1.785 | step time: 0.000
train | epoch   2 | Iter:   3459/ 13000 | global iter:   3459/ 13000 | loss: 2.1400 | ds_loss: 0.0000 | lr: 4.1864e-05 | scale: 32768.0000 | micro time: 1.801 | step time: 0.000
train | epoch   2 | Iter:   3460/ 13000 | global iter:   3460/ 13000 | loss: 2.0659 | ds_loss: 0.0000 | lr: 4.1859e-05 | scale: 32768.0000 | micro time: 1.804 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3460/ 13000 | global iter:   3460/ 13000 | loss: 1.8946 | ds_loss: 0.0000 | lr: 4.1859e-05 | scale: 32768.0000 | micro time: 1.804 | step time: 1.833
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3461/ 13000 | global iter:   3461/ 13000 | loss: 1.5780 | ds_loss: 0.0000 | lr: 4.1855e-05 | scale: 32768.0000 | micro time: 1.916 | step time: 0.000
train | epoch   2 | Iter:   3462/ 13000 | global iter:   3462/ 13000 | loss: 1.9953 | ds_loss: 0.0000 | lr: 4.1851e-05 | scale: 32768.0000 | micro time: 1.923 | step time: 0.000
train | epoch   2 | Iter:   3463/ 13000 | global iter:   3463/ 13000 | loss: 1.8151 | ds_loss: 0.0000 | lr: 4.1846e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   2 | Iter:   3464/ 13000 | global iter:   3464/ 13000 | loss: 1.2535 | ds_loss: 0.0000 | lr: 4.1842e-05 | scale: 32768.0000 | micro time: 1.739 | step time: 0.000
train | epoch   2 | Iter:   3465/ 13000 | global iter:   3465/ 13000 | loss: 1.4324 | ds_loss: 0.0000 | lr: 4.1837e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   2 | Iter:   3466/ 13000 | global iter:   3466/ 13000 | loss: 1.4658 | ds_loss: 0.0000 | lr: 4.1833e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 0.000
train | epoch   2 | Iter:   3467/ 13000 | global iter:   3467/ 13000 | loss: 1.5909 | ds_loss: 0.0000 | lr: 4.1828e-05 | scale: 32768.0000 | micro time: 1.923 | step time: 0.000
train | epoch   2 | Iter:   3468/ 13000 | global iter:   3468/ 13000 | loss: 1.8683 | ds_loss: 0.0000 | lr: 4.1824e-05 | scale: 32768.0000 | micro time: 1.735 | step time: 0.000
train | epoch   2 | Iter:   3469/ 13000 | global iter:   3469/ 13000 | loss: 2.0333 | ds_loss: 0.0000 | lr: 4.1819e-05 | scale: 32768.0000 | micro time: 1.927 | step time: 0.000
train | epoch   2 | Iter:   3470/ 13000 | global iter:   3470/ 13000 | loss: 1.5678 | ds_loss: 0.0000 | lr: 4.1815e-05 | scale: 32768.0000 | micro time: 1.846 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3470/ 13000 | global iter:   3470/ 13000 | loss: 1.6600 | ds_loss: 0.0000 | lr: 4.1815e-05 | scale: 32768.0000 | micro time: 1.846 | step time: 1.847
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3471/ 13000 | global iter:   3471/ 13000 | loss: 1.7549 | ds_loss: 0.0000 | lr: 4.1810e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   2 | Iter:   3472/ 13000 | global iter:   3472/ 13000 | loss: 1.5074 | ds_loss: 0.0000 | lr: 4.1806e-05 | scale: 32768.0000 | micro time: 1.870 | step time: 0.000
train | epoch   2 | Iter:   3473/ 13000 | global iter:   3473/ 13000 | loss: 1.7585 | ds_loss: 0.0000 | lr: 4.1801e-05 | scale: 32768.0000 | micro time: 1.758 | step time: 0.000
train | epoch   2 | Iter:   3474/ 13000 | global iter:   3474/ 13000 | loss: 2.2962 | ds_loss: 0.0000 | lr: 4.1797e-05 | scale: 32768.0000 | micro time: 1.872 | step time: 0.000
train | epoch   2 | Iter:   3475/ 13000 | global iter:   3475/ 13000 | loss: 1.7213 | ds_loss: 0.0000 | lr: 4.1793e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   2 | Iter:   3476/ 13000 | global iter:   3476/ 13000 | loss: 1.7173 | ds_loss: 0.0000 | lr: 4.1788e-05 | scale: 32768.0000 | micro time: 1.795 | step time: 0.000
train | epoch   2 | Iter:   3477/ 13000 | global iter:   3477/ 13000 | loss: 1.7360 | ds_loss: 0.0000 | lr: 4.1784e-05 | scale: 32768.0000 | micro time: 1.711 | step time: 0.000
train | epoch   2 | Iter:   3478/ 13000 | global iter:   3478/ 13000 | loss: 1.5933 | ds_loss: 0.0000 | lr: 4.1779e-05 | scale: 32768.0000 | micro time: 1.814 | step time: 0.000
train | epoch   2 | Iter:   3479/ 13000 | global iter:   3479/ 13000 | loss: 1.8781 | ds_loss: 0.0000 | lr: 4.1775e-05 | scale: 32768.0000 | micro time: 1.879 | step time: 0.000
train | epoch   2 | Iter:   3480/ 13000 | global iter:   3480/ 13000 | loss: 1.9406 | ds_loss: 0.0000 | lr: 4.1770e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3480/ 13000 | global iter:   3480/ 13000 | loss: 1.7904 | ds_loss: 0.0000 | lr: 4.1770e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 1.806
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3481/ 13000 | global iter:   3481/ 13000 | loss: 2.5331 | ds_loss: 0.0000 | lr: 4.1766e-05 | scale: 32768.0000 | micro time: 1.697 | step time: 0.000
train | epoch   2 | Iter:   3482/ 13000 | global iter:   3482/ 13000 | loss: 2.0419 | ds_loss: 0.0000 | lr: 4.1761e-05 | scale: 32768.0000 | micro time: 1.735 | step time: 0.000
train | epoch   2 | Iter:   3483/ 13000 | global iter:   3483/ 13000 | loss: 2.2862 | ds_loss: 0.0000 | lr: 4.1757e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   2 | Iter:   3484/ 13000 | global iter:   3484/ 13000 | loss: 1.8637 | ds_loss: 0.0000 | lr: 4.1752e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   2 | Iter:   3485/ 13000 | global iter:   3485/ 13000 | loss: 2.2235 | ds_loss: 0.0000 | lr: 4.1748e-05 | scale: 32768.0000 | micro time: 1.879 | step time: 0.000
train | epoch   2 | Iter:   3486/ 13000 | global iter:   3486/ 13000 | loss: 1.4271 | ds_loss: 0.0000 | lr: 4.1743e-05 | scale: 32768.0000 | micro time: 1.919 | step time: 0.000
train | epoch   2 | Iter:   3487/ 13000 | global iter:   3487/ 13000 | loss: 1.6639 | ds_loss: 0.0000 | lr: 4.1739e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   2 | Iter:   3488/ 13000 | global iter:   3488/ 13000 | loss: 1.9898 | ds_loss: 0.0000 | lr: 4.1734e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   2 | Iter:   3489/ 13000 | global iter:   3489/ 13000 | loss: 1.6619 | ds_loss: 0.0000 | lr: 4.1730e-05 | scale: 32768.0000 | micro time: 1.695 | step time: 0.000
train | epoch   2 | Iter:   3490/ 13000 | global iter:   3490/ 13000 | loss: 1.7496 | ds_loss: 0.0000 | lr: 4.1725e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3490/ 13000 | global iter:   3490/ 13000 | loss: 1.9441 | ds_loss: 0.0000 | lr: 4.1725e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 1.809
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3491/ 13000 | global iter:   3491/ 13000 | loss: 1.4585 | ds_loss: 0.0000 | lr: 4.1721e-05 | scale: 32768.0000 | micro time: 1.857 | step time: 0.000
train | epoch   2 | Iter:   3492/ 13000 | global iter:   3492/ 13000 | loss: 1.7385 | ds_loss: 0.0000 | lr: 4.1716e-05 | scale: 32768.0000 | micro time: 1.876 | step time: 0.000
train | epoch   2 | Iter:   3493/ 13000 | global iter:   3493/ 13000 | loss: 2.2907 | ds_loss: 0.0000 | lr: 4.1712e-05 | scale: 32768.0000 | micro time: 1.907 | step time: 0.000
train | epoch   2 | Iter:   3494/ 13000 | global iter:   3494/ 13000 | loss: 2.2207 | ds_loss: 0.0000 | lr: 4.1707e-05 | scale: 32768.0000 | micro time: 1.742 | step time: 0.000
train | epoch   2 | Iter:   3495/ 13000 | global iter:   3495/ 13000 | loss: 1.9508 | ds_loss: 0.0000 | lr: 4.1703e-05 | scale: 32768.0000 | micro time: 1.776 | step time: 0.000
train | epoch   2 | Iter:   3496/ 13000 | global iter:   3496/ 13000 | loss: 1.7784 | ds_loss: 0.0000 | lr: 4.1698e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   2 | Iter:   3497/ 13000 | global iter:   3497/ 13000 | loss: 2.1013 | ds_loss: 0.0000 | lr: 4.1694e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   2 | Iter:   3498/ 13000 | global iter:   3498/ 13000 | loss: 1.7040 | ds_loss: 0.0000 | lr: 4.1689e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   2 | Iter:   3499/ 13000 | global iter:   3499/ 13000 | loss: 2.0972 | ds_loss: 0.0000 | lr: 4.1685e-05 | scale: 32768.0000 | micro time: 1.856 | step time: 0.000
train | epoch   2 | Iter:   3500/ 13000 | global iter:   3500/ 13000 | loss: 1.7728 | ds_loss: 0.0000 | lr: 4.1680e-05 | scale: 32768.0000 | micro time: 1.886 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3500/ 13000 | global iter:   3500/ 13000 | loss: 1.9113 | ds_loss: 0.0000 | lr: 4.1680e-05 | scale: 32768.0000 | micro time: 1.886 | step time: 1.836
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3501/ 13000 | global iter:   3501/ 13000 | loss: 1.7808 | ds_loss: 0.0000 | lr: 4.1676e-05 | scale: 32768.0000 | micro time: 1.878 | step time: 0.000
train | epoch   2 | Iter:   3502/ 13000 | global iter:   3502/ 13000 | loss: 1.7057 | ds_loss: 0.0000 | lr: 4.1671e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   2 | Iter:   3503/ 13000 | global iter:   3503/ 13000 | loss: 2.1380 | ds_loss: 0.0000 | lr: 4.1667e-05 | scale: 32768.0000 | micro time: 1.742 | step time: 0.000
train | epoch   2 | Iter:   3504/ 13000 | global iter:   3504/ 13000 | loss: 1.9364 | ds_loss: 0.0000 | lr: 4.1662e-05 | scale: 32768.0000 | micro time: 1.798 | step time: 0.000
train | epoch   2 | Iter:   3505/ 13000 | global iter:   3505/ 13000 | loss: 1.6746 | ds_loss: 0.0000 | lr: 4.1658e-05 | scale: 32768.0000 | micro time: 1.702 | step time: 0.000
train | epoch   2 | Iter:   3506/ 13000 | global iter:   3506/ 13000 | loss: 1.8301 | ds_loss: 0.0000 | lr: 4.1653e-05 | scale: 32768.0000 | micro time: 1.726 | step time: 0.000
train | epoch   2 | Iter:   3507/ 13000 | global iter:   3507/ 13000 | loss: 2.0076 | ds_loss: 0.0000 | lr: 4.1649e-05 | scale: 32768.0000 | micro time: 1.726 | step time: 0.000
train | epoch   2 | Iter:   3508/ 13000 | global iter:   3508/ 13000 | loss: 1.7573 | ds_loss: 0.0000 | lr: 4.1644e-05 | scale: 32768.0000 | micro time: 1.701 | step time: 0.000
train | epoch   2 | Iter:   3509/ 13000 | global iter:   3509/ 13000 | loss: 1.6734 | ds_loss: 0.0000 | lr: 4.1640e-05 | scale: 32768.0000 | micro time: 1.789 | step time: 0.000
train | epoch   2 | Iter:   3510/ 13000 | global iter:   3510/ 13000 | loss: 1.9235 | ds_loss: 0.0000 | lr: 4.1635e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3510/ 13000 | global iter:   3510/ 13000 | loss: 1.8427 | ds_loss: 0.0000 | lr: 4.1635e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 1.769
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3511/ 13000 | global iter:   3511/ 13000 | loss: 1.5022 | ds_loss: 0.0000 | lr: 4.1631e-05 | scale: 32768.0000 | micro time: 1.806 | step time: 0.000
train | epoch   2 | Iter:   3512/ 13000 | global iter:   3512/ 13000 | loss: 1.5999 | ds_loss: 0.0000 | lr: 4.1626e-05 | scale: 32768.0000 | micro time: 1.795 | step time: 0.000
train | epoch   2 | Iter:   3513/ 13000 | global iter:   3513/ 13000 | loss: 2.0222 | ds_loss: 0.0000 | lr: 4.1622e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   2 | Iter:   3514/ 13000 | global iter:   3514/ 13000 | loss: 2.2565 | ds_loss: 0.0000 | lr: 4.1617e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   2 | Iter:   3515/ 13000 | global iter:   3515/ 13000 | loss: 1.6849 | ds_loss: 0.0000 | lr: 4.1613e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   2 | Iter:   3516/ 13000 | global iter:   3516/ 13000 | loss: 2.1689 | ds_loss: 0.0000 | lr: 4.1608e-05 | scale: 32768.0000 | micro time: 1.675 | step time: 0.000
train | epoch   2 | Iter:   3517/ 13000 | global iter:   3517/ 13000 | loss: 1.8990 | ds_loss: 0.0000 | lr: 4.1604e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   2 | Iter:   3518/ 13000 | global iter:   3518/ 13000 | loss: 2.0898 | ds_loss: 0.0000 | lr: 4.1599e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   2 | Iter:   3519/ 13000 | global iter:   3519/ 13000 | loss: 1.7275 | ds_loss: 0.0000 | lr: 4.1595e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 0.000
train | epoch   2 | Iter:   3520/ 13000 | global iter:   3520/ 13000 | loss: 1.8713 | ds_loss: 0.0000 | lr: 4.1590e-05 | scale: 32768.0000 | micro time: 1.814 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3520/ 13000 | global iter:   3520/ 13000 | loss: 1.8822 | ds_loss: 0.0000 | lr: 4.1590e-05 | scale: 32768.0000 | micro time: 1.814 | step time: 1.812
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3521/ 13000 | global iter:   3521/ 13000 | loss: 1.6680 | ds_loss: 0.0000 | lr: 4.1586e-05 | scale: 32768.0000 | micro time: 1.824 | step time: 0.000
train | epoch   2 | Iter:   3522/ 13000 | global iter:   3522/ 13000 | loss: 1.6093 | ds_loss: 0.0000 | lr: 4.1581e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   2 | Iter:   3523/ 13000 | global iter:   3523/ 13000 | loss: 1.6201 | ds_loss: 0.0000 | lr: 4.1577e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   2 | Iter:   3524/ 13000 | global iter:   3524/ 13000 | loss: 2.0241 | ds_loss: 0.0000 | lr: 4.1572e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   2 | Iter:   3525/ 13000 | global iter:   3525/ 13000 | loss: 2.1649 | ds_loss: 0.0000 | lr: 4.1568e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   2 | Iter:   3526/ 13000 | global iter:   3526/ 13000 | loss: 1.9605 | ds_loss: 0.0000 | lr: 4.1563e-05 | scale: 32768.0000 | micro time: 1.814 | step time: 0.000
train | epoch   2 | Iter:   3527/ 13000 | global iter:   3527/ 13000 | loss: 1.5386 | ds_loss: 0.0000 | lr: 4.1559e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   2 | Iter:   3528/ 13000 | global iter:   3528/ 13000 | loss: 1.8201 | ds_loss: 0.0000 | lr: 4.1554e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
train | epoch   2 | Iter:   3529/ 13000 | global iter:   3529/ 13000 | loss: 1.1268 | ds_loss: 0.0000 | lr: 4.1550e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 0.000
train | epoch   2 | Iter:   3530/ 13000 | global iter:   3530/ 13000 | loss: 1.8841 | ds_loss: 0.0000 | lr: 4.1545e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3530/ 13000 | global iter:   3530/ 13000 | loss: 1.7416 | ds_loss: 0.0000 | lr: 4.1545e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 1.815
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3531/ 13000 | global iter:   3531/ 13000 | loss: 2.2118 | ds_loss: 0.0000 | lr: 4.1541e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   2 | Iter:   3532/ 13000 | global iter:   3532/ 13000 | loss: 2.0184 | ds_loss: 0.0000 | lr: 4.1536e-05 | scale: 32768.0000 | micro time: 1.854 | step time: 0.000
train | epoch   2 | Iter:   3533/ 13000 | global iter:   3533/ 13000 | loss: 1.7139 | ds_loss: 0.0000 | lr: 4.1532e-05 | scale: 32768.0000 | micro time: 1.911 | step time: 0.000
train | epoch   2 | Iter:   3534/ 13000 | global iter:   3534/ 13000 | loss: 1.7606 | ds_loss: 0.0000 | lr: 4.1527e-05 | scale: 32768.0000 | micro time: 1.886 | step time: 0.000
train | epoch   2 | Iter:   3535/ 13000 | global iter:   3535/ 13000 | loss: 1.6785 | ds_loss: 0.0000 | lr: 4.1523e-05 | scale: 32768.0000 | micro time: 1.888 | step time: 0.000
train | epoch   2 | Iter:   3536/ 13000 | global iter:   3536/ 13000 | loss: 1.5681 | ds_loss: 0.0000 | lr: 4.1518e-05 | scale: 32768.0000 | micro time: 1.837 | step time: 0.000
train | epoch   2 | Iter:   3537/ 13000 | global iter:   3537/ 13000 | loss: 1.2834 | ds_loss: 0.0000 | lr: 4.1514e-05 | scale: 32768.0000 | micro time: 1.879 | step time: 0.000
train | epoch   2 | Iter:   3538/ 13000 | global iter:   3538/ 13000 | loss: 2.0204 | ds_loss: 0.0000 | lr: 4.1509e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   2 | Iter:   3539/ 13000 | global iter:   3539/ 13000 | loss: 1.6735 | ds_loss: 0.0000 | lr: 4.1504e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   2 | Iter:   3540/ 13000 | global iter:   3540/ 13000 | loss: 1.4499 | ds_loss: 0.0000 | lr: 4.1500e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3540/ 13000 | global iter:   3540/ 13000 | loss: 1.7379 | ds_loss: 0.0000 | lr: 4.1500e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 1.846
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3541/ 13000 | global iter:   3541/ 13000 | loss: 1.4734 | ds_loss: 0.0000 | lr: 4.1495e-05 | scale: 32768.0000 | micro time: 1.898 | step time: 0.000
train | epoch   2 | Iter:   3542/ 13000 | global iter:   3542/ 13000 | loss: 1.8408 | ds_loss: 0.0000 | lr: 4.1491e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   2 | Iter:   3543/ 13000 | global iter:   3543/ 13000 | loss: 1.9808 | ds_loss: 0.0000 | lr: 4.1486e-05 | scale: 32768.0000 | micro time: 1.758 | step time: 0.000
train | epoch   2 | Iter:   3544/ 13000 | global iter:   3544/ 13000 | loss: 1.3245 | ds_loss: 0.0000 | lr: 4.1482e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   2 | Iter:   3545/ 13000 | global iter:   3545/ 13000 | loss: 2.4208 | ds_loss: 0.0000 | lr: 4.1477e-05 | scale: 32768.0000 | micro time: 1.923 | step time: 0.000
train | epoch   2 | Iter:   3546/ 13000 | global iter:   3546/ 13000 | loss: 2.1853 | ds_loss: 0.0000 | lr: 4.1473e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   2 | Iter:   3547/ 13000 | global iter:   3547/ 13000 | loss: 1.4259 | ds_loss: 0.0000 | lr: 4.1468e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   2 | Iter:   3548/ 13000 | global iter:   3548/ 13000 | loss: 1.9072 | ds_loss: 0.0000 | lr: 4.1464e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   2 | Iter:   3549/ 13000 | global iter:   3549/ 13000 | loss: 2.0640 | ds_loss: 0.0000 | lr: 4.1459e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
train | epoch   2 | Iter:   3550/ 13000 | global iter:   3550/ 13000 | loss: 1.9495 | ds_loss: 0.0000 | lr: 4.1455e-05 | scale: 32768.0000 | micro time: 1.900 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3550/ 13000 | global iter:   3550/ 13000 | loss: 1.8572 | ds_loss: 0.0000 | lr: 4.1455e-05 | scale: 32768.0000 | micro time: 1.900 | step time: 1.845
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3551/ 13000 | global iter:   3551/ 13000 | loss: 1.5345 | ds_loss: 0.0000 | lr: 4.1450e-05 | scale: 32768.0000 | micro time: 1.846 | step time: 0.000
train | epoch   2 | Iter:   3552/ 13000 | global iter:   3552/ 13000 | loss: 1.4432 | ds_loss: 0.0000 | lr: 4.1445e-05 | scale: 32768.0000 | micro time: 1.809 | step time: 0.000
train | epoch   2 | Iter:   3553/ 13000 | global iter:   3553/ 13000 | loss: 1.3881 | ds_loss: 0.0000 | lr: 4.1441e-05 | scale: 32768.0000 | micro time: 1.894 | step time: 0.000
train | epoch   2 | Iter:   3554/ 13000 | global iter:   3554/ 13000 | loss: 1.4305 | ds_loss: 0.0000 | lr: 4.1436e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   2 | Iter:   3555/ 13000 | global iter:   3555/ 13000 | loss: 1.1697 | ds_loss: 0.0000 | lr: 4.1432e-05 | scale: 32768.0000 | micro time: 1.651 | step time: 0.000
train | epoch   2 | Iter:   3556/ 13000 | global iter:   3556/ 13000 | loss: 2.2715 | ds_loss: 0.0000 | lr: 4.1427e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   2 | Iter:   3557/ 13000 | global iter:   3557/ 13000 | loss: 2.3942 | ds_loss: 0.0000 | lr: 4.1423e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   2 | Iter:   3558/ 13000 | global iter:   3558/ 13000 | loss: 2.1696 | ds_loss: 0.0000 | lr: 4.1418e-05 | scale: 32768.0000 | micro time: 1.778 | step time: 0.000
train | epoch   2 | Iter:   3559/ 13000 | global iter:   3559/ 13000 | loss: 1.7183 | ds_loss: 0.0000 | lr: 4.1414e-05 | scale: 32768.0000 | micro time: 1.844 | step time: 0.000
train | epoch   2 | Iter:   3560/ 13000 | global iter:   3560/ 13000 | loss: 1.4283 | ds_loss: 0.0000 | lr: 4.1409e-05 | scale: 32768.0000 | micro time: 1.804 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3560/ 13000 | global iter:   3560/ 13000 | loss: 1.6948 | ds_loss: 0.0000 | lr: 4.1409e-05 | scale: 32768.0000 | micro time: 1.804 | step time: 1.810
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3561/ 13000 | global iter:   3561/ 13000 | loss: 2.0347 | ds_loss: 0.0000 | lr: 4.1405e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   2 | Iter:   3562/ 13000 | global iter:   3562/ 13000 | loss: 1.2767 | ds_loss: 0.0000 | lr: 4.1400e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 0.000
train | epoch   2 | Iter:   3563/ 13000 | global iter:   3563/ 13000 | loss: 1.6995 | ds_loss: 0.0000 | lr: 4.1395e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   2 | Iter:   3564/ 13000 | global iter:   3564/ 13000 | loss: 1.6946 | ds_loss: 0.0000 | lr: 4.1391e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   2 | Iter:   3565/ 13000 | global iter:   3565/ 13000 | loss: 2.2110 | ds_loss: 0.0000 | lr: 4.1386e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   2 | Iter:   3566/ 13000 | global iter:   3566/ 13000 | loss: 2.0589 | ds_loss: 0.0000 | lr: 4.1382e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   2 | Iter:   3567/ 13000 | global iter:   3567/ 13000 | loss: 2.0706 | ds_loss: 0.0000 | lr: 4.1377e-05 | scale: 32768.0000 | micro time: 1.655 | step time: 0.000
train | epoch   2 | Iter:   3568/ 13000 | global iter:   3568/ 13000 | loss: 1.9185 | ds_loss: 0.0000 | lr: 4.1373e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 0.000
train | epoch   2 | Iter:   3569/ 13000 | global iter:   3569/ 13000 | loss: 1.8493 | ds_loss: 0.0000 | lr: 4.1368e-05 | scale: 32768.0000 | micro time: 1.780 | step time: 0.000
train | epoch   2 | Iter:   3570/ 13000 | global iter:   3570/ 13000 | loss: 2.0080 | ds_loss: 0.0000 | lr: 4.1363e-05 | scale: 32768.0000 | micro time: 1.752 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3570/ 13000 | global iter:   3570/ 13000 | loss: 1.8822 | ds_loss: 0.0000 | lr: 4.1363e-05 | scale: 32768.0000 | micro time: 1.752 | step time: 1.794
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3571/ 13000 | global iter:   3571/ 13000 | loss: 1.3112 | ds_loss: 0.0000 | lr: 4.1359e-05 | scale: 32768.0000 | micro time: 1.829 | step time: 0.000
train | epoch   2 | Iter:   3572/ 13000 | global iter:   3572/ 13000 | loss: 2.2255 | ds_loss: 0.0000 | lr: 4.1354e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   2 | Iter:   3573/ 13000 | global iter:   3573/ 13000 | loss: 1.6866 | ds_loss: 0.0000 | lr: 4.1350e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   2 | Iter:   3574/ 13000 | global iter:   3574/ 13000 | loss: 2.2393 | ds_loss: 0.0000 | lr: 4.1345e-05 | scale: 32768.0000 | micro time: 1.853 | step time: 0.000
train | epoch   2 | Iter:   3575/ 13000 | global iter:   3575/ 13000 | loss: 1.4557 | ds_loss: 0.0000 | lr: 4.1341e-05 | scale: 32768.0000 | micro time: 1.757 | step time: 0.000
train | epoch   2 | Iter:   3576/ 13000 | global iter:   3576/ 13000 | loss: 2.0955 | ds_loss: 0.0000 | lr: 4.1336e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   2 | Iter:   3577/ 13000 | global iter:   3577/ 13000 | loss: 1.5678 | ds_loss: 0.0000 | lr: 4.1332e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   2 | Iter:   3578/ 13000 | global iter:   3578/ 13000 | loss: 1.9827 | ds_loss: 0.0000 | lr: 4.1327e-05 | scale: 32768.0000 | micro time: 1.959 | step time: 0.000
train | epoch   2 | Iter:   3579/ 13000 | global iter:   3579/ 13000 | loss: 2.0574 | ds_loss: 0.0000 | lr: 4.1322e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   2 | Iter:   3580/ 13000 | global iter:   3580/ 13000 | loss: 1.9008 | ds_loss: 0.0000 | lr: 4.1318e-05 | scale: 32768.0000 | micro time: 1.830 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3580/ 13000 | global iter:   3580/ 13000 | loss: 1.8522 | ds_loss: 0.0000 | lr: 4.1318e-05 | scale: 32768.0000 | micro time: 1.830 | step time: 1.824
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3581/ 13000 | global iter:   3581/ 13000 | loss: 1.6296 | ds_loss: 0.0000 | lr: 4.1313e-05 | scale: 32768.0000 | micro time: 1.908 | step time: 0.000
train | epoch   2 | Iter:   3582/ 13000 | global iter:   3582/ 13000 | loss: 2.5063 | ds_loss: 0.0000 | lr: 4.1309e-05 | scale: 32768.0000 | micro time: 1.713 | step time: 0.000
train | epoch   2 | Iter:   3583/ 13000 | global iter:   3583/ 13000 | loss: 1.1742 | ds_loss: 0.0000 | lr: 4.1304e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
train | epoch   2 | Iter:   3584/ 13000 | global iter:   3584/ 13000 | loss: 2.4862 | ds_loss: 0.0000 | lr: 4.1300e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   2 | Iter:   3585/ 13000 | global iter:   3585/ 13000 | loss: 1.9982 | ds_loss: 0.0000 | lr: 4.1295e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   2 | Iter:   3586/ 13000 | global iter:   3586/ 13000 | loss: 2.0909 | ds_loss: 0.0000 | lr: 4.1290e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   2 | Iter:   3587/ 13000 | global iter:   3587/ 13000 | loss: 1.7316 | ds_loss: 0.0000 | lr: 4.1286e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   2 | Iter:   3588/ 13000 | global iter:   3588/ 13000 | loss: 1.7075 | ds_loss: 0.0000 | lr: 4.1281e-05 | scale: 32768.0000 | micro time: 1.869 | step time: 0.000
train | epoch   2 | Iter:   3589/ 13000 | global iter:   3589/ 13000 | loss: 1.6674 | ds_loss: 0.0000 | lr: 4.1277e-05 | scale: 32768.0000 | micro time: 1.805 | step time: 0.000
train | epoch   2 | Iter:   3590/ 13000 | global iter:   3590/ 13000 | loss: 2.2184 | ds_loss: 0.0000 | lr: 4.1272e-05 | scale: 32768.0000 | micro time: 1.749 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3590/ 13000 | global iter:   3590/ 13000 | loss: 1.9210 | ds_loss: 0.0000 | lr: 4.1272e-05 | scale: 32768.0000 | micro time: 1.749 | step time: 1.801
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3591/ 13000 | global iter:   3591/ 13000 | loss: 1.4812 | ds_loss: 0.0000 | lr: 4.1267e-05 | scale: 32768.0000 | micro time: 1.720 | step time: 0.000
train | epoch   2 | Iter:   3592/ 13000 | global iter:   3592/ 13000 | loss: 1.5126 | ds_loss: 0.0000 | lr: 4.1263e-05 | scale: 32768.0000 | micro time: 1.755 | step time: 0.000
train | epoch   2 | Iter:   3593/ 13000 | global iter:   3593/ 13000 | loss: 2.1845 | ds_loss: 0.0000 | lr: 4.1258e-05 | scale: 32768.0000 | micro time: 1.869 | step time: 0.000
train | epoch   2 | Iter:   3594/ 13000 | global iter:   3594/ 13000 | loss: 2.2041 | ds_loss: 0.0000 | lr: 4.1254e-05 | scale: 32768.0000 | micro time: 1.689 | step time: 0.000
train | epoch   2 | Iter:   3595/ 13000 | global iter:   3595/ 13000 | loss: 1.7904 | ds_loss: 0.0000 | lr: 4.1249e-05 | scale: 32768.0000 | micro time: 1.808 | step time: 0.000
train | epoch   2 | Iter:   3596/ 13000 | global iter:   3596/ 13000 | loss: 1.5692 | ds_loss: 0.0000 | lr: 4.1245e-05 | scale: 32768.0000 | micro time: 1.822 | step time: 0.000
train | epoch   2 | Iter:   3597/ 13000 | global iter:   3597/ 13000 | loss: 2.1808 | ds_loss: 0.0000 | lr: 4.1240e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   2 | Iter:   3598/ 13000 | global iter:   3598/ 13000 | loss: 1.5580 | ds_loss: 0.0000 | lr: 4.1235e-05 | scale: 32768.0000 | micro time: 1.715 | step time: 0.000
train | epoch   2 | Iter:   3599/ 13000 | global iter:   3599/ 13000 | loss: 1.2915 | ds_loss: 0.0000 | lr: 4.1231e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   2 | Iter:   3600/ 13000 | global iter:   3600/ 13000 | loss: 1.5709 | ds_loss: 0.0000 | lr: 4.1226e-05 | scale: 32768.0000 | micro time: 1.747 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3600/ 13000 | global iter:   3600/ 13000 | loss: 1.7343 | ds_loss: 0.0000 | lr: 4.1226e-05 | scale: 32768.0000 | micro time: 1.747 | step time: 1.784
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3601/ 13000 | global iter:   3601/ 13000 | loss: 1.4893 | ds_loss: 0.0000 | lr: 4.1222e-05 | scale: 32768.0000 | micro time: 1.785 | step time: 0.000
train | epoch   2 | Iter:   3602/ 13000 | global iter:   3602/ 13000 | loss: 1.7205 | ds_loss: 0.0000 | lr: 4.1217e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   2 | Iter:   3603/ 13000 | global iter:   3603/ 13000 | loss: 1.6792 | ds_loss: 0.0000 | lr: 4.1212e-05 | scale: 32768.0000 | micro time: 1.742 | step time: 0.000
train | epoch   2 | Iter:   3604/ 13000 | global iter:   3604/ 13000 | loss: 1.8440 | ds_loss: 0.0000 | lr: 4.1208e-05 | scale: 32768.0000 | micro time: 1.900 | step time: 0.000
train | epoch   2 | Iter:   3605/ 13000 | global iter:   3605/ 13000 | loss: 2.0047 | ds_loss: 0.0000 | lr: 4.1203e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   2 | Iter:   3606/ 13000 | global iter:   3606/ 13000 | loss: 1.6134 | ds_loss: 0.0000 | lr: 4.1199e-05 | scale: 32768.0000 | micro time: 1.789 | step time: 0.000
train | epoch   2 | Iter:   3607/ 13000 | global iter:   3607/ 13000 | loss: 1.1605 | ds_loss: 0.0000 | lr: 4.1194e-05 | scale: 32768.0000 | micro time: 1.770 | step time: 0.000
train | epoch   2 | Iter:   3608/ 13000 | global iter:   3608/ 13000 | loss: 2.2192 | ds_loss: 0.0000 | lr: 4.1189e-05 | scale: 32768.0000 | micro time: 1.769 | step time: 0.000
train | epoch   2 | Iter:   3609/ 13000 | global iter:   3609/ 13000 | loss: 1.9693 | ds_loss: 0.0000 | lr: 4.1185e-05 | scale: 32768.0000 | micro time: 1.800 | step time: 0.000
train | epoch   2 | Iter:   3610/ 13000 | global iter:   3610/ 13000 | loss: 2.0818 | ds_loss: 0.0000 | lr: 4.1180e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3610/ 13000 | global iter:   3610/ 13000 | loss: 1.7782 | ds_loss: 0.0000 | lr: 4.1180e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 1.799
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3611/ 13000 | global iter:   3611/ 13000 | loss: 2.1936 | ds_loss: 0.0000 | lr: 4.1176e-05 | scale: 32768.0000 | micro time: 1.890 | step time: 0.000
train | epoch   2 | Iter:   3612/ 13000 | global iter:   3612/ 13000 | loss: 1.8375 | ds_loss: 0.0000 | lr: 4.1171e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   2 | Iter:   3613/ 13000 | global iter:   3613/ 13000 | loss: 2.1061 | ds_loss: 0.0000 | lr: 4.1166e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   2 | Iter:   3614/ 13000 | global iter:   3614/ 13000 | loss: 1.4443 | ds_loss: 0.0000 | lr: 4.1162e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   2 | Iter:   3615/ 13000 | global iter:   3615/ 13000 | loss: 2.2865 | ds_loss: 0.0000 | lr: 4.1157e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   2 | Iter:   3616/ 13000 | global iter:   3616/ 13000 | loss: 2.1495 | ds_loss: 0.0000 | lr: 4.1153e-05 | scale: 32768.0000 | micro time: 1.723 | step time: 0.000
train | epoch   2 | Iter:   3617/ 13000 | global iter:   3617/ 13000 | loss: 2.2840 | ds_loss: 0.0000 | lr: 4.1148e-05 | scale: 32768.0000 | micro time: 1.715 | step time: 0.000
train | epoch   2 | Iter:   3618/ 13000 | global iter:   3618/ 13000 | loss: 1.9987 | ds_loss: 0.0000 | lr: 4.1143e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   2 | Iter:   3619/ 13000 | global iter:   3619/ 13000 | loss: 1.8200 | ds_loss: 0.0000 | lr: 4.1139e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   2 | Iter:   3620/ 13000 | global iter:   3620/ 13000 | loss: 2.0023 | ds_loss: 0.0000 | lr: 4.1134e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3620/ 13000 | global iter:   3620/ 13000 | loss: 2.0123 | ds_loss: 0.0000 | lr: 4.1134e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 1.798
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3621/ 13000 | global iter:   3621/ 13000 | loss: 1.8082 | ds_loss: 0.0000 | lr: 4.1130e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   2 | Iter:   3622/ 13000 | global iter:   3622/ 13000 | loss: 1.8353 | ds_loss: 0.0000 | lr: 4.1125e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
train | epoch   2 | Iter:   3623/ 13000 | global iter:   3623/ 13000 | loss: 2.2161 | ds_loss: 0.0000 | lr: 4.1120e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   2 | Iter:   3624/ 13000 | global iter:   3624/ 13000 | loss: 2.0513 | ds_loss: 0.0000 | lr: 4.1116e-05 | scale: 32768.0000 | micro time: 1.731 | step time: 0.000
train | epoch   2 | Iter:   3625/ 13000 | global iter:   3625/ 13000 | loss: 1.9475 | ds_loss: 0.0000 | lr: 4.1111e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   2 | Iter:   3626/ 13000 | global iter:   3626/ 13000 | loss: 1.5885 | ds_loss: 0.0000 | lr: 4.1107e-05 | scale: 32768.0000 | micro time: 1.755 | step time: 0.000
train | epoch   2 | Iter:   3627/ 13000 | global iter:   3627/ 13000 | loss: 2.3814 | ds_loss: 0.0000 | lr: 4.1102e-05 | scale: 32768.0000 | micro time: 1.836 | step time: 0.000
train | epoch   2 | Iter:   3628/ 13000 | global iter:   3628/ 13000 | loss: 1.9417 | ds_loss: 0.0000 | lr: 4.1097e-05 | scale: 32768.0000 | micro time: 1.857 | step time: 0.000
train | epoch   2 | Iter:   3629/ 13000 | global iter:   3629/ 13000 | loss: 1.8280 | ds_loss: 0.0000 | lr: 4.1093e-05 | scale: 32768.0000 | micro time: 1.886 | step time: 0.000
train | epoch   2 | Iter:   3630/ 13000 | global iter:   3630/ 13000 | loss: 2.1044 | ds_loss: 0.0000 | lr: 4.1088e-05 | scale: 32768.0000 | micro time: 1.828 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3630/ 13000 | global iter:   3630/ 13000 | loss: 1.9702 | ds_loss: 0.0000 | lr: 4.1088e-05 | scale: 32768.0000 | micro time: 1.828 | step time: 1.809
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3631/ 13000 | global iter:   3631/ 13000 | loss: 1.8196 | ds_loss: 0.0000 | lr: 4.1083e-05 | scale: 32768.0000 | micro time: 1.870 | step time: 0.000
train | epoch   2 | Iter:   3632/ 13000 | global iter:   3632/ 13000 | loss: 2.1514 | ds_loss: 0.0000 | lr: 4.1079e-05 | scale: 32768.0000 | micro time: 1.663 | step time: 0.000
train | epoch   2 | Iter:   3633/ 13000 | global iter:   3633/ 13000 | loss: 1.5035 | ds_loss: 0.0000 | lr: 4.1074e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   2 | Iter:   3634/ 13000 | global iter:   3634/ 13000 | loss: 2.1789 | ds_loss: 0.0000 | lr: 4.1070e-05 | scale: 32768.0000 | micro time: 1.911 | step time: 0.000
train | epoch   2 | Iter:   3635/ 13000 | global iter:   3635/ 13000 | loss: 1.4033 | ds_loss: 0.0000 | lr: 4.1065e-05 | scale: 32768.0000 | micro time: 1.903 | step time: 0.000
train | epoch   2 | Iter:   3636/ 13000 | global iter:   3636/ 13000 | loss: 2.3019 | ds_loss: 0.0000 | lr: 4.1060e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   2 | Iter:   3637/ 13000 | global iter:   3637/ 13000 | loss: 1.8523 | ds_loss: 0.0000 | lr: 4.1056e-05 | scale: 32768.0000 | micro time: 1.711 | step time: 0.000
train | epoch   2 | Iter:   3638/ 13000 | global iter:   3638/ 13000 | loss: 1.0524 | ds_loss: 0.0000 | lr: 4.1051e-05 | scale: 32768.0000 | micro time: 1.856 | step time: 0.000
train | epoch   2 | Iter:   3639/ 13000 | global iter:   3639/ 13000 | loss: 1.3658 | ds_loss: 0.0000 | lr: 4.1046e-05 | scale: 32768.0000 | micro time: 1.818 | step time: 0.000
train | epoch   2 | Iter:   3640/ 13000 | global iter:   3640/ 13000 | loss: 1.2766 | ds_loss: 0.0000 | lr: 4.1042e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3640/ 13000 | global iter:   3640/ 13000 | loss: 1.6906 | ds_loss: 0.0000 | lr: 4.1042e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 1.811
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3641/ 13000 | global iter:   3641/ 13000 | loss: 2.0806 | ds_loss: 0.0000 | lr: 4.1037e-05 | scale: 32768.0000 | micro time: 1.784 | step time: 0.000
train | epoch   2 | Iter:   3642/ 13000 | global iter:   3642/ 13000 | loss: 2.2495 | ds_loss: 0.0000 | lr: 4.1033e-05 | scale: 32768.0000 | micro time: 1.793 | step time: 0.000
train | epoch   2 | Iter:   3643/ 13000 | global iter:   3643/ 13000 | loss: 1.4008 | ds_loss: 0.0000 | lr: 4.1028e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   2 | Iter:   3644/ 13000 | global iter:   3644/ 13000 | loss: 1.9460 | ds_loss: 0.0000 | lr: 4.1023e-05 | scale: 32768.0000 | micro time: 1.832 | step time: 0.000
train | epoch   2 | Iter:   3645/ 13000 | global iter:   3645/ 13000 | loss: 1.3369 | ds_loss: 0.0000 | lr: 4.1019e-05 | scale: 32768.0000 | micro time: 1.891 | step time: 0.000
train | epoch   2 | Iter:   3646/ 13000 | global iter:   3646/ 13000 | loss: 1.8555 | ds_loss: 0.0000 | lr: 4.1014e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 0.000
train | epoch   2 | Iter:   3647/ 13000 | global iter:   3647/ 13000 | loss: 1.9643 | ds_loss: 0.0000 | lr: 4.1009e-05 | scale: 32768.0000 | micro time: 1.776 | step time: 0.000
train | epoch   2 | Iter:   3648/ 13000 | global iter:   3648/ 13000 | loss: 1.4721 | ds_loss: 0.0000 | lr: 4.1005e-05 | scale: 32768.0000 | micro time: 1.790 | step time: 0.000
train | epoch   2 | Iter:   3649/ 13000 | global iter:   3649/ 13000 | loss: 1.8156 | ds_loss: 0.0000 | lr: 4.1000e-05 | scale: 32768.0000 | micro time: 1.820 | step time: 0.000
train | epoch   2 | Iter:   3650/ 13000 | global iter:   3650/ 13000 | loss: 1.5015 | ds_loss: 0.0000 | lr: 4.0996e-05 | scale: 32768.0000 | micro time: 1.857 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3650/ 13000 | global iter:   3650/ 13000 | loss: 1.7623 | ds_loss: 0.0000 | lr: 4.0996e-05 | scale: 32768.0000 | micro time: 1.857 | step time: 1.821
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3651/ 13000 | global iter:   3651/ 13000 | loss: 2.1453 | ds_loss: 0.0000 | lr: 4.0991e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   2 | Iter:   3652/ 13000 | global iter:   3652/ 13000 | loss: 1.6895 | ds_loss: 0.0000 | lr: 4.0986e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   2 | Iter:   3653/ 13000 | global iter:   3653/ 13000 | loss: 2.1021 | ds_loss: 0.0000 | lr: 4.0982e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   2 | Iter:   3654/ 13000 | global iter:   3654/ 13000 | loss: 1.5463 | ds_loss: 0.0000 | lr: 4.0977e-05 | scale: 32768.0000 | micro time: 1.903 | step time: 0.000
train | epoch   2 | Iter:   3655/ 13000 | global iter:   3655/ 13000 | loss: 1.3534 | ds_loss: 0.0000 | lr: 4.0972e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 0.000
train | epoch   2 | Iter:   3656/ 13000 | global iter:   3656/ 13000 | loss: 1.7970 | ds_loss: 0.0000 | lr: 4.0968e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
train | epoch   2 | Iter:   3657/ 13000 | global iter:   3657/ 13000 | loss: 1.4439 | ds_loss: 0.0000 | lr: 4.0963e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   2 | Iter:   3658/ 13000 | global iter:   3658/ 13000 | loss: 1.5281 | ds_loss: 0.0000 | lr: 4.0958e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   2 | Iter:   3659/ 13000 | global iter:   3659/ 13000 | loss: 1.6776 | ds_loss: 0.0000 | lr: 4.0954e-05 | scale: 32768.0000 | micro time: 1.679 | step time: 0.000
train | epoch   2 | Iter:   3660/ 13000 | global iter:   3660/ 13000 | loss: 2.0248 | ds_loss: 0.0000 | lr: 4.0949e-05 | scale: 32768.0000 | micro time: 1.826 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3660/ 13000 | global iter:   3660/ 13000 | loss: 1.7308 | ds_loss: 0.0000 | lr: 4.0949e-05 | scale: 32768.0000 | micro time: 1.826 | step time: 1.807
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3661/ 13000 | global iter:   3661/ 13000 | loss: 1.7065 | ds_loss: 0.0000 | lr: 4.0944e-05 | scale: 32768.0000 | micro time: 1.926 | step time: 0.000
train | epoch   2 | Iter:   3662/ 13000 | global iter:   3662/ 13000 | loss: 2.1475 | ds_loss: 0.0000 | lr: 4.0940e-05 | scale: 32768.0000 | micro time: 1.824 | step time: 0.000
train | epoch   2 | Iter:   3663/ 13000 | global iter:   3663/ 13000 | loss: 2.2523 | ds_loss: 0.0000 | lr: 4.0935e-05 | scale: 32768.0000 | micro time: 1.722 | step time: 0.000
train | epoch   2 | Iter:   3664/ 13000 | global iter:   3664/ 13000 | loss: 1.4509 | ds_loss: 0.0000 | lr: 4.0930e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   2 | Iter:   3665/ 13000 | global iter:   3665/ 13000 | loss: 2.0356 | ds_loss: 0.0000 | lr: 4.0926e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   2 | Iter:   3666/ 13000 | global iter:   3666/ 13000 | loss: 1.9722 | ds_loss: 0.0000 | lr: 4.0921e-05 | scale: 32768.0000 | micro time: 1.880 | step time: 0.000
train | epoch   2 | Iter:   3667/ 13000 | global iter:   3667/ 13000 | loss: 2.4243 | ds_loss: 0.0000 | lr: 4.0917e-05 | scale: 32768.0000 | micro time: 1.754 | step time: 0.000
train | epoch   2 | Iter:   3668/ 13000 | global iter:   3668/ 13000 | loss: 2.0795 | ds_loss: 0.0000 | lr: 4.0912e-05 | scale: 32768.0000 | micro time: 1.825 | step time: 0.000
train | epoch   2 | Iter:   3669/ 13000 | global iter:   3669/ 13000 | loss: 1.7472 | ds_loss: 0.0000 | lr: 4.0907e-05 | scale: 32768.0000 | micro time: 1.808 | step time: 0.000
train | epoch   2 | Iter:   3670/ 13000 | global iter:   3670/ 13000 | loss: 1.7001 | ds_loss: 0.0000 | lr: 4.0903e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3670/ 13000 | global iter:   3670/ 13000 | loss: 1.9516 | ds_loss: 0.0000 | lr: 4.0903e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 1.817
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3671/ 13000 | global iter:   3671/ 13000 | loss: 2.4774 | ds_loss: 0.0000 | lr: 4.0898e-05 | scale: 32768.0000 | micro time: 1.744 | step time: 0.000
train | epoch   2 | Iter:   3672/ 13000 | global iter:   3672/ 13000 | loss: 1.7789 | ds_loss: 0.0000 | lr: 4.0893e-05 | scale: 32768.0000 | micro time: 1.814 | step time: 0.000
train | epoch   2 | Iter:   3673/ 13000 | global iter:   3673/ 13000 | loss: 1.4201 | ds_loss: 0.0000 | lr: 4.0889e-05 | scale: 32768.0000 | micro time: 1.792 | step time: 0.000
train | epoch   2 | Iter:   3674/ 13000 | global iter:   3674/ 13000 | loss: 1.3367 | ds_loss: 0.0000 | lr: 4.0884e-05 | scale: 32768.0000 | micro time: 1.853 | step time: 0.000
train | epoch   2 | Iter:   3675/ 13000 | global iter:   3675/ 13000 | loss: 2.0865 | ds_loss: 0.0000 | lr: 4.0879e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   2 | Iter:   3676/ 13000 | global iter:   3676/ 13000 | loss: 1.4359 | ds_loss: 0.0000 | lr: 4.0875e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   2 | Iter:   3677/ 13000 | global iter:   3677/ 13000 | loss: 1.5994 | ds_loss: 0.0000 | lr: 4.0870e-05 | scale: 32768.0000 | micro time: 1.901 | step time: 0.000
train | epoch   2 | Iter:   3678/ 13000 | global iter:   3678/ 13000 | loss: 1.0743 | ds_loss: 0.0000 | lr: 4.0865e-05 | scale: 32768.0000 | micro time: 1.820 | step time: 0.000
train | epoch   2 | Iter:   3679/ 13000 | global iter:   3679/ 13000 | loss: 1.9654 | ds_loss: 0.0000 | lr: 4.0861e-05 | scale: 32768.0000 | micro time: 1.703 | step time: 0.000
train | epoch   2 | Iter:   3680/ 13000 | global iter:   3680/ 13000 | loss: 1.6561 | ds_loss: 0.0000 | lr: 4.0856e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3680/ 13000 | global iter:   3680/ 13000 | loss: 1.6831 | ds_loss: 0.0000 | lr: 4.0856e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 1.810
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3681/ 13000 | global iter:   3681/ 13000 | loss: 2.5444 | ds_loss: 0.0000 | lr: 4.0851e-05 | scale: 32768.0000 | micro time: 1.802 | step time: 0.000
train | epoch   2 | Iter:   3682/ 13000 | global iter:   3682/ 13000 | loss: 1.6853 | ds_loss: 0.0000 | lr: 4.0847e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   2 | Iter:   3683/ 13000 | global iter:   3683/ 13000 | loss: 2.2168 | ds_loss: 0.0000 | lr: 4.0842e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   2 | Iter:   3684/ 13000 | global iter:   3684/ 13000 | loss: 1.2196 | ds_loss: 0.0000 | lr: 4.0837e-05 | scale: 32768.0000 | micro time: 1.804 | step time: 0.000
train | epoch   2 | Iter:   3685/ 13000 | global iter:   3685/ 13000 | loss: 2.0824 | ds_loss: 0.0000 | lr: 4.0833e-05 | scale: 32768.0000 | micro time: 1.817 | step time: 0.000
train | epoch   2 | Iter:   3686/ 13000 | global iter:   3686/ 13000 | loss: 2.1420 | ds_loss: 0.0000 | lr: 4.0828e-05 | scale: 32768.0000 | micro time: 1.745 | step time: 0.000
train | epoch   2 | Iter:   3687/ 13000 | global iter:   3687/ 13000 | loss: 1.5173 | ds_loss: 0.0000 | lr: 4.0823e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   2 | Iter:   3688/ 13000 | global iter:   3688/ 13000 | loss: 1.6505 | ds_loss: 0.0000 | lr: 4.0819e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   2 | Iter:   3689/ 13000 | global iter:   3689/ 13000 | loss: 2.3161 | ds_loss: 0.0000 | lr: 4.0814e-05 | scale: 32768.0000 | micro time: 1.765 | step time: 0.000
train | epoch   2 | Iter:   3690/ 13000 | global iter:   3690/ 13000 | loss: 1.8753 | ds_loss: 0.0000 | lr: 4.0809e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3690/ 13000 | global iter:   3690/ 13000 | loss: 1.9250 | ds_loss: 0.0000 | lr: 4.0809e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 1.800
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3691/ 13000 | global iter:   3691/ 13000 | loss: 1.2123 | ds_loss: 0.0000 | lr: 4.0805e-05 | scale: 32768.0000 | micro time: 1.782 | step time: 0.000
train | epoch   2 | Iter:   3692/ 13000 | global iter:   3692/ 13000 | loss: 2.2503 | ds_loss: 0.0000 | lr: 4.0800e-05 | scale: 32768.0000 | micro time: 1.816 | step time: 0.000
train | epoch   2 | Iter:   3693/ 13000 | global iter:   3693/ 13000 | loss: 1.9067 | ds_loss: 0.0000 | lr: 4.0795e-05 | scale: 32768.0000 | micro time: 1.800 | step time: 0.000
train | epoch   2 | Iter:   3694/ 13000 | global iter:   3694/ 13000 | loss: 1.3820 | ds_loss: 0.0000 | lr: 4.0791e-05 | scale: 32768.0000 | micro time: 1.772 | step time: 0.000
train | epoch   2 | Iter:   3695/ 13000 | global iter:   3695/ 13000 | loss: 2.2510 | ds_loss: 0.0000 | lr: 4.0786e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   2 | Iter:   3696/ 13000 | global iter:   3696/ 13000 | loss: 1.8583 | ds_loss: 0.0000 | lr: 4.0781e-05 | scale: 32768.0000 | micro time: 1.790 | step time: 0.000
train | epoch   2 | Iter:   3697/ 13000 | global iter:   3697/ 13000 | loss: 1.9329 | ds_loss: 0.0000 | lr: 4.0777e-05 | scale: 32768.0000 | micro time: 1.748 | step time: 0.000
train | epoch   2 | Iter:   3698/ 13000 | global iter:   3698/ 13000 | loss: 1.7908 | ds_loss: 0.0000 | lr: 4.0772e-05 | scale: 32768.0000 | micro time: 1.838 | step time: 0.000
train | epoch   2 | Iter:   3699/ 13000 | global iter:   3699/ 13000 | loss: 1.7904 | ds_loss: 0.0000 | lr: 4.0767e-05 | scale: 32768.0000 | micro time: 1.754 | step time: 0.000
train | epoch   2 | Iter:   3700/ 13000 | global iter:   3700/ 13000 | loss: 2.0573 | ds_loss: 0.0000 | lr: 4.0762e-05 | scale: 32768.0000 | micro time: 1.728 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3700/ 13000 | global iter:   3700/ 13000 | loss: 1.8432 | ds_loss: 0.0000 | lr: 4.0762e-05 | scale: 32768.0000 | micro time: 1.728 | step time: 1.786
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3701/ 13000 | global iter:   3701/ 13000 | loss: 1.4290 | ds_loss: 0.0000 | lr: 4.0758e-05 | scale: 32768.0000 | micro time: 1.853 | step time: 0.000
train | epoch   2 | Iter:   3702/ 13000 | global iter:   3702/ 13000 | loss: 1.8168 | ds_loss: 0.0000 | lr: 4.0753e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   2 | Iter:   3703/ 13000 | global iter:   3703/ 13000 | loss: 1.0359 | ds_loss: 0.0000 | lr: 4.0748e-05 | scale: 32768.0000 | micro time: 1.825 | step time: 0.000
train | epoch   2 | Iter:   3704/ 13000 | global iter:   3704/ 13000 | loss: 1.8777 | ds_loss: 0.0000 | lr: 4.0744e-05 | scale: 32768.0000 | micro time: 1.793 | step time: 0.000
train | epoch   2 | Iter:   3705/ 13000 | global iter:   3705/ 13000 | loss: 2.1632 | ds_loss: 0.0000 | lr: 4.0739e-05 | scale: 32768.0000 | micro time: 1.790 | step time: 0.000
train | epoch   2 | Iter:   3706/ 13000 | global iter:   3706/ 13000 | loss: 2.0097 | ds_loss: 0.0000 | lr: 4.0734e-05 | scale: 32768.0000 | micro time: 1.788 | step time: 0.000
train | epoch   2 | Iter:   3707/ 13000 | global iter:   3707/ 13000 | loss: 2.0127 | ds_loss: 0.0000 | lr: 4.0730e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   2 | Iter:   3708/ 13000 | global iter:   3708/ 13000 | loss: 1.6722 | ds_loss: 0.0000 | lr: 4.0725e-05 | scale: 32768.0000 | micro time: 1.788 | step time: 0.000
train | epoch   2 | Iter:   3709/ 13000 | global iter:   3709/ 13000 | loss: 2.1661 | ds_loss: 0.0000 | lr: 4.0720e-05 | scale: 32768.0000 | micro time: 1.893 | step time: 0.000
train | epoch   2 | Iter:   3710/ 13000 | global iter:   3710/ 13000 | loss: 2.3496 | ds_loss: 0.0000 | lr: 4.0716e-05 | scale: 32768.0000 | micro time: 1.772 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3710/ 13000 | global iter:   3710/ 13000 | loss: 1.8533 | ds_loss: 0.0000 | lr: 4.0716e-05 | scale: 32768.0000 | micro time: 1.772 | step time: 1.814
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3711/ 13000 | global iter:   3711/ 13000 | loss: 1.9194 | ds_loss: 0.0000 | lr: 4.0711e-05 | scale: 32768.0000 | micro time: 1.978 | step time: 0.000
train | epoch   2 | Iter:   3712/ 13000 | global iter:   3712/ 13000 | loss: 1.8830 | ds_loss: 0.0000 | lr: 4.0706e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   2 | Iter:   3713/ 13000 | global iter:   3713/ 13000 | loss: 1.4644 | ds_loss: 0.0000 | lr: 4.0702e-05 | scale: 32768.0000 | micro time: 1.899 | step time: 0.000
train | epoch   2 | Iter:   3714/ 13000 | global iter:   3714/ 13000 | loss: 1.7189 | ds_loss: 0.0000 | lr: 4.0697e-05 | scale: 32768.0000 | micro time: 1.751 | step time: 0.000
train | epoch   2 | Iter:   3715/ 13000 | global iter:   3715/ 13000 | loss: 1.6304 | ds_loss: 0.0000 | lr: 4.0692e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   2 | Iter:   3716/ 13000 | global iter:   3716/ 13000 | loss: 2.2397 | ds_loss: 0.0000 | lr: 4.0687e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   2 | Iter:   3717/ 13000 | global iter:   3717/ 13000 | loss: 1.9580 | ds_loss: 0.0000 | lr: 4.0683e-05 | scale: 32768.0000 | micro time: 1.715 | step time: 0.000
train | epoch   2 | Iter:   3718/ 13000 | global iter:   3718/ 13000 | loss: 1.8809 | ds_loss: 0.0000 | lr: 4.0678e-05 | scale: 32768.0000 | micro time: 1.687 | step time: 0.000
train | epoch   2 | Iter:   3719/ 13000 | global iter:   3719/ 13000 | loss: 1.7997 | ds_loss: 0.0000 | lr: 4.0673e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   2 | Iter:   3720/ 13000 | global iter:   3720/ 13000 | loss: 1.8346 | ds_loss: 0.0000 | lr: 4.0669e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3720/ 13000 | global iter:   3720/ 13000 | loss: 1.8329 | ds_loss: 0.0000 | lr: 4.0669e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 1.814
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3721/ 13000 | global iter:   3721/ 13000 | loss: 1.8138 | ds_loss: 0.0000 | lr: 4.0664e-05 | scale: 32768.0000 | micro time: 1.773 | step time: 0.000
train | epoch   2 | Iter:   3722/ 13000 | global iter:   3722/ 13000 | loss: 1.0917 | ds_loss: 0.0000 | lr: 4.0659e-05 | scale: 32768.0000 | micro time: 1.826 | step time: 0.000
train | epoch   2 | Iter:   3723/ 13000 | global iter:   3723/ 13000 | loss: 1.3928 | ds_loss: 0.0000 | lr: 4.0655e-05 | scale: 32768.0000 | micro time: 1.795 | step time: 0.000
train | epoch   2 | Iter:   3724/ 13000 | global iter:   3724/ 13000 | loss: 1.9549 | ds_loss: 0.0000 | lr: 4.0650e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   2 | Iter:   3725/ 13000 | global iter:   3725/ 13000 | loss: 1.7158 | ds_loss: 0.0000 | lr: 4.0645e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   2 | Iter:   3726/ 13000 | global iter:   3726/ 13000 | loss: 2.2147 | ds_loss: 0.0000 | lr: 4.0640e-05 | scale: 32768.0000 | micro time: 1.747 | step time: 0.000
train | epoch   2 | Iter:   3727/ 13000 | global iter:   3727/ 13000 | loss: 2.0057 | ds_loss: 0.0000 | lr: 4.0636e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   2 | Iter:   3728/ 13000 | global iter:   3728/ 13000 | loss: 1.9593 | ds_loss: 0.0000 | lr: 4.0631e-05 | scale: 32768.0000 | micro time: 1.898 | step time: 0.000
train | epoch   2 | Iter:   3729/ 13000 | global iter:   3729/ 13000 | loss: 1.4331 | ds_loss: 0.0000 | lr: 4.0626e-05 | scale: 32768.0000 | micro time: 1.760 | step time: 0.000
train | epoch   2 | Iter:   3730/ 13000 | global iter:   3730/ 13000 | loss: 1.7404 | ds_loss: 0.0000 | lr: 4.0622e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3730/ 13000 | global iter:   3730/ 13000 | loss: 1.7322 | ds_loss: 0.0000 | lr: 4.0622e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 1.807
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3731/ 13000 | global iter:   3731/ 13000 | loss: 2.0168 | ds_loss: 0.0000 | lr: 4.0617e-05 | scale: 32768.0000 | micro time: 1.766 | step time: 0.000
train | epoch   2 | Iter:   3732/ 13000 | global iter:   3732/ 13000 | loss: 1.7865 | ds_loss: 0.0000 | lr: 4.0612e-05 | scale: 32768.0000 | micro time: 1.868 | step time: 0.000
train | epoch   2 | Iter:   3733/ 13000 | global iter:   3733/ 13000 | loss: 2.1498 | ds_loss: 0.0000 | lr: 4.0607e-05 | scale: 32768.0000 | micro time: 1.737 | step time: 0.000
train | epoch   2 | Iter:   3734/ 13000 | global iter:   3734/ 13000 | loss: 1.4548 | ds_loss: 0.0000 | lr: 4.0603e-05 | scale: 32768.0000 | micro time: 1.776 | step time: 0.000
train | epoch   2 | Iter:   3735/ 13000 | global iter:   3735/ 13000 | loss: 1.8703 | ds_loss: 0.0000 | lr: 4.0598e-05 | scale: 32768.0000 | micro time: 1.826 | step time: 0.000
train | epoch   2 | Iter:   3736/ 13000 | global iter:   3736/ 13000 | loss: 1.8799 | ds_loss: 0.0000 | lr: 4.0593e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 0.000
train | epoch   2 | Iter:   3737/ 13000 | global iter:   3737/ 13000 | loss: 1.2039 | ds_loss: 0.0000 | lr: 4.0589e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   2 | Iter:   3738/ 13000 | global iter:   3738/ 13000 | loss: 1.6717 | ds_loss: 0.0000 | lr: 4.0584e-05 | scale: 32768.0000 | micro time: 1.743 | step time: 0.000
train | epoch   2 | Iter:   3739/ 13000 | global iter:   3739/ 13000 | loss: 1.6574 | ds_loss: 0.0000 | lr: 4.0579e-05 | scale: 32768.0000 | micro time: 1.724 | step time: 0.000
train | epoch   2 | Iter:   3740/ 13000 | global iter:   3740/ 13000 | loss: 2.6940 | ds_loss: 0.0000 | lr: 4.0574e-05 | scale: 32768.0000 | micro time: 1.813 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3740/ 13000 | global iter:   3740/ 13000 | loss: 1.8385 | ds_loss: 0.0000 | lr: 4.0574e-05 | scale: 32768.0000 | micro time: 1.813 | step time: 1.794
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3741/ 13000 | global iter:   3741/ 13000 | loss: 1.6997 | ds_loss: 0.0000 | lr: 4.0570e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   2 | Iter:   3742/ 13000 | global iter:   3742/ 13000 | loss: 1.6167 | ds_loss: 0.0000 | lr: 4.0565e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 0.000
train | epoch   2 | Iter:   3743/ 13000 | global iter:   3743/ 13000 | loss: 1.8467 | ds_loss: 0.0000 | lr: 4.0560e-05 | scale: 32768.0000 | micro time: 1.751 | step time: 0.000
train | epoch   2 | Iter:   3744/ 13000 | global iter:   3744/ 13000 | loss: 1.6544 | ds_loss: 0.0000 | lr: 4.0556e-05 | scale: 32768.0000 | micro time: 1.743 | step time: 0.000
train | epoch   2 | Iter:   3745/ 13000 | global iter:   3745/ 13000 | loss: 1.8390 | ds_loss: 0.0000 | lr: 4.0551e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   2 | Iter:   3746/ 13000 | global iter:   3746/ 13000 | loss: 1.9281 | ds_loss: 0.0000 | lr: 4.0546e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   2 | Iter:   3747/ 13000 | global iter:   3747/ 13000 | loss: 1.4558 | ds_loss: 0.0000 | lr: 4.0541e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   2 | Iter:   3748/ 13000 | global iter:   3748/ 13000 | loss: 1.3765 | ds_loss: 0.0000 | lr: 4.0537e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 0.000
train | epoch   2 | Iter:   3749/ 13000 | global iter:   3749/ 13000 | loss: 2.0244 | ds_loss: 0.0000 | lr: 4.0532e-05 | scale: 32768.0000 | micro time: 1.809 | step time: 0.000
train | epoch   2 | Iter:   3750/ 13000 | global iter:   3750/ 13000 | loss: 1.2690 | ds_loss: 0.0000 | lr: 4.0527e-05 | scale: 32768.0000 | micro time: 1.764 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3750/ 13000 | global iter:   3750/ 13000 | loss: 1.6710 | ds_loss: 0.0000 | lr: 4.0527e-05 | scale: 32768.0000 | micro time: 1.764 | step time: 1.809
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3751/ 13000 | global iter:   3751/ 13000 | loss: 1.9406 | ds_loss: 0.0000 | lr: 4.0522e-05 | scale: 32768.0000 | micro time: 1.830 | step time: 0.000
train | epoch   2 | Iter:   3752/ 13000 | global iter:   3752/ 13000 | loss: 1.9294 | ds_loss: 0.0000 | lr: 4.0518e-05 | scale: 32768.0000 | micro time: 1.774 | step time: 0.000
train | epoch   2 | Iter:   3753/ 13000 | global iter:   3753/ 13000 | loss: 1.7432 | ds_loss: 0.0000 | lr: 4.0513e-05 | scale: 32768.0000 | micro time: 1.832 | step time: 0.000
train | epoch   2 | Iter:   3754/ 13000 | global iter:   3754/ 13000 | loss: 1.2484 | ds_loss: 0.0000 | lr: 4.0508e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   2 | Iter:   3755/ 13000 | global iter:   3755/ 13000 | loss: 2.2055 | ds_loss: 0.0000 | lr: 4.0503e-05 | scale: 32768.0000 | micro time: 1.808 | step time: 0.000
train | epoch   2 | Iter:   3756/ 13000 | global iter:   3756/ 13000 | loss: 1.6848 | ds_loss: 0.0000 | lr: 4.0499e-05 | scale: 32768.0000 | micro time: 1.797 | step time: 0.000
train | epoch   2 | Iter:   3757/ 13000 | global iter:   3757/ 13000 | loss: 1.2777 | ds_loss: 0.0000 | lr: 4.0494e-05 | scale: 32768.0000 | micro time: 1.825 | step time: 0.000
train | epoch   2 | Iter:   3758/ 13000 | global iter:   3758/ 13000 | loss: 1.2552 | ds_loss: 0.0000 | lr: 4.0489e-05 | scale: 32768.0000 | micro time: 1.841 | step time: 0.000
train | epoch   2 | Iter:   3759/ 13000 | global iter:   3759/ 13000 | loss: 1.9315 | ds_loss: 0.0000 | lr: 4.0485e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   2 | Iter:   3760/ 13000 | global iter:   3760/ 13000 | loss: 1.6440 | ds_loss: 0.0000 | lr: 4.0480e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3760/ 13000 | global iter:   3760/ 13000 | loss: 1.6860 | ds_loss: 0.0000 | lr: 4.0480e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 1.812
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3761/ 13000 | global iter:   3761/ 13000 | loss: 2.2019 | ds_loss: 0.0000 | lr: 4.0475e-05 | scale: 32768.0000 | micro time: 1.949 | step time: 0.000
train | epoch   2 | Iter:   3762/ 13000 | global iter:   3762/ 13000 | loss: 1.8490 | ds_loss: 0.0000 | lr: 4.0470e-05 | scale: 32768.0000 | micro time: 1.751 | step time: 0.000
train | epoch   2 | Iter:   3763/ 13000 | global iter:   3763/ 13000 | loss: 1.7763 | ds_loss: 0.0000 | lr: 4.0466e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   2 | Iter:   3764/ 13000 | global iter:   3764/ 13000 | loss: 2.1805 | ds_loss: 0.0000 | lr: 4.0461e-05 | scale: 32768.0000 | micro time: 1.796 | step time: 0.000
train | epoch   2 | Iter:   3765/ 13000 | global iter:   3765/ 13000 | loss: 1.9676 | ds_loss: 0.0000 | lr: 4.0456e-05 | scale: 32768.0000 | micro time: 1.800 | step time: 0.000
train | epoch   2 | Iter:   3766/ 13000 | global iter:   3766/ 13000 | loss: 2.0875 | ds_loss: 0.0000 | lr: 4.0451e-05 | scale: 32768.0000 | micro time: 1.778 | step time: 0.000
train | epoch   2 | Iter:   3767/ 13000 | global iter:   3767/ 13000 | loss: 2.1705 | ds_loss: 0.0000 | lr: 4.0447e-05 | scale: 32768.0000 | micro time: 1.793 | step time: 0.000
train | epoch   2 | Iter:   3768/ 13000 | global iter:   3768/ 13000 | loss: 1.6057 | ds_loss: 0.0000 | lr: 4.0442e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   2 | Iter:   3769/ 13000 | global iter:   3769/ 13000 | loss: 2.4126 | ds_loss: 0.0000 | lr: 4.0437e-05 | scale: 32768.0000 | micro time: 1.757 | step time: 0.000
train | epoch   2 | Iter:   3770/ 13000 | global iter:   3770/ 13000 | loss: 1.6345 | ds_loss: 0.0000 | lr: 4.0432e-05 | scale: 32768.0000 | micro time: 1.737 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3770/ 13000 | global iter:   3770/ 13000 | loss: 1.9886 | ds_loss: 0.0000 | lr: 4.0432e-05 | scale: 32768.0000 | micro time: 1.737 | step time: 1.798
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3771/ 13000 | global iter:   3771/ 13000 | loss: 1.2967 | ds_loss: 0.0000 | lr: 4.0428e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   2 | Iter:   3772/ 13000 | global iter:   3772/ 13000 | loss: 2.1902 | ds_loss: 0.0000 | lr: 4.0423e-05 | scale: 32768.0000 | micro time: 1.898 | step time: 0.000
train | epoch   2 | Iter:   3773/ 13000 | global iter:   3773/ 13000 | loss: 1.9838 | ds_loss: 0.0000 | lr: 4.0418e-05 | scale: 32768.0000 | micro time: 1.802 | step time: 0.000
train | epoch   2 | Iter:   3774/ 13000 | global iter:   3774/ 13000 | loss: 2.1722 | ds_loss: 0.0000 | lr: 4.0413e-05 | scale: 32768.0000 | micro time: 1.713 | step time: 0.000
train | epoch   2 | Iter:   3775/ 13000 | global iter:   3775/ 13000 | loss: 1.6993 | ds_loss: 0.0000 | lr: 4.0409e-05 | scale: 32768.0000 | micro time: 1.686 | step time: 0.000
train | epoch   2 | Iter:   3776/ 13000 | global iter:   3776/ 13000 | loss: 1.6545 | ds_loss: 0.0000 | lr: 4.0404e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   2 | Iter:   3777/ 13000 | global iter:   3777/ 13000 | loss: 1.6558 | ds_loss: 0.0000 | lr: 4.0399e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   2 | Iter:   3778/ 13000 | global iter:   3778/ 13000 | loss: 1.9203 | ds_loss: 0.0000 | lr: 4.0394e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   2 | Iter:   3779/ 13000 | global iter:   3779/ 13000 | loss: 1.3046 | ds_loss: 0.0000 | lr: 4.0390e-05 | scale: 32768.0000 | micro time: 1.826 | step time: 0.000
train | epoch   2 | Iter:   3780/ 13000 | global iter:   3780/ 13000 | loss: 1.6194 | ds_loss: 0.0000 | lr: 4.0385e-05 | scale: 32768.0000 | micro time: 1.687 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3780/ 13000 | global iter:   3780/ 13000 | loss: 1.7497 | ds_loss: 0.0000 | lr: 4.0385e-05 | scale: 32768.0000 | micro time: 1.687 | step time: 1.782
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3781/ 13000 | global iter:   3781/ 13000 | loss: 2.0715 | ds_loss: 0.0000 | lr: 4.0380e-05 | scale: 32768.0000 | micro time: 1.726 | step time: 0.000
train | epoch   2 | Iter:   3782/ 13000 | global iter:   3782/ 13000 | loss: 1.8176 | ds_loss: 0.0000 | lr: 4.0375e-05 | scale: 32768.0000 | micro time: 1.719 | step time: 0.000
train | epoch   2 | Iter:   3783/ 13000 | global iter:   3783/ 13000 | loss: 2.1402 | ds_loss: 0.0000 | lr: 4.0371e-05 | scale: 32768.0000 | micro time: 1.829 | step time: 0.000
train | epoch   2 | Iter:   3784/ 13000 | global iter:   3784/ 13000 | loss: 1.5661 | ds_loss: 0.0000 | lr: 4.0366e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   2 | Iter:   3785/ 13000 | global iter:   3785/ 13000 | loss: 1.9268 | ds_loss: 0.0000 | lr: 4.0361e-05 | scale: 32768.0000 | micro time: 1.804 | step time: 0.000
train | epoch   2 | Iter:   3786/ 13000 | global iter:   3786/ 13000 | loss: 2.0731 | ds_loss: 0.0000 | lr: 4.0356e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   2 | Iter:   3787/ 13000 | global iter:   3787/ 13000 | loss: 1.9147 | ds_loss: 0.0000 | lr: 4.0352e-05 | scale: 32768.0000 | micro time: 1.889 | step time: 0.000
train | epoch   2 | Iter:   3788/ 13000 | global iter:   3788/ 13000 | loss: 2.2111 | ds_loss: 0.0000 | lr: 4.0347e-05 | scale: 32768.0000 | micro time: 1.833 | step time: 0.000
train | epoch   2 | Iter:   3789/ 13000 | global iter:   3789/ 13000 | loss: 2.0337 | ds_loss: 0.0000 | lr: 4.0342e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   2 | Iter:   3790/ 13000 | global iter:   3790/ 13000 | loss: 1.9789 | ds_loss: 0.0000 | lr: 4.0337e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3790/ 13000 | global iter:   3790/ 13000 | loss: 1.9734 | ds_loss: 0.0000 | lr: 4.0337e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 1.808
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3791/ 13000 | global iter:   3791/ 13000 | loss: 1.8105 | ds_loss: 0.0000 | lr: 4.0333e-05 | scale: 32768.0000 | micro time: 1.698 | step time: 0.000
train | epoch   2 | Iter:   3792/ 13000 | global iter:   3792/ 13000 | loss: 2.0269 | ds_loss: 0.0000 | lr: 4.0328e-05 | scale: 32768.0000 | micro time: 1.740 | step time: 0.000
train | epoch   2 | Iter:   3793/ 13000 | global iter:   3793/ 13000 | loss: 2.0896 | ds_loss: 0.0000 | lr: 4.0323e-05 | scale: 32768.0000 | micro time: 1.858 | step time: 0.000
train | epoch   2 | Iter:   3794/ 13000 | global iter:   3794/ 13000 | loss: 2.2811 | ds_loss: 0.0000 | lr: 4.0318e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   2 | Iter:   3795/ 13000 | global iter:   3795/ 13000 | loss: 2.0759 | ds_loss: 0.0000 | lr: 4.0313e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 0.000
train | epoch   2 | Iter:   3796/ 13000 | global iter:   3796/ 13000 | loss: 2.0265 | ds_loss: 0.0000 | lr: 4.0309e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   2 | Iter:   3797/ 13000 | global iter:   3797/ 13000 | loss: 2.0583 | ds_loss: 0.0000 | lr: 4.0304e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   2 | Iter:   3798/ 13000 | global iter:   3798/ 13000 | loss: 1.6459 | ds_loss: 0.0000 | lr: 4.0299e-05 | scale: 32768.0000 | micro time: 1.881 | step time: 0.000
train | epoch   2 | Iter:   3799/ 13000 | global iter:   3799/ 13000 | loss: 1.6069 | ds_loss: 0.0000 | lr: 4.0294e-05 | scale: 32768.0000 | micro time: 1.750 | step time: 0.000
train | epoch   2 | Iter:   3800/ 13000 | global iter:   3800/ 13000 | loss: 1.8270 | ds_loss: 0.0000 | lr: 4.0290e-05 | scale: 32768.0000 | micro time: 1.726 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3800/ 13000 | global iter:   3800/ 13000 | loss: 1.9449 | ds_loss: 0.0000 | lr: 4.0290e-05 | scale: 32768.0000 | micro time: 1.726 | step time: 1.800
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3801/ 13000 | global iter:   3801/ 13000 | loss: 2.1493 | ds_loss: 0.0000 | lr: 4.0285e-05 | scale: 32768.0000 | micro time: 1.874 | step time: 0.000
train | epoch   2 | Iter:   3802/ 13000 | global iter:   3802/ 13000 | loss: 1.8978 | ds_loss: 0.0000 | lr: 4.0280e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 0.000
train | epoch   2 | Iter:   3803/ 13000 | global iter:   3803/ 13000 | loss: 1.5882 | ds_loss: 0.0000 | lr: 4.0275e-05 | scale: 32768.0000 | micro time: 1.892 | step time: 0.000
train | epoch   2 | Iter:   3804/ 13000 | global iter:   3804/ 13000 | loss: 1.7527 | ds_loss: 0.0000 | lr: 4.0270e-05 | scale: 32768.0000 | micro time: 1.742 | step time: 0.000
train | epoch   2 | Iter:   3805/ 13000 | global iter:   3805/ 13000 | loss: 2.2245 | ds_loss: 0.0000 | lr: 4.0266e-05 | scale: 32768.0000 | micro time: 1.826 | step time: 0.000
train | epoch   2 | Iter:   3806/ 13000 | global iter:   3806/ 13000 | loss: 1.9922 | ds_loss: 0.0000 | lr: 4.0261e-05 | scale: 32768.0000 | micro time: 1.837 | step time: 0.000
train | epoch   2 | Iter:   3807/ 13000 | global iter:   3807/ 13000 | loss: 1.8262 | ds_loss: 0.0000 | lr: 4.0256e-05 | scale: 32768.0000 | micro time: 1.707 | step time: 0.000
train | epoch   2 | Iter:   3808/ 13000 | global iter:   3808/ 13000 | loss: 2.0159 | ds_loss: 0.0000 | lr: 4.0251e-05 | scale: 32768.0000 | micro time: 1.790 | step time: 0.000
train | epoch   2 | Iter:   3809/ 13000 | global iter:   3809/ 13000 | loss: 2.2331 | ds_loss: 0.0000 | lr: 4.0247e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   2 | Iter:   3810/ 13000 | global iter:   3810/ 13000 | loss: 2.1234 | ds_loss: 0.0000 | lr: 4.0242e-05 | scale: 32768.0000 | micro time: 1.735 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3810/ 13000 | global iter:   3810/ 13000 | loss: 1.9803 | ds_loss: 0.0000 | lr: 4.0242e-05 | scale: 32768.0000 | micro time: 1.735 | step time: 1.807
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3811/ 13000 | global iter:   3811/ 13000 | loss: 1.8269 | ds_loss: 0.0000 | lr: 4.0237e-05 | scale: 32768.0000 | micro time: 1.841 | step time: 0.000
train | epoch   2 | Iter:   3812/ 13000 | global iter:   3812/ 13000 | loss: 1.8555 | ds_loss: 0.0000 | lr: 4.0232e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   2 | Iter:   3813/ 13000 | global iter:   3813/ 13000 | loss: 1.7561 | ds_loss: 0.0000 | lr: 4.0227e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   2 | Iter:   3814/ 13000 | global iter:   3814/ 13000 | loss: 1.7514 | ds_loss: 0.0000 | lr: 4.0223e-05 | scale: 32768.0000 | micro time: 1.810 | step time: 0.000
train | epoch   2 | Iter:   3815/ 13000 | global iter:   3815/ 13000 | loss: 1.5783 | ds_loss: 0.0000 | lr: 4.0218e-05 | scale: 32768.0000 | micro time: 1.794 | step time: 0.000
train | epoch   2 | Iter:   3816/ 13000 | global iter:   3816/ 13000 | loss: 2.1929 | ds_loss: 0.0000 | lr: 4.0213e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   2 | Iter:   3817/ 13000 | global iter:   3817/ 13000 | loss: 1.8385 | ds_loss: 0.0000 | lr: 4.0208e-05 | scale: 32768.0000 | micro time: 1.980 | step time: 0.000
train | epoch   2 | Iter:   3818/ 13000 | global iter:   3818/ 13000 | loss: 1.6994 | ds_loss: 0.0000 | lr: 4.0203e-05 | scale: 32768.0000 | micro time: 1.751 | step time: 0.000
train | epoch   2 | Iter:   3819/ 13000 | global iter:   3819/ 13000 | loss: 1.9954 | ds_loss: 0.0000 | lr: 4.0199e-05 | scale: 32768.0000 | micro time: 1.813 | step time: 0.000
train | epoch   2 | Iter:   3820/ 13000 | global iter:   3820/ 13000 | loss: 2.2743 | ds_loss: 0.0000 | lr: 4.0194e-05 | scale: 32768.0000 | micro time: 1.849 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3820/ 13000 | global iter:   3820/ 13000 | loss: 1.8769 | ds_loss: 0.0000 | lr: 4.0194e-05 | scale: 32768.0000 | micro time: 1.849 | step time: 1.835
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3821/ 13000 | global iter:   3821/ 13000 | loss: 2.3922 | ds_loss: 0.0000 | lr: 4.0189e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   2 | Iter:   3822/ 13000 | global iter:   3822/ 13000 | loss: 2.0663 | ds_loss: 0.0000 | lr: 4.0184e-05 | scale: 32768.0000 | micro time: 1.881 | step time: 0.000
train | epoch   2 | Iter:   3823/ 13000 | global iter:   3823/ 13000 | loss: 1.6778 | ds_loss: 0.0000 | lr: 4.0180e-05 | scale: 32768.0000 | micro time: 1.820 | step time: 0.000
train | epoch   2 | Iter:   3824/ 13000 | global iter:   3824/ 13000 | loss: 1.8731 | ds_loss: 0.0000 | lr: 4.0175e-05 | scale: 32768.0000 | micro time: 1.850 | step time: 0.000
train | epoch   2 | Iter:   3825/ 13000 | global iter:   3825/ 13000 | loss: 1.8158 | ds_loss: 0.0000 | lr: 4.0170e-05 | scale: 32768.0000 | micro time: 1.744 | step time: 0.000
train | epoch   2 | Iter:   3826/ 13000 | global iter:   3826/ 13000 | loss: 2.0520 | ds_loss: 0.0000 | lr: 4.0165e-05 | scale: 32768.0000 | micro time: 1.782 | step time: 0.000
train | epoch   2 | Iter:   3827/ 13000 | global iter:   3827/ 13000 | loss: 1.9377 | ds_loss: 0.0000 | lr: 4.0160e-05 | scale: 32768.0000 | micro time: 1.782 | step time: 0.000
train | epoch   2 | Iter:   3828/ 13000 | global iter:   3828/ 13000 | loss: 1.6233 | ds_loss: 0.0000 | lr: 4.0156e-05 | scale: 32768.0000 | micro time: 1.860 | step time: 0.000
train | epoch   2 | Iter:   3829/ 13000 | global iter:   3829/ 13000 | loss: 1.9138 | ds_loss: 0.0000 | lr: 4.0151e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   2 | Iter:   3830/ 13000 | global iter:   3830/ 13000 | loss: 1.5909 | ds_loss: 0.0000 | lr: 4.0146e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3830/ 13000 | global iter:   3830/ 13000 | loss: 1.8943 | ds_loss: 0.0000 | lr: 4.0146e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 1.814
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3831/ 13000 | global iter:   3831/ 13000 | loss: 1.9574 | ds_loss: 0.0000 | lr: 4.0141e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   2 | Iter:   3832/ 13000 | global iter:   3832/ 13000 | loss: 1.5841 | ds_loss: 0.0000 | lr: 4.0136e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   2 | Iter:   3833/ 13000 | global iter:   3833/ 13000 | loss: 1.5063 | ds_loss: 0.0000 | lr: 4.0132e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 0.000
train | epoch   2 | Iter:   3834/ 13000 | global iter:   3834/ 13000 | loss: 2.2446 | ds_loss: 0.0000 | lr: 4.0127e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   2 | Iter:   3835/ 13000 | global iter:   3835/ 13000 | loss: 1.8971 | ds_loss: 0.0000 | lr: 4.0122e-05 | scale: 32768.0000 | micro time: 1.798 | step time: 0.000
train | epoch   2 | Iter:   3836/ 13000 | global iter:   3836/ 13000 | loss: 2.2550 | ds_loss: 0.0000 | lr: 4.0117e-05 | scale: 32768.0000 | micro time: 1.864 | step time: 0.000
train | epoch   2 | Iter:   3837/ 13000 | global iter:   3837/ 13000 | loss: 2.3604 | ds_loss: 0.0000 | lr: 4.0112e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   2 | Iter:   3838/ 13000 | global iter:   3838/ 13000 | loss: 1.6269 | ds_loss: 0.0000 | lr: 4.0108e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   2 | Iter:   3839/ 13000 | global iter:   3839/ 13000 | loss: 1.4985 | ds_loss: 0.0000 | lr: 4.0103e-05 | scale: 32768.0000 | micro time: 1.850 | step time: 0.000
train | epoch   2 | Iter:   3840/ 13000 | global iter:   3840/ 13000 | loss: 2.1101 | ds_loss: 0.0000 | lr: 4.0098e-05 | scale: 32768.0000 | micro time: 1.765 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3840/ 13000 | global iter:   3840/ 13000 | loss: 1.9040 | ds_loss: 0.0000 | lr: 4.0098e-05 | scale: 32768.0000 | micro time: 1.765 | step time: 1.827
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3841/ 13000 | global iter:   3841/ 13000 | loss: 2.0944 | ds_loss: 0.0000 | lr: 4.0093e-05 | scale: 32768.0000 | micro time: 1.834 | step time: 0.000
train | epoch   2 | Iter:   3842/ 13000 | global iter:   3842/ 13000 | loss: 2.1299 | ds_loss: 0.0000 | lr: 4.0088e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   2 | Iter:   3843/ 13000 | global iter:   3843/ 13000 | loss: 1.7512 | ds_loss: 0.0000 | lr: 4.0083e-05 | scale: 32768.0000 | micro time: 1.683 | step time: 0.000
train | epoch   2 | Iter:   3844/ 13000 | global iter:   3844/ 13000 | loss: 2.0543 | ds_loss: 0.0000 | lr: 4.0079e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   2 | Iter:   3845/ 13000 | global iter:   3845/ 13000 | loss: 1.4580 | ds_loss: 0.0000 | lr: 4.0074e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   2 | Iter:   3846/ 13000 | global iter:   3846/ 13000 | loss: 1.5420 | ds_loss: 0.0000 | lr: 4.0069e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   2 | Iter:   3847/ 13000 | global iter:   3847/ 13000 | loss: 1.4365 | ds_loss: 0.0000 | lr: 4.0064e-05 | scale: 32768.0000 | micro time: 1.899 | step time: 0.000
train | epoch   2 | Iter:   3848/ 13000 | global iter:   3848/ 13000 | loss: 1.3378 | ds_loss: 0.0000 | lr: 4.0059e-05 | scale: 32768.0000 | micro time: 1.760 | step time: 0.000
train | epoch   2 | Iter:   3849/ 13000 | global iter:   3849/ 13000 | loss: 1.8168 | ds_loss: 0.0000 | lr: 4.0055e-05 | scale: 32768.0000 | micro time: 1.737 | step time: 0.000
train | epoch   2 | Iter:   3850/ 13000 | global iter:   3850/ 13000 | loss: 1.5714 | ds_loss: 0.0000 | lr: 4.0050e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3850/ 13000 | global iter:   3850/ 13000 | loss: 1.7192 | ds_loss: 0.0000 | lr: 4.0050e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 1.802
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3851/ 13000 | global iter:   3851/ 13000 | loss: 2.1492 | ds_loss: 0.0000 | lr: 4.0045e-05 | scale: 32768.0000 | micro time: 1.801 | step time: 0.000
train | epoch   2 | Iter:   3852/ 13000 | global iter:   3852/ 13000 | loss: 1.9993 | ds_loss: 0.0000 | lr: 4.0040e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   2 | Iter:   3853/ 13000 | global iter:   3853/ 13000 | loss: 1.7601 | ds_loss: 0.0000 | lr: 4.0035e-05 | scale: 32768.0000 | micro time: 1.691 | step time: 0.000
train | epoch   2 | Iter:   3854/ 13000 | global iter:   3854/ 13000 | loss: 2.0685 | ds_loss: 0.0000 | lr: 4.0030e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   2 | Iter:   3855/ 13000 | global iter:   3855/ 13000 | loss: 2.1875 | ds_loss: 0.0000 | lr: 4.0026e-05 | scale: 32768.0000 | micro time: 1.735 | step time: 0.000
train | epoch   2 | Iter:   3856/ 13000 | global iter:   3856/ 13000 | loss: 2.1567 | ds_loss: 0.0000 | lr: 4.0021e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
train | epoch   2 | Iter:   3857/ 13000 | global iter:   3857/ 13000 | loss: 2.0317 | ds_loss: 0.0000 | lr: 4.0016e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   2 | Iter:   3858/ 13000 | global iter:   3858/ 13000 | loss: 1.5826 | ds_loss: 0.0000 | lr: 4.0011e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   2 | Iter:   3859/ 13000 | global iter:   3859/ 13000 | loss: 2.0582 | ds_loss: 0.0000 | lr: 4.0006e-05 | scale: 32768.0000 | micro time: 1.817 | step time: 0.000
train | epoch   2 | Iter:   3860/ 13000 | global iter:   3860/ 13000 | loss: 1.7137 | ds_loss: 0.0000 | lr: 4.0002e-05 | scale: 32768.0000 | micro time: 1.893 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3860/ 13000 | global iter:   3860/ 13000 | loss: 1.9707 | ds_loss: 0.0000 | lr: 4.0002e-05 | scale: 32768.0000 | micro time: 1.893 | step time: 1.798
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3861/ 13000 | global iter:   3861/ 13000 | loss: 1.1972 | ds_loss: 0.0000 | lr: 3.9997e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   2 | Iter:   3862/ 13000 | global iter:   3862/ 13000 | loss: 1.7216 | ds_loss: 0.0000 | lr: 3.9992e-05 | scale: 32768.0000 | micro time: 1.685 | step time: 0.000
train | epoch   2 | Iter:   3863/ 13000 | global iter:   3863/ 13000 | loss: 2.0845 | ds_loss: 0.0000 | lr: 3.9987e-05 | scale: 32768.0000 | micro time: 1.886 | step time: 0.000
train | epoch   2 | Iter:   3864/ 13000 | global iter:   3864/ 13000 | loss: 1.7100 | ds_loss: 0.0000 | lr: 3.9982e-05 | scale: 32768.0000 | micro time: 1.777 | step time: 0.000
train | epoch   2 | Iter:   3865/ 13000 | global iter:   3865/ 13000 | loss: 1.7850 | ds_loss: 0.0000 | lr: 3.9977e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   2 | Iter:   3866/ 13000 | global iter:   3866/ 13000 | loss: 1.1176 | ds_loss: 0.0000 | lr: 3.9973e-05 | scale: 32768.0000 | micro time: 1.737 | step time: 0.000
train | epoch   2 | Iter:   3867/ 13000 | global iter:   3867/ 13000 | loss: 1.3831 | ds_loss: 0.0000 | lr: 3.9968e-05 | scale: 32768.0000 | micro time: 1.841 | step time: 0.000
train | epoch   2 | Iter:   3868/ 13000 | global iter:   3868/ 13000 | loss: 1.7085 | ds_loss: 0.0000 | lr: 3.9963e-05 | scale: 32768.0000 | micro time: 1.841 | step time: 0.000
train | epoch   2 | Iter:   3869/ 13000 | global iter:   3869/ 13000 | loss: 1.5227 | ds_loss: 0.0000 | lr: 3.9958e-05 | scale: 32768.0000 | micro time: 1.713 | step time: 0.000
train | epoch   2 | Iter:   3870/ 13000 | global iter:   3870/ 13000 | loss: 2.0068 | ds_loss: 0.0000 | lr: 3.9953e-05 | scale: 32768.0000 | micro time: 1.830 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3870/ 13000 | global iter:   3870/ 13000 | loss: 1.6237 | ds_loss: 0.0000 | lr: 3.9953e-05 | scale: 32768.0000 | micro time: 1.830 | step time: 1.796
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3871/ 13000 | global iter:   3871/ 13000 | loss: 2.0066 | ds_loss: 0.0000 | lr: 3.9948e-05 | scale: 32768.0000 | micro time: 1.758 | step time: 0.000
train | epoch   2 | Iter:   3872/ 13000 | global iter:   3872/ 13000 | loss: 2.0597 | ds_loss: 0.0000 | lr: 3.9944e-05 | scale: 32768.0000 | micro time: 1.790 | step time: 0.000
train | epoch   2 | Iter:   3873/ 13000 | global iter:   3873/ 13000 | loss: 2.1299 | ds_loss: 0.0000 | lr: 3.9939e-05 | scale: 32768.0000 | micro time: 1.722 | step time: 0.000
train | epoch   2 | Iter:   3874/ 13000 | global iter:   3874/ 13000 | loss: 1.8942 | ds_loss: 0.0000 | lr: 3.9934e-05 | scale: 32768.0000 | micro time: 1.881 | step time: 0.000
train | epoch   2 | Iter:   3875/ 13000 | global iter:   3875/ 13000 | loss: 1.7423 | ds_loss: 0.0000 | lr: 3.9929e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   2 | Iter:   3876/ 13000 | global iter:   3876/ 13000 | loss: 1.7721 | ds_loss: 0.0000 | lr: 3.9924e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   2 | Iter:   3877/ 13000 | global iter:   3877/ 13000 | loss: 1.7504 | ds_loss: 0.0000 | lr: 3.9919e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   2 | Iter:   3878/ 13000 | global iter:   3878/ 13000 | loss: 2.0173 | ds_loss: 0.0000 | lr: 3.9915e-05 | scale: 32768.0000 | micro time: 1.844 | step time: 0.000
train | epoch   2 | Iter:   3879/ 13000 | global iter:   3879/ 13000 | loss: 2.0573 | ds_loss: 0.0000 | lr: 3.9910e-05 | scale: 32768.0000 | micro time: 1.778 | step time: 0.000
train | epoch   2 | Iter:   3880/ 13000 | global iter:   3880/ 13000 | loss: 1.9290 | ds_loss: 0.0000 | lr: 3.9905e-05 | scale: 32768.0000 | micro time: 1.754 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3880/ 13000 | global iter:   3880/ 13000 | loss: 1.9359 | ds_loss: 0.0000 | lr: 3.9905e-05 | scale: 32768.0000 | micro time: 1.754 | step time: 1.800
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3881/ 13000 | global iter:   3881/ 13000 | loss: 1.9582 | ds_loss: 0.0000 | lr: 3.9900e-05 | scale: 32768.0000 | micro time: 1.729 | step time: 0.000
train | epoch   2 | Iter:   3882/ 13000 | global iter:   3882/ 13000 | loss: 1.6572 | ds_loss: 0.0000 | lr: 3.9895e-05 | scale: 32768.0000 | micro time: 1.703 | step time: 0.000
train | epoch   2 | Iter:   3883/ 13000 | global iter:   3883/ 13000 | loss: 1.8880 | ds_loss: 0.0000 | lr: 3.9890e-05 | scale: 32768.0000 | micro time: 1.723 | step time: 0.000
train | epoch   2 | Iter:   3884/ 13000 | global iter:   3884/ 13000 | loss: 1.2984 | ds_loss: 0.0000 | lr: 3.9885e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   2 | Iter:   3885/ 13000 | global iter:   3885/ 13000 | loss: 1.3175 | ds_loss: 0.0000 | lr: 3.9881e-05 | scale: 32768.0000 | micro time: 1.792 | step time: 0.000
train | epoch   2 | Iter:   3886/ 13000 | global iter:   3886/ 13000 | loss: 1.6704 | ds_loss: 0.0000 | lr: 3.9876e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   2 | Iter:   3887/ 13000 | global iter:   3887/ 13000 | loss: 1.3266 | ds_loss: 0.0000 | lr: 3.9871e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   2 | Iter:   3888/ 13000 | global iter:   3888/ 13000 | loss: 1.9208 | ds_loss: 0.0000 | lr: 3.9866e-05 | scale: 32768.0000 | micro time: 1.772 | step time: 0.000
train | epoch   2 | Iter:   3889/ 13000 | global iter:   3889/ 13000 | loss: 1.6123 | ds_loss: 0.0000 | lr: 3.9861e-05 | scale: 32768.0000 | micro time: 1.862 | step time: 0.000
train | epoch   2 | Iter:   3890/ 13000 | global iter:   3890/ 13000 | loss: 1.8045 | ds_loss: 0.0000 | lr: 3.9856e-05 | scale: 32768.0000 | micro time: 1.838 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3890/ 13000 | global iter:   3890/ 13000 | loss: 1.6454 | ds_loss: 0.0000 | lr: 3.9856e-05 | scale: 32768.0000 | micro time: 1.838 | step time: 1.782
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3891/ 13000 | global iter:   3891/ 13000 | loss: 1.4402 | ds_loss: 0.0000 | lr: 3.9851e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   2 | Iter:   3892/ 13000 | global iter:   3892/ 13000 | loss: 2.0748 | ds_loss: 0.0000 | lr: 3.9847e-05 | scale: 32768.0000 | micro time: 1.724 | step time: 0.000
train | epoch   2 | Iter:   3893/ 13000 | global iter:   3893/ 13000 | loss: 1.9989 | ds_loss: 0.0000 | lr: 3.9842e-05 | scale: 32768.0000 | micro time: 1.844 | step time: 0.000
train | epoch   2 | Iter:   3894/ 13000 | global iter:   3894/ 13000 | loss: 2.0723 | ds_loss: 0.0000 | lr: 3.9837e-05 | scale: 32768.0000 | micro time: 1.719 | step time: 0.000
train | epoch   2 | Iter:   3895/ 13000 | global iter:   3895/ 13000 | loss: 2.1857 | ds_loss: 0.0000 | lr: 3.9832e-05 | scale: 32768.0000 | micro time: 1.795 | step time: 0.000
train | epoch   2 | Iter:   3896/ 13000 | global iter:   3896/ 13000 | loss: 1.8278 | ds_loss: 0.0000 | lr: 3.9827e-05 | scale: 32768.0000 | micro time: 1.697 | step time: 0.000
train | epoch   2 | Iter:   3897/ 13000 | global iter:   3897/ 13000 | loss: 1.6831 | ds_loss: 0.0000 | lr: 3.9822e-05 | scale: 32768.0000 | micro time: 1.797 | step time: 0.000
train | epoch   2 | Iter:   3898/ 13000 | global iter:   3898/ 13000 | loss: 2.2055 | ds_loss: 0.0000 | lr: 3.9817e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   2 | Iter:   3899/ 13000 | global iter:   3899/ 13000 | loss: 2.0852 | ds_loss: 0.0000 | lr: 3.9813e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   2 | Iter:   3900/ 13000 | global iter:   3900/ 13000 | loss: 1.9540 | ds_loss: 0.0000 | lr: 3.9808e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
****************************************************************************************************
train | epoch   2 | Iter:   3900/ 13000 | global iter:   3900/ 13000 | loss: 1.9528 | ds_loss: 0.0000 | lr: 3.9808e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 1.793
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   2 | Iter:   3901/ 13000 | global iter:   3901/ 13000 | loss: 1.6249 | ds_loss: 0.0000 | lr: 3.9803e-05 | scale: 32768.0000 | micro time: 1.844 | step time: 0.000
train | epoch   2 | Iter:   3902/ 13000 | global iter:   3902/ 13000 | loss: 2.0159 | ds_loss: 0.0000 | lr: 3.9798e-05 | scale: 32768.0000 | micro time: 1.769 | step time: 0.000
train | epoch   2 | Iter:   3903/ 13000 | global iter:   3903/ 13000 | loss: 1.1600 | ds_loss: 0.0000 | lr: 3.9793e-05 | scale: 32768.0000 | micro time: 1.730 | step time: 0.000
Sat Apr 19 13:06:02 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   40C    P2            103W /  230W |   21681MiB /  23028MiB |     94%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   44C    P2            124W /  230W |   22429MiB /  23028MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   42C    P2            115W /  230W |   21691MiB /  23028MiB |     74%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   40C    P2            124W /  230W |   21059MiB /  23028MiB |     91%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3      21674MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3      22422MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3      21684MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3      21052MiB |
+-----------------------------------------------------------------------------------------+

Sat Apr 19 13:06:02 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   40C    P2            103W /  230W |   21681MiB /  23028MiB |     94%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   43C    P2            115W /  230W |   22429MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   41C    P2            107W /  230W |   21691MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   42C    P2            125W /  230W |   21059MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3      21674MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3      22422MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3      21684MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3      21052MiB |
+-----------------------------------------------------------------------------------------+
Sat Apr 19 13:06:02 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   40C    P2            103W /  230W |   21681MiB /  23028MiB |     94%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   43C    P2            122W /  230W |   22429MiB /  23028MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   42C    P2            115W /  230W |   21691MiB /  23028MiB |     74%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   40C    P2            114W /  230W |   21059MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3      21674MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3      22422MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3      21684MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3      21052MiB |
+-----------------------------------------------------------------------------------------+


Sat Apr 19 13:06:02 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   40C    P2            103W /  230W |   21681MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   43C    P2            115W /  230W |   22429MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   41C    P2            107W /  230W |   21691MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   42C    P2            125W /  230W |   21059MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3      21674MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3      22422MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3      21684MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3      21052MiB |
+-----------------------------------------------------------------------------------------+

train | epoch   3 | Iter:   3904/ 13000 | global iter:   3904/ 13000 | loss: 2.1436 | ds_loss: 0.0000 | lr: 3.9788e-05 | scale: 32768.0000 | micro time: 2.320 | step time: 0.000
train | epoch   3 | Iter:   3905/ 13000 | global iter:   3905/ 13000 | loss: 1.4459 | ds_loss: 0.0000 | lr: 3.9783e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   3 | Iter:   3906/ 13000 | global iter:   3906/ 13000 | loss: 1.1636 | ds_loss: 0.0000 | lr: 3.9779e-05 | scale: 32768.0000 | micro time: 1.854 | step time: 0.000
train | epoch   3 | Iter:   3907/ 13000 | global iter:   3907/ 13000 | loss: 1.9938 | ds_loss: 0.0000 | lr: 3.9774e-05 | scale: 32768.0000 | micro time: 1.796 | step time: 0.000
train | epoch   3 | Iter:   3908/ 13000 | global iter:   3908/ 13000 | loss: 1.5431 | ds_loss: 0.0000 | lr: 3.9769e-05 | scale: 32768.0000 | micro time: 1.800 | step time: 0.000
train | epoch   3 | Iter:   3909/ 13000 | global iter:   3909/ 13000 | loss: 1.6453 | ds_loss: 0.0000 | lr: 3.9764e-05 | scale: 32768.0000 | micro time: 1.838 | step time: 0.000
train | epoch   3 | Iter:   3910/ 13000 | global iter:   3910/ 13000 | loss: 1.3237 | ds_loss: 0.0000 | lr: 3.9759e-05 | scale: 32768.0000 | micro time: 1.754 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   3910/ 13000 | global iter:   3910/ 13000 | loss: 1.6060 | ds_loss: 0.0000 | lr: 3.9759e-05 | scale: 32768.0000 | micro time: 1.754 | step time: 1.856
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   3911/ 13000 | global iter:   3911/ 13000 | loss: 1.6029 | ds_loss: 0.0000 | lr: 3.9754e-05 | scale: 32768.0000 | micro time: 1.769 | step time: 0.000
train | epoch   3 | Iter:   3912/ 13000 | global iter:   3912/ 13000 | loss: 1.5059 | ds_loss: 0.0000 | lr: 3.9749e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   3 | Iter:   3913/ 13000 | global iter:   3913/ 13000 | loss: 1.5746 | ds_loss: 0.0000 | lr: 3.9744e-05 | scale: 32768.0000 | micro time: 1.850 | step time: 0.000
train | epoch   3 | Iter:   3914/ 13000 | global iter:   3914/ 13000 | loss: 1.8246 | ds_loss: 0.0000 | lr: 3.9740e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   3 | Iter:   3915/ 13000 | global iter:   3915/ 13000 | loss: 1.2966 | ds_loss: 0.0000 | lr: 3.9735e-05 | scale: 32768.0000 | micro time: 1.830 | step time: 0.000
train | epoch   3 | Iter:   3916/ 13000 | global iter:   3916/ 13000 | loss: 1.9003 | ds_loss: 0.0000 | lr: 3.9730e-05 | scale: 32768.0000 | micro time: 1.728 | step time: 0.000
train | epoch   3 | Iter:   3917/ 13000 | global iter:   3917/ 13000 | loss: 1.8742 | ds_loss: 0.0000 | lr: 3.9725e-05 | scale: 32768.0000 | micro time: 1.826 | step time: 0.000
train | epoch   3 | Iter:   3918/ 13000 | global iter:   3918/ 13000 | loss: 1.6976 | ds_loss: 0.0000 | lr: 3.9720e-05 | scale: 32768.0000 | micro time: 1.868 | step time: 0.000
train | epoch   3 | Iter:   3919/ 13000 | global iter:   3919/ 13000 | loss: 1.4351 | ds_loss: 0.0000 | lr: 3.9715e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   3 | Iter:   3920/ 13000 | global iter:   3920/ 13000 | loss: 1.7022 | ds_loss: 0.0000 | lr: 3.9710e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   3920/ 13000 | global iter:   3920/ 13000 | loss: 1.6414 | ds_loss: 0.0000 | lr: 3.9710e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 1.818
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   3921/ 13000 | global iter:   3921/ 13000 | loss: 1.5119 | ds_loss: 0.0000 | lr: 3.9705e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   3 | Iter:   3922/ 13000 | global iter:   3922/ 13000 | loss: 1.3259 | ds_loss: 0.0000 | lr: 3.9701e-05 | scale: 32768.0000 | micro time: 1.853 | step time: 0.000
train | epoch   3 | Iter:   3923/ 13000 | global iter:   3923/ 13000 | loss: 1.4082 | ds_loss: 0.0000 | lr: 3.9696e-05 | scale: 32768.0000 | micro time: 1.879 | step time: 0.000
train | epoch   3 | Iter:   3924/ 13000 | global iter:   3924/ 13000 | loss: 1.3295 | ds_loss: 0.0000 | lr: 3.9691e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   3 | Iter:   3925/ 13000 | global iter:   3925/ 13000 | loss: 0.9694 | ds_loss: 0.0000 | lr: 3.9686e-05 | scale: 32768.0000 | micro time: 1.794 | step time: 0.000
train | epoch   3 | Iter:   3926/ 13000 | global iter:   3926/ 13000 | loss: 1.0506 | ds_loss: 0.0000 | lr: 3.9681e-05 | scale: 32768.0000 | micro time: 1.900 | step time: 0.000
train | epoch   3 | Iter:   3927/ 13000 | global iter:   3927/ 13000 | loss: 1.0222 | ds_loss: 0.0000 | lr: 3.9676e-05 | scale: 32768.0000 | micro time: 1.829 | step time: 0.000
train | epoch   3 | Iter:   3928/ 13000 | global iter:   3928/ 13000 | loss: 1.3384 | ds_loss: 0.0000 | lr: 3.9671e-05 | scale: 32768.0000 | micro time: 1.753 | step time: 0.000
train | epoch   3 | Iter:   3929/ 13000 | global iter:   3929/ 13000 | loss: 1.4763 | ds_loss: 0.0000 | lr: 3.9666e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   3 | Iter:   3930/ 13000 | global iter:   3930/ 13000 | loss: 2.0730 | ds_loss: 0.0000 | lr: 3.9662e-05 | scale: 32768.0000 | micro time: 1.743 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   3930/ 13000 | global iter:   3930/ 13000 | loss: 1.3505 | ds_loss: 0.0000 | lr: 3.9662e-05 | scale: 32768.0000 | micro time: 1.743 | step time: 1.822
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   3931/ 13000 | global iter:   3931/ 13000 | loss: 1.5411 | ds_loss: 0.0000 | lr: 3.9657e-05 | scale: 32768.0000 | micro time: 1.798 | step time: 0.000
train | epoch   3 | Iter:   3932/ 13000 | global iter:   3932/ 13000 | loss: 1.6080 | ds_loss: 0.0000 | lr: 3.9652e-05 | scale: 32768.0000 | micro time: 1.856 | step time: 0.000
train | epoch   3 | Iter:   3933/ 13000 | global iter:   3933/ 13000 | loss: 2.0547 | ds_loss: 0.0000 | lr: 3.9647e-05 | scale: 32768.0000 | micro time: 1.672 | step time: 0.000
train | epoch   3 | Iter:   3934/ 13000 | global iter:   3934/ 13000 | loss: 1.6930 | ds_loss: 0.0000 | lr: 3.9642e-05 | scale: 32768.0000 | micro time: 1.817 | step time: 0.000
train | epoch   3 | Iter:   3935/ 13000 | global iter:   3935/ 13000 | loss: 1.8744 | ds_loss: 0.0000 | lr: 3.9637e-05 | scale: 32768.0000 | micro time: 1.861 | step time: 0.000
train | epoch   3 | Iter:   3936/ 13000 | global iter:   3936/ 13000 | loss: 1.5180 | ds_loss: 0.0000 | lr: 3.9632e-05 | scale: 32768.0000 | micro time: 1.709 | step time: 0.000
train | epoch   3 | Iter:   3937/ 13000 | global iter:   3937/ 13000 | loss: 1.4633 | ds_loss: 0.0000 | lr: 3.9627e-05 | scale: 32768.0000 | micro time: 1.903 | step time: 0.000
train | epoch   3 | Iter:   3938/ 13000 | global iter:   3938/ 13000 | loss: 1.8635 | ds_loss: 0.0000 | lr: 3.9622e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   3 | Iter:   3939/ 13000 | global iter:   3939/ 13000 | loss: 1.4158 | ds_loss: 0.0000 | lr: 3.9618e-05 | scale: 32768.0000 | micro time: 1.879 | step time: 0.000
train | epoch   3 | Iter:   3940/ 13000 | global iter:   3940/ 13000 | loss: 1.5116 | ds_loss: 0.0000 | lr: 3.9613e-05 | scale: 32768.0000 | micro time: 1.719 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   3940/ 13000 | global iter:   3940/ 13000 | loss: 1.6544 | ds_loss: 0.0000 | lr: 3.9613e-05 | scale: 32768.0000 | micro time: 1.719 | step time: 1.801
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   3941/ 13000 | global iter:   3941/ 13000 | loss: 1.5570 | ds_loss: 0.0000 | lr: 3.9608e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   3 | Iter:   3942/ 13000 | global iter:   3942/ 13000 | loss: 1.6834 | ds_loss: 0.0000 | lr: 3.9603e-05 | scale: 32768.0000 | micro time: 1.726 | step time: 0.000
train | epoch   3 | Iter:   3943/ 13000 | global iter:   3943/ 13000 | loss: 1.8955 | ds_loss: 0.0000 | lr: 3.9598e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   3 | Iter:   3944/ 13000 | global iter:   3944/ 13000 | loss: 1.5021 | ds_loss: 0.0000 | lr: 3.9593e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 0.000
train | epoch   3 | Iter:   3945/ 13000 | global iter:   3945/ 13000 | loss: 1.6122 | ds_loss: 0.0000 | lr: 3.9588e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   3 | Iter:   3946/ 13000 | global iter:   3946/ 13000 | loss: 1.9620 | ds_loss: 0.0000 | lr: 3.9583e-05 | scale: 32768.0000 | micro time: 1.740 | step time: 0.000
train | epoch   3 | Iter:   3947/ 13000 | global iter:   3947/ 13000 | loss: 1.7975 | ds_loss: 0.0000 | lr: 3.9578e-05 | scale: 32768.0000 | micro time: 1.862 | step time: 0.000
train | epoch   3 | Iter:   3948/ 13000 | global iter:   3948/ 13000 | loss: 1.2452 | ds_loss: 0.0000 | lr: 3.9573e-05 | scale: 32768.0000 | micro time: 1.830 | step time: 0.000
train | epoch   3 | Iter:   3949/ 13000 | global iter:   3949/ 13000 | loss: 1.3360 | ds_loss: 0.0000 | lr: 3.9569e-05 | scale: 32768.0000 | micro time: 1.764 | step time: 0.000
train | epoch   3 | Iter:   3950/ 13000 | global iter:   3950/ 13000 | loss: 1.4612 | ds_loss: 0.0000 | lr: 3.9564e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   3950/ 13000 | global iter:   3950/ 13000 | loss: 1.6052 | ds_loss: 0.0000 | lr: 3.9564e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 1.811
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   3951/ 13000 | global iter:   3951/ 13000 | loss: 1.6662 | ds_loss: 0.0000 | lr: 3.9559e-05 | scale: 32768.0000 | micro time: 1.841 | step time: 0.000
train | epoch   3 | Iter:   3952/ 13000 | global iter:   3952/ 13000 | loss: 1.3458 | ds_loss: 0.0000 | lr: 3.9554e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   3 | Iter:   3953/ 13000 | global iter:   3953/ 13000 | loss: 1.2238 | ds_loss: 0.0000 | lr: 3.9549e-05 | scale: 32768.0000 | micro time: 1.679 | step time: 0.000
train | epoch   3 | Iter:   3954/ 13000 | global iter:   3954/ 13000 | loss: 1.4624 | ds_loss: 0.0000 | lr: 3.9544e-05 | scale: 32768.0000 | micro time: 1.808 | step time: 0.000
train | epoch   3 | Iter:   3955/ 13000 | global iter:   3955/ 13000 | loss: 0.7402 | ds_loss: 0.0000 | lr: 3.9539e-05 | scale: 32768.0000 | micro time: 1.820 | step time: 0.000
train | epoch   3 | Iter:   3956/ 13000 | global iter:   3956/ 13000 | loss: 1.9793 | ds_loss: 0.0000 | lr: 3.9534e-05 | scale: 32768.0000 | micro time: 1.753 | step time: 0.000
train | epoch   3 | Iter:   3957/ 13000 | global iter:   3957/ 13000 | loss: 1.2066 | ds_loss: 0.0000 | lr: 3.9529e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   3 | Iter:   3958/ 13000 | global iter:   3958/ 13000 | loss: 1.0344 | ds_loss: 0.0000 | lr: 3.9524e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   3 | Iter:   3959/ 13000 | global iter:   3959/ 13000 | loss: 0.8432 | ds_loss: 0.0000 | lr: 3.9519e-05 | scale: 32768.0000 | micro time: 1.905 | step time: 0.000
train | epoch   3 | Iter:   3960/ 13000 | global iter:   3960/ 13000 | loss: 1.6738 | ds_loss: 0.0000 | lr: 3.9515e-05 | scale: 32768.0000 | micro time: 1.805 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   3960/ 13000 | global iter:   3960/ 13000 | loss: 1.3176 | ds_loss: 0.0000 | lr: 3.9515e-05 | scale: 32768.0000 | micro time: 1.805 | step time: 1.804
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   3961/ 13000 | global iter:   3961/ 13000 | loss: 1.8634 | ds_loss: 0.0000 | lr: 3.9510e-05 | scale: 32768.0000 | micro time: 1.850 | step time: 0.000
train | epoch   3 | Iter:   3962/ 13000 | global iter:   3962/ 13000 | loss: 1.5484 | ds_loss: 0.0000 | lr: 3.9505e-05 | scale: 32768.0000 | micro time: 1.742 | step time: 0.000
train | epoch   3 | Iter:   3963/ 13000 | global iter:   3963/ 13000 | loss: 1.2827 | ds_loss: 0.0000 | lr: 3.9500e-05 | scale: 32768.0000 | micro time: 1.732 | step time: 0.000
train | epoch   3 | Iter:   3964/ 13000 | global iter:   3964/ 13000 | loss: 1.7275 | ds_loss: 0.0000 | lr: 3.9495e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   3 | Iter:   3965/ 13000 | global iter:   3965/ 13000 | loss: 1.2683 | ds_loss: 0.0000 | lr: 3.9490e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
train | epoch   3 | Iter:   3966/ 13000 | global iter:   3966/ 13000 | loss: 1.4126 | ds_loss: 0.0000 | lr: 3.9485e-05 | scale: 32768.0000 | micro time: 1.716 | step time: 0.000
train | epoch   3 | Iter:   3967/ 13000 | global iter:   3967/ 13000 | loss: 1.1244 | ds_loss: 0.0000 | lr: 3.9480e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   3 | Iter:   3968/ 13000 | global iter:   3968/ 13000 | loss: 1.4755 | ds_loss: 0.0000 | lr: 3.9475e-05 | scale: 32768.0000 | micro time: 1.866 | step time: 0.000
train | epoch   3 | Iter:   3969/ 13000 | global iter:   3969/ 13000 | loss: 1.7215 | ds_loss: 0.0000 | lr: 3.9470e-05 | scale: 32768.0000 | micro time: 1.833 | step time: 0.000
train | epoch   3 | Iter:   3970/ 13000 | global iter:   3970/ 13000 | loss: 1.5499 | ds_loss: 0.0000 | lr: 3.9465e-05 | scale: 32768.0000 | micro time: 1.769 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   3970/ 13000 | global iter:   3970/ 13000 | loss: 1.4974 | ds_loss: 0.0000 | lr: 3.9465e-05 | scale: 32768.0000 | micro time: 1.769 | step time: 1.791
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   3971/ 13000 | global iter:   3971/ 13000 | loss: 1.8506 | ds_loss: 0.0000 | lr: 3.9460e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   3 | Iter:   3972/ 13000 | global iter:   3972/ 13000 | loss: 1.4746 | ds_loss: 0.0000 | lr: 3.9456e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   3 | Iter:   3973/ 13000 | global iter:   3973/ 13000 | loss: 1.1697 | ds_loss: 0.0000 | lr: 3.9451e-05 | scale: 32768.0000 | micro time: 1.752 | step time: 0.000
train | epoch   3 | Iter:   3974/ 13000 | global iter:   3974/ 13000 | loss: 1.2790 | ds_loss: 0.0000 | lr: 3.9446e-05 | scale: 32768.0000 | micro time: 1.887 | step time: 0.000
train | epoch   3 | Iter:   3975/ 13000 | global iter:   3975/ 13000 | loss: 1.7121 | ds_loss: 0.0000 | lr: 3.9441e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   3 | Iter:   3976/ 13000 | global iter:   3976/ 13000 | loss: 1.0492 | ds_loss: 0.0000 | lr: 3.9436e-05 | scale: 32768.0000 | micro time: 1.683 | step time: 0.000
train | epoch   3 | Iter:   3977/ 13000 | global iter:   3977/ 13000 | loss: 1.5763 | ds_loss: 0.0000 | lr: 3.9431e-05 | scale: 32768.0000 | micro time: 1.857 | step time: 0.000
train | epoch   3 | Iter:   3978/ 13000 | global iter:   3978/ 13000 | loss: 1.9982 | ds_loss: 0.0000 | lr: 3.9426e-05 | scale: 32768.0000 | micro time: 1.777 | step time: 0.000
train | epoch   3 | Iter:   3979/ 13000 | global iter:   3979/ 13000 | loss: 1.5524 | ds_loss: 0.0000 | lr: 3.9421e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   3 | Iter:   3980/ 13000 | global iter:   3980/ 13000 | loss: 1.4106 | ds_loss: 0.0000 | lr: 3.9416e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   3980/ 13000 | global iter:   3980/ 13000 | loss: 1.5073 | ds_loss: 0.0000 | lr: 3.9416e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 1.803
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   3981/ 13000 | global iter:   3981/ 13000 | loss: 0.8907 | ds_loss: 0.0000 | lr: 3.9411e-05 | scale: 32768.0000 | micro time: 1.794 | step time: 0.000
train | epoch   3 | Iter:   3982/ 13000 | global iter:   3982/ 13000 | loss: 1.2997 | ds_loss: 0.0000 | lr: 3.9406e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
train | epoch   3 | Iter:   3983/ 13000 | global iter:   3983/ 13000 | loss: 1.6856 | ds_loss: 0.0000 | lr: 3.9401e-05 | scale: 32768.0000 | micro time: 1.723 | step time: 0.000
train | epoch   3 | Iter:   3984/ 13000 | global iter:   3984/ 13000 | loss: 0.8081 | ds_loss: 0.0000 | lr: 3.9396e-05 | scale: 32768.0000 | micro time: 1.732 | step time: 0.000
train | epoch   3 | Iter:   3985/ 13000 | global iter:   3985/ 13000 | loss: 1.5790 | ds_loss: 0.0000 | lr: 3.9391e-05 | scale: 32768.0000 | micro time: 1.782 | step time: 0.000
train | epoch   3 | Iter:   3986/ 13000 | global iter:   3986/ 13000 | loss: 1.9914 | ds_loss: 0.0000 | lr: 3.9387e-05 | scale: 32768.0000 | micro time: 1.721 | step time: 0.000
train | epoch   3 | Iter:   3987/ 13000 | global iter:   3987/ 13000 | loss: 1.7077 | ds_loss: 0.0000 | lr: 3.9382e-05 | scale: 32768.0000 | micro time: 1.861 | step time: 0.000
train | epoch   3 | Iter:   3988/ 13000 | global iter:   3988/ 13000 | loss: 1.4042 | ds_loss: 0.0000 | lr: 3.9377e-05 | scale: 32768.0000 | micro time: 1.846 | step time: 0.000
train | epoch   3 | Iter:   3989/ 13000 | global iter:   3989/ 13000 | loss: 1.7659 | ds_loss: 0.0000 | lr: 3.9372e-05 | scale: 32768.0000 | micro time: 1.788 | step time: 0.000
train | epoch   3 | Iter:   3990/ 13000 | global iter:   3990/ 13000 | loss: 1.9001 | ds_loss: 0.0000 | lr: 3.9367e-05 | scale: 32768.0000 | micro time: 1.841 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   3990/ 13000 | global iter:   3990/ 13000 | loss: 1.5032 | ds_loss: 0.0000 | lr: 3.9367e-05 | scale: 32768.0000 | micro time: 1.841 | step time: 1.785
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   3991/ 13000 | global iter:   3991/ 13000 | loss: 2.1572 | ds_loss: 0.0000 | lr: 3.9362e-05 | scale: 32768.0000 | micro time: 1.785 | step time: 0.000
train | epoch   3 | Iter:   3992/ 13000 | global iter:   3992/ 13000 | loss: 1.4429 | ds_loss: 0.0000 | lr: 3.9357e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   3 | Iter:   3993/ 13000 | global iter:   3993/ 13000 | loss: 1.7311 | ds_loss: 0.0000 | lr: 3.9352e-05 | scale: 32768.0000 | micro time: 1.903 | step time: 0.000
train | epoch   3 | Iter:   3994/ 13000 | global iter:   3994/ 13000 | loss: 0.8582 | ds_loss: 0.0000 | lr: 3.9347e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   3 | Iter:   3995/ 13000 | global iter:   3995/ 13000 | loss: 1.4552 | ds_loss: 0.0000 | lr: 3.9342e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 0.000
train | epoch   3 | Iter:   3996/ 13000 | global iter:   3996/ 13000 | loss: 1.8727 | ds_loss: 0.0000 | lr: 3.9337e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   3 | Iter:   3997/ 13000 | global iter:   3997/ 13000 | loss: 1.8815 | ds_loss: 0.0000 | lr: 3.9332e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   3 | Iter:   3998/ 13000 | global iter:   3998/ 13000 | loss: 1.2549 | ds_loss: 0.0000 | lr: 3.9327e-05 | scale: 32768.0000 | micro time: 1.887 | step time: 0.000
train | epoch   3 | Iter:   3999/ 13000 | global iter:   3999/ 13000 | loss: 1.4352 | ds_loss: 0.0000 | lr: 3.9322e-05 | scale: 32768.0000 | micro time: 1.876 | step time: 0.000
train | epoch   3 | Iter:   4000/ 13000 | global iter:   4000/ 13000 | loss: 1.3092 | ds_loss: 0.0000 | lr: 3.9317e-05 | scale: 32768.0000 | micro time: 1.899 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4000/ 13000 | global iter:   4000/ 13000 | loss: 1.5398 | ds_loss: 0.0000 | lr: 3.9317e-05 | scale: 32768.0000 | micro time: 1.899 | step time: 1.850
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
Model save to ./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1/4000
dp size 4
0/63
1/63
2/63
3/63
4/63
5/63
6/63
7/63
8/63
9/63
10/63
11/63
12/63
13/63
14/63
15/63
Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]Evaluating:   2%|         | 1/63 [00:14<15:10, 14.68s/it]Evaluating:   3%|         | 2/63 [00:30<15:42, 15.46s/it]Evaluating:   5%|         | 3/63 [00:47<16:12, 16.21s/it]Evaluating:   6%|         | 4/63 [00:57<13:22, 13.60s/it]Evaluating:   8%|         | 5/63 [01:13<14:09, 14.65s/it]Evaluating:  10%|         | 6/63 [01:20<11:14, 11.83s/it]Evaluating:  11%|         | 7/63 [01:32<11:08, 11.94s/it]Evaluating:  13%|        | 8/63 [01:49<12:22, 13.49s/it]Evaluating:  14%|        | 9/63 [02:04<12:35, 14.00s/it]Evaluating:  16%|        | 10/63 [02:21<13:19, 15.08s/it]Evaluating:  17%|        | 11/63 [02:38<13:24, 15.47s/it]Evaluating:  19%|        | 12/63 [02:54<13:29, 15.86s/it]Evaluating:  21%|        | 13/63 [03:11<13:21, 16.03s/it]Evaluating:  22%|       | 14/63 [03:26<12:53, 15.78s/it]Evaluating:  24%|       | 15/63 [03:43<12:59, 16.24s/it]Evaluating:  25%|     16/63
17/63
18/63
19/63
20/63
21/63
22/63
23/63
24/63
25/63
26/63
27/63
28/63
29/63
30/63
  | 16/63 [04:00<12:45, 16.29s/it]Evaluating:  27%|       | 17/63 [04:15<12:19, 16.08s/it]Evaluating:  29%|       | 18/63 [04:33<12:26, 16.59s/it]Evaluating:  30%|       | 19/63 [04:47<11:29, 15.68s/it]Evaluating:  32%|      | 20/63 [05:04<11:32, 16.10s/it]Evaluating:  33%|      | 21/63 [05:17<10:34, 15.12s/it]Evaluating:  35%|      | 22/63 [05:29<09:48, 14.35s/it]Evaluating:  37%|      | 23/63 [05:42<09:13, 13.83s/it]Evaluating:  38%|      | 24/63 [05:58<09:28, 14.59s/it]Evaluating:  40%|      | 25/63 [06:15<09:37, 15.21s/it]Evaluating:  41%|     | 26/63 [06:30<09:18, 15.10s/it]Evaluating:  43%|     | 27/63 [06:42<08:32, 14.24s/it]Evaluating:  44%|     | 28/63 [06:57<08:29, 14.57s/it]Evaluating:  46%|     | 29/63 [07:08<07:37, 13.45s/it]Evaluating:  48%|     | 30/63 [07:23<07:38, 13.88s/it]Evaluating:  49%| 31/63
32/63
33/63
34/63
35/63
36/63
37/63
38/63
39/63
40/63
41/63
42/63
43/63
44/63
    | 31/63 [07:38<07:32, 14.14s/it]Evaluating:  51%|     | 32/63 [07:46<06:20, 12.27s/it]Evaluating:  52%|    | 33/63 [08:00<06:30, 13.01s/it]Evaluating:  54%|    | 34/63 [08:15<06:27, 13.37s/it]Evaluating:  56%|    | 35/63 [08:28<06:12, 13.31s/it]Evaluating:  57%|    | 36/63 [08:43<06:15, 13.91s/it]Evaluating:  59%|    | 37/63 [09:00<06:28, 14.96s/it]Evaluating:  60%|    | 38/63 [09:18<06:29, 15.60s/it]Evaluating:  62%|   | 39/63 [09:36<06:33, 16.40s/it]Evaluating:  63%|   | 40/63 [09:54<06:27, 16.85s/it]Evaluating:  65%|   | 41/63 [10:10<06:04, 16.55s/it]Evaluating:  67%|   | 42/63 [10:25<05:38, 16.14s/it]Evaluating:  68%|   | 43/63 [10:41<05:25, 16.25s/it]Evaluating:  70%|   | 44/63 [10:57<05:03, 15.98s/it]Evaluating:  71%|45/63
46/63
47/63
Distributed index stop interation. Idx: 778 Total_length: 777
Distributed index stop interation. Idx: 777 Total_length: 777Distributed index stop interation. Idx: 779 Total_length: 777

Distributed index stop interation. Idx: 780 Total_length: 777
  | 45/63 [11:08<04:20, 14.46s/it]Evaluating:  73%|  | 46/63 [11:24<04:16, 15.10s/it]Evaluating:  75%|  | 47/63 [11:37<03:50, 14.40s/it]Evaluating:  76%|  | 48/63 [11:52<03:40, 14.69s/it]Evaluating:  76%|  | 48/63 [11:52<03:42, 14.85s/it]
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1/eval/3
dev | avg_loss: 2.25927734375 | {'exact_match': 1.4323, 'rougeL': 19.0183}
train | epoch   3 | Iter:   4001/ 13000 | global iter:   4001/ 13000 | loss: 1.7150 | ds_loss: 0.0000 | lr: 3.9312e-05 | scale: 32768.0000 | micro time: 1.829 | step time: 0.000
train | epoch   3 | Iter:   4002/ 13000 | global iter:   4002/ 13000 | loss: 1.1837 | ds_loss: 0.0000 | lr: 3.9307e-05 | scale: 32768.0000 | micro time: 1.761 | step time: 0.000
train | epoch   3 | Iter:   4003/ 13000 | global iter:   4003/ 13000 | loss: 1.9584 | ds_loss: 0.0000 | lr: 3.9303e-05 | scale: 32768.0000 | micro time: 1.812 | step time: 0.000
train | epoch   3 | Iter:   4004/ 13000 | global iter:   4004/ 13000 | loss: 1.7221 | ds_loss: 0.0000 | lr: 3.9298e-05 | scale: 32768.0000 | micro time: 1.903 | step time: 0.000
train | epoch   3 | Iter:   4005/ 13000 | global iter:   4005/ 13000 | loss: 0.7191 | ds_loss: 0.0000 | lr: 3.9293e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   3 | Iter:   4006/ 13000 | global iter:   4006/ 13000 | loss: 1.8032 | ds_loss: 0.0000 | lr: 3.9288e-05 | scale: 32768.0000 | micro time: 1.923 | step time: 0.000
train | epoch   3 | Iter:   4007/ 13000 | global iter:   4007/ 13000 | loss: 1.6221 | ds_loss: 0.0000 | lr: 3.9283e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   3 | Iter:   4008/ 13000 | global iter:   4008/ 13000 | loss: 1.8030 | ds_loss: 0.0000 | lr: 3.9278e-05 | scale: 32768.0000 | micro time: 1.891 | step time: 0.000
train | epoch   3 | Iter:   4009/ 13000 | global iter:   4009/ 13000 | loss: 1.4689 | ds_loss: 0.0000 | lr: 3.9273e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   3 | Iter:   4010/ 13000 | global iter:   4010/ 13000 | loss: 2.1395 | ds_loss: 0.0000 | lr: 3.9268e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4010/ 13000 | global iter:   4010/ 13000 | loss: 1.6135 | ds_loss: 0.0000 | lr: 3.9268e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 1.841
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4011/ 13000 | global iter:   4011/ 13000 | loss: 1.6436 | ds_loss: 0.0000 | lr: 3.9263e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   3 | Iter:   4012/ 13000 | global iter:   4012/ 13000 | loss: 2.0319 | ds_loss: 0.0000 | lr: 3.9258e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 0.000
train | epoch   3 | Iter:   4013/ 13000 | global iter:   4013/ 13000 | loss: 1.2128 | ds_loss: 0.0000 | lr: 3.9253e-05 | scale: 32768.0000 | micro time: 1.951 | step time: 0.000
train | epoch   3 | Iter:   4014/ 13000 | global iter:   4014/ 13000 | loss: 2.1458 | ds_loss: 0.0000 | lr: 3.9248e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   3 | Iter:   4015/ 13000 | global iter:   4015/ 13000 | loss: 1.2998 | ds_loss: 0.0000 | lr: 3.9243e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
train | epoch   3 | Iter:   4016/ 13000 | global iter:   4016/ 13000 | loss: 1.3356 | ds_loss: 0.0000 | lr: 3.9238e-05 | scale: 32768.0000 | micro time: 1.825 | step time: 0.000
train | epoch   3 | Iter:   4017/ 13000 | global iter:   4017/ 13000 | loss: 1.7753 | ds_loss: 0.0000 | lr: 3.9233e-05 | scale: 32768.0000 | micro time: 1.845 | step time: 0.000
train | epoch   3 | Iter:   4018/ 13000 | global iter:   4018/ 13000 | loss: 2.0141 | ds_loss: 0.0000 | lr: 3.9228e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   3 | Iter:   4019/ 13000 | global iter:   4019/ 13000 | loss: 1.7275 | ds_loss: 0.0000 | lr: 3.9223e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   3 | Iter:   4020/ 13000 | global iter:   4020/ 13000 | loss: 1.9082 | ds_loss: 0.0000 | lr: 3.9218e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4020/ 13000 | global iter:   4020/ 13000 | loss: 1.7095 | ds_loss: 0.0000 | lr: 3.9218e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 1.844
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4021/ 13000 | global iter:   4021/ 13000 | loss: 1.9037 | ds_loss: 0.0000 | lr: 3.9213e-05 | scale: 32768.0000 | micro time: 1.858 | step time: 0.000
train | epoch   3 | Iter:   4022/ 13000 | global iter:   4022/ 13000 | loss: 1.0309 | ds_loss: 0.0000 | lr: 3.9208e-05 | scale: 32768.0000 | micro time: 1.678 | step time: 0.000
train | epoch   3 | Iter:   4023/ 13000 | global iter:   4023/ 13000 | loss: 1.9273 | ds_loss: 0.0000 | lr: 3.9203e-05 | scale: 32768.0000 | micro time: 1.824 | step time: 0.000
train | epoch   3 | Iter:   4024/ 13000 | global iter:   4024/ 13000 | loss: 1.6924 | ds_loss: 0.0000 | lr: 3.9198e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   3 | Iter:   4025/ 13000 | global iter:   4025/ 13000 | loss: 1.5875 | ds_loss: 0.0000 | lr: 3.9193e-05 | scale: 32768.0000 | micro time: 1.899 | step time: 0.000
train | epoch   3 | Iter:   4026/ 13000 | global iter:   4026/ 13000 | loss: 1.3112 | ds_loss: 0.0000 | lr: 3.9188e-05 | scale: 32768.0000 | micro time: 1.703 | step time: 0.000
train | epoch   3 | Iter:   4027/ 13000 | global iter:   4027/ 13000 | loss: 1.4092 | ds_loss: 0.0000 | lr: 3.9183e-05 | scale: 32768.0000 | micro time: 1.891 | step time: 0.000
train | epoch   3 | Iter:   4028/ 13000 | global iter:   4028/ 13000 | loss: 1.8958 | ds_loss: 0.0000 | lr: 3.9179e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 0.000
train | epoch   3 | Iter:   4029/ 13000 | global iter:   4029/ 13000 | loss: 1.5601 | ds_loss: 0.0000 | lr: 3.9174e-05 | scale: 32768.0000 | micro time: 1.781 | step time: 0.000
train | epoch   3 | Iter:   4030/ 13000 | global iter:   4030/ 13000 | loss: 1.4616 | ds_loss: 0.0000 | lr: 3.9169e-05 | scale: 32768.0000 | micro time: 1.897 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4030/ 13000 | global iter:   4030/ 13000 | loss: 1.5780 | ds_loss: 0.0000 | lr: 3.9169e-05 | scale: 32768.0000 | micro time: 1.897 | step time: 1.825
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4031/ 13000 | global iter:   4031/ 13000 | loss: 1.5748 | ds_loss: 0.0000 | lr: 3.9164e-05 | scale: 32768.0000 | micro time: 1.878 | step time: 0.000
train | epoch   3 | Iter:   4032/ 13000 | global iter:   4032/ 13000 | loss: 1.9888 | ds_loss: 0.0000 | lr: 3.9159e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   3 | Iter:   4033/ 13000 | global iter:   4033/ 13000 | loss: 1.3829 | ds_loss: 0.0000 | lr: 3.9154e-05 | scale: 32768.0000 | micro time: 1.846 | step time: 0.000
train | epoch   3 | Iter:   4034/ 13000 | global iter:   4034/ 13000 | loss: 1.2650 | ds_loss: 0.0000 | lr: 3.9149e-05 | scale: 32768.0000 | micro time: 1.908 | step time: 0.000
train | epoch   3 | Iter:   4035/ 13000 | global iter:   4035/ 13000 | loss: 1.7403 | ds_loss: 0.0000 | lr: 3.9144e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   3 | Iter:   4036/ 13000 | global iter:   4036/ 13000 | loss: 1.3682 | ds_loss: 0.0000 | lr: 3.9139e-05 | scale: 32768.0000 | micro time: 1.837 | step time: 0.000
train | epoch   3 | Iter:   4037/ 13000 | global iter:   4037/ 13000 | loss: 2.1514 | ds_loss: 0.0000 | lr: 3.9134e-05 | scale: 32768.0000 | micro time: 1.885 | step time: 0.000
train | epoch   3 | Iter:   4038/ 13000 | global iter:   4038/ 13000 | loss: 1.4380 | ds_loss: 0.0000 | lr: 3.9129e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   3 | Iter:   4039/ 13000 | global iter:   4039/ 13000 | loss: 1.6729 | ds_loss: 0.0000 | lr: 3.9124e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   3 | Iter:   4040/ 13000 | global iter:   4040/ 13000 | loss: 1.6818 | ds_loss: 0.0000 | lr: 3.9119e-05 | scale: 32768.0000 | micro time: 1.794 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4040/ 13000 | global iter:   4040/ 13000 | loss: 1.6264 | ds_loss: 0.0000 | lr: 3.9119e-05 | scale: 32768.0000 | micro time: 1.794 | step time: 1.852
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4041/ 13000 | global iter:   4041/ 13000 | loss: 1.8404 | ds_loss: 0.0000 | lr: 3.9114e-05 | scale: 32768.0000 | micro time: 1.738 | step time: 0.000
train | epoch   3 | Iter:   4042/ 13000 | global iter:   4042/ 13000 | loss: 1.4386 | ds_loss: 0.0000 | lr: 3.9109e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 0.000
train | epoch   3 | Iter:   4043/ 13000 | global iter:   4043/ 13000 | loss: 1.4385 | ds_loss: 0.0000 | lr: 3.9104e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   3 | Iter:   4044/ 13000 | global iter:   4044/ 13000 | loss: 1.5539 | ds_loss: 0.0000 | lr: 3.9099e-05 | scale: 32768.0000 | micro time: 1.755 | step time: 0.000
train | epoch   3 | Iter:   4045/ 13000 | global iter:   4045/ 13000 | loss: 1.6595 | ds_loss: 0.0000 | lr: 3.9094e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   3 | Iter:   4046/ 13000 | global iter:   4046/ 13000 | loss: 1.0133 | ds_loss: 0.0000 | lr: 3.9089e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   3 | Iter:   4047/ 13000 | global iter:   4047/ 13000 | loss: 1.9836 | ds_loss: 0.0000 | lr: 3.9084e-05 | scale: 32768.0000 | micro time: 1.817 | step time: 0.000
train | epoch   3 | Iter:   4048/ 13000 | global iter:   4048/ 13000 | loss: 1.7382 | ds_loss: 0.0000 | lr: 3.9079e-05 | scale: 32768.0000 | micro time: 1.812 | step time: 0.000
train | epoch   3 | Iter:   4049/ 13000 | global iter:   4049/ 13000 | loss: 1.7595 | ds_loss: 0.0000 | lr: 3.9074e-05 | scale: 32768.0000 | micro time: 1.904 | step time: 0.000
train | epoch   3 | Iter:   4050/ 13000 | global iter:   4050/ 13000 | loss: 0.6634 | ds_loss: 0.0000 | lr: 3.9069e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4050/ 13000 | global iter:   4050/ 13000 | loss: 1.5089 | ds_loss: 0.0000 | lr: 3.9069e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 1.820
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4051/ 13000 | global iter:   4051/ 13000 | loss: 1.6094 | ds_loss: 0.0000 | lr: 3.9064e-05 | scale: 32768.0000 | micro time: 1.882 | step time: 0.000
train | epoch   3 | Iter:   4052/ 13000 | global iter:   4052/ 13000 | loss: 1.7296 | ds_loss: 0.0000 | lr: 3.9059e-05 | scale: 32768.0000 | micro time: 1.932 | step time: 0.000
train | epoch   3 | Iter:   4053/ 13000 | global iter:   4053/ 13000 | loss: 2.1007 | ds_loss: 0.0000 | lr: 3.9054e-05 | scale: 32768.0000 | micro time: 1.862 | step time: 0.000
train | epoch   3 | Iter:   4054/ 13000 | global iter:   4054/ 13000 | loss: 1.4390 | ds_loss: 0.0000 | lr: 3.9049e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   3 | Iter:   4055/ 13000 | global iter:   4055/ 13000 | loss: 1.3058 | ds_loss: 0.0000 | lr: 3.9044e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
train | epoch   3 | Iter:   4056/ 13000 | global iter:   4056/ 13000 | loss: 1.3494 | ds_loss: 0.0000 | lr: 3.9039e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   3 | Iter:   4057/ 13000 | global iter:   4057/ 13000 | loss: 2.1331 | ds_loss: 0.0000 | lr: 3.9034e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 0.000
train | epoch   3 | Iter:   4058/ 13000 | global iter:   4058/ 13000 | loss: 1.8995 | ds_loss: 0.0000 | lr: 3.9029e-05 | scale: 32768.0000 | micro time: 1.930 | step time: 0.000
train | epoch   3 | Iter:   4059/ 13000 | global iter:   4059/ 13000 | loss: 1.7883 | ds_loss: 0.0000 | lr: 3.9024e-05 | scale: 32768.0000 | micro time: 1.693 | step time: 0.000
train | epoch   3 | Iter:   4060/ 13000 | global iter:   4060/ 13000 | loss: 1.3551 | ds_loss: 0.0000 | lr: 3.9019e-05 | scale: 32768.0000 | micro time: 1.833 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4060/ 13000 | global iter:   4060/ 13000 | loss: 1.6710 | ds_loss: 0.0000 | lr: 3.9019e-05 | scale: 32768.0000 | micro time: 1.833 | step time: 1.843
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4061/ 13000 | global iter:   4061/ 13000 | loss: 1.4745 | ds_loss: 0.0000 | lr: 3.9014e-05 | scale: 32768.0000 | micro time: 1.854 | step time: 0.000
train | epoch   3 | Iter:   4062/ 13000 | global iter:   4062/ 13000 | loss: 1.6590 | ds_loss: 0.0000 | lr: 3.9009e-05 | scale: 32768.0000 | micro time: 1.774 | step time: 0.000
train | epoch   3 | Iter:   4063/ 13000 | global iter:   4063/ 13000 | loss: 2.1451 | ds_loss: 0.0000 | lr: 3.9004e-05 | scale: 32768.0000 | micro time: 1.820 | step time: 0.000
train | epoch   3 | Iter:   4064/ 13000 | global iter:   4064/ 13000 | loss: 1.1584 | ds_loss: 0.0000 | lr: 3.8999e-05 | scale: 32768.0000 | micro time: 1.765 | step time: 0.000
train | epoch   3 | Iter:   4065/ 13000 | global iter:   4065/ 13000 | loss: 1.8343 | ds_loss: 0.0000 | lr: 3.8994e-05 | scale: 32768.0000 | micro time: 1.776 | step time: 0.000
train | epoch   3 | Iter:   4066/ 13000 | global iter:   4066/ 13000 | loss: 1.1834 | ds_loss: 0.0000 | lr: 3.8989e-05 | scale: 32768.0000 | micro time: 1.920 | step time: 0.000
train | epoch   3 | Iter:   4067/ 13000 | global iter:   4067/ 13000 | loss: 1.8583 | ds_loss: 0.0000 | lr: 3.8984e-05 | scale: 32768.0000 | micro time: 1.713 | step time: 0.000
train | epoch   3 | Iter:   4068/ 13000 | global iter:   4068/ 13000 | loss: 1.3321 | ds_loss: 0.0000 | lr: 3.8979e-05 | scale: 32768.0000 | micro time: 1.817 | step time: 0.000
train | epoch   3 | Iter:   4069/ 13000 | global iter:   4069/ 13000 | loss: 1.6003 | ds_loss: 0.0000 | lr: 3.8974e-05 | scale: 32768.0000 | micro time: 1.795 | step time: 0.000
train | epoch   3 | Iter:   4070/ 13000 | global iter:   4070/ 13000 | loss: 1.7388 | ds_loss: 0.0000 | lr: 3.8969e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4070/ 13000 | global iter:   4070/ 13000 | loss: 1.5984 | ds_loss: 0.0000 | lr: 3.8969e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 1.806
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4071/ 13000 | global iter:   4071/ 13000 | loss: 1.5968 | ds_loss: 0.0000 | lr: 3.8964e-05 | scale: 32768.0000 | micro time: 1.810 | step time: 0.000
train | epoch   3 | Iter:   4072/ 13000 | global iter:   4072/ 13000 | loss: 1.3072 | ds_loss: 0.0000 | lr: 3.8959e-05 | scale: 32768.0000 | micro time: 1.849 | step time: 0.000
train | epoch   3 | Iter:   4073/ 13000 | global iter:   4073/ 13000 | loss: 1.1038 | ds_loss: 0.0000 | lr: 3.8954e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   3 | Iter:   4074/ 13000 | global iter:   4074/ 13000 | loss: 1.5994 | ds_loss: 0.0000 | lr: 3.8949e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 0.000
train | epoch   3 | Iter:   4075/ 13000 | global iter:   4075/ 13000 | loss: 1.6607 | ds_loss: 0.0000 | lr: 3.8944e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   3 | Iter:   4076/ 13000 | global iter:   4076/ 13000 | loss: 1.5967 | ds_loss: 0.0000 | lr: 3.8939e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
train | epoch   3 | Iter:   4077/ 13000 | global iter:   4077/ 13000 | loss: 1.9263 | ds_loss: 0.0000 | lr: 3.8934e-05 | scale: 32768.0000 | micro time: 1.887 | step time: 0.000
train | epoch   3 | Iter:   4078/ 13000 | global iter:   4078/ 13000 | loss: 1.7263 | ds_loss: 0.0000 | lr: 3.8929e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   3 | Iter:   4079/ 13000 | global iter:   4079/ 13000 | loss: 1.1708 | ds_loss: 0.0000 | lr: 3.8924e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   3 | Iter:   4080/ 13000 | global iter:   4080/ 13000 | loss: 1.4126 | ds_loss: 0.0000 | lr: 3.8919e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4080/ 13000 | global iter:   4080/ 13000 | loss: 1.5101 | ds_loss: 0.0000 | lr: 3.8919e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 1.828
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4081/ 13000 | global iter:   4081/ 13000 | loss: 1.4928 | ds_loss: 0.0000 | lr: 3.8914e-05 | scale: 32768.0000 | micro time: 1.777 | step time: 0.000
train | epoch   3 | Iter:   4082/ 13000 | global iter:   4082/ 13000 | loss: 1.4803 | ds_loss: 0.0000 | lr: 3.8909e-05 | scale: 32768.0000 | micro time: 2.055 | step time: 0.000
train | epoch   3 | Iter:   4083/ 13000 | global iter:   4083/ 13000 | loss: 1.2887 | ds_loss: 0.0000 | lr: 3.8904e-05 | scale: 32768.0000 | micro time: 1.801 | step time: 0.000
train | epoch   3 | Iter:   4084/ 13000 | global iter:   4084/ 13000 | loss: 1.6172 | ds_loss: 0.0000 | lr: 3.8899e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   3 | Iter:   4085/ 13000 | global iter:   4085/ 13000 | loss: 1.3340 | ds_loss: 0.0000 | lr: 3.8894e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
train | epoch   3 | Iter:   4086/ 13000 | global iter:   4086/ 13000 | loss: 1.4033 | ds_loss: 0.0000 | lr: 3.8889e-05 | scale: 32768.0000 | micro time: 1.769 | step time: 0.000
train | epoch   3 | Iter:   4087/ 13000 | global iter:   4087/ 13000 | loss: 1.4337 | ds_loss: 0.0000 | lr: 3.8884e-05 | scale: 32768.0000 | micro time: 1.903 | step time: 0.000
train | epoch   3 | Iter:   4088/ 13000 | global iter:   4088/ 13000 | loss: 1.2535 | ds_loss: 0.0000 | lr: 3.8879e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   3 | Iter:   4089/ 13000 | global iter:   4089/ 13000 | loss: 1.6104 | ds_loss: 0.0000 | lr: 3.8874e-05 | scale: 32768.0000 | micro time: 1.753 | step time: 0.000
train | epoch   3 | Iter:   4090/ 13000 | global iter:   4090/ 13000 | loss: 1.7466 | ds_loss: 0.0000 | lr: 3.8869e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4090/ 13000 | global iter:   4090/ 13000 | loss: 1.4661 | ds_loss: 0.0000 | lr: 3.8869e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 1.837
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4091/ 13000 | global iter:   4091/ 13000 | loss: 1.0347 | ds_loss: 0.0000 | lr: 3.8864e-05 | scale: 32768.0000 | micro time: 1.914 | step time: 0.000
train | epoch   3 | Iter:   4092/ 13000 | global iter:   4092/ 13000 | loss: 1.4566 | ds_loss: 0.0000 | lr: 3.8859e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   3 | Iter:   4093/ 13000 | global iter:   4093/ 13000 | loss: 1.9011 | ds_loss: 0.0000 | lr: 3.8854e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   3 | Iter:   4094/ 13000 | global iter:   4094/ 13000 | loss: 2.0023 | ds_loss: 0.0000 | lr: 3.8849e-05 | scale: 32768.0000 | micro time: 1.853 | step time: 0.000
train | epoch   3 | Iter:   4095/ 13000 | global iter:   4095/ 13000 | loss: 1.6073 | ds_loss: 0.0000 | lr: 3.8844e-05 | scale: 32768.0000 | micro time: 1.837 | step time: 0.000
train | epoch   3 | Iter:   4096/ 13000 | global iter:   4096/ 13000 | loss: 1.3781 | ds_loss: 0.0000 | lr: 3.8839e-05 | scale: 32768.0000 | micro time: 1.657 | step time: 0.000
train | epoch   3 | Iter:   4097/ 13000 | global iter:   4097/ 13000 | loss: 1.1299 | ds_loss: 0.0000 | lr: 3.8834e-05 | scale: 32768.0000 | micro time: 1.773 | step time: 0.000
train | epoch   3 | Iter:   4098/ 13000 | global iter:   4098/ 13000 | loss: 1.1453 | ds_loss: 0.0000 | lr: 3.8829e-05 | scale: 32768.0000 | micro time: 1.795 | step time: 0.000
train | epoch   3 | Iter:   4099/ 13000 | global iter:   4099/ 13000 | loss: 1.9864 | ds_loss: 0.0000 | lr: 3.8824e-05 | scale: 32768.0000 | micro time: 1.795 | step time: 0.000
train | epoch   3 | Iter:   4100/ 13000 | global iter:   4100/ 13000 | loss: 1.7533 | ds_loss: 0.0000 | lr: 3.8819e-05 | scale: 32768.0000 | micro time: 1.895 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4100/ 13000 | global iter:   4100/ 13000 | loss: 1.5395 | ds_loss: 0.0000 | lr: 3.8819e-05 | scale: 32768.0000 | micro time: 1.895 | step time: 1.814
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4101/ 13000 | global iter:   4101/ 13000 | loss: 1.9984 | ds_loss: 0.0000 | lr: 3.8814e-05 | scale: 32768.0000 | micro time: 1.919 | step time: 0.000
train | epoch   3 | Iter:   4102/ 13000 | global iter:   4102/ 13000 | loss: 1.5623 | ds_loss: 0.0000 | lr: 3.8809e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   3 | Iter:   4103/ 13000 | global iter:   4103/ 13000 | loss: 0.6728 | ds_loss: 0.0000 | lr: 3.8804e-05 | scale: 32768.0000 | micro time: 1.891 | step time: 0.000
train | epoch   3 | Iter:   4104/ 13000 | global iter:   4104/ 13000 | loss: 1.5386 | ds_loss: 0.0000 | lr: 3.8798e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   3 | Iter:   4105/ 13000 | global iter:   4105/ 13000 | loss: 1.5626 | ds_loss: 0.0000 | lr: 3.8793e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   3 | Iter:   4106/ 13000 | global iter:   4106/ 13000 | loss: 1.6345 | ds_loss: 0.0000 | lr: 3.8788e-05 | scale: 32768.0000 | micro time: 1.715 | step time: 0.000
train | epoch   3 | Iter:   4107/ 13000 | global iter:   4107/ 13000 | loss: 1.1348 | ds_loss: 0.0000 | lr: 3.8783e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   3 | Iter:   4108/ 13000 | global iter:   4108/ 13000 | loss: 1.5721 | ds_loss: 0.0000 | lr: 3.8778e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   3 | Iter:   4109/ 13000 | global iter:   4109/ 13000 | loss: 1.8287 | ds_loss: 0.0000 | lr: 3.8773e-05 | scale: 32768.0000 | micro time: 1.765 | step time: 0.000
train | epoch   3 | Iter:   4110/ 13000 | global iter:   4110/ 13000 | loss: 1.9303 | ds_loss: 0.0000 | lr: 3.8768e-05 | scale: 32768.0000 | micro time: 1.881 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4110/ 13000 | global iter:   4110/ 13000 | loss: 1.5435 | ds_loss: 0.0000 | lr: 3.8768e-05 | scale: 32768.0000 | micro time: 1.881 | step time: 1.830
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4111/ 13000 | global iter:   4111/ 13000 | loss: 1.5029 | ds_loss: 0.0000 | lr: 3.8763e-05 | scale: 32768.0000 | micro time: 1.746 | step time: 0.000
train | epoch   3 | Iter:   4112/ 13000 | global iter:   4112/ 13000 | loss: 1.8817 | ds_loss: 0.0000 | lr: 3.8758e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   3 | Iter:   4113/ 13000 | global iter:   4113/ 13000 | loss: 1.6841 | ds_loss: 0.0000 | lr: 3.8753e-05 | scale: 32768.0000 | micro time: 1.755 | step time: 0.000
train | epoch   3 | Iter:   4114/ 13000 | global iter:   4114/ 13000 | loss: 1.2058 | ds_loss: 0.0000 | lr: 3.8748e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   3 | Iter:   4115/ 13000 | global iter:   4115/ 13000 | loss: 1.8064 | ds_loss: 0.0000 | lr: 3.8743e-05 | scale: 32768.0000 | micro time: 1.837 | step time: 0.000
train | epoch   3 | Iter:   4116/ 13000 | global iter:   4116/ 13000 | loss: 1.4202 | ds_loss: 0.0000 | lr: 3.8738e-05 | scale: 32768.0000 | micro time: 1.828 | step time: 0.000
train | epoch   3 | Iter:   4117/ 13000 | global iter:   4117/ 13000 | loss: 1.9603 | ds_loss: 0.0000 | lr: 3.8733e-05 | scale: 32768.0000 | micro time: 1.869 | step time: 0.000
train | epoch   3 | Iter:   4118/ 13000 | global iter:   4118/ 13000 | loss: 1.3511 | ds_loss: 0.0000 | lr: 3.8728e-05 | scale: 32768.0000 | micro time: 2.089 | step time: 0.000
train | epoch   3 | Iter:   4119/ 13000 | global iter:   4119/ 13000 | loss: 0.9280 | ds_loss: 0.0000 | lr: 3.8723e-05 | scale: 32768.0000 | micro time: 1.849 | step time: 0.000
train | epoch   3 | Iter:   4120/ 13000 | global iter:   4120/ 13000 | loss: 1.6148 | ds_loss: 0.0000 | lr: 3.8718e-05 | scale: 32768.0000 | micro time: 1.747 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4120/ 13000 | global iter:   4120/ 13000 | loss: 1.5355 | ds_loss: 0.0000 | lr: 3.8718e-05 | scale: 32768.0000 | micro time: 1.747 | step time: 1.833
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4121/ 13000 | global iter:   4121/ 13000 | loss: 1.4633 | ds_loss: 0.0000 | lr: 3.8713e-05 | scale: 32768.0000 | micro time: 1.837 | step time: 0.000
train | epoch   3 | Iter:   4122/ 13000 | global iter:   4122/ 13000 | loss: 1.6702 | ds_loss: 0.0000 | lr: 3.8708e-05 | scale: 32768.0000 | micro time: 1.836 | step time: 0.000
train | epoch   3 | Iter:   4123/ 13000 | global iter:   4123/ 13000 | loss: 2.0126 | ds_loss: 0.0000 | lr: 3.8703e-05 | scale: 32768.0000 | micro time: 2.220 | step time: 0.000
train | epoch   3 | Iter:   4124/ 13000 | global iter:   4124/ 13000 | loss: 1.8377 | ds_loss: 0.0000 | lr: 3.8698e-05 | scale: 32768.0000 | micro time: 1.813 | step time: 0.000
train | epoch   3 | Iter:   4125/ 13000 | global iter:   4125/ 13000 | loss: 1.5236 | ds_loss: 0.0000 | lr: 3.8693e-05 | scale: 32768.0000 | micro time: 1.882 | step time: 0.000
train | epoch   3 | Iter:   4126/ 13000 | global iter:   4126/ 13000 | loss: 1.4475 | ds_loss: 0.0000 | lr: 3.8688e-05 | scale: 32768.0000 | micro time: 1.887 | step time: 0.000
train | epoch   3 | Iter:   4127/ 13000 | global iter:   4127/ 13000 | loss: 1.1482 | ds_loss: 0.0000 | lr: 3.8683e-05 | scale: 32768.0000 | micro time: 1.800 | step time: 0.000
train | epoch   3 | Iter:   4128/ 13000 | global iter:   4128/ 13000 | loss: 1.9177 | ds_loss: 0.0000 | lr: 3.8677e-05 | scale: 32768.0000 | micro time: 1.730 | step time: 0.000
train | epoch   3 | Iter:   4129/ 13000 | global iter:   4129/ 13000 | loss: 1.9327 | ds_loss: 0.0000 | lr: 3.8672e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   3 | Iter:   4130/ 13000 | global iter:   4130/ 13000 | loss: 1.5689 | ds_loss: 0.0000 | lr: 3.8667e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4130/ 13000 | global iter:   4130/ 13000 | loss: 1.6522 | ds_loss: 0.0000 | lr: 3.8667e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 1.871
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4131/ 13000 | global iter:   4131/ 13000 | loss: 1.7438 | ds_loss: 0.0000 | lr: 3.8662e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   3 | Iter:   4132/ 13000 | global iter:   4132/ 13000 | loss: 1.2214 | ds_loss: 0.0000 | lr: 3.8657e-05 | scale: 32768.0000 | micro time: 1.747 | step time: 0.000
train | epoch   3 | Iter:   4133/ 13000 | global iter:   4133/ 13000 | loss: 1.2124 | ds_loss: 0.0000 | lr: 3.8652e-05 | scale: 32768.0000 | micro time: 1.874 | step time: 0.000
train | epoch   3 | Iter:   4134/ 13000 | global iter:   4134/ 13000 | loss: 2.0084 | ds_loss: 0.0000 | lr: 3.8647e-05 | scale: 32768.0000 | micro time: 1.786 | step time: 0.000
train | epoch   3 | Iter:   4135/ 13000 | global iter:   4135/ 13000 | loss: 1.6957 | ds_loss: 0.0000 | lr: 3.8642e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   3 | Iter:   4136/ 13000 | global iter:   4136/ 13000 | loss: 1.4070 | ds_loss: 0.0000 | lr: 3.8637e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   3 | Iter:   4137/ 13000 | global iter:   4137/ 13000 | loss: 1.5403 | ds_loss: 0.0000 | lr: 3.8632e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   3 | Iter:   4138/ 13000 | global iter:   4138/ 13000 | loss: 1.3032 | ds_loss: 0.0000 | lr: 3.8627e-05 | scale: 32768.0000 | micro time: 1.739 | step time: 0.000
train | epoch   3 | Iter:   4139/ 13000 | global iter:   4139/ 13000 | loss: 1.8538 | ds_loss: 0.0000 | lr: 3.8622e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   3 | Iter:   4140/ 13000 | global iter:   4140/ 13000 | loss: 1.9737 | ds_loss: 0.0000 | lr: 3.8617e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4140/ 13000 | global iter:   4140/ 13000 | loss: 1.5960 | ds_loss: 0.0000 | lr: 3.8617e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 1.797
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4141/ 13000 | global iter:   4141/ 13000 | loss: 1.7938 | ds_loss: 0.0000 | lr: 3.8612e-05 | scale: 32768.0000 | micro time: 1.785 | step time: 0.000
train | epoch   3 | Iter:   4142/ 13000 | global iter:   4142/ 13000 | loss: 1.8890 | ds_loss: 0.0000 | lr: 3.8607e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   3 | Iter:   4143/ 13000 | global iter:   4143/ 13000 | loss: 1.7285 | ds_loss: 0.0000 | lr: 3.8602e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   3 | Iter:   4144/ 13000 | global iter:   4144/ 13000 | loss: 1.7774 | ds_loss: 0.0000 | lr: 3.8597e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   3 | Iter:   4145/ 13000 | global iter:   4145/ 13000 | loss: 1.7357 | ds_loss: 0.0000 | lr: 3.8592e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   3 | Iter:   4146/ 13000 | global iter:   4146/ 13000 | loss: 1.1179 | ds_loss: 0.0000 | lr: 3.8586e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   3 | Iter:   4147/ 13000 | global iter:   4147/ 13000 | loss: 1.7836 | ds_loss: 0.0000 | lr: 3.8581e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   3 | Iter:   4148/ 13000 | global iter:   4148/ 13000 | loss: 1.7564 | ds_loss: 0.0000 | lr: 3.8576e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
train | epoch   3 | Iter:   4149/ 13000 | global iter:   4149/ 13000 | loss: 1.8580 | ds_loss: 0.0000 | lr: 3.8571e-05 | scale: 32768.0000 | micro time: 1.857 | step time: 0.000
train | epoch   3 | Iter:   4150/ 13000 | global iter:   4150/ 13000 | loss: 2.0291 | ds_loss: 0.0000 | lr: 3.8566e-05 | scale: 32768.0000 | micro time: 1.881 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4150/ 13000 | global iter:   4150/ 13000 | loss: 1.7469 | ds_loss: 0.0000 | lr: 3.8566e-05 | scale: 32768.0000 | micro time: 1.881 | step time: 1.804
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4151/ 13000 | global iter:   4151/ 13000 | loss: 1.2164 | ds_loss: 0.0000 | lr: 3.8561e-05 | scale: 65536.0000 | micro time: 1.894 | step time: 0.000
train | epoch   3 | Iter:   4152/ 13000 | global iter:   4152/ 13000 | loss: 1.3914 | ds_loss: 0.0000 | lr: 3.8556e-05 | scale: 65536.0000 | micro time: 1.843 | step time: 0.000
train | epoch   3 | Iter:   4153/ 13000 | global iter:   4153/ 13000 | loss: 1.9149 | ds_loss: 0.0000 | lr: 3.8551e-05 | scale: 65536.0000 | micro time: 1.751 | step time: 0.000
train | epoch   3 | Iter:   4154/ 13000 | global iter:   4154/ 13000 | loss: 1.6954 | ds_loss: 0.0000 | lr: 3.8546e-05 | scale: 65536.0000 | micro time: 1.797 | step time: 0.000
train | epoch   3 | Iter:   4155/ 13000 | global iter:   4155/ 13000 | loss: 2.0940 | ds_loss: 0.0000 | lr: 3.8541e-05 | scale: 65536.0000 | micro time: 1.785 | step time: 0.000
train | epoch   3 | Iter:   4156/ 13000 | global iter:   4156/ 13000 | loss: 1.5883 | ds_loss: 0.0000 | lr: 3.8536e-05 | scale: 65536.0000 | micro time: 1.742 | step time: 0.000
train | epoch   3 | Iter:   4157/ 13000 | global iter:   4157/ 13000 | loss: 1.9073 | ds_loss: 0.0000 | lr: 3.8531e-05 | scale: 65536.0000 | micro time: 1.844 | step time: 0.000
train | epoch   3 | Iter:   4158/ 13000 | global iter:   4158/ 13000 | loss: 1.2013 | ds_loss: 0.0000 | lr: 3.8526e-05 | scale: 65536.0000 | micro time: 1.774 | step time: 0.000
train | epoch   3 | Iter:   4159/ 13000 | global iter:   4159/ 13000 | loss: 1.4701 | ds_loss: 0.0000 | lr: 3.8521e-05 | scale: 65536.0000 | micro time: 1.776 | step time: 0.000
train | epoch   3 | Iter:   4160/ 13000 | global iter:   4160/ 13000 | loss: 1.7185 | ds_loss: 0.0000 | lr: 3.8515e-05 | scale: 65536.0000 | micro time: 1.771 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4160/ 13000 | global iter:   4160/ 13000 | loss: 1.6197 | ds_loss: 0.0000 | lr: 3.8515e-05 | scale: 65536.0000 | micro time: 1.771 | step time: 1.798
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4161/ 13000 | global iter:   4161/ 13000 | loss: 1.8962 | ds_loss: 0.0000 | lr: 3.8510e-05 | scale: 65536.0000 | micro time: 1.769 | step time: 0.000
train | epoch   3 | Iter:   4162/ 13000 | global iter:   4162/ 13000 | loss: 1.0836 | ds_loss: 0.0000 | lr: 3.8505e-05 | scale: 65536.0000 | micro time: 1.871 | step time: 0.000
train | epoch   3 | Iter:   4163/ 13000 | global iter:   4163/ 13000 | loss: 0.9163 | ds_loss: 0.0000 | lr: 3.8500e-05 | scale: 65536.0000 | micro time: 1.775 | step time: 0.000
train | epoch   3 | Iter:   4164/ 13000 | global iter:   4164/ 13000 | loss: 1.6001 | ds_loss: 0.0000 | lr: 3.8495e-05 | scale: 65536.0000 | micro time: 1.743 | step time: 0.000
train | epoch   3 | Iter:   4165/ 13000 | global iter:   4165/ 13000 | loss: 1.5603 | ds_loss: 0.0000 | lr: 3.8490e-05 | scale: 65536.0000 | micro time: 1.864 | step time: 0.000
train | epoch   3 | Iter:   4166/ 13000 | global iter:   4166/ 13000 | loss: 1.3300 | ds_loss: 0.0000 | lr: 3.8485e-05 | scale: 65536.0000 | micro time: 1.793 | step time: 0.000
train | epoch   3 | Iter:   4167/ 13000 | global iter:   4167/ 13000 | loss: 1.6581 | ds_loss: 0.0000 | lr: 3.8480e-05 | scale: 65536.0000 | micro time: 1.781 | step time: 0.000
train | epoch   3 | Iter:   4168/ 13000 | global iter:   4168/ 13000 | loss: 1.6385 | ds_loss: 0.0000 | lr: 3.8475e-05 | scale: 65536.0000 | micro time: 1.826 | step time: 0.000
train | epoch   3 | Iter:   4169/ 13000 | global iter:   4169/ 13000 | loss: 1.9426 | ds_loss: 0.0000 | lr: 3.8470e-05 | scale: 65536.0000 | micro time: 1.798 | step time: 0.000
train | epoch   3 | Iter:   4170/ 13000 | global iter:   4170/ 13000 | loss: 1.5555 | ds_loss: 0.0000 | lr: 3.8465e-05 | scale: 65536.0000 | micro time: 1.799 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4170/ 13000 | global iter:   4170/ 13000 | loss: 1.5181 | ds_loss: 0.0000 | lr: 3.8465e-05 | scale: 65536.0000 | micro time: 1.799 | step time: 1.802
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4171/ 13000 | global iter:   4171/ 13000 | loss: 1.1937 | ds_loss: 0.0000 | lr: 3.8460e-05 | scale: 65536.0000 | micro time: 1.869 | step time: 0.000
train | epoch   3 | Iter:   4172/ 13000 | global iter:   4172/ 13000 | loss: 1.5449 | ds_loss: 0.0000 | lr: 3.8454e-05 | scale: 65536.0000 | micro time: 1.825 | step time: 0.000
train | epoch   3 | Iter:   4173/ 13000 | global iter:   4173/ 13000 | loss: 1.1744 | ds_loss: 0.0000 | lr: 3.8449e-05 | scale: 65536.0000 | micro time: 1.746 | step time: 0.000
train | epoch   3 | Iter:   4174/ 13000 | global iter:   4174/ 13000 | loss: 1.9896 | ds_loss: 0.0000 | lr: 3.8444e-05 | scale: 65536.0000 | micro time: 1.828 | step time: 0.000
train | epoch   3 | Iter:   4175/ 13000 | global iter:   4175/ 13000 | loss: 1.2340 | ds_loss: 0.0000 | lr: 3.8439e-05 | scale: 65536.0000 | micro time: 1.859 | step time: 0.000
train | epoch   3 | Iter:   4176/ 13000 | global iter:   4176/ 13000 | loss: 1.4725 | ds_loss: 0.0000 | lr: 3.8434e-05 | scale: 65536.0000 | micro time: 1.857 | step time: 0.000
train | epoch   3 | Iter:   4177/ 13000 | global iter:   4177/ 13000 | loss: 1.6010 | ds_loss: 0.0000 | lr: 3.8429e-05 | scale: 65536.0000 | micro time: 1.914 | step time: 0.000
train | epoch   3 | Iter:   4178/ 13000 | global iter:   4178/ 13000 | loss: 0.9751 | ds_loss: 0.0000 | lr: 3.8424e-05 | scale: 65536.0000 | micro time: 1.846 | step time: 0.000
train | epoch   3 | Iter:   4179/ 13000 | global iter:   4179/ 13000 | loss: 1.7878 | ds_loss: 0.0000 | lr: 3.8419e-05 | scale: 65536.0000 | micro time: 1.780 | step time: 0.000
train | epoch   3 | Iter:   4180/ 13000 | global iter:   4180/ 13000 | loss: 1.2493 | ds_loss: 0.0000 | lr: 3.8414e-05 | scale: 65536.0000 | micro time: 1.761 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4180/ 13000 | global iter:   4180/ 13000 | loss: 1.4222 | ds_loss: 0.0000 | lr: 3.8414e-05 | scale: 65536.0000 | micro time: 1.761 | step time: 1.829
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
[2025-04-19 13:26:42,060] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
train | epoch   3 | Iter:   4181/ 13000 | global iter:   4181/ 13000 | loss: 1.8104 | ds_loss: 0.0000 | lr: 3.8414e-05 | scale: 32768.0000 | micro time: 1.491 | step time: 0.000
train | epoch   3 | Iter:   4182/ 13000 | global iter:   4182/ 13000 | loss: 1.2519 | ds_loss: 0.0000 | lr: 3.8409e-05 | scale: 32768.0000 | micro time: 1.806 | step time: 0.000
train | epoch   3 | Iter:   4183/ 13000 | global iter:   4183/ 13000 | loss: 1.7355 | ds_loss: 0.0000 | lr: 3.8404e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   3 | Iter:   4184/ 13000 | global iter:   4184/ 13000 | loss: 1.2680 | ds_loss: 0.0000 | lr: 3.8399e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   3 | Iter:   4185/ 13000 | global iter:   4185/ 13000 | loss: 1.7059 | ds_loss: 0.0000 | lr: 3.8393e-05 | scale: 32768.0000 | micro time: 1.707 | step time: 0.000
train | epoch   3 | Iter:   4186/ 13000 | global iter:   4186/ 13000 | loss: 1.7565 | ds_loss: 0.0000 | lr: 3.8388e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   3 | Iter:   4187/ 13000 | global iter:   4187/ 13000 | loss: 1.0373 | ds_loss: 0.0000 | lr: 3.8383e-05 | scale: 32768.0000 | micro time: 1.816 | step time: 0.000
train | epoch   3 | Iter:   4188/ 13000 | global iter:   4188/ 13000 | loss: 1.3534 | ds_loss: 0.0000 | lr: 3.8378e-05 | scale: 32768.0000 | micro time: 1.786 | step time: 0.000
train | epoch   3 | Iter:   4189/ 13000 | global iter:   4189/ 13000 | loss: 1.7769 | ds_loss: 0.0000 | lr: 3.8373e-05 | scale: 32768.0000 | micro time: 1.743 | step time: 0.000
train | epoch   3 | Iter:   4190/ 13000 | global iter:   4190/ 13000 | loss: 1.6341 | ds_loss: 0.0000 | lr: 3.8368e-05 | scale: 32768.0000 | micro time: 1.804 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4190/ 13000 | global iter:   4190/ 13000 | loss: 1.5330 | ds_loss: 0.0000 | lr: 3.8368e-05 | scale: 32768.0000 | micro time: 1.804 | step time: 1.768
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4191/ 13000 | global iter:   4191/ 13000 | loss: 1.5059 | ds_loss: 0.0000 | lr: 3.8363e-05 | scale: 32768.0000 | micro time: 1.784 | step time: 0.000
train | epoch   3 | Iter:   4192/ 13000 | global iter:   4192/ 13000 | loss: 2.0846 | ds_loss: 0.0000 | lr: 3.8358e-05 | scale: 32768.0000 | micro time: 1.736 | step time: 0.000
train | epoch   3 | Iter:   4193/ 13000 | global iter:   4193/ 13000 | loss: 1.2792 | ds_loss: 0.0000 | lr: 3.8353e-05 | scale: 32768.0000 | micro time: 1.842 | step time: 0.000
train | epoch   3 | Iter:   4194/ 13000 | global iter:   4194/ 13000 | loss: 1.7629 | ds_loss: 0.0000 | lr: 3.8348e-05 | scale: 32768.0000 | micro time: 1.685 | step time: 0.000
train | epoch   3 | Iter:   4195/ 13000 | global iter:   4195/ 13000 | loss: 0.8394 | ds_loss: 0.0000 | lr: 3.8342e-05 | scale: 32768.0000 | micro time: 1.852 | step time: 0.000
train | epoch   3 | Iter:   4196/ 13000 | global iter:   4196/ 13000 | loss: 1.3571 | ds_loss: 0.0000 | lr: 3.8337e-05 | scale: 32768.0000 | micro time: 1.865 | step time: 0.000
train | epoch   3 | Iter:   4197/ 13000 | global iter:   4197/ 13000 | loss: 1.5764 | ds_loss: 0.0000 | lr: 3.8332e-05 | scale: 32768.0000 | micro time: 1.800 | step time: 0.000
train | epoch   3 | Iter:   4198/ 13000 | global iter:   4198/ 13000 | loss: 1.7925 | ds_loss: 0.0000 | lr: 3.8327e-05 | scale: 32768.0000 | micro time: 1.801 | step time: 0.000
train | epoch   3 | Iter:   4199/ 13000 | global iter:   4199/ 13000 | loss: 1.4341 | ds_loss: 0.0000 | lr: 3.8322e-05 | scale: 32768.0000 | micro time: 1.837 | step time: 0.000
train | epoch   3 | Iter:   4200/ 13000 | global iter:   4200/ 13000 | loss: 1.5968 | ds_loss: 0.0000 | lr: 3.8317e-05 | scale: 32768.0000 | micro time: 1.841 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4200/ 13000 | global iter:   4200/ 13000 | loss: 1.5229 | ds_loss: 0.0000 | lr: 3.8317e-05 | scale: 32768.0000 | micro time: 1.841 | step time: 1.804
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4201/ 13000 | global iter:   4201/ 13000 | loss: 1.5693 | ds_loss: 0.0000 | lr: 3.8312e-05 | scale: 32768.0000 | micro time: 1.749 | step time: 0.000
train | epoch   3 | Iter:   4202/ 13000 | global iter:   4202/ 13000 | loss: 1.8220 | ds_loss: 0.0000 | lr: 3.8307e-05 | scale: 32768.0000 | micro time: 1.900 | step time: 0.000
train | epoch   3 | Iter:   4203/ 13000 | global iter:   4203/ 13000 | loss: 1.7537 | ds_loss: 0.0000 | lr: 3.8302e-05 | scale: 32768.0000 | micro time: 1.830 | step time: 0.000
train | epoch   3 | Iter:   4204/ 13000 | global iter:   4204/ 13000 | loss: 1.8361 | ds_loss: 0.0000 | lr: 3.8296e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 0.000
train | epoch   3 | Iter:   4205/ 13000 | global iter:   4205/ 13000 | loss: 1.4024 | ds_loss: 0.0000 | lr: 3.8291e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
train | epoch   3 | Iter:   4206/ 13000 | global iter:   4206/ 13000 | loss: 1.6972 | ds_loss: 0.0000 | lr: 3.8286e-05 | scale: 32768.0000 | micro time: 1.813 | step time: 0.000
train | epoch   3 | Iter:   4207/ 13000 | global iter:   4207/ 13000 | loss: 1.3097 | ds_loss: 0.0000 | lr: 3.8281e-05 | scale: 32768.0000 | micro time: 1.743 | step time: 0.000
train | epoch   3 | Iter:   4208/ 13000 | global iter:   4208/ 13000 | loss: 1.7601 | ds_loss: 0.0000 | lr: 3.8276e-05 | scale: 32768.0000 | micro time: 1.758 | step time: 0.000
train | epoch   3 | Iter:   4209/ 13000 | global iter:   4209/ 13000 | loss: 1.4462 | ds_loss: 0.0000 | lr: 3.8271e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   3 | Iter:   4210/ 13000 | global iter:   4210/ 13000 | loss: 1.8333 | ds_loss: 0.0000 | lr: 3.8266e-05 | scale: 32768.0000 | micro time: 1.804 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4210/ 13000 | global iter:   4210/ 13000 | loss: 1.6430 | ds_loss: 0.0000 | lr: 3.8266e-05 | scale: 32768.0000 | micro time: 1.804 | step time: 1.809
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4211/ 13000 | global iter:   4211/ 13000 | loss: 1.6675 | ds_loss: 0.0000 | lr: 3.8261e-05 | scale: 32768.0000 | micro time: 1.795 | step time: 0.000
train | epoch   3 | Iter:   4212/ 13000 | global iter:   4212/ 13000 | loss: 1.3289 | ds_loss: 0.0000 | lr: 3.8256e-05 | scale: 32768.0000 | micro time: 1.816 | step time: 0.000
train | epoch   3 | Iter:   4213/ 13000 | global iter:   4213/ 13000 | loss: 1.8315 | ds_loss: 0.0000 | lr: 3.8250e-05 | scale: 32768.0000 | micro time: 1.753 | step time: 0.000
train | epoch   3 | Iter:   4214/ 13000 | global iter:   4214/ 13000 | loss: 0.8006 | ds_loss: 0.0000 | lr: 3.8245e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
train | epoch   3 | Iter:   4215/ 13000 | global iter:   4215/ 13000 | loss: 1.5620 | ds_loss: 0.0000 | lr: 3.8240e-05 | scale: 32768.0000 | micro time: 1.743 | step time: 0.000
train | epoch   3 | Iter:   4216/ 13000 | global iter:   4216/ 13000 | loss: 2.2242 | ds_loss: 0.0000 | lr: 3.8235e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   3 | Iter:   4217/ 13000 | global iter:   4217/ 13000 | loss: 1.7206 | ds_loss: 0.0000 | lr: 3.8230e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   3 | Iter:   4218/ 13000 | global iter:   4218/ 13000 | loss: 1.8990 | ds_loss: 0.0000 | lr: 3.8225e-05 | scale: 32768.0000 | micro time: 1.899 | step time: 0.000
train | epoch   3 | Iter:   4219/ 13000 | global iter:   4219/ 13000 | loss: 1.1183 | ds_loss: 0.0000 | lr: 3.8220e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   3 | Iter:   4220/ 13000 | global iter:   4220/ 13000 | loss: 1.4163 | ds_loss: 0.0000 | lr: 3.8215e-05 | scale: 32768.0000 | micro time: 1.746 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4220/ 13000 | global iter:   4220/ 13000 | loss: 1.5569 | ds_loss: 0.0000 | lr: 3.8215e-05 | scale: 32768.0000 | micro time: 1.746 | step time: 1.804
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4221/ 13000 | global iter:   4221/ 13000 | loss: 1.3475 | ds_loss: 0.0000 | lr: 3.8210e-05 | scale: 32768.0000 | micro time: 1.846 | step time: 0.000
train | epoch   3 | Iter:   4222/ 13000 | global iter:   4222/ 13000 | loss: 1.5846 | ds_loss: 0.0000 | lr: 3.8204e-05 | scale: 32768.0000 | micro time: 1.902 | step time: 0.000
train | epoch   3 | Iter:   4223/ 13000 | global iter:   4223/ 13000 | loss: 1.2681 | ds_loss: 0.0000 | lr: 3.8199e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   3 | Iter:   4224/ 13000 | global iter:   4224/ 13000 | loss: 1.1757 | ds_loss: 0.0000 | lr: 3.8194e-05 | scale: 32768.0000 | micro time: 1.923 | step time: 0.000
train | epoch   3 | Iter:   4225/ 13000 | global iter:   4225/ 13000 | loss: 1.6849 | ds_loss: 0.0000 | lr: 3.8189e-05 | scale: 32768.0000 | micro time: 1.739 | step time: 0.000
train | epoch   3 | Iter:   4226/ 13000 | global iter:   4226/ 13000 | loss: 1.8992 | ds_loss: 0.0000 | lr: 3.8184e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
train | epoch   3 | Iter:   4227/ 13000 | global iter:   4227/ 13000 | loss: 1.7337 | ds_loss: 0.0000 | lr: 3.8179e-05 | scale: 32768.0000 | micro time: 1.869 | step time: 0.000
train | epoch   3 | Iter:   4228/ 13000 | global iter:   4228/ 13000 | loss: 1.7392 | ds_loss: 0.0000 | lr: 3.8174e-05 | scale: 32768.0000 | micro time: 1.897 | step time: 0.000
train | epoch   3 | Iter:   4229/ 13000 | global iter:   4229/ 13000 | loss: 1.1288 | ds_loss: 0.0000 | lr: 3.8168e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   3 | Iter:   4230/ 13000 | global iter:   4230/ 13000 | loss: 0.8773 | ds_loss: 0.0000 | lr: 3.8163e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4230/ 13000 | global iter:   4230/ 13000 | loss: 1.4439 | ds_loss: 0.0000 | lr: 3.8163e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 1.845
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4231/ 13000 | global iter:   4231/ 13000 | loss: 1.3380 | ds_loss: 0.0000 | lr: 3.8158e-05 | scale: 32768.0000 | micro time: 1.790 | step time: 0.000
train | epoch   3 | Iter:   4232/ 13000 | global iter:   4232/ 13000 | loss: 1.7944 | ds_loss: 0.0000 | lr: 3.8153e-05 | scale: 32768.0000 | micro time: 1.919 | step time: 0.000
train | epoch   3 | Iter:   4233/ 13000 | global iter:   4233/ 13000 | loss: 1.1862 | ds_loss: 0.0000 | lr: 3.8148e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   3 | Iter:   4234/ 13000 | global iter:   4234/ 13000 | loss: 1.9516 | ds_loss: 0.0000 | lr: 3.8143e-05 | scale: 32768.0000 | micro time: 1.836 | step time: 0.000
train | epoch   3 | Iter:   4235/ 13000 | global iter:   4235/ 13000 | loss: 1.0022 | ds_loss: 0.0000 | lr: 3.8138e-05 | scale: 32768.0000 | micro time: 1.798 | step time: 0.000
train | epoch   3 | Iter:   4236/ 13000 | global iter:   4236/ 13000 | loss: 1.8503 | ds_loss: 0.0000 | lr: 3.8133e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   3 | Iter:   4237/ 13000 | global iter:   4237/ 13000 | loss: 1.5260 | ds_loss: 0.0000 | lr: 3.8127e-05 | scale: 32768.0000 | micro time: 1.751 | step time: 0.000
train | epoch   3 | Iter:   4238/ 13000 | global iter:   4238/ 13000 | loss: 1.6951 | ds_loss: 0.0000 | lr: 3.8122e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   3 | Iter:   4239/ 13000 | global iter:   4239/ 13000 | loss: 1.7321 | ds_loss: 0.0000 | lr: 3.8117e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   3 | Iter:   4240/ 13000 | global iter:   4240/ 13000 | loss: 1.1013 | ds_loss: 0.0000 | lr: 3.8112e-05 | scale: 32768.0000 | micro time: 1.707 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4240/ 13000 | global iter:   4240/ 13000 | loss: 1.5177 | ds_loss: 0.0000 | lr: 3.8112e-05 | scale: 32768.0000 | micro time: 1.707 | step time: 1.806
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4241/ 13000 | global iter:   4241/ 13000 | loss: 1.8761 | ds_loss: 0.0000 | lr: 3.8107e-05 | scale: 32768.0000 | micro time: 1.793 | step time: 0.000
train | epoch   3 | Iter:   4242/ 13000 | global iter:   4242/ 13000 | loss: 1.6714 | ds_loss: 0.0000 | lr: 3.8102e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   3 | Iter:   4243/ 13000 | global iter:   4243/ 13000 | loss: 1.4239 | ds_loss: 0.0000 | lr: 3.8097e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   3 | Iter:   4244/ 13000 | global iter:   4244/ 13000 | loss: 1.5774 | ds_loss: 0.0000 | lr: 3.8091e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   3 | Iter:   4245/ 13000 | global iter:   4245/ 13000 | loss: 1.2104 | ds_loss: 0.0000 | lr: 3.8086e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   3 | Iter:   4246/ 13000 | global iter:   4246/ 13000 | loss: 1.8097 | ds_loss: 0.0000 | lr: 3.8081e-05 | scale: 32768.0000 | micro time: 1.735 | step time: 0.000
train | epoch   3 | Iter:   4247/ 13000 | global iter:   4247/ 13000 | loss: 1.7502 | ds_loss: 0.0000 | lr: 3.8076e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   3 | Iter:   4248/ 13000 | global iter:   4248/ 13000 | loss: 1.1848 | ds_loss: 0.0000 | lr: 3.8071e-05 | scale: 32768.0000 | micro time: 1.915 | step time: 0.000
train | epoch   3 | Iter:   4249/ 13000 | global iter:   4249/ 13000 | loss: 0.9974 | ds_loss: 0.0000 | lr: 3.8066e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   3 | Iter:   4250/ 13000 | global iter:   4250/ 13000 | loss: 1.9012 | ds_loss: 0.0000 | lr: 3.8061e-05 | scale: 32768.0000 | micro time: 1.761 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4250/ 13000 | global iter:   4250/ 13000 | loss: 1.5403 | ds_loss: 0.0000 | lr: 3.8061e-05 | scale: 32768.0000 | micro time: 1.761 | step time: 1.814
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4251/ 13000 | global iter:   4251/ 13000 | loss: 1.5057 | ds_loss: 0.0000 | lr: 3.8055e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   3 | Iter:   4252/ 13000 | global iter:   4252/ 13000 | loss: 1.3780 | ds_loss: 0.0000 | lr: 3.8050e-05 | scale: 32768.0000 | micro time: 1.739 | step time: 0.000
train | epoch   3 | Iter:   4253/ 13000 | global iter:   4253/ 13000 | loss: 1.8151 | ds_loss: 0.0000 | lr: 3.8045e-05 | scale: 32768.0000 | micro time: 1.845 | step time: 0.000
train | epoch   3 | Iter:   4254/ 13000 | global iter:   4254/ 13000 | loss: 2.1260 | ds_loss: 0.0000 | lr: 3.8040e-05 | scale: 32768.0000 | micro time: 1.808 | step time: 0.000
train | epoch   3 | Iter:   4255/ 13000 | global iter:   4255/ 13000 | loss: 1.9776 | ds_loss: 0.0000 | lr: 3.8035e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   3 | Iter:   4256/ 13000 | global iter:   4256/ 13000 | loss: 1.2968 | ds_loss: 0.0000 | lr: 3.8030e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   3 | Iter:   4257/ 13000 | global iter:   4257/ 13000 | loss: 1.6548 | ds_loss: 0.0000 | lr: 3.8025e-05 | scale: 32768.0000 | micro time: 1.755 | step time: 0.000
train | epoch   3 | Iter:   4258/ 13000 | global iter:   4258/ 13000 | loss: 1.5719 | ds_loss: 0.0000 | lr: 3.8019e-05 | scale: 32768.0000 | micro time: 1.723 | step time: 0.000
train | epoch   3 | Iter:   4259/ 13000 | global iter:   4259/ 13000 | loss: 1.5430 | ds_loss: 0.0000 | lr: 3.8014e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   3 | Iter:   4260/ 13000 | global iter:   4260/ 13000 | loss: 1.4204 | ds_loss: 0.0000 | lr: 3.8009e-05 | scale: 32768.0000 | micro time: 1.801 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4260/ 13000 | global iter:   4260/ 13000 | loss: 1.6289 | ds_loss: 0.0000 | lr: 3.8009e-05 | scale: 32768.0000 | micro time: 1.801 | step time: 1.791
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4261/ 13000 | global iter:   4261/ 13000 | loss: 1.5789 | ds_loss: 0.0000 | lr: 3.8004e-05 | scale: 32768.0000 | micro time: 1.804 | step time: 0.000
train | epoch   3 | Iter:   4262/ 13000 | global iter:   4262/ 13000 | loss: 1.2962 | ds_loss: 0.0000 | lr: 3.7999e-05 | scale: 32768.0000 | micro time: 1.785 | step time: 0.000
train | epoch   3 | Iter:   4263/ 13000 | global iter:   4263/ 13000 | loss: 1.5753 | ds_loss: 0.0000 | lr: 3.7994e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   3 | Iter:   4264/ 13000 | global iter:   4264/ 13000 | loss: 1.3288 | ds_loss: 0.0000 | lr: 3.7989e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   3 | Iter:   4265/ 13000 | global iter:   4265/ 13000 | loss: 1.5032 | ds_loss: 0.0000 | lr: 3.7983e-05 | scale: 32768.0000 | micro time: 1.730 | step time: 0.000
train | epoch   3 | Iter:   4266/ 13000 | global iter:   4266/ 13000 | loss: 1.1788 | ds_loss: 0.0000 | lr: 3.7978e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   3 | Iter:   4267/ 13000 | global iter:   4267/ 13000 | loss: 1.7527 | ds_loss: 0.0000 | lr: 3.7973e-05 | scale: 32768.0000 | micro time: 1.795 | step time: 0.000
train | epoch   3 | Iter:   4268/ 13000 | global iter:   4268/ 13000 | loss: 1.5710 | ds_loss: 0.0000 | lr: 3.7968e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
train | epoch   3 | Iter:   4269/ 13000 | global iter:   4269/ 13000 | loss: 1.4612 | ds_loss: 0.0000 | lr: 3.7963e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   3 | Iter:   4270/ 13000 | global iter:   4270/ 13000 | loss: 1.9193 | ds_loss: 0.0000 | lr: 3.7958e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4270/ 13000 | global iter:   4270/ 13000 | loss: 1.5165 | ds_loss: 0.0000 | lr: 3.7958e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 1.797
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4271/ 13000 | global iter:   4271/ 13000 | loss: 1.9697 | ds_loss: 0.0000 | lr: 3.7952e-05 | scale: 32768.0000 | micro time: 1.832 | step time: 0.000
train | epoch   3 | Iter:   4272/ 13000 | global iter:   4272/ 13000 | loss: 1.4243 | ds_loss: 0.0000 | lr: 3.7947e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   3 | Iter:   4273/ 13000 | global iter:   4273/ 13000 | loss: 1.5208 | ds_loss: 0.0000 | lr: 3.7942e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   3 | Iter:   4274/ 13000 | global iter:   4274/ 13000 | loss: 1.3795 | ds_loss: 0.0000 | lr: 3.7937e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   3 | Iter:   4275/ 13000 | global iter:   4275/ 13000 | loss: 1.5979 | ds_loss: 0.0000 | lr: 3.7932e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   3 | Iter:   4276/ 13000 | global iter:   4276/ 13000 | loss: 1.3302 | ds_loss: 0.0000 | lr: 3.7927e-05 | scale: 32768.0000 | micro time: 1.735 | step time: 0.000
train | epoch   3 | Iter:   4277/ 13000 | global iter:   4277/ 13000 | loss: 1.5955 | ds_loss: 0.0000 | lr: 3.7921e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   3 | Iter:   4278/ 13000 | global iter:   4278/ 13000 | loss: 1.2265 | ds_loss: 0.0000 | lr: 3.7916e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 0.000
train | epoch   3 | Iter:   4279/ 13000 | global iter:   4279/ 13000 | loss: 1.2671 | ds_loss: 0.0000 | lr: 3.7911e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 0.000
train | epoch   3 | Iter:   4280/ 13000 | global iter:   4280/ 13000 | loss: 1.3198 | ds_loss: 0.0000 | lr: 3.7906e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4280/ 13000 | global iter:   4280/ 13000 | loss: 1.4631 | ds_loss: 0.0000 | lr: 3.7906e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 1.806
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4281/ 13000 | global iter:   4281/ 13000 | loss: 1.8053 | ds_loss: 0.0000 | lr: 3.7901e-05 | scale: 32768.0000 | micro time: 1.829 | step time: 0.000
train | epoch   3 | Iter:   4282/ 13000 | global iter:   4282/ 13000 | loss: 1.9529 | ds_loss: 0.0000 | lr: 3.7896e-05 | scale: 32768.0000 | micro time: 1.728 | step time: 0.000
train | epoch   3 | Iter:   4283/ 13000 | global iter:   4283/ 13000 | loss: 2.2808 | ds_loss: 0.0000 | lr: 3.7890e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   3 | Iter:   4284/ 13000 | global iter:   4284/ 13000 | loss: 1.5745 | ds_loss: 0.0000 | lr: 3.7885e-05 | scale: 32768.0000 | micro time: 1.901 | step time: 0.000
train | epoch   3 | Iter:   4285/ 13000 | global iter:   4285/ 13000 | loss: 0.7602 | ds_loss: 0.0000 | lr: 3.7880e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
train | epoch   3 | Iter:   4286/ 13000 | global iter:   4286/ 13000 | loss: 1.4127 | ds_loss: 0.0000 | lr: 3.7875e-05 | scale: 32768.0000 | micro time: 1.897 | step time: 0.000
train | epoch   3 | Iter:   4287/ 13000 | global iter:   4287/ 13000 | loss: 1.8604 | ds_loss: 0.0000 | lr: 3.7870e-05 | scale: 32768.0000 | micro time: 1.778 | step time: 0.000
train | epoch   3 | Iter:   4288/ 13000 | global iter:   4288/ 13000 | loss: 1.1014 | ds_loss: 0.0000 | lr: 3.7865e-05 | scale: 32768.0000 | micro time: 1.774 | step time: 0.000
train | epoch   3 | Iter:   4289/ 13000 | global iter:   4289/ 13000 | loss: 2.0033 | ds_loss: 0.0000 | lr: 3.7859e-05 | scale: 32768.0000 | micro time: 1.802 | step time: 0.000
train | epoch   3 | Iter:   4290/ 13000 | global iter:   4290/ 13000 | loss: 1.2097 | ds_loss: 0.0000 | lr: 3.7854e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4290/ 13000 | global iter:   4290/ 13000 | loss: 1.5961 | ds_loss: 0.0000 | lr: 3.7854e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 1.812
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4291/ 13000 | global iter:   4291/ 13000 | loss: 1.3450 | ds_loss: 0.0000 | lr: 3.7849e-05 | scale: 32768.0000 | micro time: 1.834 | step time: 0.000
train | epoch   3 | Iter:   4292/ 13000 | global iter:   4292/ 13000 | loss: 1.5749 | ds_loss: 0.0000 | lr: 3.7844e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   3 | Iter:   4293/ 13000 | global iter:   4293/ 13000 | loss: 1.5676 | ds_loss: 0.0000 | lr: 3.7839e-05 | scale: 32768.0000 | micro time: 1.822 | step time: 0.000
train | epoch   3 | Iter:   4294/ 13000 | global iter:   4294/ 13000 | loss: 1.3162 | ds_loss: 0.0000 | lr: 3.7834e-05 | scale: 32768.0000 | micro time: 1.852 | step time: 0.000
train | epoch   3 | Iter:   4295/ 13000 | global iter:   4295/ 13000 | loss: 1.4248 | ds_loss: 0.0000 | lr: 3.7828e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   3 | Iter:   4296/ 13000 | global iter:   4296/ 13000 | loss: 1.1447 | ds_loss: 0.0000 | lr: 3.7823e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   3 | Iter:   4297/ 13000 | global iter:   4297/ 13000 | loss: 1.6251 | ds_loss: 0.0000 | lr: 3.7818e-05 | scale: 32768.0000 | micro time: 1.891 | step time: 0.000
train | epoch   3 | Iter:   4298/ 13000 | global iter:   4298/ 13000 | loss: 1.8991 | ds_loss: 0.0000 | lr: 3.7813e-05 | scale: 32768.0000 | micro time: 1.836 | step time: 0.000
train | epoch   3 | Iter:   4299/ 13000 | global iter:   4299/ 13000 | loss: 1.1702 | ds_loss: 0.0000 | lr: 3.7808e-05 | scale: 32768.0000 | micro time: 1.789 | step time: 0.000
train | epoch   3 | Iter:   4300/ 13000 | global iter:   4300/ 13000 | loss: 1.5263 | ds_loss: 0.0000 | lr: 3.7802e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4300/ 13000 | global iter:   4300/ 13000 | loss: 1.4594 | ds_loss: 0.0000 | lr: 3.7802e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 1.836
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4301/ 13000 | global iter:   4301/ 13000 | loss: 1.7881 | ds_loss: 0.0000 | lr: 3.7797e-05 | scale: 32768.0000 | micro time: 1.802 | step time: 0.000
train | epoch   3 | Iter:   4302/ 13000 | global iter:   4302/ 13000 | loss: 1.2973 | ds_loss: 0.0000 | lr: 3.7792e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 0.000
train | epoch   3 | Iter:   4303/ 13000 | global iter:   4303/ 13000 | loss: 1.2948 | ds_loss: 0.0000 | lr: 3.7787e-05 | scale: 32768.0000 | micro time: 1.886 | step time: 0.000
train | epoch   3 | Iter:   4304/ 13000 | global iter:   4304/ 13000 | loss: 1.7827 | ds_loss: 0.0000 | lr: 3.7782e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   3 | Iter:   4305/ 13000 | global iter:   4305/ 13000 | loss: 1.5307 | ds_loss: 0.0000 | lr: 3.7777e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   3 | Iter:   4306/ 13000 | global iter:   4306/ 13000 | loss: 0.8713 | ds_loss: 0.0000 | lr: 3.7771e-05 | scale: 32768.0000 | micro time: 1.795 | step time: 0.000
train | epoch   3 | Iter:   4307/ 13000 | global iter:   4307/ 13000 | loss: 1.5305 | ds_loss: 0.0000 | lr: 3.7766e-05 | scale: 32768.0000 | micro time: 1.778 | step time: 0.000
train | epoch   3 | Iter:   4308/ 13000 | global iter:   4308/ 13000 | loss: 1.3854 | ds_loss: 0.0000 | lr: 3.7761e-05 | scale: 32768.0000 | micro time: 1.796 | step time: 0.000
train | epoch   3 | Iter:   4309/ 13000 | global iter:   4309/ 13000 | loss: 1.2289 | ds_loss: 0.0000 | lr: 3.7756e-05 | scale: 32768.0000 | micro time: 1.873 | step time: 0.000
train | epoch   3 | Iter:   4310/ 13000 | global iter:   4310/ 13000 | loss: 1.3134 | ds_loss: 0.0000 | lr: 3.7751e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4310/ 13000 | global iter:   4310/ 13000 | loss: 1.4023 | ds_loss: 0.0000 | lr: 3.7751e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 1.831
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4311/ 13000 | global iter:   4311/ 13000 | loss: 1.6641 | ds_loss: 0.0000 | lr: 3.7745e-05 | scale: 32768.0000 | micro time: 1.900 | step time: 0.000
train | epoch   3 | Iter:   4312/ 13000 | global iter:   4312/ 13000 | loss: 1.2145 | ds_loss: 0.0000 | lr: 3.7740e-05 | scale: 32768.0000 | micro time: 1.726 | step time: 0.000
train | epoch   3 | Iter:   4313/ 13000 | global iter:   4313/ 13000 | loss: 1.6410 | ds_loss: 0.0000 | lr: 3.7735e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   3 | Iter:   4314/ 13000 | global iter:   4314/ 13000 | loss: 1.6657 | ds_loss: 0.0000 | lr: 3.7730e-05 | scale: 32768.0000 | micro time: 1.876 | step time: 0.000
train | epoch   3 | Iter:   4315/ 13000 | global iter:   4315/ 13000 | loss: 1.7319 | ds_loss: 0.0000 | lr: 3.7725e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   3 | Iter:   4316/ 13000 | global iter:   4316/ 13000 | loss: 1.4353 | ds_loss: 0.0000 | lr: 3.7719e-05 | scale: 32768.0000 | micro time: 1.820 | step time: 0.000
train | epoch   3 | Iter:   4317/ 13000 | global iter:   4317/ 13000 | loss: 1.7636 | ds_loss: 0.0000 | lr: 3.7714e-05 | scale: 32768.0000 | micro time: 1.900 | step time: 0.000
train | epoch   3 | Iter:   4318/ 13000 | global iter:   4318/ 13000 | loss: 1.6087 | ds_loss: 0.0000 | lr: 3.7709e-05 | scale: 32768.0000 | micro time: 1.660 | step time: 0.000
train | epoch   3 | Iter:   4319/ 13000 | global iter:   4319/ 13000 | loss: 1.4599 | ds_loss: 0.0000 | lr: 3.7704e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   3 | Iter:   4320/ 13000 | global iter:   4320/ 13000 | loss: 1.4035 | ds_loss: 0.0000 | lr: 3.7699e-05 | scale: 32768.0000 | micro time: 1.794 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4320/ 13000 | global iter:   4320/ 13000 | loss: 1.5588 | ds_loss: 0.0000 | lr: 3.7699e-05 | scale: 32768.0000 | micro time: 1.794 | step time: 1.806
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4321/ 13000 | global iter:   4321/ 13000 | loss: 1.4943 | ds_loss: 0.0000 | lr: 3.7693e-05 | scale: 32768.0000 | micro time: 1.853 | step time: 0.000
train | epoch   3 | Iter:   4322/ 13000 | global iter:   4322/ 13000 | loss: 1.1506 | ds_loss: 0.0000 | lr: 3.7688e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   3 | Iter:   4323/ 13000 | global iter:   4323/ 13000 | loss: 1.4790 | ds_loss: 0.0000 | lr: 3.7683e-05 | scale: 32768.0000 | micro time: 1.899 | step time: 0.000
train | epoch   3 | Iter:   4324/ 13000 | global iter:   4324/ 13000 | loss: 1.6401 | ds_loss: 0.0000 | lr: 3.7678e-05 | scale: 32768.0000 | micro time: 1.796 | step time: 0.000
train | epoch   3 | Iter:   4325/ 13000 | global iter:   4325/ 13000 | loss: 1.8355 | ds_loss: 0.0000 | lr: 3.7673e-05 | scale: 32768.0000 | micro time: 1.800 | step time: 0.000
train | epoch   3 | Iter:   4326/ 13000 | global iter:   4326/ 13000 | loss: 1.3726 | ds_loss: 0.0000 | lr: 3.7667e-05 | scale: 32768.0000 | micro time: 1.864 | step time: 0.000
train | epoch   3 | Iter:   4327/ 13000 | global iter:   4327/ 13000 | loss: 1.5527 | ds_loss: 0.0000 | lr: 3.7662e-05 | scale: 32768.0000 | micro time: 1.853 | step time: 0.000
train | epoch   3 | Iter:   4328/ 13000 | global iter:   4328/ 13000 | loss: 1.5700 | ds_loss: 0.0000 | lr: 3.7657e-05 | scale: 32768.0000 | micro time: 1.857 | step time: 0.000
train | epoch   3 | Iter:   4329/ 13000 | global iter:   4329/ 13000 | loss: 1.4099 | ds_loss: 0.0000 | lr: 3.7652e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   3 | Iter:   4330/ 13000 | global iter:   4330/ 13000 | loss: 1.7976 | ds_loss: 0.0000 | lr: 3.7647e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4330/ 13000 | global iter:   4330/ 13000 | loss: 1.5302 | ds_loss: 0.0000 | lr: 3.7647e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 1.842
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4331/ 13000 | global iter:   4331/ 13000 | loss: 1.5217 | ds_loss: 0.0000 | lr: 3.7641e-05 | scale: 32768.0000 | micro time: 1.731 | step time: 0.000
train | epoch   3 | Iter:   4332/ 13000 | global iter:   4332/ 13000 | loss: 1.9372 | ds_loss: 0.0000 | lr: 3.7636e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   3 | Iter:   4333/ 13000 | global iter:   4333/ 13000 | loss: 1.3938 | ds_loss: 0.0000 | lr: 3.7631e-05 | scale: 32768.0000 | micro time: 1.838 | step time: 0.000
train | epoch   3 | Iter:   4334/ 13000 | global iter:   4334/ 13000 | loss: 1.3210 | ds_loss: 0.0000 | lr: 3.7626e-05 | scale: 32768.0000 | micro time: 1.828 | step time: 0.000
train | epoch   3 | Iter:   4335/ 13000 | global iter:   4335/ 13000 | loss: 1.4827 | ds_loss: 0.0000 | lr: 3.7621e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   3 | Iter:   4336/ 13000 | global iter:   4336/ 13000 | loss: 1.6445 | ds_loss: 0.0000 | lr: 3.7615e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   3 | Iter:   4337/ 13000 | global iter:   4337/ 13000 | loss: 1.6339 | ds_loss: 0.0000 | lr: 3.7610e-05 | scale: 32768.0000 | micro time: 1.719 | step time: 0.000
train | epoch   3 | Iter:   4338/ 13000 | global iter:   4338/ 13000 | loss: 1.7147 | ds_loss: 0.0000 | lr: 3.7605e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   3 | Iter:   4339/ 13000 | global iter:   4339/ 13000 | loss: 1.8409 | ds_loss: 0.0000 | lr: 3.7600e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 0.000
train | epoch   3 | Iter:   4340/ 13000 | global iter:   4340/ 13000 | loss: 1.2645 | ds_loss: 0.0000 | lr: 3.7595e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4340/ 13000 | global iter:   4340/ 13000 | loss: 1.5755 | ds_loss: 0.0000 | lr: 3.7595e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 1.813
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4341/ 13000 | global iter:   4341/ 13000 | loss: 2.0832 | ds_loss: 0.0000 | lr: 3.7589e-05 | scale: 32768.0000 | micro time: 1.836 | step time: 0.000
train | epoch   3 | Iter:   4342/ 13000 | global iter:   4342/ 13000 | loss: 1.6472 | ds_loss: 0.0000 | lr: 3.7584e-05 | scale: 32768.0000 | micro time: 1.805 | step time: 0.000
train | epoch   3 | Iter:   4343/ 13000 | global iter:   4343/ 13000 | loss: 1.6066 | ds_loss: 0.0000 | lr: 3.7579e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   3 | Iter:   4344/ 13000 | global iter:   4344/ 13000 | loss: 1.9561 | ds_loss: 0.0000 | lr: 3.7574e-05 | scale: 32768.0000 | micro time: 1.907 | step time: 0.000
train | epoch   3 | Iter:   4345/ 13000 | global iter:   4345/ 13000 | loss: 2.1222 | ds_loss: 0.0000 | lr: 3.7568e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 0.000
train | epoch   3 | Iter:   4346/ 13000 | global iter:   4346/ 13000 | loss: 1.2328 | ds_loss: 0.0000 | lr: 3.7563e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   3 | Iter:   4347/ 13000 | global iter:   4347/ 13000 | loss: 1.8800 | ds_loss: 0.0000 | lr: 3.7558e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   3 | Iter:   4348/ 13000 | global iter:   4348/ 13000 | loss: 1.4538 | ds_loss: 0.0000 | lr: 3.7553e-05 | scale: 32768.0000 | micro time: 1.743 | step time: 0.000
train | epoch   3 | Iter:   4349/ 13000 | global iter:   4349/ 13000 | loss: 2.0494 | ds_loss: 0.0000 | lr: 3.7548e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   3 | Iter:   4350/ 13000 | global iter:   4350/ 13000 | loss: 1.5378 | ds_loss: 0.0000 | lr: 3.7542e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4350/ 13000 | global iter:   4350/ 13000 | loss: 1.7569 | ds_loss: 0.0000 | lr: 3.7542e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 1.826
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4351/ 13000 | global iter:   4351/ 13000 | loss: 1.1556 | ds_loss: 0.0000 | lr: 3.7537e-05 | scale: 32768.0000 | micro time: 1.806 | step time: 0.000
train | epoch   3 | Iter:   4352/ 13000 | global iter:   4352/ 13000 | loss: 2.0087 | ds_loss: 0.0000 | lr: 3.7532e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 0.000
train | epoch   3 | Iter:   4353/ 13000 | global iter:   4353/ 13000 | loss: 1.8771 | ds_loss: 0.0000 | lr: 3.7527e-05 | scale: 32768.0000 | micro time: 1.888 | step time: 0.000
train | epoch   3 | Iter:   4354/ 13000 | global iter:   4354/ 13000 | loss: 1.8107 | ds_loss: 0.0000 | lr: 3.7522e-05 | scale: 32768.0000 | micro time: 1.878 | step time: 0.000
train | epoch   3 | Iter:   4355/ 13000 | global iter:   4355/ 13000 | loss: 1.5465 | ds_loss: 0.0000 | lr: 3.7516e-05 | scale: 32768.0000 | micro time: 1.911 | step time: 0.000
train | epoch   3 | Iter:   4356/ 13000 | global iter:   4356/ 13000 | loss: 1.3187 | ds_loss: 0.0000 | lr: 3.7511e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   3 | Iter:   4357/ 13000 | global iter:   4357/ 13000 | loss: 1.0996 | ds_loss: 0.0000 | lr: 3.7506e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   3 | Iter:   4358/ 13000 | global iter:   4358/ 13000 | loss: 1.5101 | ds_loss: 0.0000 | lr: 3.7501e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   3 | Iter:   4359/ 13000 | global iter:   4359/ 13000 | loss: 1.9173 | ds_loss: 0.0000 | lr: 3.7495e-05 | scale: 32768.0000 | micro time: 1.749 | step time: 0.000
train | epoch   3 | Iter:   4360/ 13000 | global iter:   4360/ 13000 | loss: 1.5629 | ds_loss: 0.0000 | lr: 3.7490e-05 | scale: 32768.0000 | micro time: 1.757 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4360/ 13000 | global iter:   4360/ 13000 | loss: 1.5807 | ds_loss: 0.0000 | lr: 3.7490e-05 | scale: 32768.0000 | micro time: 1.757 | step time: 1.833
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4361/ 13000 | global iter:   4361/ 13000 | loss: 1.9838 | ds_loss: 0.0000 | lr: 3.7485e-05 | scale: 32768.0000 | micro time: 1.796 | step time: 0.000
train | epoch   3 | Iter:   4362/ 13000 | global iter:   4362/ 13000 | loss: 1.4756 | ds_loss: 0.0000 | lr: 3.7480e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   3 | Iter:   4363/ 13000 | global iter:   4363/ 13000 | loss: 1.3633 | ds_loss: 0.0000 | lr: 3.7474e-05 | scale: 32768.0000 | micro time: 1.776 | step time: 0.000
train | epoch   3 | Iter:   4364/ 13000 | global iter:   4364/ 13000 | loss: 1.3727 | ds_loss: 0.0000 | lr: 3.7469e-05 | scale: 32768.0000 | micro time: 1.806 | step time: 0.000
train | epoch   3 | Iter:   4365/ 13000 | global iter:   4365/ 13000 | loss: 1.4721 | ds_loss: 0.0000 | lr: 3.7464e-05 | scale: 32768.0000 | micro time: 1.768 | step time: 0.000
train | epoch   3 | Iter:   4366/ 13000 | global iter:   4366/ 13000 | loss: 2.3741 | ds_loss: 0.0000 | lr: 3.7459e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   3 | Iter:   4367/ 13000 | global iter:   4367/ 13000 | loss: 1.6777 | ds_loss: 0.0000 | lr: 3.7454e-05 | scale: 32768.0000 | micro time: 1.740 | step time: 0.000
train | epoch   3 | Iter:   4368/ 13000 | global iter:   4368/ 13000 | loss: 1.2851 | ds_loss: 0.0000 | lr: 3.7448e-05 | scale: 32768.0000 | micro time: 1.870 | step time: 0.000
train | epoch   3 | Iter:   4369/ 13000 | global iter:   4369/ 13000 | loss: 1.1622 | ds_loss: 0.0000 | lr: 3.7443e-05 | scale: 32768.0000 | micro time: 1.808 | step time: 0.000
train | epoch   3 | Iter:   4370/ 13000 | global iter:   4370/ 13000 | loss: 1.5280 | ds_loss: 0.0000 | lr: 3.7438e-05 | scale: 32768.0000 | micro time: 1.806 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4370/ 13000 | global iter:   4370/ 13000 | loss: 1.5695 | ds_loss: 0.0000 | lr: 3.7438e-05 | scale: 32768.0000 | micro time: 1.806 | step time: 1.800
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4371/ 13000 | global iter:   4371/ 13000 | loss: 1.7283 | ds_loss: 0.0000 | lr: 3.7433e-05 | scale: 32768.0000 | micro time: 1.717 | step time: 0.000
train | epoch   3 | Iter:   4372/ 13000 | global iter:   4372/ 13000 | loss: 1.4618 | ds_loss: 0.0000 | lr: 3.7427e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   3 | Iter:   4373/ 13000 | global iter:   4373/ 13000 | loss: 1.6783 | ds_loss: 0.0000 | lr: 3.7422e-05 | scale: 32768.0000 | micro time: 1.891 | step time: 0.000
train | epoch   3 | Iter:   4374/ 13000 | global iter:   4374/ 13000 | loss: 1.6546 | ds_loss: 0.0000 | lr: 3.7417e-05 | scale: 32768.0000 | micro time: 1.661 | step time: 0.000
train | epoch   3 | Iter:   4375/ 13000 | global iter:   4375/ 13000 | loss: 1.8176 | ds_loss: 0.0000 | lr: 3.7412e-05 | scale: 32768.0000 | micro time: 1.753 | step time: 0.000
train | epoch   3 | Iter:   4376/ 13000 | global iter:   4376/ 13000 | loss: 1.6565 | ds_loss: 0.0000 | lr: 3.7406e-05 | scale: 32768.0000 | micro time: 1.825 | step time: 0.000
train | epoch   3 | Iter:   4377/ 13000 | global iter:   4377/ 13000 | loss: 1.7893 | ds_loss: 0.0000 | lr: 3.7401e-05 | scale: 32768.0000 | micro time: 1.665 | step time: 0.000
train | epoch   3 | Iter:   4378/ 13000 | global iter:   4378/ 13000 | loss: 1.6554 | ds_loss: 0.0000 | lr: 3.7396e-05 | scale: 32768.0000 | micro time: 1.751 | step time: 0.000
train | epoch   3 | Iter:   4379/ 13000 | global iter:   4379/ 13000 | loss: 1.4885 | ds_loss: 0.0000 | lr: 3.7391e-05 | scale: 32768.0000 | micro time: 1.830 | step time: 0.000
train | epoch   3 | Iter:   4380/ 13000 | global iter:   4380/ 13000 | loss: 1.3407 | ds_loss: 0.0000 | lr: 3.7385e-05 | scale: 32768.0000 | micro time: 1.857 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4380/ 13000 | global iter:   4380/ 13000 | loss: 1.6271 | ds_loss: 0.0000 | lr: 3.7385e-05 | scale: 32768.0000 | micro time: 1.857 | step time: 1.779
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4381/ 13000 | global iter:   4381/ 13000 | loss: 1.7798 | ds_loss: 0.0000 | lr: 3.7380e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   3 | Iter:   4382/ 13000 | global iter:   4382/ 13000 | loss: 2.2080 | ds_loss: 0.0000 | lr: 3.7375e-05 | scale: 32768.0000 | micro time: 1.888 | step time: 0.000
train | epoch   3 | Iter:   4383/ 13000 | global iter:   4383/ 13000 | loss: 1.5121 | ds_loss: 0.0000 | lr: 3.7370e-05 | scale: 32768.0000 | micro time: 1.886 | step time: 0.000
train | epoch   3 | Iter:   4384/ 13000 | global iter:   4384/ 13000 | loss: 1.7006 | ds_loss: 0.0000 | lr: 3.7365e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   3 | Iter:   4385/ 13000 | global iter:   4385/ 13000 | loss: 1.6938 | ds_loss: 0.0000 | lr: 3.7359e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   3 | Iter:   4386/ 13000 | global iter:   4386/ 13000 | loss: 2.0117 | ds_loss: 0.0000 | lr: 3.7354e-05 | scale: 32768.0000 | micro time: 1.765 | step time: 0.000
train | epoch   3 | Iter:   4387/ 13000 | global iter:   4387/ 13000 | loss: 1.8407 | ds_loss: 0.0000 | lr: 3.7349e-05 | scale: 32768.0000 | micro time: 1.861 | step time: 0.000
train | epoch   3 | Iter:   4388/ 13000 | global iter:   4388/ 13000 | loss: 1.3903 | ds_loss: 0.0000 | lr: 3.7344e-05 | scale: 32768.0000 | micro time: 1.912 | step time: 0.000
train | epoch   3 | Iter:   4389/ 13000 | global iter:   4389/ 13000 | loss: 1.2828 | ds_loss: 0.0000 | lr: 3.7338e-05 | scale: 32768.0000 | micro time: 1.853 | step time: 0.000
train | epoch   3 | Iter:   4390/ 13000 | global iter:   4390/ 13000 | loss: 1.5600 | ds_loss: 0.0000 | lr: 3.7333e-05 | scale: 32768.0000 | micro time: 1.849 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4390/ 13000 | global iter:   4390/ 13000 | loss: 1.6980 | ds_loss: 0.0000 | lr: 3.7333e-05 | scale: 32768.0000 | micro time: 1.849 | step time: 1.851
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4391/ 13000 | global iter:   4391/ 13000 | loss: 1.3743 | ds_loss: 0.0000 | lr: 3.7328e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   3 | Iter:   4392/ 13000 | global iter:   4392/ 13000 | loss: 0.7749 | ds_loss: 0.0000 | lr: 3.7323e-05 | scale: 32768.0000 | micro time: 1.672 | step time: 0.000
train | epoch   3 | Iter:   4393/ 13000 | global iter:   4393/ 13000 | loss: 1.9045 | ds_loss: 0.0000 | lr: 3.7317e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 0.000
train | epoch   3 | Iter:   4394/ 13000 | global iter:   4394/ 13000 | loss: 1.6292 | ds_loss: 0.0000 | lr: 3.7312e-05 | scale: 32768.0000 | micro time: 1.806 | step time: 0.000
train | epoch   3 | Iter:   4395/ 13000 | global iter:   4395/ 13000 | loss: 1.8723 | ds_loss: 0.0000 | lr: 3.7307e-05 | scale: 32768.0000 | micro time: 1.711 | step time: 0.000
train | epoch   3 | Iter:   4396/ 13000 | global iter:   4396/ 13000 | loss: 1.2224 | ds_loss: 0.0000 | lr: 3.7302e-05 | scale: 32768.0000 | micro time: 1.816 | step time: 0.000
train | epoch   3 | Iter:   4397/ 13000 | global iter:   4397/ 13000 | loss: 1.5202 | ds_loss: 0.0000 | lr: 3.7296e-05 | scale: 32768.0000 | micro time: 1.837 | step time: 0.000
train | epoch   3 | Iter:   4398/ 13000 | global iter:   4398/ 13000 | loss: 1.1269 | ds_loss: 0.0000 | lr: 3.7291e-05 | scale: 32768.0000 | micro time: 1.919 | step time: 0.000
train | epoch   3 | Iter:   4399/ 13000 | global iter:   4399/ 13000 | loss: 2.0581 | ds_loss: 0.0000 | lr: 3.7286e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   3 | Iter:   4400/ 13000 | global iter:   4400/ 13000 | loss: 1.9572 | ds_loss: 0.0000 | lr: 3.7281e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4400/ 13000 | global iter:   4400/ 13000 | loss: 1.5440 | ds_loss: 0.0000 | lr: 3.7281e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 1.805
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4401/ 13000 | global iter:   4401/ 13000 | loss: 1.5216 | ds_loss: 0.0000 | lr: 3.7275e-05 | scale: 32768.0000 | micro time: 1.861 | step time: 0.000
train | epoch   3 | Iter:   4402/ 13000 | global iter:   4402/ 13000 | loss: 1.8153 | ds_loss: 0.0000 | lr: 3.7270e-05 | scale: 32768.0000 | micro time: 1.695 | step time: 0.000
train | epoch   3 | Iter:   4403/ 13000 | global iter:   4403/ 13000 | loss: 1.6626 | ds_loss: 0.0000 | lr: 3.7265e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   3 | Iter:   4404/ 13000 | global iter:   4404/ 13000 | loss: 2.5889 | ds_loss: 0.0000 | lr: 3.7260e-05 | scale: 32768.0000 | micro time: 1.860 | step time: 0.000
train | epoch   3 | Iter:   4405/ 13000 | global iter:   4405/ 13000 | loss: 2.1898 | ds_loss: 0.0000 | lr: 3.7254e-05 | scale: 32768.0000 | micro time: 1.789 | step time: 0.000
train | epoch   3 | Iter:   4406/ 13000 | global iter:   4406/ 13000 | loss: 1.2910 | ds_loss: 0.0000 | lr: 3.7249e-05 | scale: 32768.0000 | micro time: 1.915 | step time: 0.000
train | epoch   3 | Iter:   4407/ 13000 | global iter:   4407/ 13000 | loss: 1.5961 | ds_loss: 0.0000 | lr: 3.7244e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   3 | Iter:   4408/ 13000 | global iter:   4408/ 13000 | loss: 1.3208 | ds_loss: 0.0000 | lr: 3.7238e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   3 | Iter:   4409/ 13000 | global iter:   4409/ 13000 | loss: 1.5076 | ds_loss: 0.0000 | lr: 3.7233e-05 | scale: 32768.0000 | micro time: 1.679 | step time: 0.000
train | epoch   3 | Iter:   4410/ 13000 | global iter:   4410/ 13000 | loss: 1.0179 | ds_loss: 0.0000 | lr: 3.7228e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4410/ 13000 | global iter:   4410/ 13000 | loss: 1.6512 | ds_loss: 0.0000 | lr: 3.7228e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 1.793
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4411/ 13000 | global iter:   4411/ 13000 | loss: 0.8070 | ds_loss: 0.0000 | lr: 3.7223e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   3 | Iter:   4412/ 13000 | global iter:   4412/ 13000 | loss: 1.2336 | ds_loss: 0.0000 | lr: 3.7217e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   3 | Iter:   4413/ 13000 | global iter:   4413/ 13000 | loss: 1.6692 | ds_loss: 0.0000 | lr: 3.7212e-05 | scale: 32768.0000 | micro time: 1.869 | step time: 0.000
train | epoch   3 | Iter:   4414/ 13000 | global iter:   4414/ 13000 | loss: 1.8780 | ds_loss: 0.0000 | lr: 3.7207e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 0.000
train | epoch   3 | Iter:   4415/ 13000 | global iter:   4415/ 13000 | loss: 2.2558 | ds_loss: 0.0000 | lr: 3.7202e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   3 | Iter:   4416/ 13000 | global iter:   4416/ 13000 | loss: 1.3748 | ds_loss: 0.0000 | lr: 3.7196e-05 | scale: 32768.0000 | micro time: 1.755 | step time: 0.000
train | epoch   3 | Iter:   4417/ 13000 | global iter:   4417/ 13000 | loss: 1.8526 | ds_loss: 0.0000 | lr: 3.7191e-05 | scale: 32768.0000 | micro time: 1.840 | step time: 0.000
train | epoch   3 | Iter:   4418/ 13000 | global iter:   4418/ 13000 | loss: 1.7342 | ds_loss: 0.0000 | lr: 3.7186e-05 | scale: 32768.0000 | micro time: 1.765 | step time: 0.000
train | epoch   3 | Iter:   4419/ 13000 | global iter:   4419/ 13000 | loss: 1.8369 | ds_loss: 0.0000 | lr: 3.7181e-05 | scale: 32768.0000 | micro time: 1.915 | step time: 0.000
train | epoch   3 | Iter:   4420/ 13000 | global iter:   4420/ 13000 | loss: 1.9711 | ds_loss: 0.0000 | lr: 3.7175e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4420/ 13000 | global iter:   4420/ 13000 | loss: 1.6613 | ds_loss: 0.0000 | lr: 3.7175e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 1.839
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4421/ 13000 | global iter:   4421/ 13000 | loss: 1.7362 | ds_loss: 0.0000 | lr: 3.7170e-05 | scale: 32768.0000 | micro time: 1.802 | step time: 0.000
train | epoch   3 | Iter:   4422/ 13000 | global iter:   4422/ 13000 | loss: 1.5635 | ds_loss: 0.0000 | lr: 3.7165e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   3 | Iter:   4423/ 13000 | global iter:   4423/ 13000 | loss: 1.5144 | ds_loss: 0.0000 | lr: 3.7159e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   3 | Iter:   4424/ 13000 | global iter:   4424/ 13000 | loss: 1.1716 | ds_loss: 0.0000 | lr: 3.7154e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   3 | Iter:   4425/ 13000 | global iter:   4425/ 13000 | loss: 1.3071 | ds_loss: 0.0000 | lr: 3.7149e-05 | scale: 32768.0000 | micro time: 1.735 | step time: 0.000
train | epoch   3 | Iter:   4426/ 13000 | global iter:   4426/ 13000 | loss: 1.2219 | ds_loss: 0.0000 | lr: 3.7144e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   3 | Iter:   4427/ 13000 | global iter:   4427/ 13000 | loss: 1.9176 | ds_loss: 0.0000 | lr: 3.7138e-05 | scale: 32768.0000 | micro time: 1.907 | step time: 0.000
train | epoch   3 | Iter:   4428/ 13000 | global iter:   4428/ 13000 | loss: 0.9166 | ds_loss: 0.0000 | lr: 3.7133e-05 | scale: 32768.0000 | micro time: 1.782 | step time: 0.000
train | epoch   3 | Iter:   4429/ 13000 | global iter:   4429/ 13000 | loss: 1.5757 | ds_loss: 0.0000 | lr: 3.7128e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 0.000
train | epoch   3 | Iter:   4430/ 13000 | global iter:   4430/ 13000 | loss: 1.6879 | ds_loss: 0.0000 | lr: 3.7123e-05 | scale: 32768.0000 | micro time: 1.852 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4430/ 13000 | global iter:   4430/ 13000 | loss: 1.4612 | ds_loss: 0.0000 | lr: 3.7123e-05 | scale: 32768.0000 | micro time: 1.852 | step time: 1.820
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4431/ 13000 | global iter:   4431/ 13000 | loss: 1.4711 | ds_loss: 0.0000 | lr: 3.7117e-05 | scale: 32768.0000 | micro time: 1.924 | step time: 0.000
train | epoch   3 | Iter:   4432/ 13000 | global iter:   4432/ 13000 | loss: 0.8063 | ds_loss: 0.0000 | lr: 3.7112e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   3 | Iter:   4433/ 13000 | global iter:   4433/ 13000 | loss: 1.4030 | ds_loss: 0.0000 | lr: 3.7107e-05 | scale: 32768.0000 | micro time: 1.833 | step time: 0.000
train | epoch   3 | Iter:   4434/ 13000 | global iter:   4434/ 13000 | loss: 1.4227 | ds_loss: 0.0000 | lr: 3.7101e-05 | scale: 32768.0000 | micro time: 1.673 | step time: 0.000
train | epoch   3 | Iter:   4435/ 13000 | global iter:   4435/ 13000 | loss: 1.2985 | ds_loss: 0.0000 | lr: 3.7096e-05 | scale: 32768.0000 | micro time: 1.804 | step time: 0.000
train | epoch   3 | Iter:   4436/ 13000 | global iter:   4436/ 13000 | loss: 2.0270 | ds_loss: 0.0000 | lr: 3.7091e-05 | scale: 32768.0000 | micro time: 1.934 | step time: 0.000
train | epoch   3 | Iter:   4437/ 13000 | global iter:   4437/ 13000 | loss: 1.6613 | ds_loss: 0.0000 | lr: 3.7086e-05 | scale: 32768.0000 | micro time: 1.821 | step time: 0.000
train | epoch   3 | Iter:   4438/ 13000 | global iter:   4438/ 13000 | loss: 1.5791 | ds_loss: 0.0000 | lr: 3.7080e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   3 | Iter:   4439/ 13000 | global iter:   4439/ 13000 | loss: 1.5872 | ds_loss: 0.0000 | lr: 3.7075e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   3 | Iter:   4440/ 13000 | global iter:   4440/ 13000 | loss: 1.6037 | ds_loss: 0.0000 | lr: 3.7070e-05 | scale: 32768.0000 | micro time: 1.719 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4440/ 13000 | global iter:   4440/ 13000 | loss: 1.4860 | ds_loss: 0.0000 | lr: 3.7070e-05 | scale: 32768.0000 | micro time: 1.719 | step time: 1.814
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4441/ 13000 | global iter:   4441/ 13000 | loss: 1.8170 | ds_loss: 0.0000 | lr: 3.7064e-05 | scale: 32768.0000 | micro time: 1.738 | step time: 0.000
train | epoch   3 | Iter:   4442/ 13000 | global iter:   4442/ 13000 | loss: 1.4242 | ds_loss: 0.0000 | lr: 3.7059e-05 | scale: 32768.0000 | micro time: 1.899 | step time: 0.000
train | epoch   3 | Iter:   4443/ 13000 | global iter:   4443/ 13000 | loss: 1.6187 | ds_loss: 0.0000 | lr: 3.7054e-05 | scale: 32768.0000 | micro time: 1.879 | step time: 0.000
train | epoch   3 | Iter:   4444/ 13000 | global iter:   4444/ 13000 | loss: 1.7939 | ds_loss: 0.0000 | lr: 3.7049e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
train | epoch   3 | Iter:   4445/ 13000 | global iter:   4445/ 13000 | loss: 1.4391 | ds_loss: 0.0000 | lr: 3.7043e-05 | scale: 32768.0000 | micro time: 1.790 | step time: 0.000
train | epoch   3 | Iter:   4446/ 13000 | global iter:   4446/ 13000 | loss: 1.9757 | ds_loss: 0.0000 | lr: 3.7038e-05 | scale: 32768.0000 | micro time: 1.711 | step time: 0.000
train | epoch   3 | Iter:   4447/ 13000 | global iter:   4447/ 13000 | loss: 1.5619 | ds_loss: 0.0000 | lr: 3.7033e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   3 | Iter:   4448/ 13000 | global iter:   4448/ 13000 | loss: 1.6035 | ds_loss: 0.0000 | lr: 3.7027e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   3 | Iter:   4449/ 13000 | global iter:   4449/ 13000 | loss: 1.7003 | ds_loss: 0.0000 | lr: 3.7022e-05 | scale: 32768.0000 | micro time: 1.848 | step time: 0.000
train | epoch   3 | Iter:   4450/ 13000 | global iter:   4450/ 13000 | loss: 1.5093 | ds_loss: 0.0000 | lr: 3.7017e-05 | scale: 32768.0000 | micro time: 1.826 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4450/ 13000 | global iter:   4450/ 13000 | loss: 1.6444 | ds_loss: 0.0000 | lr: 3.7017e-05 | scale: 32768.0000 | micro time: 1.826 | step time: 1.819
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4451/ 13000 | global iter:   4451/ 13000 | loss: 1.1067 | ds_loss: 0.0000 | lr: 3.7012e-05 | scale: 32768.0000 | micro time: 1.842 | step time: 0.000
train | epoch   3 | Iter:   4452/ 13000 | global iter:   4452/ 13000 | loss: 1.4247 | ds_loss: 0.0000 | lr: 3.7006e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   3 | Iter:   4453/ 13000 | global iter:   4453/ 13000 | loss: 2.0049 | ds_loss: 0.0000 | lr: 3.7001e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   3 | Iter:   4454/ 13000 | global iter:   4454/ 13000 | loss: 1.8410 | ds_loss: 0.0000 | lr: 3.6996e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   3 | Iter:   4455/ 13000 | global iter:   4455/ 13000 | loss: 1.3498 | ds_loss: 0.0000 | lr: 3.6990e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   3 | Iter:   4456/ 13000 | global iter:   4456/ 13000 | loss: 1.8902 | ds_loss: 0.0000 | lr: 3.6985e-05 | scale: 32768.0000 | micro time: 1.792 | step time: 0.000
train | epoch   3 | Iter:   4457/ 13000 | global iter:   4457/ 13000 | loss: 1.8719 | ds_loss: 0.0000 | lr: 3.6980e-05 | scale: 32768.0000 | micro time: 1.792 | step time: 0.000
train | epoch   3 | Iter:   4458/ 13000 | global iter:   4458/ 13000 | loss: 1.8728 | ds_loss: 0.0000 | lr: 3.6975e-05 | scale: 32768.0000 | micro time: 1.710 | step time: 0.000
train | epoch   3 | Iter:   4459/ 13000 | global iter:   4459/ 13000 | loss: 1.6387 | ds_loss: 0.0000 | lr: 3.6969e-05 | scale: 32768.0000 | micro time: 1.822 | step time: 0.000
train | epoch   3 | Iter:   4460/ 13000 | global iter:   4460/ 13000 | loss: 2.1305 | ds_loss: 0.0000 | lr: 3.6964e-05 | scale: 32768.0000 | micro time: 1.731 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4460/ 13000 | global iter:   4460/ 13000 | loss: 1.7131 | ds_loss: 0.0000 | lr: 3.6964e-05 | scale: 32768.0000 | micro time: 1.731 | step time: 1.801
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4461/ 13000 | global iter:   4461/ 13000 | loss: 1.7485 | ds_loss: 0.0000 | lr: 3.6959e-05 | scale: 32768.0000 | micro time: 1.765 | step time: 0.000
train | epoch   3 | Iter:   4462/ 13000 | global iter:   4462/ 13000 | loss: 1.7849 | ds_loss: 0.0000 | lr: 3.6953e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   3 | Iter:   4463/ 13000 | global iter:   4463/ 13000 | loss: 1.6429 | ds_loss: 0.0000 | lr: 3.6948e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   3 | Iter:   4464/ 13000 | global iter:   4464/ 13000 | loss: 1.6965 | ds_loss: 0.0000 | lr: 3.6943e-05 | scale: 32768.0000 | micro time: 1.921 | step time: 0.000
train | epoch   3 | Iter:   4465/ 13000 | global iter:   4465/ 13000 | loss: 1.2779 | ds_loss: 0.0000 | lr: 3.6937e-05 | scale: 32768.0000 | micro time: 1.865 | step time: 0.000
train | epoch   3 | Iter:   4466/ 13000 | global iter:   4466/ 13000 | loss: 0.9895 | ds_loss: 0.0000 | lr: 3.6932e-05 | scale: 32768.0000 | micro time: 1.798 | step time: 0.000
train | epoch   3 | Iter:   4467/ 13000 | global iter:   4467/ 13000 | loss: 2.0728 | ds_loss: 0.0000 | lr: 3.6927e-05 | scale: 32768.0000 | micro time: 1.748 | step time: 0.000
train | epoch   3 | Iter:   4468/ 13000 | global iter:   4468/ 13000 | loss: 1.9696 | ds_loss: 0.0000 | lr: 3.6922e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   3 | Iter:   4469/ 13000 | global iter:   4469/ 13000 | loss: 0.9941 | ds_loss: 0.0000 | lr: 3.6916e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   3 | Iter:   4470/ 13000 | global iter:   4470/ 13000 | loss: 1.4272 | ds_loss: 0.0000 | lr: 3.6911e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4470/ 13000 | global iter:   4470/ 13000 | loss: 1.5604 | ds_loss: 0.0000 | lr: 3.6911e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 1.814
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4471/ 13000 | global iter:   4471/ 13000 | loss: 1.0926 | ds_loss: 0.0000 | lr: 3.6906e-05 | scale: 32768.0000 | micro time: 1.794 | step time: 0.000
train | epoch   3 | Iter:   4472/ 13000 | global iter:   4472/ 13000 | loss: 1.9085 | ds_loss: 0.0000 | lr: 3.6900e-05 | scale: 32768.0000 | micro time: 1.880 | step time: 0.000
train | epoch   3 | Iter:   4473/ 13000 | global iter:   4473/ 13000 | loss: 1.7133 | ds_loss: 0.0000 | lr: 3.6895e-05 | scale: 32768.0000 | micro time: 1.746 | step time: 0.000
train | epoch   3 | Iter:   4474/ 13000 | global iter:   4474/ 13000 | loss: 1.7539 | ds_loss: 0.0000 | lr: 3.6890e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
train | epoch   3 | Iter:   4475/ 13000 | global iter:   4475/ 13000 | loss: 1.6026 | ds_loss: 0.0000 | lr: 3.6884e-05 | scale: 32768.0000 | micro time: 1.798 | step time: 0.000
train | epoch   3 | Iter:   4476/ 13000 | global iter:   4476/ 13000 | loss: 1.8873 | ds_loss: 0.0000 | lr: 3.6879e-05 | scale: 32768.0000 | micro time: 1.703 | step time: 0.000
train | epoch   3 | Iter:   4477/ 13000 | global iter:   4477/ 13000 | loss: 1.9667 | ds_loss: 0.0000 | lr: 3.6874e-05 | scale: 32768.0000 | micro time: 1.814 | step time: 0.000
train | epoch   3 | Iter:   4478/ 13000 | global iter:   4478/ 13000 | loss: 1.3619 | ds_loss: 0.0000 | lr: 3.6868e-05 | scale: 32768.0000 | micro time: 1.919 | step time: 0.000
train | epoch   3 | Iter:   4479/ 13000 | global iter:   4479/ 13000 | loss: 1.6399 | ds_loss: 0.0000 | lr: 3.6863e-05 | scale: 32768.0000 | micro time: 1.798 | step time: 0.000
train | epoch   3 | Iter:   4480/ 13000 | global iter:   4480/ 13000 | loss: 1.6881 | ds_loss: 0.0000 | lr: 3.6858e-05 | scale: 32768.0000 | micro time: 1.836 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4480/ 13000 | global iter:   4480/ 13000 | loss: 1.6615 | ds_loss: 0.0000 | lr: 3.6858e-05 | scale: 32768.0000 | micro time: 1.836 | step time: 1.806
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4481/ 13000 | global iter:   4481/ 13000 | loss: 1.4961 | ds_loss: 0.0000 | lr: 3.6853e-05 | scale: 32768.0000 | micro time: 1.853 | step time: 0.000
train | epoch   3 | Iter:   4482/ 13000 | global iter:   4482/ 13000 | loss: 1.6364 | ds_loss: 0.0000 | lr: 3.6847e-05 | scale: 32768.0000 | micro time: 1.769 | step time: 0.000
train | epoch   3 | Iter:   4483/ 13000 | global iter:   4483/ 13000 | loss: 1.7518 | ds_loss: 0.0000 | lr: 3.6842e-05 | scale: 32768.0000 | micro time: 1.769 | step time: 0.000
train | epoch   3 | Iter:   4484/ 13000 | global iter:   4484/ 13000 | loss: 1.5120 | ds_loss: 0.0000 | lr: 3.6837e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   3 | Iter:   4485/ 13000 | global iter:   4485/ 13000 | loss: 1.9893 | ds_loss: 0.0000 | lr: 3.6831e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   3 | Iter:   4486/ 13000 | global iter:   4486/ 13000 | loss: 1.2645 | ds_loss: 0.0000 | lr: 3.6826e-05 | scale: 32768.0000 | micro time: 1.723 | step time: 0.000
train | epoch   3 | Iter:   4487/ 13000 | global iter:   4487/ 13000 | loss: 1.8353 | ds_loss: 0.0000 | lr: 3.6821e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   3 | Iter:   4488/ 13000 | global iter:   4488/ 13000 | loss: 1.8295 | ds_loss: 0.0000 | lr: 3.6815e-05 | scale: 32768.0000 | micro time: 1.888 | step time: 0.000
train | epoch   3 | Iter:   4489/ 13000 | global iter:   4489/ 13000 | loss: 1.2218 | ds_loss: 0.0000 | lr: 3.6810e-05 | scale: 32768.0000 | micro time: 1.821 | step time: 0.000
train | epoch   3 | Iter:   4490/ 13000 | global iter:   4490/ 13000 | loss: 1.3556 | ds_loss: 0.0000 | lr: 3.6805e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4490/ 13000 | global iter:   4490/ 13000 | loss: 1.5892 | ds_loss: 0.0000 | lr: 3.6805e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 1.808
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4491/ 13000 | global iter:   4491/ 13000 | loss: 1.8555 | ds_loss: 0.0000 | lr: 3.6799e-05 | scale: 32768.0000 | micro time: 1.833 | step time: 0.000
train | epoch   3 | Iter:   4492/ 13000 | global iter:   4492/ 13000 | loss: 1.3527 | ds_loss: 0.0000 | lr: 3.6794e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   3 | Iter:   4493/ 13000 | global iter:   4493/ 13000 | loss: 1.5104 | ds_loss: 0.0000 | lr: 3.6789e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   3 | Iter:   4494/ 13000 | global iter:   4494/ 13000 | loss: 1.4389 | ds_loss: 0.0000 | lr: 3.6783e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   3 | Iter:   4495/ 13000 | global iter:   4495/ 13000 | loss: 1.1951 | ds_loss: 0.0000 | lr: 3.6778e-05 | scale: 32768.0000 | micro time: 1.711 | step time: 0.000
train | epoch   3 | Iter:   4496/ 13000 | global iter:   4496/ 13000 | loss: 1.5325 | ds_loss: 0.0000 | lr: 3.6773e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   3 | Iter:   4497/ 13000 | global iter:   4497/ 13000 | loss: 1.4117 | ds_loss: 0.0000 | lr: 3.6767e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   3 | Iter:   4498/ 13000 | global iter:   4498/ 13000 | loss: 1.0545 | ds_loss: 0.0000 | lr: 3.6762e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   3 | Iter:   4499/ 13000 | global iter:   4499/ 13000 | loss: 1.3854 | ds_loss: 0.0000 | lr: 3.6757e-05 | scale: 32768.0000 | micro time: 1.864 | step time: 0.000
train | epoch   3 | Iter:   4500/ 13000 | global iter:   4500/ 13000 | loss: 2.1276 | ds_loss: 0.0000 | lr: 3.6751e-05 | scale: 32768.0000 | micro time: 1.840 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4500/ 13000 | global iter:   4500/ 13000 | loss: 1.4864 | ds_loss: 0.0000 | lr: 3.6751e-05 | scale: 32768.0000 | micro time: 1.840 | step time: 1.816
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4501/ 13000 | global iter:   4501/ 13000 | loss: 1.3559 | ds_loss: 0.0000 | lr: 3.6746e-05 | scale: 32768.0000 | micro time: 1.808 | step time: 0.000
train | epoch   3 | Iter:   4502/ 13000 | global iter:   4502/ 13000 | loss: 1.3148 | ds_loss: 0.0000 | lr: 3.6741e-05 | scale: 32768.0000 | micro time: 1.853 | step time: 0.000
train | epoch   3 | Iter:   4503/ 13000 | global iter:   4503/ 13000 | loss: 2.1255 | ds_loss: 0.0000 | lr: 3.6736e-05 | scale: 32768.0000 | micro time: 1.800 | step time: 0.000
train | epoch   3 | Iter:   4504/ 13000 | global iter:   4504/ 13000 | loss: 1.0226 | ds_loss: 0.0000 | lr: 3.6730e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   3 | Iter:   4505/ 13000 | global iter:   4505/ 13000 | loss: 1.8235 | ds_loss: 0.0000 | lr: 3.6725e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   3 | Iter:   4506/ 13000 | global iter:   4506/ 13000 | loss: 1.6773 | ds_loss: 0.0000 | lr: 3.6720e-05 | scale: 32768.0000 | micro time: 1.816 | step time: 0.000
train | epoch   3 | Iter:   4507/ 13000 | global iter:   4507/ 13000 | loss: 1.4772 | ds_loss: 0.0000 | lr: 3.6714e-05 | scale: 32768.0000 | micro time: 1.805 | step time: 0.000
train | epoch   3 | Iter:   4508/ 13000 | global iter:   4508/ 13000 | loss: 1.5912 | ds_loss: 0.0000 | lr: 3.6709e-05 | scale: 32768.0000 | micro time: 1.751 | step time: 0.000
train | epoch   3 | Iter:   4509/ 13000 | global iter:   4509/ 13000 | loss: 2.1368 | ds_loss: 0.0000 | lr: 3.6704e-05 | scale: 32768.0000 | micro time: 1.716 | step time: 0.000
train | epoch   3 | Iter:   4510/ 13000 | global iter:   4510/ 13000 | loss: 1.8175 | ds_loss: 0.0000 | lr: 3.6698e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4510/ 13000 | global iter:   4510/ 13000 | loss: 1.6342 | ds_loss: 0.0000 | lr: 3.6698e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 1.804
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4511/ 13000 | global iter:   4511/ 13000 | loss: 1.2097 | ds_loss: 0.0000 | lr: 3.6693e-05 | scale: 32768.0000 | micro time: 1.740 | step time: 0.000
train | epoch   3 | Iter:   4512/ 13000 | global iter:   4512/ 13000 | loss: 1.5203 | ds_loss: 0.0000 | lr: 3.6688e-05 | scale: 32768.0000 | micro time: 1.752 | step time: 0.000
train | epoch   3 | Iter:   4513/ 13000 | global iter:   4513/ 13000 | loss: 1.6231 | ds_loss: 0.0000 | lr: 3.6682e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   3 | Iter:   4514/ 13000 | global iter:   4514/ 13000 | loss: 1.5850 | ds_loss: 0.0000 | lr: 3.6677e-05 | scale: 32768.0000 | micro time: 1.806 | step time: 0.000
train | epoch   3 | Iter:   4515/ 13000 | global iter:   4515/ 13000 | loss: 1.2723 | ds_loss: 0.0000 | lr: 3.6672e-05 | scale: 32768.0000 | micro time: 1.900 | step time: 0.000
train | epoch   3 | Iter:   4516/ 13000 | global iter:   4516/ 13000 | loss: 1.6642 | ds_loss: 0.0000 | lr: 3.6666e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   3 | Iter:   4517/ 13000 | global iter:   4517/ 13000 | loss: 1.7434 | ds_loss: 0.0000 | lr: 3.6661e-05 | scale: 32768.0000 | micro time: 1.895 | step time: 0.000
train | epoch   3 | Iter:   4518/ 13000 | global iter:   4518/ 13000 | loss: 1.7310 | ds_loss: 0.0000 | lr: 3.6656e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   3 | Iter:   4519/ 13000 | global iter:   4519/ 13000 | loss: 1.7321 | ds_loss: 0.0000 | lr: 3.6650e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   3 | Iter:   4520/ 13000 | global iter:   4520/ 13000 | loss: 1.7798 | ds_loss: 0.0000 | lr: 3.6645e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4520/ 13000 | global iter:   4520/ 13000 | loss: 1.5861 | ds_loss: 0.0000 | lr: 3.6645e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 1.816
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4521/ 13000 | global iter:   4521/ 13000 | loss: 1.6500 | ds_loss: 0.0000 | lr: 3.6640e-05 | scale: 32768.0000 | micro time: 1.872 | step time: 0.000
train | epoch   3 | Iter:   4522/ 13000 | global iter:   4522/ 13000 | loss: 1.7818 | ds_loss: 0.0000 | lr: 3.6634e-05 | scale: 32768.0000 | micro time: 1.880 | step time: 0.000
train | epoch   3 | Iter:   4523/ 13000 | global iter:   4523/ 13000 | loss: 1.6198 | ds_loss: 0.0000 | lr: 3.6629e-05 | scale: 32768.0000 | micro time: 1.800 | step time: 0.000
train | epoch   3 | Iter:   4524/ 13000 | global iter:   4524/ 13000 | loss: 1.1770 | ds_loss: 0.0000 | lr: 3.6623e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   3 | Iter:   4525/ 13000 | global iter:   4525/ 13000 | loss: 2.0556 | ds_loss: 0.0000 | lr: 3.6618e-05 | scale: 32768.0000 | micro time: 1.850 | step time: 0.000
train | epoch   3 | Iter:   4526/ 13000 | global iter:   4526/ 13000 | loss: 1.6865 | ds_loss: 0.0000 | lr: 3.6613e-05 | scale: 32768.0000 | micro time: 1.743 | step time: 0.000
train | epoch   3 | Iter:   4527/ 13000 | global iter:   4527/ 13000 | loss: 1.7063 | ds_loss: 0.0000 | lr: 3.6607e-05 | scale: 32768.0000 | micro time: 1.782 | step time: 0.000
train | epoch   3 | Iter:   4528/ 13000 | global iter:   4528/ 13000 | loss: 1.9183 | ds_loss: 0.0000 | lr: 3.6602e-05 | scale: 32768.0000 | micro time: 1.666 | step time: 0.000
train | epoch   3 | Iter:   4529/ 13000 | global iter:   4529/ 13000 | loss: 1.7700 | ds_loss: 0.0000 | lr: 3.6597e-05 | scale: 32768.0000 | micro time: 1.800 | step time: 0.000
train | epoch   3 | Iter:   4530/ 13000 | global iter:   4530/ 13000 | loss: 1.4919 | ds_loss: 0.0000 | lr: 3.6591e-05 | scale: 32768.0000 | micro time: 1.936 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4530/ 13000 | global iter:   4530/ 13000 | loss: 1.6857 | ds_loss: 0.0000 | lr: 3.6591e-05 | scale: 32768.0000 | micro time: 1.936 | step time: 1.812
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4531/ 13000 | global iter:   4531/ 13000 | loss: 2.0915 | ds_loss: 0.0000 | lr: 3.6586e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   3 | Iter:   4532/ 13000 | global iter:   4532/ 13000 | loss: 1.7574 | ds_loss: 0.0000 | lr: 3.6581e-05 | scale: 32768.0000 | micro time: 1.854 | step time: 0.000
train | epoch   3 | Iter:   4533/ 13000 | global iter:   4533/ 13000 | loss: 2.0153 | ds_loss: 0.0000 | lr: 3.6575e-05 | scale: 32768.0000 | micro time: 1.672 | step time: 0.000
train | epoch   3 | Iter:   4534/ 13000 | global iter:   4534/ 13000 | loss: 1.3459 | ds_loss: 0.0000 | lr: 3.6570e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   3 | Iter:   4535/ 13000 | global iter:   4535/ 13000 | loss: 1.7739 | ds_loss: 0.0000 | lr: 3.6565e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   3 | Iter:   4536/ 13000 | global iter:   4536/ 13000 | loss: 1.4166 | ds_loss: 0.0000 | lr: 3.6559e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   3 | Iter:   4537/ 13000 | global iter:   4537/ 13000 | loss: 1.6146 | ds_loss: 0.0000 | lr: 3.6554e-05 | scale: 32768.0000 | micro time: 1.850 | step time: 0.000
train | epoch   3 | Iter:   4538/ 13000 | global iter:   4538/ 13000 | loss: 1.7344 | ds_loss: 0.0000 | lr: 3.6549e-05 | scale: 32768.0000 | micro time: 1.888 | step time: 0.000
train | epoch   3 | Iter:   4539/ 13000 | global iter:   4539/ 13000 | loss: 1.6935 | ds_loss: 0.0000 | lr: 3.6543e-05 | scale: 32768.0000 | micro time: 1.776 | step time: 0.000
train | epoch   3 | Iter:   4540/ 13000 | global iter:   4540/ 13000 | loss: 1.7378 | ds_loss: 0.0000 | lr: 3.6538e-05 | scale: 32768.0000 | micro time: 1.756 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4540/ 13000 | global iter:   4540/ 13000 | loss: 1.7181 | ds_loss: 0.0000 | lr: 3.6538e-05 | scale: 32768.0000 | micro time: 1.756 | step time: 1.803
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4541/ 13000 | global iter:   4541/ 13000 | loss: 0.8166 | ds_loss: 0.0000 | lr: 3.6533e-05 | scale: 32768.0000 | micro time: 1.778 | step time: 0.000
train | epoch   3 | Iter:   4542/ 13000 | global iter:   4542/ 13000 | loss: 1.9639 | ds_loss: 0.0000 | lr: 3.6527e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   3 | Iter:   4543/ 13000 | global iter:   4543/ 13000 | loss: 1.5072 | ds_loss: 0.0000 | lr: 3.6522e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   3 | Iter:   4544/ 13000 | global iter:   4544/ 13000 | loss: 1.5608 | ds_loss: 0.0000 | lr: 3.6517e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   3 | Iter:   4545/ 13000 | global iter:   4545/ 13000 | loss: 1.8469 | ds_loss: 0.0000 | lr: 3.6511e-05 | scale: 32768.0000 | micro time: 1.789 | step time: 0.000
train | epoch   3 | Iter:   4546/ 13000 | global iter:   4546/ 13000 | loss: 1.2704 | ds_loss: 0.0000 | lr: 3.6506e-05 | scale: 32768.0000 | micro time: 1.844 | step time: 0.000
train | epoch   3 | Iter:   4547/ 13000 | global iter:   4547/ 13000 | loss: 2.0052 | ds_loss: 0.0000 | lr: 3.6500e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   3 | Iter:   4548/ 13000 | global iter:   4548/ 13000 | loss: 1.4658 | ds_loss: 0.0000 | lr: 3.6495e-05 | scale: 32768.0000 | micro time: 1.781 | step time: 0.000
train | epoch   3 | Iter:   4549/ 13000 | global iter:   4549/ 13000 | loss: 1.7860 | ds_loss: 0.0000 | lr: 3.6490e-05 | scale: 32768.0000 | micro time: 1.901 | step time: 0.000
train | epoch   3 | Iter:   4550/ 13000 | global iter:   4550/ 13000 | loss: 1.5227 | ds_loss: 0.0000 | lr: 3.6484e-05 | scale: 32768.0000 | micro time: 1.734 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4550/ 13000 | global iter:   4550/ 13000 | loss: 1.5746 | ds_loss: 0.0000 | lr: 3.6484e-05 | scale: 32768.0000 | micro time: 1.734 | step time: 1.802
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4551/ 13000 | global iter:   4551/ 13000 | loss: 1.8181 | ds_loss: 0.0000 | lr: 3.6479e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   3 | Iter:   4552/ 13000 | global iter:   4552/ 13000 | loss: 1.5715 | ds_loss: 0.0000 | lr: 3.6474e-05 | scale: 32768.0000 | micro time: 1.922 | step time: 0.000
train | epoch   3 | Iter:   4553/ 13000 | global iter:   4553/ 13000 | loss: 1.4881 | ds_loss: 0.0000 | lr: 3.6468e-05 | scale: 32768.0000 | micro time: 1.802 | step time: 0.000
train | epoch   3 | Iter:   4554/ 13000 | global iter:   4554/ 13000 | loss: 1.9540 | ds_loss: 0.0000 | lr: 3.6463e-05 | scale: 32768.0000 | micro time: 1.772 | step time: 0.000
train | epoch   3 | Iter:   4555/ 13000 | global iter:   4555/ 13000 | loss: 1.2308 | ds_loss: 0.0000 | lr: 3.6458e-05 | scale: 32768.0000 | micro time: 1.828 | step time: 0.000
train | epoch   3 | Iter:   4556/ 13000 | global iter:   4556/ 13000 | loss: 1.7492 | ds_loss: 0.0000 | lr: 3.6452e-05 | scale: 32768.0000 | micro time: 1.739 | step time: 0.000
train | epoch   3 | Iter:   4557/ 13000 | global iter:   4557/ 13000 | loss: 1.6090 | ds_loss: 0.0000 | lr: 3.6447e-05 | scale: 32768.0000 | micro time: 1.850 | step time: 0.000
train | epoch   3 | Iter:   4558/ 13000 | global iter:   4558/ 13000 | loss: 1.7900 | ds_loss: 0.0000 | lr: 3.6441e-05 | scale: 32768.0000 | micro time: 1.751 | step time: 0.000
train | epoch   3 | Iter:   4559/ 13000 | global iter:   4559/ 13000 | loss: 1.2746 | ds_loss: 0.0000 | lr: 3.6436e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 0.000
train | epoch   3 | Iter:   4560/ 13000 | global iter:   4560/ 13000 | loss: 1.6242 | ds_loss: 0.0000 | lr: 3.6431e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4560/ 13000 | global iter:   4560/ 13000 | loss: 1.6110 | ds_loss: 0.0000 | lr: 3.6431e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 1.814
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4561/ 13000 | global iter:   4561/ 13000 | loss: 1.2472 | ds_loss: 0.0000 | lr: 3.6425e-05 | scale: 32768.0000 | micro time: 1.761 | step time: 0.000
train | epoch   3 | Iter:   4562/ 13000 | global iter:   4562/ 13000 | loss: 1.5034 | ds_loss: 0.0000 | lr: 3.6420e-05 | scale: 32768.0000 | micro time: 1.772 | step time: 0.000
train | epoch   3 | Iter:   4563/ 13000 | global iter:   4563/ 13000 | loss: 1.3325 | ds_loss: 0.0000 | lr: 3.6415e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   3 | Iter:   4564/ 13000 | global iter:   4564/ 13000 | loss: 1.6949 | ds_loss: 0.0000 | lr: 3.6409e-05 | scale: 32768.0000 | micro time: 1.798 | step time: 0.000
train | epoch   3 | Iter:   4565/ 13000 | global iter:   4565/ 13000 | loss: 1.4155 | ds_loss: 0.0000 | lr: 3.6404e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   3 | Iter:   4566/ 13000 | global iter:   4566/ 13000 | loss: 1.8946 | ds_loss: 0.0000 | lr: 3.6399e-05 | scale: 32768.0000 | micro time: 1.964 | step time: 0.000
train | epoch   3 | Iter:   4567/ 13000 | global iter:   4567/ 13000 | loss: 1.7082 | ds_loss: 0.0000 | lr: 3.6393e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   3 | Iter:   4568/ 13000 | global iter:   4568/ 13000 | loss: 2.0849 | ds_loss: 0.0000 | lr: 3.6388e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   3 | Iter:   4569/ 13000 | global iter:   4569/ 13000 | loss: 1.4831 | ds_loss: 0.0000 | lr: 3.6382e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   3 | Iter:   4570/ 13000 | global iter:   4570/ 13000 | loss: 1.4804 | ds_loss: 0.0000 | lr: 3.6377e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4570/ 13000 | global iter:   4570/ 13000 | loss: 1.5845 | ds_loss: 0.0000 | lr: 3.6377e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 1.828
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4571/ 13000 | global iter:   4571/ 13000 | loss: 1.4168 | ds_loss: 0.0000 | lr: 3.6372e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   3 | Iter:   4572/ 13000 | global iter:   4572/ 13000 | loss: 1.7939 | ds_loss: 0.0000 | lr: 3.6366e-05 | scale: 32768.0000 | micro time: 1.846 | step time: 0.000
train | epoch   3 | Iter:   4573/ 13000 | global iter:   4573/ 13000 | loss: 1.9470 | ds_loss: 0.0000 | lr: 3.6361e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 0.000
train | epoch   3 | Iter:   4574/ 13000 | global iter:   4574/ 13000 | loss: 1.2978 | ds_loss: 0.0000 | lr: 3.6356e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   3 | Iter:   4575/ 13000 | global iter:   4575/ 13000 | loss: 1.5343 | ds_loss: 0.0000 | lr: 3.6350e-05 | scale: 32768.0000 | micro time: 1.848 | step time: 0.000
train | epoch   3 | Iter:   4576/ 13000 | global iter:   4576/ 13000 | loss: 1.0569 | ds_loss: 0.0000 | lr: 3.6345e-05 | scale: 32768.0000 | micro time: 1.738 | step time: 0.000
train | epoch   3 | Iter:   4577/ 13000 | global iter:   4577/ 13000 | loss: 1.7458 | ds_loss: 0.0000 | lr: 3.6339e-05 | scale: 32768.0000 | micro time: 1.837 | step time: 0.000
train | epoch   3 | Iter:   4578/ 13000 | global iter:   4578/ 13000 | loss: 1.3645 | ds_loss: 0.0000 | lr: 3.6334e-05 | scale: 32768.0000 | micro time: 1.723 | step time: 0.000
train | epoch   3 | Iter:   4579/ 13000 | global iter:   4579/ 13000 | loss: 1.3556 | ds_loss: 0.0000 | lr: 3.6329e-05 | scale: 32768.0000 | micro time: 1.769 | step time: 0.000
train | epoch   3 | Iter:   4580/ 13000 | global iter:   4580/ 13000 | loss: 1.5492 | ds_loss: 0.0000 | lr: 3.6323e-05 | scale: 32768.0000 | micro time: 1.864 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4580/ 13000 | global iter:   4580/ 13000 | loss: 1.5062 | ds_loss: 0.0000 | lr: 3.6323e-05 | scale: 32768.0000 | micro time: 1.864 | step time: 1.807
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4581/ 13000 | global iter:   4581/ 13000 | loss: 1.9033 | ds_loss: 0.0000 | lr: 3.6318e-05 | scale: 32768.0000 | micro time: 1.901 | step time: 0.000
train | epoch   3 | Iter:   4582/ 13000 | global iter:   4582/ 13000 | loss: 1.4138 | ds_loss: 0.0000 | lr: 3.6313e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
train | epoch   3 | Iter:   4583/ 13000 | global iter:   4583/ 13000 | loss: 1.7151 | ds_loss: 0.0000 | lr: 3.6307e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   3 | Iter:   4584/ 13000 | global iter:   4584/ 13000 | loss: 1.3109 | ds_loss: 0.0000 | lr: 3.6302e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   3 | Iter:   4585/ 13000 | global iter:   4585/ 13000 | loss: 1.2605 | ds_loss: 0.0000 | lr: 3.6296e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   3 | Iter:   4586/ 13000 | global iter:   4586/ 13000 | loss: 1.5167 | ds_loss: 0.0000 | lr: 3.6291e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   3 | Iter:   4587/ 13000 | global iter:   4587/ 13000 | loss: 1.7186 | ds_loss: 0.0000 | lr: 3.6286e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   3 | Iter:   4588/ 13000 | global iter:   4588/ 13000 | loss: 1.5069 | ds_loss: 0.0000 | lr: 3.6280e-05 | scale: 32768.0000 | micro time: 1.896 | step time: 0.000
train | epoch   3 | Iter:   4589/ 13000 | global iter:   4589/ 13000 | loss: 1.4767 | ds_loss: 0.0000 | lr: 3.6275e-05 | scale: 32768.0000 | micro time: 1.887 | step time: 0.000
train | epoch   3 | Iter:   4590/ 13000 | global iter:   4590/ 13000 | loss: 1.9409 | ds_loss: 0.0000 | lr: 3.6269e-05 | scale: 32768.0000 | micro time: 1.850 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4590/ 13000 | global iter:   4590/ 13000 | loss: 1.5763 | ds_loss: 0.0000 | lr: 3.6269e-05 | scale: 32768.0000 | micro time: 1.850 | step time: 1.836
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4591/ 13000 | global iter:   4591/ 13000 | loss: 1.7937 | ds_loss: 0.0000 | lr: 3.6264e-05 | scale: 32768.0000 | micro time: 1.834 | step time: 0.000
train | epoch   3 | Iter:   4592/ 13000 | global iter:   4592/ 13000 | loss: 1.8082 | ds_loss: 0.0000 | lr: 3.6259e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   3 | Iter:   4593/ 13000 | global iter:   4593/ 13000 | loss: 1.9478 | ds_loss: 0.0000 | lr: 3.6253e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   3 | Iter:   4594/ 13000 | global iter:   4594/ 13000 | loss: 1.8786 | ds_loss: 0.0000 | lr: 3.6248e-05 | scale: 32768.0000 | micro time: 1.874 | step time: 0.000
train | epoch   3 | Iter:   4595/ 13000 | global iter:   4595/ 13000 | loss: 1.6744 | ds_loss: 0.0000 | lr: 3.6243e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   3 | Iter:   4596/ 13000 | global iter:   4596/ 13000 | loss: 1.8695 | ds_loss: 0.0000 | lr: 3.6237e-05 | scale: 32768.0000 | micro time: 1.838 | step time: 0.000
train | epoch   3 | Iter:   4597/ 13000 | global iter:   4597/ 13000 | loss: 1.6838 | ds_loss: 0.0000 | lr: 3.6232e-05 | scale: 32768.0000 | micro time: 1.731 | step time: 0.000
train | epoch   3 | Iter:   4598/ 13000 | global iter:   4598/ 13000 | loss: 1.7571 | ds_loss: 0.0000 | lr: 3.6226e-05 | scale: 32768.0000 | micro time: 1.895 | step time: 0.000
train | epoch   3 | Iter:   4599/ 13000 | global iter:   4599/ 13000 | loss: 1.3053 | ds_loss: 0.0000 | lr: 3.6221e-05 | scale: 32768.0000 | micro time: 1.841 | step time: 0.000
train | epoch   3 | Iter:   4600/ 13000 | global iter:   4600/ 13000 | loss: 1.7887 | ds_loss: 0.0000 | lr: 3.6216e-05 | scale: 32768.0000 | micro time: 1.671 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4600/ 13000 | global iter:   4600/ 13000 | loss: 1.7507 | ds_loss: 0.0000 | lr: 3.6216e-05 | scale: 32768.0000 | micro time: 1.671 | step time: 1.827
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4601/ 13000 | global iter:   4601/ 13000 | loss: 1.5027 | ds_loss: 0.0000 | lr: 3.6210e-05 | scale: 32768.0000 | micro time: 1.719 | step time: 0.000
train | epoch   3 | Iter:   4602/ 13000 | global iter:   4602/ 13000 | loss: 1.2613 | ds_loss: 0.0000 | lr: 3.6205e-05 | scale: 32768.0000 | micro time: 1.747 | step time: 0.000
train | epoch   3 | Iter:   4603/ 13000 | global iter:   4603/ 13000 | loss: 1.9107 | ds_loss: 0.0000 | lr: 3.6199e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   3 | Iter:   4604/ 13000 | global iter:   4604/ 13000 | loss: 1.8766 | ds_loss: 0.0000 | lr: 3.6194e-05 | scale: 32768.0000 | micro time: 1.804 | step time: 0.000
train | epoch   3 | Iter:   4605/ 13000 | global iter:   4605/ 13000 | loss: 1.5602 | ds_loss: 0.0000 | lr: 3.6189e-05 | scale: 32768.0000 | micro time: 1.846 | step time: 0.000
train | epoch   3 | Iter:   4606/ 13000 | global iter:   4606/ 13000 | loss: 1.8182 | ds_loss: 0.0000 | lr: 3.6183e-05 | scale: 32768.0000 | micro time: 1.817 | step time: 0.000
train | epoch   3 | Iter:   4607/ 13000 | global iter:   4607/ 13000 | loss: 1.8002 | ds_loss: 0.0000 | lr: 3.6178e-05 | scale: 32768.0000 | micro time: 1.801 | step time: 0.000
train | epoch   3 | Iter:   4608/ 13000 | global iter:   4608/ 13000 | loss: 1.4683 | ds_loss: 0.0000 | lr: 3.6172e-05 | scale: 32768.0000 | micro time: 1.832 | step time: 0.000
train | epoch   3 | Iter:   4609/ 13000 | global iter:   4609/ 13000 | loss: 1.3544 | ds_loss: 0.0000 | lr: 3.6167e-05 | scale: 32768.0000 | micro time: 1.886 | step time: 0.000
train | epoch   3 | Iter:   4610/ 13000 | global iter:   4610/ 13000 | loss: 1.5041 | ds_loss: 0.0000 | lr: 3.6162e-05 | scale: 32768.0000 | micro time: 1.785 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4610/ 13000 | global iter:   4610/ 13000 | loss: 1.6057 | ds_loss: 0.0000 | lr: 3.6162e-05 | scale: 32768.0000 | micro time: 1.785 | step time: 1.809
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4611/ 13000 | global iter:   4611/ 13000 | loss: 1.6119 | ds_loss: 0.0000 | lr: 3.6156e-05 | scale: 32768.0000 | micro time: 1.720 | step time: 0.000
train | epoch   3 | Iter:   4612/ 13000 | global iter:   4612/ 13000 | loss: 1.8475 | ds_loss: 0.0000 | lr: 3.6151e-05 | scale: 32768.0000 | micro time: 1.747 | step time: 0.000
train | epoch   3 | Iter:   4613/ 13000 | global iter:   4613/ 13000 | loss: 1.9674 | ds_loss: 0.0000 | lr: 3.6145e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   3 | Iter:   4614/ 13000 | global iter:   4614/ 13000 | loss: 1.4951 | ds_loss: 0.0000 | lr: 3.6140e-05 | scale: 32768.0000 | micro time: 1.849 | step time: 0.000
train | epoch   3 | Iter:   4615/ 13000 | global iter:   4615/ 13000 | loss: 1.7902 | ds_loss: 0.0000 | lr: 3.6135e-05 | scale: 32768.0000 | micro time: 1.869 | step time: 0.000
train | epoch   3 | Iter:   4616/ 13000 | global iter:   4616/ 13000 | loss: 1.3868 | ds_loss: 0.0000 | lr: 3.6129e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   3 | Iter:   4617/ 13000 | global iter:   4617/ 13000 | loss: 1.6620 | ds_loss: 0.0000 | lr: 3.6124e-05 | scale: 32768.0000 | micro time: 1.861 | step time: 0.000
train | epoch   3 | Iter:   4618/ 13000 | global iter:   4618/ 13000 | loss: 1.1368 | ds_loss: 0.0000 | lr: 3.6118e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   3 | Iter:   4619/ 13000 | global iter:   4619/ 13000 | loss: 1.7004 | ds_loss: 0.0000 | lr: 3.6113e-05 | scale: 32768.0000 | micro time: 1.792 | step time: 0.000
train | epoch   3 | Iter:   4620/ 13000 | global iter:   4620/ 13000 | loss: 1.7668 | ds_loss: 0.0000 | lr: 3.6108e-05 | scale: 32768.0000 | micro time: 1.844 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4620/ 13000 | global iter:   4620/ 13000 | loss: 1.6365 | ds_loss: 0.0000 | lr: 3.6108e-05 | scale: 32768.0000 | micro time: 1.844 | step time: 1.811
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4621/ 13000 | global iter:   4621/ 13000 | loss: 1.3192 | ds_loss: 0.0000 | lr: 3.6102e-05 | scale: 32768.0000 | micro time: 1.874 | step time: 0.000
train | epoch   3 | Iter:   4622/ 13000 | global iter:   4622/ 13000 | loss: 1.7222 | ds_loss: 0.0000 | lr: 3.6097e-05 | scale: 32768.0000 | micro time: 1.743 | step time: 0.000
train | epoch   3 | Iter:   4623/ 13000 | global iter:   4623/ 13000 | loss: 1.3794 | ds_loss: 0.0000 | lr: 3.6091e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
train | epoch   3 | Iter:   4624/ 13000 | global iter:   4624/ 13000 | loss: 1.4860 | ds_loss: 0.0000 | lr: 3.6086e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   3 | Iter:   4625/ 13000 | global iter:   4625/ 13000 | loss: 2.0450 | ds_loss: 0.0000 | lr: 3.6081e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   3 | Iter:   4626/ 13000 | global iter:   4626/ 13000 | loss: 1.8974 | ds_loss: 0.0000 | lr: 3.6075e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   3 | Iter:   4627/ 13000 | global iter:   4627/ 13000 | loss: 1.4833 | ds_loss: 0.0000 | lr: 3.6070e-05 | scale: 32768.0000 | micro time: 1.879 | step time: 0.000
train | epoch   3 | Iter:   4628/ 13000 | global iter:   4628/ 13000 | loss: 1.6662 | ds_loss: 0.0000 | lr: 3.6064e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   3 | Iter:   4629/ 13000 | global iter:   4629/ 13000 | loss: 1.8036 | ds_loss: 0.0000 | lr: 3.6059e-05 | scale: 32768.0000 | micro time: 1.802 | step time: 0.000
train | epoch   3 | Iter:   4630/ 13000 | global iter:   4630/ 13000 | loss: 1.5716 | ds_loss: 0.0000 | lr: 3.6054e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4630/ 13000 | global iter:   4630/ 13000 | loss: 1.6374 | ds_loss: 0.0000 | lr: 3.6054e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 1.815
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4631/ 13000 | global iter:   4631/ 13000 | loss: 2.1604 | ds_loss: 0.0000 | lr: 3.6048e-05 | scale: 32768.0000 | micro time: 1.790 | step time: 0.000
train | epoch   3 | Iter:   4632/ 13000 | global iter:   4632/ 13000 | loss: 1.5743 | ds_loss: 0.0000 | lr: 3.6043e-05 | scale: 32768.0000 | micro time: 1.891 | step time: 0.000
train | epoch   3 | Iter:   4633/ 13000 | global iter:   4633/ 13000 | loss: 1.3697 | ds_loss: 0.0000 | lr: 3.6037e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   3 | Iter:   4634/ 13000 | global iter:   4634/ 13000 | loss: 1.6205 | ds_loss: 0.0000 | lr: 3.6032e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   3 | Iter:   4635/ 13000 | global iter:   4635/ 13000 | loss: 1.2134 | ds_loss: 0.0000 | lr: 3.6026e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   3 | Iter:   4636/ 13000 | global iter:   4636/ 13000 | loss: 1.7171 | ds_loss: 0.0000 | lr: 3.6021e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   3 | Iter:   4637/ 13000 | global iter:   4637/ 13000 | loss: 1.5314 | ds_loss: 0.0000 | lr: 3.6016e-05 | scale: 32768.0000 | micro time: 1.898 | step time: 0.000
train | epoch   3 | Iter:   4638/ 13000 | global iter:   4638/ 13000 | loss: 1.3708 | ds_loss: 0.0000 | lr: 3.6010e-05 | scale: 32768.0000 | micro time: 1.854 | step time: 0.000
train | epoch   3 | Iter:   4639/ 13000 | global iter:   4639/ 13000 | loss: 2.0183 | ds_loss: 0.0000 | lr: 3.6005e-05 | scale: 32768.0000 | micro time: 1.724 | step time: 0.000
train | epoch   3 | Iter:   4640/ 13000 | global iter:   4640/ 13000 | loss: 1.3612 | ds_loss: 0.0000 | lr: 3.5999e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4640/ 13000 | global iter:   4640/ 13000 | loss: 1.5937 | ds_loss: 0.0000 | lr: 3.5999e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 1.834
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4641/ 13000 | global iter:   4641/ 13000 | loss: 1.9469 | ds_loss: 0.0000 | lr: 3.5994e-05 | scale: 32768.0000 | micro time: 1.920 | step time: 0.000
train | epoch   3 | Iter:   4642/ 13000 | global iter:   4642/ 13000 | loss: 1.9653 | ds_loss: 0.0000 | lr: 3.5989e-05 | scale: 32768.0000 | micro time: 1.817 | step time: 0.000
train | epoch   3 | Iter:   4643/ 13000 | global iter:   4643/ 13000 | loss: 1.5613 | ds_loss: 0.0000 | lr: 3.5983e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   3 | Iter:   4644/ 13000 | global iter:   4644/ 13000 | loss: 1.6244 | ds_loss: 0.0000 | lr: 3.5978e-05 | scale: 32768.0000 | micro time: 1.757 | step time: 0.000
train | epoch   3 | Iter:   4645/ 13000 | global iter:   4645/ 13000 | loss: 1.5886 | ds_loss: 0.0000 | lr: 3.5972e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   3 | Iter:   4646/ 13000 | global iter:   4646/ 13000 | loss: 0.9778 | ds_loss: 0.0000 | lr: 3.5967e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   3 | Iter:   4647/ 13000 | global iter:   4647/ 13000 | loss: 1.5500 | ds_loss: 0.0000 | lr: 3.5961e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   3 | Iter:   4648/ 13000 | global iter:   4648/ 13000 | loss: 1.3195 | ds_loss: 0.0000 | lr: 3.5956e-05 | scale: 32768.0000 | micro time: 1.714 | step time: 0.000
train | epoch   3 | Iter:   4649/ 13000 | global iter:   4649/ 13000 | loss: 1.4874 | ds_loss: 0.0000 | lr: 3.5951e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 0.000
train | epoch   3 | Iter:   4650/ 13000 | global iter:   4650/ 13000 | loss: 1.2127 | ds_loss: 0.0000 | lr: 3.5945e-05 | scale: 32768.0000 | micro time: 1.747 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4650/ 13000 | global iter:   4650/ 13000 | loss: 1.5234 | ds_loss: 0.0000 | lr: 3.5945e-05 | scale: 32768.0000 | micro time: 1.747 | step time: 1.810
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4651/ 13000 | global iter:   4651/ 13000 | loss: 1.5935 | ds_loss: 0.0000 | lr: 3.5940e-05 | scale: 32768.0000 | micro time: 1.881 | step time: 0.000
train | epoch   3 | Iter:   4652/ 13000 | global iter:   4652/ 13000 | loss: 1.6017 | ds_loss: 0.0000 | lr: 3.5934e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   3 | Iter:   4653/ 13000 | global iter:   4653/ 13000 | loss: 1.1247 | ds_loss: 0.0000 | lr: 3.5929e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   3 | Iter:   4654/ 13000 | global iter:   4654/ 13000 | loss: 1.2643 | ds_loss: 0.0000 | lr: 3.5923e-05 | scale: 32768.0000 | micro time: 1.911 | step time: 0.000
train | epoch   3 | Iter:   4655/ 13000 | global iter:   4655/ 13000 | loss: 1.7632 | ds_loss: 0.0000 | lr: 3.5918e-05 | scale: 32768.0000 | micro time: 1.712 | step time: 0.000
train | epoch   3 | Iter:   4656/ 13000 | global iter:   4656/ 13000 | loss: 1.2036 | ds_loss: 0.0000 | lr: 3.5913e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   3 | Iter:   4657/ 13000 | global iter:   4657/ 13000 | loss: 1.5932 | ds_loss: 0.0000 | lr: 3.5907e-05 | scale: 32768.0000 | micro time: 1.758 | step time: 0.000
train | epoch   3 | Iter:   4658/ 13000 | global iter:   4658/ 13000 | loss: 1.7332 | ds_loss: 0.0000 | lr: 3.5902e-05 | scale: 32768.0000 | micro time: 1.723 | step time: 0.000
train | epoch   3 | Iter:   4659/ 13000 | global iter:   4659/ 13000 | loss: 1.1129 | ds_loss: 0.0000 | lr: 3.5896e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   3 | Iter:   4660/ 13000 | global iter:   4660/ 13000 | loss: 1.5709 | ds_loss: 0.0000 | lr: 3.5891e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4660/ 13000 | global iter:   4660/ 13000 | loss: 1.4561 | ds_loss: 0.0000 | lr: 3.5891e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 1.809
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4661/ 13000 | global iter:   4661/ 13000 | loss: 1.4481 | ds_loss: 0.0000 | lr: 3.5885e-05 | scale: 32768.0000 | micro time: 1.793 | step time: 0.000
train | epoch   3 | Iter:   4662/ 13000 | global iter:   4662/ 13000 | loss: 1.5650 | ds_loss: 0.0000 | lr: 3.5880e-05 | scale: 32768.0000 | micro time: 1.904 | step time: 0.000
train | epoch   3 | Iter:   4663/ 13000 | global iter:   4663/ 13000 | loss: 1.2947 | ds_loss: 0.0000 | lr: 3.5875e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   3 | Iter:   4664/ 13000 | global iter:   4664/ 13000 | loss: 2.0961 | ds_loss: 0.0000 | lr: 3.5869e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   3 | Iter:   4665/ 13000 | global iter:   4665/ 13000 | loss: 1.6691 | ds_loss: 0.0000 | lr: 3.5864e-05 | scale: 32768.0000 | micro time: 1.887 | step time: 0.000
train | epoch   3 | Iter:   4666/ 13000 | global iter:   4666/ 13000 | loss: 1.3729 | ds_loss: 0.0000 | lr: 3.5858e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   3 | Iter:   4667/ 13000 | global iter:   4667/ 13000 | loss: 1.4685 | ds_loss: 0.0000 | lr: 3.5853e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   3 | Iter:   4668/ 13000 | global iter:   4668/ 13000 | loss: 1.4582 | ds_loss: 0.0000 | lr: 3.5847e-05 | scale: 32768.0000 | micro time: 1.758 | step time: 0.000
train | epoch   3 | Iter:   4669/ 13000 | global iter:   4669/ 13000 | loss: 2.1003 | ds_loss: 0.0000 | lr: 3.5842e-05 | scale: 32768.0000 | micro time: 1.904 | step time: 0.000
train | epoch   3 | Iter:   4670/ 13000 | global iter:   4670/ 13000 | loss: 1.4272 | ds_loss: 0.0000 | lr: 3.5837e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4670/ 13000 | global iter:   4670/ 13000 | loss: 1.5900 | ds_loss: 0.0000 | lr: 3.5837e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 1.841
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4671/ 13000 | global iter:   4671/ 13000 | loss: 1.4769 | ds_loss: 0.0000 | lr: 3.5831e-05 | scale: 32768.0000 | micro time: 1.802 | step time: 0.000
train | epoch   3 | Iter:   4672/ 13000 | global iter:   4672/ 13000 | loss: 1.7418 | ds_loss: 0.0000 | lr: 3.5826e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
train | epoch   3 | Iter:   4673/ 13000 | global iter:   4673/ 13000 | loss: 1.7404 | ds_loss: 0.0000 | lr: 3.5820e-05 | scale: 32768.0000 | micro time: 1.856 | step time: 0.000
train | epoch   3 | Iter:   4674/ 13000 | global iter:   4674/ 13000 | loss: 1.6968 | ds_loss: 0.0000 | lr: 3.5815e-05 | scale: 32768.0000 | micro time: 1.841 | step time: 0.000
train | epoch   3 | Iter:   4675/ 13000 | global iter:   4675/ 13000 | loss: 1.7866 | ds_loss: 0.0000 | lr: 3.5809e-05 | scale: 32768.0000 | micro time: 1.764 | step time: 0.000
train | epoch   3 | Iter:   4676/ 13000 | global iter:   4676/ 13000 | loss: 0.8309 | ds_loss: 0.0000 | lr: 3.5804e-05 | scale: 32768.0000 | micro time: 1.834 | step time: 0.000
train | epoch   3 | Iter:   4677/ 13000 | global iter:   4677/ 13000 | loss: 1.1330 | ds_loss: 0.0000 | lr: 3.5799e-05 | scale: 32768.0000 | micro time: 1.768 | step time: 0.000
train | epoch   3 | Iter:   4678/ 13000 | global iter:   4678/ 13000 | loss: 1.5927 | ds_loss: 0.0000 | lr: 3.5793e-05 | scale: 32768.0000 | micro time: 1.790 | step time: 0.000
train | epoch   3 | Iter:   4679/ 13000 | global iter:   4679/ 13000 | loss: 1.7784 | ds_loss: 0.0000 | lr: 3.5788e-05 | scale: 32768.0000 | micro time: 1.786 | step time: 0.000
train | epoch   3 | Iter:   4680/ 13000 | global iter:   4680/ 13000 | loss: 1.2495 | ds_loss: 0.0000 | lr: 3.5782e-05 | scale: 32768.0000 | micro time: 1.760 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4680/ 13000 | global iter:   4680/ 13000 | loss: 1.5027 | ds_loss: 0.0000 | lr: 3.5782e-05 | scale: 32768.0000 | micro time: 1.760 | step time: 1.798
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4681/ 13000 | global iter:   4681/ 13000 | loss: 1.7305 | ds_loss: 0.0000 | lr: 3.5777e-05 | scale: 32768.0000 | micro time: 1.857 | step time: 0.000
train | epoch   3 | Iter:   4682/ 13000 | global iter:   4682/ 13000 | loss: 1.7601 | ds_loss: 0.0000 | lr: 3.5771e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   3 | Iter:   4683/ 13000 | global iter:   4683/ 13000 | loss: 1.4211 | ds_loss: 0.0000 | lr: 3.5766e-05 | scale: 32768.0000 | micro time: 1.762 | step time: 0.000
train | epoch   3 | Iter:   4684/ 13000 | global iter:   4684/ 13000 | loss: 1.7632 | ds_loss: 0.0000 | lr: 3.5760e-05 | scale: 32768.0000 | micro time: 1.793 | step time: 0.000
train | epoch   3 | Iter:   4685/ 13000 | global iter:   4685/ 13000 | loss: 1.8282 | ds_loss: 0.0000 | lr: 3.5755e-05 | scale: 32768.0000 | micro time: 1.742 | step time: 0.000
train | epoch   3 | Iter:   4686/ 13000 | global iter:   4686/ 13000 | loss: 1.9753 | ds_loss: 0.0000 | lr: 3.5750e-05 | scale: 32768.0000 | micro time: 1.840 | step time: 0.000
train | epoch   3 | Iter:   4687/ 13000 | global iter:   4687/ 13000 | loss: 1.4878 | ds_loss: 0.0000 | lr: 3.5744e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   3 | Iter:   4688/ 13000 | global iter:   4688/ 13000 | loss: 0.9334 | ds_loss: 0.0000 | lr: 3.5739e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   3 | Iter:   4689/ 13000 | global iter:   4689/ 13000 | loss: 1.6421 | ds_loss: 0.0000 | lr: 3.5733e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   3 | Iter:   4690/ 13000 | global iter:   4690/ 13000 | loss: 1.7681 | ds_loss: 0.0000 | lr: 3.5728e-05 | scale: 32768.0000 | micro time: 1.778 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4690/ 13000 | global iter:   4690/ 13000 | loss: 1.6310 | ds_loss: 0.0000 | lr: 3.5728e-05 | scale: 32768.0000 | micro time: 1.778 | step time: 1.787
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4691/ 13000 | global iter:   4691/ 13000 | loss: 1.9154 | ds_loss: 0.0000 | lr: 3.5722e-05 | scale: 32768.0000 | micro time: 1.869 | step time: 0.000
train | epoch   3 | Iter:   4692/ 13000 | global iter:   4692/ 13000 | loss: 1.7303 | ds_loss: 0.0000 | lr: 3.5717e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   3 | Iter:   4693/ 13000 | global iter:   4693/ 13000 | loss: 1.5158 | ds_loss: 0.0000 | lr: 3.5711e-05 | scale: 32768.0000 | micro time: 1.802 | step time: 0.000
train | epoch   3 | Iter:   4694/ 13000 | global iter:   4694/ 13000 | loss: 1.4329 | ds_loss: 0.0000 | lr: 3.5706e-05 | scale: 32768.0000 | micro time: 1.800 | step time: 0.000
train | epoch   3 | Iter:   4695/ 13000 | global iter:   4695/ 13000 | loss: 2.1678 | ds_loss: 0.0000 | lr: 3.5700e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 0.000
train | epoch   3 | Iter:   4696/ 13000 | global iter:   4696/ 13000 | loss: 2.0884 | ds_loss: 0.0000 | lr: 3.5695e-05 | scale: 32768.0000 | micro time: 1.911 | step time: 0.000
train | epoch   3 | Iter:   4697/ 13000 | global iter:   4697/ 13000 | loss: 1.9339 | ds_loss: 0.0000 | lr: 3.5690e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 0.000
train | epoch   3 | Iter:   4698/ 13000 | global iter:   4698/ 13000 | loss: 2.0058 | ds_loss: 0.0000 | lr: 3.5684e-05 | scale: 32768.0000 | micro time: 1.870 | step time: 0.000
train | epoch   3 | Iter:   4699/ 13000 | global iter:   4699/ 13000 | loss: 1.1912 | ds_loss: 0.0000 | lr: 3.5679e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   3 | Iter:   4700/ 13000 | global iter:   4700/ 13000 | loss: 1.4964 | ds_loss: 0.0000 | lr: 3.5673e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4700/ 13000 | global iter:   4700/ 13000 | loss: 1.7478 | ds_loss: 0.0000 | lr: 3.5673e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 1.840
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4701/ 13000 | global iter:   4701/ 13000 | loss: 1.6886 | ds_loss: 0.0000 | lr: 3.5668e-05 | scale: 32768.0000 | micro time: 1.862 | step time: 0.000
train | epoch   3 | Iter:   4702/ 13000 | global iter:   4702/ 13000 | loss: 1.1795 | ds_loss: 0.0000 | lr: 3.5662e-05 | scale: 32768.0000 | micro time: 1.715 | step time: 0.000
train | epoch   3 | Iter:   4703/ 13000 | global iter:   4703/ 13000 | loss: 1.5886 | ds_loss: 0.0000 | lr: 3.5657e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   3 | Iter:   4704/ 13000 | global iter:   4704/ 13000 | loss: 1.8578 | ds_loss: 0.0000 | lr: 3.5651e-05 | scale: 32768.0000 | micro time: 1.800 | step time: 0.000
train | epoch   3 | Iter:   4705/ 13000 | global iter:   4705/ 13000 | loss: 1.4607 | ds_loss: 0.0000 | lr: 3.5646e-05 | scale: 32768.0000 | micro time: 1.720 | step time: 0.000
train | epoch   3 | Iter:   4706/ 13000 | global iter:   4706/ 13000 | loss: 1.3311 | ds_loss: 0.0000 | lr: 3.5640e-05 | scale: 32768.0000 | micro time: 1.892 | step time: 0.000
train | epoch   3 | Iter:   4707/ 13000 | global iter:   4707/ 13000 | loss: 1.1125 | ds_loss: 0.0000 | lr: 3.5635e-05 | scale: 32768.0000 | micro time: 1.774 | step time: 0.000
train | epoch   3 | Iter:   4708/ 13000 | global iter:   4708/ 13000 | loss: 1.9020 | ds_loss: 0.0000 | lr: 3.5630e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   3 | Iter:   4709/ 13000 | global iter:   4709/ 13000 | loss: 1.5986 | ds_loss: 0.0000 | lr: 3.5624e-05 | scale: 32768.0000 | micro time: 1.879 | step time: 0.000
train | epoch   3 | Iter:   4710/ 13000 | global iter:   4710/ 13000 | loss: 1.7478 | ds_loss: 0.0000 | lr: 3.5619e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4710/ 13000 | global iter:   4710/ 13000 | loss: 1.5467 | ds_loss: 0.0000 | lr: 3.5619e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 1.809
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4711/ 13000 | global iter:   4711/ 13000 | loss: 1.6945 | ds_loss: 0.0000 | lr: 3.5613e-05 | scale: 32768.0000 | micro time: 1.862 | step time: 0.000
train | epoch   3 | Iter:   4712/ 13000 | global iter:   4712/ 13000 | loss: 1.4997 | ds_loss: 0.0000 | lr: 3.5608e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   3 | Iter:   4713/ 13000 | global iter:   4713/ 13000 | loss: 0.9595 | ds_loss: 0.0000 | lr: 3.5602e-05 | scale: 32768.0000 | micro time: 1.943 | step time: 0.000
train | epoch   3 | Iter:   4714/ 13000 | global iter:   4714/ 13000 | loss: 1.6503 | ds_loss: 0.0000 | lr: 3.5597e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   3 | Iter:   4715/ 13000 | global iter:   4715/ 13000 | loss: 1.2988 | ds_loss: 0.0000 | lr: 3.5591e-05 | scale: 32768.0000 | micro time: 1.806 | step time: 0.000
train | epoch   3 | Iter:   4716/ 13000 | global iter:   4716/ 13000 | loss: 1.2350 | ds_loss: 0.0000 | lr: 3.5586e-05 | scale: 32768.0000 | micro time: 1.884 | step time: 0.000
train | epoch   3 | Iter:   4717/ 13000 | global iter:   4717/ 13000 | loss: 1.6162 | ds_loss: 0.0000 | lr: 3.5580e-05 | scale: 32768.0000 | micro time: 1.747 | step time: 0.000
train | epoch   3 | Iter:   4718/ 13000 | global iter:   4718/ 13000 | loss: 1.9465 | ds_loss: 0.0000 | lr: 3.5575e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   3 | Iter:   4719/ 13000 | global iter:   4719/ 13000 | loss: 1.5656 | ds_loss: 0.0000 | lr: 3.5569e-05 | scale: 32768.0000 | micro time: 1.747 | step time: 0.000
train | epoch   3 | Iter:   4720/ 13000 | global iter:   4720/ 13000 | loss: 1.6637 | ds_loss: 0.0000 | lr: 3.5564e-05 | scale: 32768.0000 | micro time: 1.942 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4720/ 13000 | global iter:   4720/ 13000 | loss: 1.5130 | ds_loss: 0.0000 | lr: 3.5564e-05 | scale: 32768.0000 | micro time: 1.942 | step time: 1.832
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4721/ 13000 | global iter:   4721/ 13000 | loss: 1.5994 | ds_loss: 0.0000 | lr: 3.5558e-05 | scale: 32768.0000 | micro time: 1.806 | step time: 0.000
train | epoch   3 | Iter:   4722/ 13000 | global iter:   4722/ 13000 | loss: 1.6765 | ds_loss: 0.0000 | lr: 3.5553e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   3 | Iter:   4723/ 13000 | global iter:   4723/ 13000 | loss: 1.3941 | ds_loss: 0.0000 | lr: 3.5548e-05 | scale: 32768.0000 | micro time: 1.895 | step time: 0.000
train | epoch   3 | Iter:   4724/ 13000 | global iter:   4724/ 13000 | loss: 1.7637 | ds_loss: 0.0000 | lr: 3.5542e-05 | scale: 32768.0000 | micro time: 1.895 | step time: 0.000
train | epoch   3 | Iter:   4725/ 13000 | global iter:   4725/ 13000 | loss: 1.9578 | ds_loss: 0.0000 | lr: 3.5537e-05 | scale: 32768.0000 | micro time: 1.872 | step time: 0.000
train | epoch   3 | Iter:   4726/ 13000 | global iter:   4726/ 13000 | loss: 2.1882 | ds_loss: 0.0000 | lr: 3.5531e-05 | scale: 32768.0000 | micro time: 1.938 | step time: 0.000
train | epoch   3 | Iter:   4727/ 13000 | global iter:   4727/ 13000 | loss: 1.2101 | ds_loss: 0.0000 | lr: 3.5526e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 0.000
train | epoch   3 | Iter:   4728/ 13000 | global iter:   4728/ 13000 | loss: 1.1641 | ds_loss: 0.0000 | lr: 3.5520e-05 | scale: 32768.0000 | micro time: 1.963 | step time: 0.000
train | epoch   3 | Iter:   4729/ 13000 | global iter:   4729/ 13000 | loss: 1.7066 | ds_loss: 0.0000 | lr: 3.5515e-05 | scale: 32768.0000 | micro time: 1.870 | step time: 0.000
train | epoch   3 | Iter:   4730/ 13000 | global iter:   4730/ 13000 | loss: 1.3748 | ds_loss: 0.0000 | lr: 3.5509e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4730/ 13000 | global iter:   4730/ 13000 | loss: 1.6035 | ds_loss: 0.0000 | lr: 3.5509e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 1.873
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4731/ 13000 | global iter:   4731/ 13000 | loss: 2.2618 | ds_loss: 0.0000 | lr: 3.5504e-05 | scale: 32768.0000 | micro time: 1.898 | step time: 0.000
train | epoch   3 | Iter:   4732/ 13000 | global iter:   4732/ 13000 | loss: 1.5637 | ds_loss: 0.0000 | lr: 3.5498e-05 | scale: 32768.0000 | micro time: 1.887 | step time: 0.000
train | epoch   3 | Iter:   4733/ 13000 | global iter:   4733/ 13000 | loss: 1.8622 | ds_loss: 0.0000 | lr: 3.5493e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
train | epoch   3 | Iter:   4734/ 13000 | global iter:   4734/ 13000 | loss: 1.1388 | ds_loss: 0.0000 | lr: 3.5487e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
[2025-04-19 13:43:29,212] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
train | epoch   3 | Iter:   4735/ 13000 | global iter:   4735/ 13000 | loss: 1.3199 | ds_loss: 0.0000 | lr: 3.5487e-05 | scale: 16384.0000 | micro time: 1.620 | step time: 0.000
train | epoch   3 | Iter:   4736/ 13000 | global iter:   4736/ 13000 | loss: 1.5497 | ds_loss: 0.0000 | lr: 3.5482e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   3 | Iter:   4737/ 13000 | global iter:   4737/ 13000 | loss: 1.1849 | ds_loss: 0.0000 | lr: 3.5476e-05 | scale: 16384.0000 | micro time: 1.908 | step time: 0.000
train | epoch   3 | Iter:   4738/ 13000 | global iter:   4738/ 13000 | loss: 1.6023 | ds_loss: 0.0000 | lr: 3.5471e-05 | scale: 16384.0000 | micro time: 1.858 | step time: 0.000
train | epoch   3 | Iter:   4739/ 13000 | global iter:   4739/ 13000 | loss: 1.7243 | ds_loss: 0.0000 | lr: 3.5465e-05 | scale: 16384.0000 | micro time: 1.919 | step time: 0.000
train | epoch   3 | Iter:   4740/ 13000 | global iter:   4740/ 13000 | loss: 1.5794 | ds_loss: 0.0000 | lr: 3.5460e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4740/ 13000 | global iter:   4740/ 13000 | loss: 1.5787 | ds_loss: 0.0000 | lr: 3.5460e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 1.830
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4741/ 13000 | global iter:   4741/ 13000 | loss: 1.4296 | ds_loss: 0.0000 | lr: 3.5454e-05 | scale: 16384.0000 | micro time: 1.857 | step time: 0.000
train | epoch   3 | Iter:   4742/ 13000 | global iter:   4742/ 13000 | loss: 1.4226 | ds_loss: 0.0000 | lr: 3.5449e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
train | epoch   3 | Iter:   4743/ 13000 | global iter:   4743/ 13000 | loss: 1.6126 | ds_loss: 0.0000 | lr: 3.5444e-05 | scale: 16384.0000 | micro time: 1.887 | step time: 0.000
train | epoch   3 | Iter:   4744/ 13000 | global iter:   4744/ 13000 | loss: 1.5959 | ds_loss: 0.0000 | lr: 3.5438e-05 | scale: 16384.0000 | micro time: 1.834 | step time: 0.000
train | epoch   3 | Iter:   4745/ 13000 | global iter:   4745/ 13000 | loss: 1.3963 | ds_loss: 0.0000 | lr: 3.5433e-05 | scale: 16384.0000 | micro time: 1.924 | step time: 0.000
train | epoch   3 | Iter:   4746/ 13000 | global iter:   4746/ 13000 | loss: 1.7292 | ds_loss: 0.0000 | lr: 3.5427e-05 | scale: 16384.0000 | micro time: 1.723 | step time: 0.000
train | epoch   3 | Iter:   4747/ 13000 | global iter:   4747/ 13000 | loss: 1.4689 | ds_loss: 0.0000 | lr: 3.5422e-05 | scale: 16384.0000 | micro time: 1.899 | step time: 0.000
train | epoch   3 | Iter:   4748/ 13000 | global iter:   4748/ 13000 | loss: 1.8184 | ds_loss: 0.0000 | lr: 3.5416e-05 | scale: 16384.0000 | micro time: 1.853 | step time: 0.000
train | epoch   3 | Iter:   4749/ 13000 | global iter:   4749/ 13000 | loss: 1.2253 | ds_loss: 0.0000 | lr: 3.5411e-05 | scale: 16384.0000 | micro time: 1.865 | step time: 0.000
train | epoch   3 | Iter:   4750/ 13000 | global iter:   4750/ 13000 | loss: 1.3884 | ds_loss: 0.0000 | lr: 3.5405e-05 | scale: 16384.0000 | micro time: 1.729 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4750/ 13000 | global iter:   4750/ 13000 | loss: 1.5087 | ds_loss: 0.0000 | lr: 3.5405e-05 | scale: 16384.0000 | micro time: 1.729 | step time: 1.845
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4751/ 13000 | global iter:   4751/ 13000 | loss: 1.9770 | ds_loss: 0.0000 | lr: 3.5400e-05 | scale: 16384.0000 | micro time: 1.884 | step time: 0.000
train | epoch   3 | Iter:   4752/ 13000 | global iter:   4752/ 13000 | loss: 1.7278 | ds_loss: 0.0000 | lr: 3.5394e-05 | scale: 16384.0000 | micro time: 1.737 | step time: 0.000
train | epoch   3 | Iter:   4753/ 13000 | global iter:   4753/ 13000 | loss: 1.6418 | ds_loss: 0.0000 | lr: 3.5389e-05 | scale: 16384.0000 | micro time: 1.821 | step time: 0.000
train | epoch   3 | Iter:   4754/ 13000 | global iter:   4754/ 13000 | loss: 1.8441 | ds_loss: 0.0000 | lr: 3.5383e-05 | scale: 16384.0000 | micro time: 1.899 | step time: 0.000
train | epoch   3 | Iter:   4755/ 13000 | global iter:   4755/ 13000 | loss: 2.1094 | ds_loss: 0.0000 | lr: 3.5378e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
train | epoch   3 | Iter:   4756/ 13000 | global iter:   4756/ 13000 | loss: 1.5450 | ds_loss: 0.0000 | lr: 3.5372e-05 | scale: 16384.0000 | micro time: 1.810 | step time: 0.000
train | epoch   3 | Iter:   4757/ 13000 | global iter:   4757/ 13000 | loss: 0.7243 | ds_loss: 0.0000 | lr: 3.5367e-05 | scale: 16384.0000 | micro time: 1.715 | step time: 0.000
train | epoch   3 | Iter:   4758/ 13000 | global iter:   4758/ 13000 | loss: 1.7135 | ds_loss: 0.0000 | lr: 3.5361e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   3 | Iter:   4759/ 13000 | global iter:   4759/ 13000 | loss: 1.2066 | ds_loss: 0.0000 | lr: 3.5356e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   3 | Iter:   4760/ 13000 | global iter:   4760/ 13000 | loss: 1.8675 | ds_loss: 0.0000 | lr: 3.5350e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4760/ 13000 | global iter:   4760/ 13000 | loss: 1.6357 | ds_loss: 0.0000 | lr: 3.5350e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 1.829
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4761/ 13000 | global iter:   4761/ 13000 | loss: 1.3073 | ds_loss: 0.0000 | lr: 3.5345e-05 | scale: 16384.0000 | micro time: 1.833 | step time: 0.000
train | epoch   3 | Iter:   4762/ 13000 | global iter:   4762/ 13000 | loss: 1.4837 | ds_loss: 0.0000 | lr: 3.5339e-05 | scale: 16384.0000 | micro time: 1.838 | step time: 0.000
train | epoch   3 | Iter:   4763/ 13000 | global iter:   4763/ 13000 | loss: 1.6779 | ds_loss: 0.0000 | lr: 3.5334e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   3 | Iter:   4764/ 13000 | global iter:   4764/ 13000 | loss: 1.5454 | ds_loss: 0.0000 | lr: 3.5328e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   3 | Iter:   4765/ 13000 | global iter:   4765/ 13000 | loss: 1.5793 | ds_loss: 0.0000 | lr: 3.5323e-05 | scale: 16384.0000 | micro time: 1.879 | step time: 0.000
train | epoch   3 | Iter:   4766/ 13000 | global iter:   4766/ 13000 | loss: 1.8349 | ds_loss: 0.0000 | lr: 3.5317e-05 | scale: 16384.0000 | micro time: 1.890 | step time: 0.000
train | epoch   3 | Iter:   4767/ 13000 | global iter:   4767/ 13000 | loss: 2.0606 | ds_loss: 0.0000 | lr: 3.5312e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   3 | Iter:   4768/ 13000 | global iter:   4768/ 13000 | loss: 1.4712 | ds_loss: 0.0000 | lr: 3.5306e-05 | scale: 16384.0000 | micro time: 1.903 | step time: 0.000
train | epoch   3 | Iter:   4769/ 13000 | global iter:   4769/ 13000 | loss: 1.5026 | ds_loss: 0.0000 | lr: 3.5301e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   3 | Iter:   4770/ 13000 | global iter:   4770/ 13000 | loss: 1.4609 | ds_loss: 0.0000 | lr: 3.5295e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4770/ 13000 | global iter:   4770/ 13000 | loss: 1.5924 | ds_loss: 0.0000 | lr: 3.5295e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 1.841
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4771/ 13000 | global iter:   4771/ 13000 | loss: 1.4339 | ds_loss: 0.0000 | lr: 3.5290e-05 | scale: 16384.0000 | micro time: 1.956 | step time: 0.000
train | epoch   3 | Iter:   4772/ 13000 | global iter:   4772/ 13000 | loss: 2.1673 | ds_loss: 0.0000 | lr: 3.5284e-05 | scale: 16384.0000 | micro time: 1.828 | step time: 0.000
train | epoch   3 | Iter:   4773/ 13000 | global iter:   4773/ 13000 | loss: 1.3274 | ds_loss: 0.0000 | lr: 3.5279e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   3 | Iter:   4774/ 13000 | global iter:   4774/ 13000 | loss: 1.7541 | ds_loss: 0.0000 | lr: 3.5273e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   3 | Iter:   4775/ 13000 | global iter:   4775/ 13000 | loss: 1.9721 | ds_loss: 0.0000 | lr: 3.5268e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   3 | Iter:   4776/ 13000 | global iter:   4776/ 13000 | loss: 1.3628 | ds_loss: 0.0000 | lr: 3.5262e-05 | scale: 16384.0000 | micro time: 1.809 | step time: 0.000
train | epoch   3 | Iter:   4777/ 13000 | global iter:   4777/ 13000 | loss: 1.3058 | ds_loss: 0.0000 | lr: 3.5257e-05 | scale: 16384.0000 | micro time: 1.776 | step time: 0.000
train | epoch   3 | Iter:   4778/ 13000 | global iter:   4778/ 13000 | loss: 1.1344 | ds_loss: 0.0000 | lr: 3.5251e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   3 | Iter:   4779/ 13000 | global iter:   4779/ 13000 | loss: 2.0367 | ds_loss: 0.0000 | lr: 3.5246e-05 | scale: 16384.0000 | micro time: 1.785 | step time: 0.000
train | epoch   3 | Iter:   4780/ 13000 | global iter:   4780/ 13000 | loss: 1.9478 | ds_loss: 0.0000 | lr: 3.5240e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4780/ 13000 | global iter:   4780/ 13000 | loss: 1.6442 | ds_loss: 0.0000 | lr: 3.5240e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 1.834
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4781/ 13000 | global iter:   4781/ 13000 | loss: 1.4972 | ds_loss: 0.0000 | lr: 3.5235e-05 | scale: 16384.0000 | micro time: 1.879 | step time: 0.000
train | epoch   3 | Iter:   4782/ 13000 | global iter:   4782/ 13000 | loss: 1.6245 | ds_loss: 0.0000 | lr: 3.5229e-05 | scale: 16384.0000 | micro time: 1.761 | step time: 0.000
train | epoch   3 | Iter:   4783/ 13000 | global iter:   4783/ 13000 | loss: 1.0812 | ds_loss: 0.0000 | lr: 3.5224e-05 | scale: 16384.0000 | micro time: 1.828 | step time: 0.000
train | epoch   3 | Iter:   4784/ 13000 | global iter:   4784/ 13000 | loss: 2.0785 | ds_loss: 0.0000 | lr: 3.5218e-05 | scale: 16384.0000 | micro time: 1.880 | step time: 0.000
train | epoch   3 | Iter:   4785/ 13000 | global iter:   4785/ 13000 | loss: 1.8888 | ds_loss: 0.0000 | lr: 3.5213e-05 | scale: 16384.0000 | micro time: 1.704 | step time: 0.000
train | epoch   3 | Iter:   4786/ 13000 | global iter:   4786/ 13000 | loss: 1.9308 | ds_loss: 0.0000 | lr: 3.5207e-05 | scale: 16384.0000 | micro time: 1.824 | step time: 0.000
train | epoch   3 | Iter:   4787/ 13000 | global iter:   4787/ 13000 | loss: 1.3245 | ds_loss: 0.0000 | lr: 3.5202e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   3 | Iter:   4788/ 13000 | global iter:   4788/ 13000 | loss: 1.9395 | ds_loss: 0.0000 | lr: 3.5196e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   3 | Iter:   4789/ 13000 | global iter:   4789/ 13000 | loss: 1.4959 | ds_loss: 0.0000 | lr: 3.5191e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   3 | Iter:   4790/ 13000 | global iter:   4790/ 13000 | loss: 1.7807 | ds_loss: 0.0000 | lr: 3.5185e-05 | scale: 16384.0000 | micro time: 1.789 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4790/ 13000 | global iter:   4790/ 13000 | loss: 1.6642 | ds_loss: 0.0000 | lr: 3.5185e-05 | scale: 16384.0000 | micro time: 1.789 | step time: 1.815
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4791/ 13000 | global iter:   4791/ 13000 | loss: 1.8250 | ds_loss: 0.0000 | lr: 3.5180e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   3 | Iter:   4792/ 13000 | global iter:   4792/ 13000 | loss: 2.1010 | ds_loss: 0.0000 | lr: 3.5174e-05 | scale: 16384.0000 | micro time: 1.752 | step time: 0.000
train | epoch   3 | Iter:   4793/ 13000 | global iter:   4793/ 13000 | loss: 1.9701 | ds_loss: 0.0000 | lr: 3.5169e-05 | scale: 16384.0000 | micro time: 1.757 | step time: 0.000
train | epoch   3 | Iter:   4794/ 13000 | global iter:   4794/ 13000 | loss: 1.0748 | ds_loss: 0.0000 | lr: 3.5163e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   3 | Iter:   4795/ 13000 | global iter:   4795/ 13000 | loss: 1.4313 | ds_loss: 0.0000 | lr: 3.5158e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   3 | Iter:   4796/ 13000 | global iter:   4796/ 13000 | loss: 1.6437 | ds_loss: 0.0000 | lr: 3.5152e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   3 | Iter:   4797/ 13000 | global iter:   4797/ 13000 | loss: 1.7873 | ds_loss: 0.0000 | lr: 3.5147e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   3 | Iter:   4798/ 13000 | global iter:   4798/ 13000 | loss: 1.5880 | ds_loss: 0.0000 | lr: 3.5141e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   3 | Iter:   4799/ 13000 | global iter:   4799/ 13000 | loss: 1.5302 | ds_loss: 0.0000 | lr: 3.5136e-05 | scale: 16384.0000 | micro time: 1.723 | step time: 0.000
train | epoch   3 | Iter:   4800/ 13000 | global iter:   4800/ 13000 | loss: 1.1792 | ds_loss: 0.0000 | lr: 3.5130e-05 | scale: 16384.0000 | micro time: 1.818 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4800/ 13000 | global iter:   4800/ 13000 | loss: 1.6131 | ds_loss: 0.0000 | lr: 3.5130e-05 | scale: 16384.0000 | micro time: 1.818 | step time: 1.790
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4801/ 13000 | global iter:   4801/ 13000 | loss: 1.6363 | ds_loss: 0.0000 | lr: 3.5125e-05 | scale: 16384.0000 | micro time: 1.794 | step time: 0.000
train | epoch   3 | Iter:   4802/ 13000 | global iter:   4802/ 13000 | loss: 1.3733 | ds_loss: 0.0000 | lr: 3.5119e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   3 | Iter:   4803/ 13000 | global iter:   4803/ 13000 | loss: 1.6177 | ds_loss: 0.0000 | lr: 3.5114e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   3 | Iter:   4804/ 13000 | global iter:   4804/ 13000 | loss: 0.9086 | ds_loss: 0.0000 | lr: 3.5108e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   3 | Iter:   4805/ 13000 | global iter:   4805/ 13000 | loss: 2.0262 | ds_loss: 0.0000 | lr: 3.5103e-05 | scale: 16384.0000 | micro time: 1.887 | step time: 0.000
train | epoch   3 | Iter:   4806/ 13000 | global iter:   4806/ 13000 | loss: 1.9660 | ds_loss: 0.0000 | lr: 3.5097e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   3 | Iter:   4807/ 13000 | global iter:   4807/ 13000 | loss: 1.4253 | ds_loss: 0.0000 | lr: 3.5091e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   3 | Iter:   4808/ 13000 | global iter:   4808/ 13000 | loss: 1.4849 | ds_loss: 0.0000 | lr: 3.5086e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   3 | Iter:   4809/ 13000 | global iter:   4809/ 13000 | loss: 1.7311 | ds_loss: 0.0000 | lr: 3.5080e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   3 | Iter:   4810/ 13000 | global iter:   4810/ 13000 | loss: 1.8903 | ds_loss: 0.0000 | lr: 3.5075e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4810/ 13000 | global iter:   4810/ 13000 | loss: 1.6060 | ds_loss: 0.0000 | lr: 3.5075e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 1.821
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4811/ 13000 | global iter:   4811/ 13000 | loss: 1.6388 | ds_loss: 0.0000 | lr: 3.5069e-05 | scale: 16384.0000 | micro time: 1.895 | step time: 0.000
train | epoch   3 | Iter:   4812/ 13000 | global iter:   4812/ 13000 | loss: 1.5193 | ds_loss: 0.0000 | lr: 3.5064e-05 | scale: 16384.0000 | micro time: 1.765 | step time: 0.000
train | epoch   3 | Iter:   4813/ 13000 | global iter:   4813/ 13000 | loss: 1.4846 | ds_loss: 0.0000 | lr: 3.5058e-05 | scale: 16384.0000 | micro time: 1.711 | step time: 0.000
train | epoch   3 | Iter:   4814/ 13000 | global iter:   4814/ 13000 | loss: 2.1381 | ds_loss: 0.0000 | lr: 3.5053e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   3 | Iter:   4815/ 13000 | global iter:   4815/ 13000 | loss: 1.4430 | ds_loss: 0.0000 | lr: 3.5047e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   3 | Iter:   4816/ 13000 | global iter:   4816/ 13000 | loss: 1.8680 | ds_loss: 0.0000 | lr: 3.5042e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
train | epoch   3 | Iter:   4817/ 13000 | global iter:   4817/ 13000 | loss: 1.0746 | ds_loss: 0.0000 | lr: 3.5036e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   3 | Iter:   4818/ 13000 | global iter:   4818/ 13000 | loss: 1.3804 | ds_loss: 0.0000 | lr: 3.5031e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   3 | Iter:   4819/ 13000 | global iter:   4819/ 13000 | loss: 1.8590 | ds_loss: 0.0000 | lr: 3.5025e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   3 | Iter:   4820/ 13000 | global iter:   4820/ 13000 | loss: 1.9176 | ds_loss: 0.0000 | lr: 3.5020e-05 | scale: 16384.0000 | micro time: 1.824 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4820/ 13000 | global iter:   4820/ 13000 | loss: 1.6323 | ds_loss: 0.0000 | lr: 3.5020e-05 | scale: 16384.0000 | micro time: 1.824 | step time: 1.806
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4821/ 13000 | global iter:   4821/ 13000 | loss: 1.9645 | ds_loss: 0.0000 | lr: 3.5014e-05 | scale: 16384.0000 | micro time: 1.759 | step time: 0.000
train | epoch   3 | Iter:   4822/ 13000 | global iter:   4822/ 13000 | loss: 1.8065 | ds_loss: 0.0000 | lr: 3.5009e-05 | scale: 16384.0000 | micro time: 1.814 | step time: 0.000
train | epoch   3 | Iter:   4823/ 13000 | global iter:   4823/ 13000 | loss: 1.1929 | ds_loss: 0.0000 | lr: 3.5003e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 0.000
train | epoch   3 | Iter:   4824/ 13000 | global iter:   4824/ 13000 | loss: 1.1089 | ds_loss: 0.0000 | lr: 3.4998e-05 | scale: 16384.0000 | micro time: 1.786 | step time: 0.000
train | epoch   3 | Iter:   4825/ 13000 | global iter:   4825/ 13000 | loss: 1.2615 | ds_loss: 0.0000 | lr: 3.4992e-05 | scale: 16384.0000 | micro time: 1.762 | step time: 0.000
train | epoch   3 | Iter:   4826/ 13000 | global iter:   4826/ 13000 | loss: 1.3336 | ds_loss: 0.0000 | lr: 3.4987e-05 | scale: 16384.0000 | micro time: 1.743 | step time: 0.000
train | epoch   3 | Iter:   4827/ 13000 | global iter:   4827/ 13000 | loss: 1.7876 | ds_loss: 0.0000 | lr: 3.4981e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   3 | Iter:   4828/ 13000 | global iter:   4828/ 13000 | loss: 1.5578 | ds_loss: 0.0000 | lr: 3.4975e-05 | scale: 16384.0000 | micro time: 1.759 | step time: 0.000
train | epoch   3 | Iter:   4829/ 13000 | global iter:   4829/ 13000 | loss: 1.6386 | ds_loss: 0.0000 | lr: 3.4970e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   3 | Iter:   4830/ 13000 | global iter:   4830/ 13000 | loss: 1.7466 | ds_loss: 0.0000 | lr: 3.4964e-05 | scale: 16384.0000 | micro time: 1.828 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4830/ 13000 | global iter:   4830/ 13000 | loss: 1.5399 | ds_loss: 0.0000 | lr: 3.4964e-05 | scale: 16384.0000 | micro time: 1.828 | step time: 1.787
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4831/ 13000 | global iter:   4831/ 13000 | loss: 1.7604 | ds_loss: 0.0000 | lr: 3.4959e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   3 | Iter:   4832/ 13000 | global iter:   4832/ 13000 | loss: 1.1232 | ds_loss: 0.0000 | lr: 3.4953e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   3 | Iter:   4833/ 13000 | global iter:   4833/ 13000 | loss: 1.9695 | ds_loss: 0.0000 | lr: 3.4948e-05 | scale: 16384.0000 | micro time: 1.817 | step time: 0.000
train | epoch   3 | Iter:   4834/ 13000 | global iter:   4834/ 13000 | loss: 1.9727 | ds_loss: 0.0000 | lr: 3.4942e-05 | scale: 16384.0000 | micro time: 1.773 | step time: 0.000
train | epoch   3 | Iter:   4835/ 13000 | global iter:   4835/ 13000 | loss: 1.6112 | ds_loss: 0.0000 | lr: 3.4937e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   3 | Iter:   4836/ 13000 | global iter:   4836/ 13000 | loss: 2.0987 | ds_loss: 0.0000 | lr: 3.4931e-05 | scale: 16384.0000 | micro time: 1.903 | step time: 0.000
train | epoch   3 | Iter:   4837/ 13000 | global iter:   4837/ 13000 | loss: 1.8847 | ds_loss: 0.0000 | lr: 3.4926e-05 | scale: 16384.0000 | micro time: 1.782 | step time: 0.000
train | epoch   3 | Iter:   4838/ 13000 | global iter:   4838/ 13000 | loss: 1.9555 | ds_loss: 0.0000 | lr: 3.4920e-05 | scale: 16384.0000 | micro time: 1.844 | step time: 0.000
train | epoch   3 | Iter:   4839/ 13000 | global iter:   4839/ 13000 | loss: 1.8643 | ds_loss: 0.0000 | lr: 3.4915e-05 | scale: 16384.0000 | micro time: 1.654 | step time: 0.000
train | epoch   3 | Iter:   4840/ 13000 | global iter:   4840/ 13000 | loss: 1.7209 | ds_loss: 0.0000 | lr: 3.4909e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4840/ 13000 | global iter:   4840/ 13000 | loss: 1.7961 | ds_loss: 0.0000 | lr: 3.4909e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 1.817
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4841/ 13000 | global iter:   4841/ 13000 | loss: 1.8502 | ds_loss: 0.0000 | lr: 3.4903e-05 | scale: 16384.0000 | micro time: 1.749 | step time: 0.000
train | epoch   3 | Iter:   4842/ 13000 | global iter:   4842/ 13000 | loss: 1.5892 | ds_loss: 0.0000 | lr: 3.4898e-05 | scale: 16384.0000 | micro time: 1.849 | step time: 0.000
train | epoch   3 | Iter:   4843/ 13000 | global iter:   4843/ 13000 | loss: 2.1034 | ds_loss: 0.0000 | lr: 3.4892e-05 | scale: 16384.0000 | micro time: 1.796 | step time: 0.000
train | epoch   3 | Iter:   4844/ 13000 | global iter:   4844/ 13000 | loss: 1.4543 | ds_loss: 0.0000 | lr: 3.4887e-05 | scale: 16384.0000 | micro time: 1.899 | step time: 0.000
train | epoch   3 | Iter:   4845/ 13000 | global iter:   4845/ 13000 | loss: 1.9420 | ds_loss: 0.0000 | lr: 3.4881e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   3 | Iter:   4846/ 13000 | global iter:   4846/ 13000 | loss: 1.9438 | ds_loss: 0.0000 | lr: 3.4876e-05 | scale: 16384.0000 | micro time: 1.857 | step time: 0.000
train | epoch   3 | Iter:   4847/ 13000 | global iter:   4847/ 13000 | loss: 1.4091 | ds_loss: 0.0000 | lr: 3.4870e-05 | scale: 16384.0000 | micro time: 1.905 | step time: 0.000
train | epoch   3 | Iter:   4848/ 13000 | global iter:   4848/ 13000 | loss: 1.2440 | ds_loss: 0.0000 | lr: 3.4865e-05 | scale: 16384.0000 | micro time: 1.743 | step time: 0.000
train | epoch   3 | Iter:   4849/ 13000 | global iter:   4849/ 13000 | loss: 1.7317 | ds_loss: 0.0000 | lr: 3.4859e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 0.000
train | epoch   3 | Iter:   4850/ 13000 | global iter:   4850/ 13000 | loss: 1.9737 | ds_loss: 0.0000 | lr: 3.4854e-05 | scale: 16384.0000 | micro time: 1.701 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4850/ 13000 | global iter:   4850/ 13000 | loss: 1.7241 | ds_loss: 0.0000 | lr: 3.4854e-05 | scale: 16384.0000 | micro time: 1.701 | step time: 1.808
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4851/ 13000 | global iter:   4851/ 13000 | loss: 1.6199 | ds_loss: 0.0000 | lr: 3.4848e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   3 | Iter:   4852/ 13000 | global iter:   4852/ 13000 | loss: 1.3980 | ds_loss: 0.0000 | lr: 3.4843e-05 | scale: 16384.0000 | micro time: 1.860 | step time: 0.000
train | epoch   3 | Iter:   4853/ 13000 | global iter:   4853/ 13000 | loss: 1.9449 | ds_loss: 0.0000 | lr: 3.4837e-05 | scale: 16384.0000 | micro time: 1.793 | step time: 0.000
train | epoch   3 | Iter:   4854/ 13000 | global iter:   4854/ 13000 | loss: 1.7716 | ds_loss: 0.0000 | lr: 3.4831e-05 | scale: 16384.0000 | micro time: 1.836 | step time: 0.000
train | epoch   3 | Iter:   4855/ 13000 | global iter:   4855/ 13000 | loss: 1.8711 | ds_loss: 0.0000 | lr: 3.4826e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   3 | Iter:   4856/ 13000 | global iter:   4856/ 13000 | loss: 1.2703 | ds_loss: 0.0000 | lr: 3.4820e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   3 | Iter:   4857/ 13000 | global iter:   4857/ 13000 | loss: 1.4382 | ds_loss: 0.0000 | lr: 3.4815e-05 | scale: 16384.0000 | micro time: 1.834 | step time: 0.000
train | epoch   3 | Iter:   4858/ 13000 | global iter:   4858/ 13000 | loss: 1.8430 | ds_loss: 0.0000 | lr: 3.4809e-05 | scale: 16384.0000 | micro time: 1.899 | step time: 0.000
train | epoch   3 | Iter:   4859/ 13000 | global iter:   4859/ 13000 | loss: 1.8943 | ds_loss: 0.0000 | lr: 3.4804e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   3 | Iter:   4860/ 13000 | global iter:   4860/ 13000 | loss: 1.3515 | ds_loss: 0.0000 | lr: 3.4798e-05 | scale: 16384.0000 | micro time: 1.891 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4860/ 13000 | global iter:   4860/ 13000 | loss: 1.6403 | ds_loss: 0.0000 | lr: 3.4798e-05 | scale: 16384.0000 | micro time: 1.891 | step time: 1.832
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4861/ 13000 | global iter:   4861/ 13000 | loss: 2.0964 | ds_loss: 0.0000 | lr: 3.4793e-05 | scale: 16384.0000 | micro time: 1.762 | step time: 0.000
train | epoch   3 | Iter:   4862/ 13000 | global iter:   4862/ 13000 | loss: 2.1580 | ds_loss: 0.0000 | lr: 3.4787e-05 | scale: 16384.0000 | micro time: 1.715 | step time: 0.000
train | epoch   3 | Iter:   4863/ 13000 | global iter:   4863/ 13000 | loss: 1.0481 | ds_loss: 0.0000 | lr: 3.4781e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   3 | Iter:   4864/ 13000 | global iter:   4864/ 13000 | loss: 1.7883 | ds_loss: 0.0000 | lr: 3.4776e-05 | scale: 16384.0000 | micro time: 1.786 | step time: 0.000
train | epoch   3 | Iter:   4865/ 13000 | global iter:   4865/ 13000 | loss: 1.2740 | ds_loss: 0.0000 | lr: 3.4770e-05 | scale: 16384.0000 | micro time: 1.796 | step time: 0.000
train | epoch   3 | Iter:   4866/ 13000 | global iter:   4866/ 13000 | loss: 1.1411 | ds_loss: 0.0000 | lr: 3.4765e-05 | scale: 16384.0000 | micro time: 1.822 | step time: 0.000
train | epoch   3 | Iter:   4867/ 13000 | global iter:   4867/ 13000 | loss: 1.8842 | ds_loss: 0.0000 | lr: 3.4759e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   3 | Iter:   4868/ 13000 | global iter:   4868/ 13000 | loss: 1.2960 | ds_loss: 0.0000 | lr: 3.4754e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   3 | Iter:   4869/ 13000 | global iter:   4869/ 13000 | loss: 1.5090 | ds_loss: 0.0000 | lr: 3.4748e-05 | scale: 16384.0000 | micro time: 1.891 | step time: 0.000
train | epoch   3 | Iter:   4870/ 13000 | global iter:   4870/ 13000 | loss: 1.5577 | ds_loss: 0.0000 | lr: 3.4743e-05 | scale: 16384.0000 | micro time: 1.923 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4870/ 13000 | global iter:   4870/ 13000 | loss: 1.5753 | ds_loss: 0.0000 | lr: 3.4743e-05 | scale: 16384.0000 | micro time: 1.923 | step time: 1.809
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4871/ 13000 | global iter:   4871/ 13000 | loss: 1.6547 | ds_loss: 0.0000 | lr: 3.4737e-05 | scale: 16384.0000 | micro time: 1.822 | step time: 0.000
train | epoch   3 | Iter:   4872/ 13000 | global iter:   4872/ 13000 | loss: 1.4681 | ds_loss: 0.0000 | lr: 3.4731e-05 | scale: 16384.0000 | micro time: 1.824 | step time: 0.000
train | epoch   3 | Iter:   4873/ 13000 | global iter:   4873/ 13000 | loss: 1.4767 | ds_loss: 0.0000 | lr: 3.4726e-05 | scale: 16384.0000 | micro time: 1.903 | step time: 0.000
train | epoch   3 | Iter:   4874/ 13000 | global iter:   4874/ 13000 | loss: 0.8263 | ds_loss: 0.0000 | lr: 3.4720e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   3 | Iter:   4875/ 13000 | global iter:   4875/ 13000 | loss: 1.3612 | ds_loss: 0.0000 | lr: 3.4715e-05 | scale: 16384.0000 | micro time: 1.789 | step time: 0.000
train | epoch   3 | Iter:   4876/ 13000 | global iter:   4876/ 13000 | loss: 1.5471 | ds_loss: 0.0000 | lr: 3.4709e-05 | scale: 16384.0000 | micro time: 1.713 | step time: 0.000
train | epoch   3 | Iter:   4877/ 13000 | global iter:   4877/ 13000 | loss: 1.4917 | ds_loss: 0.0000 | lr: 3.4704e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   3 | Iter:   4878/ 13000 | global iter:   4878/ 13000 | loss: 1.2316 | ds_loss: 0.0000 | lr: 3.4698e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   3 | Iter:   4879/ 13000 | global iter:   4879/ 13000 | loss: 2.2344 | ds_loss: 0.0000 | lr: 3.4693e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   3 | Iter:   4880/ 13000 | global iter:   4880/ 13000 | loss: 2.2119 | ds_loss: 0.0000 | lr: 3.4687e-05 | scale: 16384.0000 | micro time: 1.907 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4880/ 13000 | global iter:   4880/ 13000 | loss: 1.5504 | ds_loss: 0.0000 | lr: 3.4687e-05 | scale: 16384.0000 | micro time: 1.907 | step time: 1.814
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4881/ 13000 | global iter:   4881/ 13000 | loss: 1.7956 | ds_loss: 0.0000 | lr: 3.4681e-05 | scale: 16384.0000 | micro time: 1.757 | step time: 0.000
train | epoch   3 | Iter:   4882/ 13000 | global iter:   4882/ 13000 | loss: 1.5408 | ds_loss: 0.0000 | lr: 3.4676e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   3 | Iter:   4883/ 13000 | global iter:   4883/ 13000 | loss: 1.9642 | ds_loss: 0.0000 | lr: 3.4670e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   3 | Iter:   4884/ 13000 | global iter:   4884/ 13000 | loss: 1.4245 | ds_loss: 0.0000 | lr: 3.4665e-05 | scale: 16384.0000 | micro time: 1.861 | step time: 0.000
train | epoch   3 | Iter:   4885/ 13000 | global iter:   4885/ 13000 | loss: 2.1001 | ds_loss: 0.0000 | lr: 3.4659e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   3 | Iter:   4886/ 13000 | global iter:   4886/ 13000 | loss: 1.4046 | ds_loss: 0.0000 | lr: 3.4654e-05 | scale: 16384.0000 | micro time: 1.757 | step time: 0.000
train | epoch   3 | Iter:   4887/ 13000 | global iter:   4887/ 13000 | loss: 1.4267 | ds_loss: 0.0000 | lr: 3.4648e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   3 | Iter:   4888/ 13000 | global iter:   4888/ 13000 | loss: 1.4733 | ds_loss: 0.0000 | lr: 3.4642e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   3 | Iter:   4889/ 13000 | global iter:   4889/ 13000 | loss: 2.0418 | ds_loss: 0.0000 | lr: 3.4637e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   3 | Iter:   4890/ 13000 | global iter:   4890/ 13000 | loss: 1.8988 | ds_loss: 0.0000 | lr: 3.4631e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4890/ 13000 | global iter:   4890/ 13000 | loss: 1.7070 | ds_loss: 0.0000 | lr: 3.4631e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 1.818
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4891/ 13000 | global iter:   4891/ 13000 | loss: 1.4518 | ds_loss: 0.0000 | lr: 3.4626e-05 | scale: 16384.0000 | micro time: 1.813 | step time: 0.000
train | epoch   3 | Iter:   4892/ 13000 | global iter:   4892/ 13000 | loss: 1.4268 | ds_loss: 0.0000 | lr: 3.4620e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   3 | Iter:   4893/ 13000 | global iter:   4893/ 13000 | loss: 1.3359 | ds_loss: 0.0000 | lr: 3.4615e-05 | scale: 16384.0000 | micro time: 1.911 | step time: 0.000
train | epoch   3 | Iter:   4894/ 13000 | global iter:   4894/ 13000 | loss: 1.0467 | ds_loss: 0.0000 | lr: 3.4609e-05 | scale: 16384.0000 | micro time: 1.792 | step time: 0.000
train | epoch   3 | Iter:   4895/ 13000 | global iter:   4895/ 13000 | loss: 1.5508 | ds_loss: 0.0000 | lr: 3.4604e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   3 | Iter:   4896/ 13000 | global iter:   4896/ 13000 | loss: 1.3693 | ds_loss: 0.0000 | lr: 3.4598e-05 | scale: 16384.0000 | micro time: 1.758 | step time: 0.000
train | epoch   3 | Iter:   4897/ 13000 | global iter:   4897/ 13000 | loss: 1.9660 | ds_loss: 0.0000 | lr: 3.4592e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   3 | Iter:   4898/ 13000 | global iter:   4898/ 13000 | loss: 1.0796 | ds_loss: 0.0000 | lr: 3.4587e-05 | scale: 16384.0000 | micro time: 1.860 | step time: 0.000
train | epoch   3 | Iter:   4899/ 13000 | global iter:   4899/ 13000 | loss: 1.9768 | ds_loss: 0.0000 | lr: 3.4581e-05 | scale: 16384.0000 | micro time: 1.758 | step time: 0.000
train | epoch   3 | Iter:   4900/ 13000 | global iter:   4900/ 13000 | loss: 1.6319 | ds_loss: 0.0000 | lr: 3.4576e-05 | scale: 16384.0000 | micro time: 1.757 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4900/ 13000 | global iter:   4900/ 13000 | loss: 1.4835 | ds_loss: 0.0000 | lr: 3.4576e-05 | scale: 16384.0000 | micro time: 1.757 | step time: 1.811
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4901/ 13000 | global iter:   4901/ 13000 | loss: 1.6711 | ds_loss: 0.0000 | lr: 3.4570e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   3 | Iter:   4902/ 13000 | global iter:   4902/ 13000 | loss: 1.6256 | ds_loss: 0.0000 | lr: 3.4565e-05 | scale: 16384.0000 | micro time: 1.756 | step time: 0.000
train | epoch   3 | Iter:   4903/ 13000 | global iter:   4903/ 13000 | loss: 1.4186 | ds_loss: 0.0000 | lr: 3.4559e-05 | scale: 16384.0000 | micro time: 1.893 | step time: 0.000
train | epoch   3 | Iter:   4904/ 13000 | global iter:   4904/ 13000 | loss: 0.7286 | ds_loss: 0.0000 | lr: 3.4553e-05 | scale: 16384.0000 | micro time: 1.737 | step time: 0.000
train | epoch   3 | Iter:   4905/ 13000 | global iter:   4905/ 13000 | loss: 1.5013 | ds_loss: 0.0000 | lr: 3.4548e-05 | scale: 16384.0000 | micro time: 1.922 | step time: 0.000
train | epoch   3 | Iter:   4906/ 13000 | global iter:   4906/ 13000 | loss: 2.0298 | ds_loss: 0.0000 | lr: 3.4542e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   3 | Iter:   4907/ 13000 | global iter:   4907/ 13000 | loss: 2.0930 | ds_loss: 0.0000 | lr: 3.4537e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   3 | Iter:   4908/ 13000 | global iter:   4908/ 13000 | loss: 2.0910 | ds_loss: 0.0000 | lr: 3.4531e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 0.000
train | epoch   3 | Iter:   4909/ 13000 | global iter:   4909/ 13000 | loss: 1.6387 | ds_loss: 0.0000 | lr: 3.4525e-05 | scale: 16384.0000 | micro time: 1.913 | step time: 0.000
train | epoch   3 | Iter:   4910/ 13000 | global iter:   4910/ 13000 | loss: 2.0173 | ds_loss: 0.0000 | lr: 3.4520e-05 | scale: 16384.0000 | micro time: 1.801 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4910/ 13000 | global iter:   4910/ 13000 | loss: 1.6815 | ds_loss: 0.0000 | lr: 3.4520e-05 | scale: 16384.0000 | micro time: 1.801 | step time: 1.833
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4911/ 13000 | global iter:   4911/ 13000 | loss: 1.3885 | ds_loss: 0.0000 | lr: 3.4514e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   3 | Iter:   4912/ 13000 | global iter:   4912/ 13000 | loss: 1.2119 | ds_loss: 0.0000 | lr: 3.4509e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   3 | Iter:   4913/ 13000 | global iter:   4913/ 13000 | loss: 1.3581 | ds_loss: 0.0000 | lr: 3.4503e-05 | scale: 16384.0000 | micro time: 1.740 | step time: 0.000
train | epoch   3 | Iter:   4914/ 13000 | global iter:   4914/ 13000 | loss: 1.9463 | ds_loss: 0.0000 | lr: 3.4498e-05 | scale: 16384.0000 | micro time: 1.797 | step time: 0.000
train | epoch   3 | Iter:   4915/ 13000 | global iter:   4915/ 13000 | loss: 1.5825 | ds_loss: 0.0000 | lr: 3.4492e-05 | scale: 16384.0000 | micro time: 1.732 | step time: 0.000
train | epoch   3 | Iter:   4916/ 13000 | global iter:   4916/ 13000 | loss: 1.9597 | ds_loss: 0.0000 | lr: 3.4486e-05 | scale: 16384.0000 | micro time: 1.806 | step time: 0.000
train | epoch   3 | Iter:   4917/ 13000 | global iter:   4917/ 13000 | loss: 1.5184 | ds_loss: 0.0000 | lr: 3.4481e-05 | scale: 16384.0000 | micro time: 1.862 | step time: 0.000
train | epoch   3 | Iter:   4918/ 13000 | global iter:   4918/ 13000 | loss: 1.3261 | ds_loss: 0.0000 | lr: 3.4475e-05 | scale: 16384.0000 | micro time: 1.940 | step time: 0.000
train | epoch   3 | Iter:   4919/ 13000 | global iter:   4919/ 13000 | loss: 1.7198 | ds_loss: 0.0000 | lr: 3.4470e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   3 | Iter:   4920/ 13000 | global iter:   4920/ 13000 | loss: 1.9485 | ds_loss: 0.0000 | lr: 3.4464e-05 | scale: 16384.0000 | micro time: 1.761 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4920/ 13000 | global iter:   4920/ 13000 | loss: 1.5960 | ds_loss: 0.0000 | lr: 3.4464e-05 | scale: 16384.0000 | micro time: 1.761 | step time: 1.805
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4921/ 13000 | global iter:   4921/ 13000 | loss: 1.3341 | ds_loss: 0.0000 | lr: 3.4459e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   3 | Iter:   4922/ 13000 | global iter:   4922/ 13000 | loss: 1.7659 | ds_loss: 0.0000 | lr: 3.4453e-05 | scale: 16384.0000 | micro time: 1.907 | step time: 0.000
train | epoch   3 | Iter:   4923/ 13000 | global iter:   4923/ 13000 | loss: 1.4188 | ds_loss: 0.0000 | lr: 3.4447e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   3 | Iter:   4924/ 13000 | global iter:   4924/ 13000 | loss: 2.0880 | ds_loss: 0.0000 | lr: 3.4442e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   3 | Iter:   4925/ 13000 | global iter:   4925/ 13000 | loss: 1.7288 | ds_loss: 0.0000 | lr: 3.4436e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   3 | Iter:   4926/ 13000 | global iter:   4926/ 13000 | loss: 1.5886 | ds_loss: 0.0000 | lr: 3.4431e-05 | scale: 16384.0000 | micro time: 1.788 | step time: 0.000
train | epoch   3 | Iter:   4927/ 13000 | global iter:   4927/ 13000 | loss: 1.6497 | ds_loss: 0.0000 | lr: 3.4425e-05 | scale: 16384.0000 | micro time: 1.881 | step time: 0.000
train | epoch   3 | Iter:   4928/ 13000 | global iter:   4928/ 13000 | loss: 1.4985 | ds_loss: 0.0000 | lr: 3.4419e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   3 | Iter:   4929/ 13000 | global iter:   4929/ 13000 | loss: 1.9896 | ds_loss: 0.0000 | lr: 3.4414e-05 | scale: 16384.0000 | micro time: 1.857 | step time: 0.000
train | epoch   3 | Iter:   4930/ 13000 | global iter:   4930/ 13000 | loss: 0.8409 | ds_loss: 0.0000 | lr: 3.4408e-05 | scale: 16384.0000 | micro time: 1.830 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4930/ 13000 | global iter:   4930/ 13000 | loss: 1.5903 | ds_loss: 0.0000 | lr: 3.4408e-05 | scale: 16384.0000 | micro time: 1.830 | step time: 1.833
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4931/ 13000 | global iter:   4931/ 13000 | loss: 1.7049 | ds_loss: 0.0000 | lr: 3.4403e-05 | scale: 16384.0000 | micro time: 1.780 | step time: 0.000
train | epoch   3 | Iter:   4932/ 13000 | global iter:   4932/ 13000 | loss: 1.7853 | ds_loss: 0.0000 | lr: 3.4397e-05 | scale: 16384.0000 | micro time: 1.840 | step time: 0.000
train | epoch   3 | Iter:   4933/ 13000 | global iter:   4933/ 13000 | loss: 1.5910 | ds_loss: 0.0000 | lr: 3.4391e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   3 | Iter:   4934/ 13000 | global iter:   4934/ 13000 | loss: 2.0190 | ds_loss: 0.0000 | lr: 3.4386e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   3 | Iter:   4935/ 13000 | global iter:   4935/ 13000 | loss: 1.5608 | ds_loss: 0.0000 | lr: 3.4380e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   3 | Iter:   4936/ 13000 | global iter:   4936/ 13000 | loss: 1.9383 | ds_loss: 0.0000 | lr: 3.4375e-05 | scale: 16384.0000 | micro time: 1.904 | step time: 0.000
train | epoch   3 | Iter:   4937/ 13000 | global iter:   4937/ 13000 | loss: 1.5850 | ds_loss: 0.0000 | lr: 3.4369e-05 | scale: 16384.0000 | micro time: 1.716 | step time: 0.000
train | epoch   3 | Iter:   4938/ 13000 | global iter:   4938/ 13000 | loss: 1.7379 | ds_loss: 0.0000 | lr: 3.4364e-05 | scale: 16384.0000 | micro time: 1.768 | step time: 0.000
train | epoch   3 | Iter:   4939/ 13000 | global iter:   4939/ 13000 | loss: 1.6312 | ds_loss: 0.0000 | lr: 3.4358e-05 | scale: 16384.0000 | micro time: 1.912 | step time: 0.000
train | epoch   3 | Iter:   4940/ 13000 | global iter:   4940/ 13000 | loss: 1.9281 | ds_loss: 0.0000 | lr: 3.4352e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4940/ 13000 | global iter:   4940/ 13000 | loss: 1.7481 | ds_loss: 0.0000 | lr: 3.4352e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 1.805
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4941/ 13000 | global iter:   4941/ 13000 | loss: 1.8441 | ds_loss: 0.0000 | lr: 3.4347e-05 | scale: 16384.0000 | micro time: 1.768 | step time: 0.000
train | epoch   3 | Iter:   4942/ 13000 | global iter:   4942/ 13000 | loss: 1.5493 | ds_loss: 0.0000 | lr: 3.4341e-05 | scale: 16384.0000 | micro time: 1.876 | step time: 0.000
train | epoch   3 | Iter:   4943/ 13000 | global iter:   4943/ 13000 | loss: 1.3344 | ds_loss: 0.0000 | lr: 3.4336e-05 | scale: 16384.0000 | micro time: 1.727 | step time: 0.000
train | epoch   3 | Iter:   4944/ 13000 | global iter:   4944/ 13000 | loss: 1.4422 | ds_loss: 0.0000 | lr: 3.4330e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   3 | Iter:   4945/ 13000 | global iter:   4945/ 13000 | loss: 1.8379 | ds_loss: 0.0000 | lr: 3.4324e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   3 | Iter:   4946/ 13000 | global iter:   4946/ 13000 | loss: 2.0545 | ds_loss: 0.0000 | lr: 3.4319e-05 | scale: 16384.0000 | micro time: 1.809 | step time: 0.000
train | epoch   3 | Iter:   4947/ 13000 | global iter:   4947/ 13000 | loss: 2.2248 | ds_loss: 0.0000 | lr: 3.4313e-05 | scale: 16384.0000 | micro time: 1.794 | step time: 0.000
train | epoch   3 | Iter:   4948/ 13000 | global iter:   4948/ 13000 | loss: 1.0017 | ds_loss: 0.0000 | lr: 3.4308e-05 | scale: 16384.0000 | micro time: 1.683 | step time: 0.000
train | epoch   3 | Iter:   4949/ 13000 | global iter:   4949/ 13000 | loss: 1.1205 | ds_loss: 0.0000 | lr: 3.4302e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   3 | Iter:   4950/ 13000 | global iter:   4950/ 13000 | loss: 1.6658 | ds_loss: 0.0000 | lr: 3.4296e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4950/ 13000 | global iter:   4950/ 13000 | loss: 1.6075 | ds_loss: 0.0000 | lr: 3.4296e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 1.785
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4951/ 13000 | global iter:   4951/ 13000 | loss: 1.7754 | ds_loss: 0.0000 | lr: 3.4291e-05 | scale: 16384.0000 | micro time: 1.834 | step time: 0.000
train | epoch   3 | Iter:   4952/ 13000 | global iter:   4952/ 13000 | loss: 1.4858 | ds_loss: 0.0000 | lr: 3.4285e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   3 | Iter:   4953/ 13000 | global iter:   4953/ 13000 | loss: 1.4632 | ds_loss: 0.0000 | lr: 3.4280e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   3 | Iter:   4954/ 13000 | global iter:   4954/ 13000 | loss: 1.7090 | ds_loss: 0.0000 | lr: 3.4274e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   3 | Iter:   4955/ 13000 | global iter:   4955/ 13000 | loss: 1.4022 | ds_loss: 0.0000 | lr: 3.4268e-05 | scale: 16384.0000 | micro time: 1.770 | step time: 0.000
train | epoch   3 | Iter:   4956/ 13000 | global iter:   4956/ 13000 | loss: 1.7151 | ds_loss: 0.0000 | lr: 3.4263e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   3 | Iter:   4957/ 13000 | global iter:   4957/ 13000 | loss: 1.7919 | ds_loss: 0.0000 | lr: 3.4257e-05 | scale: 16384.0000 | micro time: 1.743 | step time: 0.000
train | epoch   3 | Iter:   4958/ 13000 | global iter:   4958/ 13000 | loss: 1.7344 | ds_loss: 0.0000 | lr: 3.4252e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   3 | Iter:   4959/ 13000 | global iter:   4959/ 13000 | loss: 1.6169 | ds_loss: 0.0000 | lr: 3.4246e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   3 | Iter:   4960/ 13000 | global iter:   4960/ 13000 | loss: 1.9281 | ds_loss: 0.0000 | lr: 3.4240e-05 | scale: 16384.0000 | micro time: 1.895 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4960/ 13000 | global iter:   4960/ 13000 | loss: 1.6622 | ds_loss: 0.0000 | lr: 3.4240e-05 | scale: 16384.0000 | micro time: 1.895 | step time: 1.811
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4961/ 13000 | global iter:   4961/ 13000 | loss: 1.9772 | ds_loss: 0.0000 | lr: 3.4235e-05 | scale: 16384.0000 | micro time: 1.876 | step time: 0.000
train | epoch   3 | Iter:   4962/ 13000 | global iter:   4962/ 13000 | loss: 1.4934 | ds_loss: 0.0000 | lr: 3.4229e-05 | scale: 16384.0000 | micro time: 1.765 | step time: 0.000
train | epoch   3 | Iter:   4963/ 13000 | global iter:   4963/ 13000 | loss: 1.5544 | ds_loss: 0.0000 | lr: 3.4223e-05 | scale: 16384.0000 | micro time: 1.780 | step time: 0.000
train | epoch   3 | Iter:   4964/ 13000 | global iter:   4964/ 13000 | loss: 1.4299 | ds_loss: 0.0000 | lr: 3.4218e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   3 | Iter:   4965/ 13000 | global iter:   4965/ 13000 | loss: 1.6103 | ds_loss: 0.0000 | lr: 3.4212e-05 | scale: 16384.0000 | micro time: 1.834 | step time: 0.000
train | epoch   3 | Iter:   4966/ 13000 | global iter:   4966/ 13000 | loss: 1.3408 | ds_loss: 0.0000 | lr: 3.4207e-05 | scale: 16384.0000 | micro time: 1.862 | step time: 0.000
train | epoch   3 | Iter:   4967/ 13000 | global iter:   4967/ 13000 | loss: 1.4588 | ds_loss: 0.0000 | lr: 3.4201e-05 | scale: 16384.0000 | micro time: 1.895 | step time: 0.000
train | epoch   3 | Iter:   4968/ 13000 | global iter:   4968/ 13000 | loss: 1.9260 | ds_loss: 0.0000 | lr: 3.4195e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   3 | Iter:   4969/ 13000 | global iter:   4969/ 13000 | loss: 1.7245 | ds_loss: 0.0000 | lr: 3.4190e-05 | scale: 16384.0000 | micro time: 1.825 | step time: 0.000
train | epoch   3 | Iter:   4970/ 13000 | global iter:   4970/ 13000 | loss: 1.4246 | ds_loss: 0.0000 | lr: 3.4184e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4970/ 13000 | global iter:   4970/ 13000 | loss: 1.5940 | ds_loss: 0.0000 | lr: 3.4184e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 1.830
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4971/ 13000 | global iter:   4971/ 13000 | loss: 1.6937 | ds_loss: 0.0000 | lr: 3.4179e-05 | scale: 16384.0000 | micro time: 1.845 | step time: 0.000
train | epoch   3 | Iter:   4972/ 13000 | global iter:   4972/ 13000 | loss: 1.5764 | ds_loss: 0.0000 | lr: 3.4173e-05 | scale: 16384.0000 | micro time: 1.816 | step time: 0.000
train | epoch   3 | Iter:   4973/ 13000 | global iter:   4973/ 13000 | loss: 1.3344 | ds_loss: 0.0000 | lr: 3.4167e-05 | scale: 16384.0000 | micro time: 1.809 | step time: 0.000
train | epoch   3 | Iter:   4974/ 13000 | global iter:   4974/ 13000 | loss: 1.8768 | ds_loss: 0.0000 | lr: 3.4162e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   3 | Iter:   4975/ 13000 | global iter:   4975/ 13000 | loss: 1.7962 | ds_loss: 0.0000 | lr: 3.4156e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   3 | Iter:   4976/ 13000 | global iter:   4976/ 13000 | loss: 1.8423 | ds_loss: 0.0000 | lr: 3.4151e-05 | scale: 16384.0000 | micro time: 1.894 | step time: 0.000
train | epoch   3 | Iter:   4977/ 13000 | global iter:   4977/ 13000 | loss: 1.7834 | ds_loss: 0.0000 | lr: 3.4145e-05 | scale: 16384.0000 | micro time: 1.841 | step time: 0.000
train | epoch   3 | Iter:   4978/ 13000 | global iter:   4978/ 13000 | loss: 1.7177 | ds_loss: 0.0000 | lr: 3.4139e-05 | scale: 16384.0000 | micro time: 1.768 | step time: 0.000
train | epoch   3 | Iter:   4979/ 13000 | global iter:   4979/ 13000 | loss: 1.2647 | ds_loss: 0.0000 | lr: 3.4134e-05 | scale: 16384.0000 | micro time: 1.828 | step time: 0.000
train | epoch   3 | Iter:   4980/ 13000 | global iter:   4980/ 13000 | loss: 1.2223 | ds_loss: 0.0000 | lr: 3.4128e-05 | scale: 16384.0000 | micro time: 1.772 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4980/ 13000 | global iter:   4980/ 13000 | loss: 1.6108 | ds_loss: 0.0000 | lr: 3.4128e-05 | scale: 16384.0000 | micro time: 1.772 | step time: 1.819
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4981/ 13000 | global iter:   4981/ 13000 | loss: 1.7155 | ds_loss: 0.0000 | lr: 3.4122e-05 | scale: 16384.0000 | micro time: 1.870 | step time: 0.000
train | epoch   3 | Iter:   4982/ 13000 | global iter:   4982/ 13000 | loss: 1.1758 | ds_loss: 0.0000 | lr: 3.4117e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   3 | Iter:   4983/ 13000 | global iter:   4983/ 13000 | loss: 1.7960 | ds_loss: 0.0000 | lr: 3.4111e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   3 | Iter:   4984/ 13000 | global iter:   4984/ 13000 | loss: 1.5278 | ds_loss: 0.0000 | lr: 3.4106e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   3 | Iter:   4985/ 13000 | global iter:   4985/ 13000 | loss: 1.1424 | ds_loss: 0.0000 | lr: 3.4100e-05 | scale: 16384.0000 | micro time: 1.891 | step time: 0.000
train | epoch   3 | Iter:   4986/ 13000 | global iter:   4986/ 13000 | loss: 1.7207 | ds_loss: 0.0000 | lr: 3.4094e-05 | scale: 16384.0000 | micro time: 1.754 | step time: 0.000
train | epoch   3 | Iter:   4987/ 13000 | global iter:   4987/ 13000 | loss: 1.8215 | ds_loss: 0.0000 | lr: 3.4089e-05 | scale: 16384.0000 | micro time: 1.844 | step time: 0.000
train | epoch   3 | Iter:   4988/ 13000 | global iter:   4988/ 13000 | loss: 2.1482 | ds_loss: 0.0000 | lr: 3.4083e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   3 | Iter:   4989/ 13000 | global iter:   4989/ 13000 | loss: 1.1788 | ds_loss: 0.0000 | lr: 3.4078e-05 | scale: 16384.0000 | micro time: 1.903 | step time: 0.000
train | epoch   3 | Iter:   4990/ 13000 | global iter:   4990/ 13000 | loss: 1.5001 | ds_loss: 0.0000 | lr: 3.4072e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   4990/ 13000 | global iter:   4990/ 13000 | loss: 1.5727 | ds_loss: 0.0000 | lr: 3.4072e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 1.838
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   4991/ 13000 | global iter:   4991/ 13000 | loss: 1.9472 | ds_loss: 0.0000 | lr: 3.4066e-05 | scale: 16384.0000 | micro time: 1.746 | step time: 0.000
train | epoch   3 | Iter:   4992/ 13000 | global iter:   4992/ 13000 | loss: 1.8081 | ds_loss: 0.0000 | lr: 3.4061e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   3 | Iter:   4993/ 13000 | global iter:   4993/ 13000 | loss: 0.9990 | ds_loss: 0.0000 | lr: 3.4055e-05 | scale: 16384.0000 | micro time: 1.685 | step time: 0.000
train | epoch   3 | Iter:   4994/ 13000 | global iter:   4994/ 13000 | loss: 1.8101 | ds_loss: 0.0000 | lr: 3.4049e-05 | scale: 16384.0000 | micro time: 1.877 | step time: 0.000
train | epoch   3 | Iter:   4995/ 13000 | global iter:   4995/ 13000 | loss: 1.4527 | ds_loss: 0.0000 | lr: 3.4044e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   3 | Iter:   4996/ 13000 | global iter:   4996/ 13000 | loss: 1.2723 | ds_loss: 0.0000 | lr: 3.4038e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
train | epoch   3 | Iter:   4997/ 13000 | global iter:   4997/ 13000 | loss: 2.0392 | ds_loss: 0.0000 | lr: 3.4033e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   3 | Iter:   4998/ 13000 | global iter:   4998/ 13000 | loss: 1.7894 | ds_loss: 0.0000 | lr: 3.4027e-05 | scale: 16384.0000 | micro time: 1.719 | step time: 0.000
train | epoch   3 | Iter:   4999/ 13000 | global iter:   4999/ 13000 | loss: 2.2613 | ds_loss: 0.0000 | lr: 3.4021e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   3 | Iter:   5000/ 13000 | global iter:   5000/ 13000 | loss: 0.9250 | ds_loss: 0.0000 | lr: 3.4016e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   5000/ 13000 | global iter:   5000/ 13000 | loss: 1.6304 | ds_loss: 0.0000 | lr: 3.4016e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 1.789
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   5001/ 13000 | global iter:   5001/ 13000 | loss: 1.6041 | ds_loss: 0.0000 | lr: 3.4010e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   3 | Iter:   5002/ 13000 | global iter:   5002/ 13000 | loss: 1.8016 | ds_loss: 0.0000 | lr: 3.4004e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   3 | Iter:   5003/ 13000 | global iter:   5003/ 13000 | loss: 1.4100 | ds_loss: 0.0000 | lr: 3.3999e-05 | scale: 16384.0000 | micro time: 1.824 | step time: 0.000
train | epoch   3 | Iter:   5004/ 13000 | global iter:   5004/ 13000 | loss: 1.8851 | ds_loss: 0.0000 | lr: 3.3993e-05 | scale: 16384.0000 | micro time: 1.861 | step time: 0.000
train | epoch   3 | Iter:   5005/ 13000 | global iter:   5005/ 13000 | loss: 1.9301 | ds_loss: 0.0000 | lr: 3.3988e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   3 | Iter:   5006/ 13000 | global iter:   5006/ 13000 | loss: 1.3544 | ds_loss: 0.0000 | lr: 3.3982e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   3 | Iter:   5007/ 13000 | global iter:   5007/ 13000 | loss: 1.8558 | ds_loss: 0.0000 | lr: 3.3976e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   3 | Iter:   5008/ 13000 | global iter:   5008/ 13000 | loss: 1.9469 | ds_loss: 0.0000 | lr: 3.3971e-05 | scale: 16384.0000 | micro time: 1.897 | step time: 0.000
train | epoch   3 | Iter:   5009/ 13000 | global iter:   5009/ 13000 | loss: 1.3090 | ds_loss: 0.0000 | lr: 3.3965e-05 | scale: 16384.0000 | micro time: 1.821 | step time: 0.000
train | epoch   3 | Iter:   5010/ 13000 | global iter:   5010/ 13000 | loss: 1.4516 | ds_loss: 0.0000 | lr: 3.3959e-05 | scale: 16384.0000 | micro time: 1.730 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   5010/ 13000 | global iter:   5010/ 13000 | loss: 1.6548 | ds_loss: 0.0000 | lr: 3.3959e-05 | scale: 16384.0000 | micro time: 1.730 | step time: 1.811
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   5011/ 13000 | global iter:   5011/ 13000 | loss: 2.0012 | ds_loss: 0.0000 | lr: 3.3954e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   3 | Iter:   5012/ 13000 | global iter:   5012/ 13000 | loss: 1.3356 | ds_loss: 0.0000 | lr: 3.3948e-05 | scale: 16384.0000 | micro time: 1.891 | step time: 0.000
train | epoch   3 | Iter:   5013/ 13000 | global iter:   5013/ 13000 | loss: 1.9132 | ds_loss: 0.0000 | lr: 3.3942e-05 | scale: 16384.0000 | micro time: 1.663 | step time: 0.000
train | epoch   3 | Iter:   5014/ 13000 | global iter:   5014/ 13000 | loss: 1.5557 | ds_loss: 0.0000 | lr: 3.3937e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   3 | Iter:   5015/ 13000 | global iter:   5015/ 13000 | loss: 1.1443 | ds_loss: 0.0000 | lr: 3.3931e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   3 | Iter:   5016/ 13000 | global iter:   5016/ 13000 | loss: 1.3739 | ds_loss: 0.0000 | lr: 3.3926e-05 | scale: 16384.0000 | micro time: 1.743 | step time: 0.000
train | epoch   3 | Iter:   5017/ 13000 | global iter:   5017/ 13000 | loss: 1.3785 | ds_loss: 0.0000 | lr: 3.3920e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   3 | Iter:   5018/ 13000 | global iter:   5018/ 13000 | loss: 1.8491 | ds_loss: 0.0000 | lr: 3.3914e-05 | scale: 16384.0000 | micro time: 1.722 | step time: 0.000
train | epoch   3 | Iter:   5019/ 13000 | global iter:   5019/ 13000 | loss: 1.8518 | ds_loss: 0.0000 | lr: 3.3909e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   3 | Iter:   5020/ 13000 | global iter:   5020/ 13000 | loss: 1.4693 | ds_loss: 0.0000 | lr: 3.3903e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   5020/ 13000 | global iter:   5020/ 13000 | loss: 1.5872 | ds_loss: 0.0000 | lr: 3.3903e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 1.791
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   5021/ 13000 | global iter:   5021/ 13000 | loss: 2.1616 | ds_loss: 0.0000 | lr: 3.3897e-05 | scale: 16384.0000 | micro time: 1.833 | step time: 0.000
train | epoch   3 | Iter:   5022/ 13000 | global iter:   5022/ 13000 | loss: 1.6157 | ds_loss: 0.0000 | lr: 3.3892e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   3 | Iter:   5023/ 13000 | global iter:   5023/ 13000 | loss: 1.1700 | ds_loss: 0.0000 | lr: 3.3886e-05 | scale: 16384.0000 | micro time: 1.830 | step time: 0.000
train | epoch   3 | Iter:   5024/ 13000 | global iter:   5024/ 13000 | loss: 2.1164 | ds_loss: 0.0000 | lr: 3.3880e-05 | scale: 16384.0000 | micro time: 1.683 | step time: 0.000
train | epoch   3 | Iter:   5025/ 13000 | global iter:   5025/ 13000 | loss: 1.4512 | ds_loss: 0.0000 | lr: 3.3875e-05 | scale: 16384.0000 | micro time: 1.824 | step time: 0.000
train | epoch   3 | Iter:   5026/ 13000 | global iter:   5026/ 13000 | loss: 1.9416 | ds_loss: 0.0000 | lr: 3.3869e-05 | scale: 16384.0000 | micro time: 1.770 | step time: 0.000
train | epoch   3 | Iter:   5027/ 13000 | global iter:   5027/ 13000 | loss: 1.6929 | ds_loss: 0.0000 | lr: 3.3864e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   3 | Iter:   5028/ 13000 | global iter:   5028/ 13000 | loss: 0.8588 | ds_loss: 0.0000 | lr: 3.3858e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   3 | Iter:   5029/ 13000 | global iter:   5029/ 13000 | loss: 1.0769 | ds_loss: 0.0000 | lr: 3.3852e-05 | scale: 16384.0000 | micro time: 1.854 | step time: 0.000
train | epoch   3 | Iter:   5030/ 13000 | global iter:   5030/ 13000 | loss: 1.7861 | ds_loss: 0.0000 | lr: 3.3847e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   5030/ 13000 | global iter:   5030/ 13000 | loss: 1.5871 | ds_loss: 0.0000 | lr: 3.3847e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 1.804
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   5031/ 13000 | global iter:   5031/ 13000 | loss: 1.2211 | ds_loss: 0.0000 | lr: 3.3841e-05 | scale: 16384.0000 | micro time: 1.792 | step time: 0.000
train | epoch   3 | Iter:   5032/ 13000 | global iter:   5032/ 13000 | loss: 1.6787 | ds_loss: 0.0000 | lr: 3.3835e-05 | scale: 16384.0000 | micro time: 1.742 | step time: 0.000
train | epoch   3 | Iter:   5033/ 13000 | global iter:   5033/ 13000 | loss: 1.9391 | ds_loss: 0.0000 | lr: 3.3830e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 0.000
train | epoch   3 | Iter:   5034/ 13000 | global iter:   5034/ 13000 | loss: 1.1726 | ds_loss: 0.0000 | lr: 3.3824e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   3 | Iter:   5035/ 13000 | global iter:   5035/ 13000 | loss: 1.2605 | ds_loss: 0.0000 | lr: 3.3818e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   3 | Iter:   5036/ 13000 | global iter:   5036/ 13000 | loss: 1.0609 | ds_loss: 0.0000 | lr: 3.3813e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   3 | Iter:   5037/ 13000 | global iter:   5037/ 13000 | loss: 1.6933 | ds_loss: 0.0000 | lr: 3.3807e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   3 | Iter:   5038/ 13000 | global iter:   5038/ 13000 | loss: 1.9236 | ds_loss: 0.0000 | lr: 3.3801e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   3 | Iter:   5039/ 13000 | global iter:   5039/ 13000 | loss: 1.9883 | ds_loss: 0.0000 | lr: 3.3796e-05 | scale: 16384.0000 | micro time: 1.826 | step time: 0.000
train | epoch   3 | Iter:   5040/ 13000 | global iter:   5040/ 13000 | loss: 1.9565 | ds_loss: 0.0000 | lr: 3.3790e-05 | scale: 16384.0000 | micro time: 1.734 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   5040/ 13000 | global iter:   5040/ 13000 | loss: 1.5894 | ds_loss: 0.0000 | lr: 3.3790e-05 | scale: 16384.0000 | micro time: 1.734 | step time: 1.794
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   5041/ 13000 | global iter:   5041/ 13000 | loss: 1.5094 | ds_loss: 0.0000 | lr: 3.3785e-05 | scale: 16384.0000 | micro time: 1.808 | step time: 0.000
train | epoch   3 | Iter:   5042/ 13000 | global iter:   5042/ 13000 | loss: 1.4885 | ds_loss: 0.0000 | lr: 3.3779e-05 | scale: 16384.0000 | micro time: 1.804 | step time: 0.000
train | epoch   3 | Iter:   5043/ 13000 | global iter:   5043/ 13000 | loss: 1.6365 | ds_loss: 0.0000 | lr: 3.3773e-05 | scale: 16384.0000 | micro time: 1.753 | step time: 0.000
train | epoch   3 | Iter:   5044/ 13000 | global iter:   5044/ 13000 | loss: 1.6089 | ds_loss: 0.0000 | lr: 3.3768e-05 | scale: 16384.0000 | micro time: 1.841 | step time: 0.000
train | epoch   3 | Iter:   5045/ 13000 | global iter:   5045/ 13000 | loss: 1.9648 | ds_loss: 0.0000 | lr: 3.3762e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   3 | Iter:   5046/ 13000 | global iter:   5046/ 13000 | loss: 1.7276 | ds_loss: 0.0000 | lr: 3.3756e-05 | scale: 16384.0000 | micro time: 1.790 | step time: 0.000
train | epoch   3 | Iter:   5047/ 13000 | global iter:   5047/ 13000 | loss: 0.4810 | ds_loss: 0.0000 | lr: 3.3751e-05 | scale: 16384.0000 | micro time: 1.690 | step time: 0.000
train | epoch   3 | Iter:   5048/ 13000 | global iter:   5048/ 13000 | loss: 1.7828 | ds_loss: 0.0000 | lr: 3.3745e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   3 | Iter:   5049/ 13000 | global iter:   5049/ 13000 | loss: 1.8481 | ds_loss: 0.0000 | lr: 3.3739e-05 | scale: 16384.0000 | micro time: 1.834 | step time: 0.000
train | epoch   3 | Iter:   5050/ 13000 | global iter:   5050/ 13000 | loss: 1.5278 | ds_loss: 0.0000 | lr: 3.3734e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   5050/ 13000 | global iter:   5050/ 13000 | loss: 1.5575 | ds_loss: 0.0000 | lr: 3.3734e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 1.787
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   5051/ 13000 | global iter:   5051/ 13000 | loss: 1.5143 | ds_loss: 0.0000 | lr: 3.3728e-05 | scale: 16384.0000 | micro time: 1.698 | step time: 0.000
train | epoch   3 | Iter:   5052/ 13000 | global iter:   5052/ 13000 | loss: 1.4152 | ds_loss: 0.0000 | lr: 3.3722e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   3 | Iter:   5053/ 13000 | global iter:   5053/ 13000 | loss: 1.2322 | ds_loss: 0.0000 | lr: 3.3717e-05 | scale: 16384.0000 | micro time: 1.660 | step time: 0.000
train | epoch   3 | Iter:   5054/ 13000 | global iter:   5054/ 13000 | loss: 1.7361 | ds_loss: 0.0000 | lr: 3.3711e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   3 | Iter:   5055/ 13000 | global iter:   5055/ 13000 | loss: 1.1643 | ds_loss: 0.0000 | lr: 3.3705e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   3 | Iter:   5056/ 13000 | global iter:   5056/ 13000 | loss: 1.4204 | ds_loss: 0.0000 | lr: 3.3700e-05 | scale: 16384.0000 | micro time: 1.927 | step time: 0.000
train | epoch   3 | Iter:   5057/ 13000 | global iter:   5057/ 13000 | loss: 1.5111 | ds_loss: 0.0000 | lr: 3.3694e-05 | scale: 16384.0000 | micro time: 1.850 | step time: 0.000
train | epoch   3 | Iter:   5058/ 13000 | global iter:   5058/ 13000 | loss: 1.7909 | ds_loss: 0.0000 | lr: 3.3688e-05 | scale: 16384.0000 | micro time: 1.864 | step time: 0.000
train | epoch   3 | Iter:   5059/ 13000 | global iter:   5059/ 13000 | loss: 1.6323 | ds_loss: 0.0000 | lr: 3.3683e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   3 | Iter:   5060/ 13000 | global iter:   5060/ 13000 | loss: 1.2304 | ds_loss: 0.0000 | lr: 3.3677e-05 | scale: 16384.0000 | micro time: 1.715 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   5060/ 13000 | global iter:   5060/ 13000 | loss: 1.4647 | ds_loss: 0.0000 | lr: 3.3677e-05 | scale: 16384.0000 | micro time: 1.715 | step time: 1.800
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   5061/ 13000 | global iter:   5061/ 13000 | loss: 1.3768 | ds_loss: 0.0000 | lr: 3.3671e-05 | scale: 16384.0000 | micro time: 1.866 | step time: 0.000
train | epoch   3 | Iter:   5062/ 13000 | global iter:   5062/ 13000 | loss: 1.6488 | ds_loss: 0.0000 | lr: 3.3666e-05 | scale: 16384.0000 | micro time: 1.794 | step time: 0.000
train | epoch   3 | Iter:   5063/ 13000 | global iter:   5063/ 13000 | loss: 1.5631 | ds_loss: 0.0000 | lr: 3.3660e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   3 | Iter:   5064/ 13000 | global iter:   5064/ 13000 | loss: 1.3165 | ds_loss: 0.0000 | lr: 3.3655e-05 | scale: 16384.0000 | micro time: 1.798 | step time: 0.000
train | epoch   3 | Iter:   5065/ 13000 | global iter:   5065/ 13000 | loss: 1.2116 | ds_loss: 0.0000 | lr: 3.3649e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 0.000
train | epoch   3 | Iter:   5066/ 13000 | global iter:   5066/ 13000 | loss: 1.9673 | ds_loss: 0.0000 | lr: 3.3643e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   3 | Iter:   5067/ 13000 | global iter:   5067/ 13000 | loss: 2.2322 | ds_loss: 0.0000 | lr: 3.3638e-05 | scale: 16384.0000 | micro time: 1.781 | step time: 0.000
train | epoch   3 | Iter:   5068/ 13000 | global iter:   5068/ 13000 | loss: 1.7180 | ds_loss: 0.0000 | lr: 3.3632e-05 | scale: 16384.0000 | micro time: 1.861 | step time: 0.000
train | epoch   3 | Iter:   5069/ 13000 | global iter:   5069/ 13000 | loss: 1.4783 | ds_loss: 0.0000 | lr: 3.3626e-05 | scale: 16384.0000 | micro time: 1.872 | step time: 0.000
train | epoch   3 | Iter:   5070/ 13000 | global iter:   5070/ 13000 | loss: 1.3824 | ds_loss: 0.0000 | lr: 3.3621e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   5070/ 13000 | global iter:   5070/ 13000 | loss: 1.5895 | ds_loss: 0.0000 | lr: 3.3621e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 1.829
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   5071/ 13000 | global iter:   5071/ 13000 | loss: 1.1656 | ds_loss: 0.0000 | lr: 3.3615e-05 | scale: 16384.0000 | micro time: 1.902 | step time: 0.000
train | epoch   3 | Iter:   5072/ 13000 | global iter:   5072/ 13000 | loss: 1.8478 | ds_loss: 0.0000 | lr: 3.3609e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   3 | Iter:   5073/ 13000 | global iter:   5073/ 13000 | loss: 1.4322 | ds_loss: 0.0000 | lr: 3.3604e-05 | scale: 16384.0000 | micro time: 1.826 | step time: 0.000
train | epoch   3 | Iter:   5074/ 13000 | global iter:   5074/ 13000 | loss: 1.7889 | ds_loss: 0.0000 | lr: 3.3598e-05 | scale: 16384.0000 | micro time: 1.864 | step time: 0.000
train | epoch   3 | Iter:   5075/ 13000 | global iter:   5075/ 13000 | loss: 1.3317 | ds_loss: 0.0000 | lr: 3.3592e-05 | scale: 16384.0000 | micro time: 1.862 | step time: 0.000
train | epoch   3 | Iter:   5076/ 13000 | global iter:   5076/ 13000 | loss: 2.0292 | ds_loss: 0.0000 | lr: 3.3587e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   3 | Iter:   5077/ 13000 | global iter:   5077/ 13000 | loss: 1.7431 | ds_loss: 0.0000 | lr: 3.3581e-05 | scale: 16384.0000 | micro time: 1.728 | step time: 0.000
train | epoch   3 | Iter:   5078/ 13000 | global iter:   5078/ 13000 | loss: 1.6421 | ds_loss: 0.0000 | lr: 3.3575e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   3 | Iter:   5079/ 13000 | global iter:   5079/ 13000 | loss: 1.6542 | ds_loss: 0.0000 | lr: 3.3570e-05 | scale: 16384.0000 | micro time: 1.853 | step time: 0.000
train | epoch   3 | Iter:   5080/ 13000 | global iter:   5080/ 13000 | loss: 1.4754 | ds_loss: 0.0000 | lr: 3.3564e-05 | scale: 16384.0000 | micro time: 1.696 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   5080/ 13000 | global iter:   5080/ 13000 | loss: 1.6110 | ds_loss: 0.0000 | lr: 3.3564e-05 | scale: 16384.0000 | micro time: 1.696 | step time: 1.816
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   5081/ 13000 | global iter:   5081/ 13000 | loss: 1.4252 | ds_loss: 0.0000 | lr: 3.3558e-05 | scale: 16384.0000 | micro time: 1.829 | step time: 0.000
train | epoch   3 | Iter:   5082/ 13000 | global iter:   5082/ 13000 | loss: 1.2127 | ds_loss: 0.0000 | lr: 3.3553e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   3 | Iter:   5083/ 13000 | global iter:   5083/ 13000 | loss: 1.4099 | ds_loss: 0.0000 | lr: 3.3547e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   3 | Iter:   5084/ 13000 | global iter:   5084/ 13000 | loss: 1.6152 | ds_loss: 0.0000 | lr: 3.3541e-05 | scale: 16384.0000 | micro time: 1.820 | step time: 0.000
train | epoch   3 | Iter:   5085/ 13000 | global iter:   5085/ 13000 | loss: 1.8654 | ds_loss: 0.0000 | lr: 3.3536e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   3 | Iter:   5086/ 13000 | global iter:   5086/ 13000 | loss: 1.2374 | ds_loss: 0.0000 | lr: 3.3530e-05 | scale: 16384.0000 | micro time: 1.824 | step time: 0.000
train | epoch   3 | Iter:   5087/ 13000 | global iter:   5087/ 13000 | loss: 1.3473 | ds_loss: 0.0000 | lr: 3.3524e-05 | scale: 16384.0000 | micro time: 1.832 | step time: 0.000
train | epoch   3 | Iter:   5088/ 13000 | global iter:   5088/ 13000 | loss: 1.7770 | ds_loss: 0.0000 | lr: 3.3519e-05 | scale: 16384.0000 | micro time: 1.845 | step time: 0.000
train | epoch   3 | Iter:   5089/ 13000 | global iter:   5089/ 13000 | loss: 1.3369 | ds_loss: 0.0000 | lr: 3.3513e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   3 | Iter:   5090/ 13000 | global iter:   5090/ 13000 | loss: 1.9514 | ds_loss: 0.0000 | lr: 3.3507e-05 | scale: 16384.0000 | micro time: 1.833 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   5090/ 13000 | global iter:   5090/ 13000 | loss: 1.5179 | ds_loss: 0.0000 | lr: 3.3507e-05 | scale: 16384.0000 | micro time: 1.833 | step time: 1.828
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   5091/ 13000 | global iter:   5091/ 13000 | loss: 1.5443 | ds_loss: 0.0000 | lr: 3.3502e-05 | scale: 16384.0000 | micro time: 1.800 | step time: 0.000
train | epoch   3 | Iter:   5092/ 13000 | global iter:   5092/ 13000 | loss: 1.5171 | ds_loss: 0.0000 | lr: 3.3496e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   3 | Iter:   5093/ 13000 | global iter:   5093/ 13000 | loss: 1.6810 | ds_loss: 0.0000 | lr: 3.3490e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   3 | Iter:   5094/ 13000 | global iter:   5094/ 13000 | loss: 1.0837 | ds_loss: 0.0000 | lr: 3.3484e-05 | scale: 16384.0000 | micro time: 1.719 | step time: 0.000
train | epoch   3 | Iter:   5095/ 13000 | global iter:   5095/ 13000 | loss: 1.2074 | ds_loss: 0.0000 | lr: 3.3479e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   3 | Iter:   5096/ 13000 | global iter:   5096/ 13000 | loss: 1.3544 | ds_loss: 0.0000 | lr: 3.3473e-05 | scale: 16384.0000 | micro time: 1.768 | step time: 0.000
train | epoch   3 | Iter:   5097/ 13000 | global iter:   5097/ 13000 | loss: 0.9402 | ds_loss: 0.0000 | lr: 3.3467e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   3 | Iter:   5098/ 13000 | global iter:   5098/ 13000 | loss: 1.4240 | ds_loss: 0.0000 | lr: 3.3462e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   3 | Iter:   5099/ 13000 | global iter:   5099/ 13000 | loss: 1.9013 | ds_loss: 0.0000 | lr: 3.3456e-05 | scale: 16384.0000 | micro time: 1.884 | step time: 0.000
train | epoch   3 | Iter:   5100/ 13000 | global iter:   5100/ 13000 | loss: 1.4876 | ds_loss: 0.0000 | lr: 3.3450e-05 | scale: 16384.0000 | micro time: 1.733 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   5100/ 13000 | global iter:   5100/ 13000 | loss: 1.4141 | ds_loss: 0.0000 | lr: 3.3450e-05 | scale: 16384.0000 | micro time: 1.733 | step time: 1.803
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   5101/ 13000 | global iter:   5101/ 13000 | loss: 1.3789 | ds_loss: 0.0000 | lr: 3.3445e-05 | scale: 16384.0000 | micro time: 1.850 | step time: 0.000
train | epoch   3 | Iter:   5102/ 13000 | global iter:   5102/ 13000 | loss: 1.6200 | ds_loss: 0.0000 | lr: 3.3439e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   3 | Iter:   5103/ 13000 | global iter:   5103/ 13000 | loss: 1.9109 | ds_loss: 0.0000 | lr: 3.3433e-05 | scale: 16384.0000 | micro time: 1.899 | step time: 0.000
train | epoch   3 | Iter:   5104/ 13000 | global iter:   5104/ 13000 | loss: 1.8521 | ds_loss: 0.0000 | lr: 3.3428e-05 | scale: 16384.0000 | micro time: 1.877 | step time: 0.000
train | epoch   3 | Iter:   5105/ 13000 | global iter:   5105/ 13000 | loss: 1.3324 | ds_loss: 0.0000 | lr: 3.3422e-05 | scale: 16384.0000 | micro time: 1.834 | step time: 0.000
train | epoch   3 | Iter:   5106/ 13000 | global iter:   5106/ 13000 | loss: 1.3937 | ds_loss: 0.0000 | lr: 3.3416e-05 | scale: 16384.0000 | micro time: 1.792 | step time: 0.000
train | epoch   3 | Iter:   5107/ 13000 | global iter:   5107/ 13000 | loss: 1.4837 | ds_loss: 0.0000 | lr: 3.3411e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   3 | Iter:   5108/ 13000 | global iter:   5108/ 13000 | loss: 1.1941 | ds_loss: 0.0000 | lr: 3.3405e-05 | scale: 16384.0000 | micro time: 1.856 | step time: 0.000
train | epoch   3 | Iter:   5109/ 13000 | global iter:   5109/ 13000 | loss: 1.0437 | ds_loss: 0.0000 | lr: 3.3399e-05 | scale: 16384.0000 | micro time: 1.746 | step time: 0.000
train | epoch   3 | Iter:   5110/ 13000 | global iter:   5110/ 13000 | loss: 1.3630 | ds_loss: 0.0000 | lr: 3.3394e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   5110/ 13000 | global iter:   5110/ 13000 | loss: 1.4572 | ds_loss: 0.0000 | lr: 3.3394e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 1.828
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   5111/ 13000 | global iter:   5111/ 13000 | loss: 1.5941 | ds_loss: 0.0000 | lr: 3.3388e-05 | scale: 16384.0000 | micro time: 1.802 | step time: 0.000
train | epoch   3 | Iter:   5112/ 13000 | global iter:   5112/ 13000 | loss: 1.6155 | ds_loss: 0.0000 | lr: 3.3382e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   3 | Iter:   5113/ 13000 | global iter:   5113/ 13000 | loss: 1.4178 | ds_loss: 0.0000 | lr: 3.3377e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   3 | Iter:   5114/ 13000 | global iter:   5114/ 13000 | loss: 1.6400 | ds_loss: 0.0000 | lr: 3.3371e-05 | scale: 16384.0000 | micro time: 1.794 | step time: 0.000
train | epoch   3 | Iter:   5115/ 13000 | global iter:   5115/ 13000 | loss: 1.2018 | ds_loss: 0.0000 | lr: 3.3365e-05 | scale: 16384.0000 | micro time: 1.836 | step time: 0.000
train | epoch   3 | Iter:   5116/ 13000 | global iter:   5116/ 13000 | loss: 1.3759 | ds_loss: 0.0000 | lr: 3.3360e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   3 | Iter:   5117/ 13000 | global iter:   5117/ 13000 | loss: 1.3065 | ds_loss: 0.0000 | lr: 3.3354e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   3 | Iter:   5118/ 13000 | global iter:   5118/ 13000 | loss: 1.5535 | ds_loss: 0.0000 | lr: 3.3348e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   3 | Iter:   5119/ 13000 | global iter:   5119/ 13000 | loss: 1.7864 | ds_loss: 0.0000 | lr: 3.3342e-05 | scale: 16384.0000 | micro time: 1.736 | step time: 0.000
train | epoch   3 | Iter:   5120/ 13000 | global iter:   5120/ 13000 | loss: 2.0124 | ds_loss: 0.0000 | lr: 3.3337e-05 | scale: 16384.0000 | micro time: 1.834 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   5120/ 13000 | global iter:   5120/ 13000 | loss: 1.5504 | ds_loss: 0.0000 | lr: 3.3337e-05 | scale: 16384.0000 | micro time: 1.834 | step time: 1.815
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   5121/ 13000 | global iter:   5121/ 13000 | loss: 1.7646 | ds_loss: 0.0000 | lr: 3.3331e-05 | scale: 16384.0000 | micro time: 1.850 | step time: 0.000
train | epoch   3 | Iter:   5122/ 13000 | global iter:   5122/ 13000 | loss: 2.1221 | ds_loss: 0.0000 | lr: 3.3325e-05 | scale: 16384.0000 | micro time: 1.666 | step time: 0.000
train | epoch   3 | Iter:   5123/ 13000 | global iter:   5123/ 13000 | loss: 1.9707 | ds_loss: 0.0000 | lr: 3.3320e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   3 | Iter:   5124/ 13000 | global iter:   5124/ 13000 | loss: 1.3079 | ds_loss: 0.0000 | lr: 3.3314e-05 | scale: 16384.0000 | micro time: 1.880 | step time: 0.000
train | epoch   3 | Iter:   5125/ 13000 | global iter:   5125/ 13000 | loss: 1.5409 | ds_loss: 0.0000 | lr: 3.3308e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   3 | Iter:   5126/ 13000 | global iter:   5126/ 13000 | loss: 1.7515 | ds_loss: 0.0000 | lr: 3.3303e-05 | scale: 16384.0000 | micro time: 1.830 | step time: 0.000
train | epoch   3 | Iter:   5127/ 13000 | global iter:   5127/ 13000 | loss: 1.5597 | ds_loss: 0.0000 | lr: 3.3297e-05 | scale: 16384.0000 | micro time: 1.810 | step time: 0.000
train | epoch   3 | Iter:   5128/ 13000 | global iter:   5128/ 13000 | loss: 1.5734 | ds_loss: 0.0000 | lr: 3.3291e-05 | scale: 16384.0000 | micro time: 1.774 | step time: 0.000
train | epoch   3 | Iter:   5129/ 13000 | global iter:   5129/ 13000 | loss: 1.7208 | ds_loss: 0.0000 | lr: 3.3286e-05 | scale: 16384.0000 | micro time: 1.917 | step time: 0.000
train | epoch   3 | Iter:   5130/ 13000 | global iter:   5130/ 13000 | loss: 1.2622 | ds_loss: 0.0000 | lr: 3.3280e-05 | scale: 16384.0000 | micro time: 1.806 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   5130/ 13000 | global iter:   5130/ 13000 | loss: 1.6574 | ds_loss: 0.0000 | lr: 3.3280e-05 | scale: 16384.0000 | micro time: 1.806 | step time: 1.810
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   5131/ 13000 | global iter:   5131/ 13000 | loss: 1.5439 | ds_loss: 0.0000 | lr: 3.3274e-05 | scale: 16384.0000 | micro time: 1.778 | step time: 0.000
train | epoch   3 | Iter:   5132/ 13000 | global iter:   5132/ 13000 | loss: 1.2010 | ds_loss: 0.0000 | lr: 3.3269e-05 | scale: 16384.0000 | micro time: 2.142 | step time: 0.000
train | epoch   3 | Iter:   5133/ 13000 | global iter:   5133/ 13000 | loss: 1.8046 | ds_loss: 0.0000 | lr: 3.3263e-05 | scale: 16384.0000 | micro time: 1.752 | step time: 0.000
train | epoch   3 | Iter:   5134/ 13000 | global iter:   5134/ 13000 | loss: 1.7376 | ds_loss: 0.0000 | lr: 3.3257e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   3 | Iter:   5135/ 13000 | global iter:   5135/ 13000 | loss: 1.1222 | ds_loss: 0.0000 | lr: 3.3251e-05 | scale: 16384.0000 | micro time: 1.881 | step time: 0.000
train | epoch   3 | Iter:   5136/ 13000 | global iter:   5136/ 13000 | loss: 1.2269 | ds_loss: 0.0000 | lr: 3.3246e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   3 | Iter:   5137/ 13000 | global iter:   5137/ 13000 | loss: 1.5082 | ds_loss: 0.0000 | lr: 3.3240e-05 | scale: 16384.0000 | micro time: 1.845 | step time: 0.000
train | epoch   3 | Iter:   5138/ 13000 | global iter:   5138/ 13000 | loss: 1.4100 | ds_loss: 0.0000 | lr: 3.3234e-05 | scale: 16384.0000 | micro time: 1.842 | step time: 0.000
train | epoch   3 | Iter:   5139/ 13000 | global iter:   5139/ 13000 | loss: 1.2166 | ds_loss: 0.0000 | lr: 3.3229e-05 | scale: 16384.0000 | micro time: 1.911 | step time: 0.000
train | epoch   3 | Iter:   5140/ 13000 | global iter:   5140/ 13000 | loss: 1.4519 | ds_loss: 0.0000 | lr: 3.3223e-05 | scale: 16384.0000 | micro time: 1.830 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   5140/ 13000 | global iter:   5140/ 13000 | loss: 1.4223 | ds_loss: 0.0000 | lr: 3.3223e-05 | scale: 16384.0000 | micro time: 1.830 | step time: 1.864
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   5141/ 13000 | global iter:   5141/ 13000 | loss: 1.7946 | ds_loss: 0.0000 | lr: 3.3217e-05 | scale: 16384.0000 | micro time: 1.834 | step time: 0.000
train | epoch   3 | Iter:   5142/ 13000 | global iter:   5142/ 13000 | loss: 1.6457 | ds_loss: 0.0000 | lr: 3.3212e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   3 | Iter:   5143/ 13000 | global iter:   5143/ 13000 | loss: 1.5718 | ds_loss: 0.0000 | lr: 3.3206e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   3 | Iter:   5144/ 13000 | global iter:   5144/ 13000 | loss: 1.4006 | ds_loss: 0.0000 | lr: 3.3200e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   3 | Iter:   5145/ 13000 | global iter:   5145/ 13000 | loss: 1.8899 | ds_loss: 0.0000 | lr: 3.3194e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 0.000
train | epoch   3 | Iter:   5146/ 13000 | global iter:   5146/ 13000 | loss: 1.9552 | ds_loss: 0.0000 | lr: 3.3189e-05 | scale: 16384.0000 | micro time: 1.772 | step time: 0.000
train | epoch   3 | Iter:   5147/ 13000 | global iter:   5147/ 13000 | loss: 1.6701 | ds_loss: 0.0000 | lr: 3.3183e-05 | scale: 16384.0000 | micro time: 1.850 | step time: 0.000
train | epoch   3 | Iter:   5148/ 13000 | global iter:   5148/ 13000 | loss: 1.5419 | ds_loss: 0.0000 | lr: 3.3177e-05 | scale: 16384.0000 | micro time: 1.725 | step time: 0.000
train | epoch   3 | Iter:   5149/ 13000 | global iter:   5149/ 13000 | loss: 1.7703 | ds_loss: 0.0000 | lr: 3.3172e-05 | scale: 16384.0000 | micro time: 1.845 | step time: 0.000
train | epoch   3 | Iter:   5150/ 13000 | global iter:   5150/ 13000 | loss: 0.7571 | ds_loss: 0.0000 | lr: 3.3166e-05 | scale: 16384.0000 | micro time: 1.821 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   5150/ 13000 | global iter:   5150/ 13000 | loss: 1.5997 | ds_loss: 0.0000 | lr: 3.3166e-05 | scale: 16384.0000 | micro time: 1.821 | step time: 1.803
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   5151/ 13000 | global iter:   5151/ 13000 | loss: 1.4611 | ds_loss: 0.0000 | lr: 3.3160e-05 | scale: 16384.0000 | micro time: 1.834 | step time: 0.000
train | epoch   3 | Iter:   5152/ 13000 | global iter:   5152/ 13000 | loss: 1.2053 | ds_loss: 0.0000 | lr: 3.3155e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   3 | Iter:   5153/ 13000 | global iter:   5153/ 13000 | loss: 1.5653 | ds_loss: 0.0000 | lr: 3.3149e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   3 | Iter:   5154/ 13000 | global iter:   5154/ 13000 | loss: 1.6206 | ds_loss: 0.0000 | lr: 3.3143e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   3 | Iter:   5155/ 13000 | global iter:   5155/ 13000 | loss: 1.7790 | ds_loss: 0.0000 | lr: 3.3137e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   3 | Iter:   5156/ 13000 | global iter:   5156/ 13000 | loss: 1.2472 | ds_loss: 0.0000 | lr: 3.3132e-05 | scale: 16384.0000 | micro time: 1.719 | step time: 0.000
train | epoch   3 | Iter:   5157/ 13000 | global iter:   5157/ 13000 | loss: 1.9644 | ds_loss: 0.0000 | lr: 3.3126e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   3 | Iter:   5158/ 13000 | global iter:   5158/ 13000 | loss: 1.3341 | ds_loss: 0.0000 | lr: 3.3120e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   3 | Iter:   5159/ 13000 | global iter:   5159/ 13000 | loss: 1.7405 | ds_loss: 0.0000 | lr: 3.3115e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   3 | Iter:   5160/ 13000 | global iter:   5160/ 13000 | loss: 1.3588 | ds_loss: 0.0000 | lr: 3.3109e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   5160/ 13000 | global iter:   5160/ 13000 | loss: 1.5276 | ds_loss: 0.0000 | lr: 3.3109e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 1.798
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   5161/ 13000 | global iter:   5161/ 13000 | loss: 1.1764 | ds_loss: 0.0000 | lr: 3.3103e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   3 | Iter:   5162/ 13000 | global iter:   5162/ 13000 | loss: 1.5662 | ds_loss: 0.0000 | lr: 3.3098e-05 | scale: 16384.0000 | micro time: 1.781 | step time: 0.000
train | epoch   3 | Iter:   5163/ 13000 | global iter:   5163/ 13000 | loss: 1.5420 | ds_loss: 0.0000 | lr: 3.3092e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   3 | Iter:   5164/ 13000 | global iter:   5164/ 13000 | loss: 1.9357 | ds_loss: 0.0000 | lr: 3.3086e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   3 | Iter:   5165/ 13000 | global iter:   5165/ 13000 | loss: 1.3308 | ds_loss: 0.0000 | lr: 3.3080e-05 | scale: 16384.0000 | micro time: 1.887 | step time: 0.000
train | epoch   3 | Iter:   5166/ 13000 | global iter:   5166/ 13000 | loss: 1.0842 | ds_loss: 0.0000 | lr: 3.3075e-05 | scale: 16384.0000 | micro time: 1.802 | step time: 0.000
train | epoch   3 | Iter:   5167/ 13000 | global iter:   5167/ 13000 | loss: 1.4584 | ds_loss: 0.0000 | lr: 3.3069e-05 | scale: 16384.0000 | micro time: 1.743 | step time: 0.000
train | epoch   3 | Iter:   5168/ 13000 | global iter:   5168/ 13000 | loss: 1.3344 | ds_loss: 0.0000 | lr: 3.3063e-05 | scale: 16384.0000 | micro time: 2.229 | step time: 0.000
train | epoch   3 | Iter:   5169/ 13000 | global iter:   5169/ 13000 | loss: 1.7380 | ds_loss: 0.0000 | lr: 3.3058e-05 | scale: 16384.0000 | micro time: 1.736 | step time: 0.000
train | epoch   3 | Iter:   5170/ 13000 | global iter:   5170/ 13000 | loss: 1.4571 | ds_loss: 0.0000 | lr: 3.3052e-05 | scale: 16384.0000 | micro time: 1.816 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   5170/ 13000 | global iter:   5170/ 13000 | loss: 1.4623 | ds_loss: 0.0000 | lr: 3.3052e-05 | scale: 16384.0000 | micro time: 1.816 | step time: 1.833
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   5171/ 13000 | global iter:   5171/ 13000 | loss: 1.6909 | ds_loss: 0.0000 | lr: 3.3046e-05 | scale: 16384.0000 | micro time: 1.802 | step time: 0.000
train | epoch   3 | Iter:   5172/ 13000 | global iter:   5172/ 13000 | loss: 1.5479 | ds_loss: 0.0000 | lr: 3.3040e-05 | scale: 16384.0000 | micro time: 1.801 | step time: 0.000
train | epoch   3 | Iter:   5173/ 13000 | global iter:   5173/ 13000 | loss: 1.4592 | ds_loss: 0.0000 | lr: 3.3035e-05 | scale: 16384.0000 | micro time: 2.079 | step time: 0.000
train | epoch   3 | Iter:   5174/ 13000 | global iter:   5174/ 13000 | loss: 1.2300 | ds_loss: 0.0000 | lr: 3.3029e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   3 | Iter:   5175/ 13000 | global iter:   5175/ 13000 | loss: 1.6875 | ds_loss: 0.0000 | lr: 3.3023e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   3 | Iter:   5176/ 13000 | global iter:   5176/ 13000 | loss: 1.0076 | ds_loss: 0.0000 | lr: 3.3018e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   3 | Iter:   5177/ 13000 | global iter:   5177/ 13000 | loss: 1.6319 | ds_loss: 0.0000 | lr: 3.3012e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   3 | Iter:   5178/ 13000 | global iter:   5178/ 13000 | loss: 1.6364 | ds_loss: 0.0000 | lr: 3.3006e-05 | scale: 16384.0000 | micro time: 1.711 | step time: 0.000
train | epoch   3 | Iter:   5179/ 13000 | global iter:   5179/ 13000 | loss: 1.5019 | ds_loss: 0.0000 | lr: 3.3000e-05 | scale: 16384.0000 | micro time: 1.735 | step time: 0.000
train | epoch   3 | Iter:   5180/ 13000 | global iter:   5180/ 13000 | loss: 1.5956 | ds_loss: 0.0000 | lr: 3.2995e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   5180/ 13000 | global iter:   5180/ 13000 | loss: 1.4989 | ds_loss: 0.0000 | lr: 3.2995e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 1.812
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   5181/ 13000 | global iter:   5181/ 13000 | loss: 1.1618 | ds_loss: 0.0000 | lr: 3.2989e-05 | scale: 16384.0000 | micro time: 1.830 | step time: 0.000
train | epoch   3 | Iter:   5182/ 13000 | global iter:   5182/ 13000 | loss: 1.5876 | ds_loss: 0.0000 | lr: 3.2983e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   3 | Iter:   5183/ 13000 | global iter:   5183/ 13000 | loss: 1.5741 | ds_loss: 0.0000 | lr: 3.2978e-05 | scale: 16384.0000 | micro time: 1.793 | step time: 0.000
train | epoch   3 | Iter:   5184/ 13000 | global iter:   5184/ 13000 | loss: 1.9616 | ds_loss: 0.0000 | lr: 3.2972e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   3 | Iter:   5185/ 13000 | global iter:   5185/ 13000 | loss: 1.4839 | ds_loss: 0.0000 | lr: 3.2966e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   3 | Iter:   5186/ 13000 | global iter:   5186/ 13000 | loss: 1.6226 | ds_loss: 0.0000 | lr: 3.2960e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   3 | Iter:   5187/ 13000 | global iter:   5187/ 13000 | loss: 1.0146 | ds_loss: 0.0000 | lr: 3.2955e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   3 | Iter:   5188/ 13000 | global iter:   5188/ 13000 | loss: 1.5513 | ds_loss: 0.0000 | lr: 3.2949e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   3 | Iter:   5189/ 13000 | global iter:   5189/ 13000 | loss: 1.2324 | ds_loss: 0.0000 | lr: 3.2943e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   3 | Iter:   5190/ 13000 | global iter:   5190/ 13000 | loss: 1.7968 | ds_loss: 0.0000 | lr: 3.2938e-05 | scale: 16384.0000 | micro time: 1.899 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   5190/ 13000 | global iter:   5190/ 13000 | loss: 1.4987 | ds_loss: 0.0000 | lr: 3.2938e-05 | scale: 16384.0000 | micro time: 1.899 | step time: 1.824
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   5191/ 13000 | global iter:   5191/ 13000 | loss: 1.7158 | ds_loss: 0.0000 | lr: 3.2932e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   3 | Iter:   5192/ 13000 | global iter:   5192/ 13000 | loss: 1.5758 | ds_loss: 0.0000 | lr: 3.2926e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   3 | Iter:   5193/ 13000 | global iter:   5193/ 13000 | loss: 1.6199 | ds_loss: 0.0000 | lr: 3.2920e-05 | scale: 16384.0000 | micro time: 1.794 | step time: 0.000
train | epoch   3 | Iter:   5194/ 13000 | global iter:   5194/ 13000 | loss: 1.9840 | ds_loss: 0.0000 | lr: 3.2915e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   3 | Iter:   5195/ 13000 | global iter:   5195/ 13000 | loss: 1.3603 | ds_loss: 0.0000 | lr: 3.2909e-05 | scale: 16384.0000 | micro time: 1.842 | step time: 0.000
train | epoch   3 | Iter:   5196/ 13000 | global iter:   5196/ 13000 | loss: 1.5826 | ds_loss: 0.0000 | lr: 3.2903e-05 | scale: 16384.0000 | micro time: 1.800 | step time: 0.000
train | epoch   3 | Iter:   5197/ 13000 | global iter:   5197/ 13000 | loss: 1.2591 | ds_loss: 0.0000 | lr: 3.2897e-05 | scale: 16384.0000 | micro time: 1.833 | step time: 0.000
train | epoch   3 | Iter:   5198/ 13000 | global iter:   5198/ 13000 | loss: 1.5695 | ds_loss: 0.0000 | lr: 3.2892e-05 | scale: 16384.0000 | micro time: 1.734 | step time: 0.000
train | epoch   3 | Iter:   5199/ 13000 | global iter:   5199/ 13000 | loss: 1.9085 | ds_loss: 0.0000 | lr: 3.2886e-05 | scale: 16384.0000 | micro time: 1.868 | step time: 0.000
train | epoch   3 | Iter:   5200/ 13000 | global iter:   5200/ 13000 | loss: 1.9484 | ds_loss: 0.0000 | lr: 3.2880e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
****************************************************************************************************
train | epoch   3 | Iter:   5200/ 13000 | global iter:   5200/ 13000 | loss: 1.6524 | ds_loss: 0.0000 | lr: 3.2880e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 1.807
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   3 | Iter:   5201/ 13000 | global iter:   5201/ 13000 | loss: 1.6954 | ds_loss: 0.0000 | lr: 3.2875e-05 | scale: 16384.0000 | micro time: 1.857 | step time: 0.000
train | epoch   3 | Iter:   5202/ 13000 | global iter:   5202/ 13000 | loss: 1.4646 | ds_loss: 0.0000 | lr: 3.2869e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   3 | Iter:   5203/ 13000 | global iter:   5203/ 13000 | loss: 1.9485 | ds_loss: 0.0000 | lr: 3.2863e-05 | scale: 16384.0000 | micro time: 1.814 | step time: 0.000
train | epoch   3 | Iter:   5204/ 13000 | global iter:   5204/ 13000 | loss: 1.4591 | ds_loss: 0.0000 | lr: 3.2857e-05 | scale: 16384.0000 | micro time: 1.544 | step time: 0.000
Sat Apr 19 13:57:41 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   40C    P2            109W /  230W |   21631MiB /  23028MiB |     96%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   44C    P2            129W /  230W |   22433MiB /  23028MiB |     91%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   42C    P2            117W /  230W |   21691MiB /  23028MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   40C    P2            126W /  230W |   21059MiB /  23028MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3      21624MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3      22426MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3      21684MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3      21052MiB |
+-----------------------------------------------------------------------------------------+

Sat Apr 19 13:57:41 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   40C    P2            109W /  230W |   21631MiB /  23028MiB |     96%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   44C    P2            129W /  230W |   22433MiB /  23028MiB |     91%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   42C    P2            117W /  230W |   21691MiB /  23028MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   40C    P2            126W /  230W |   21059MiB /  23028MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3      21624MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3      22426MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3      21684MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3      21052MiB |
+-----------------------------------------------------------------------------------------+

Sat Apr 19 13:57:41 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   40C    P2             96W /  230W |   21631MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   43C    P2            110W /  230W |   22433MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   41C    P2            100W /  230W |   21691MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   39C    P2            101W /  230W |   21059MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3      21624MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3      22426MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3      21684MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3      21052MiB |
+-----------------------------------------------------------------------------------------+
Sat Apr 19 13:57:41 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   40C    P2            109W /  230W |   21631MiB /  23028MiB |     96%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   44C    P2            129W /  230W |   22433MiB /  23028MiB |     91%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   42C    P2            111W /  230W |   21691MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   39C    P2            106W /  230W |   21059MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3      21624MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3      22426MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3      21684MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3      21052MiB |
+-----------------------------------------------------------------------------------------+


train | epoch   4 | Iter:   5205/ 13000 | global iter:   5205/ 13000 | loss: 1.5032 | ds_loss: 0.0000 | lr: 3.2852e-05 | scale: 16384.0000 | micro time: 2.490 | step time: 0.000
train | epoch   4 | Iter:   5206/ 13000 | global iter:   5206/ 13000 | loss: 1.6384 | ds_loss: 0.0000 | lr: 3.2846e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   4 | Iter:   5207/ 13000 | global iter:   5207/ 13000 | loss: 1.2122 | ds_loss: 0.0000 | lr: 3.2840e-05 | scale: 16384.0000 | micro time: 1.911 | step time: 0.000
train | epoch   4 | Iter:   5208/ 13000 | global iter:   5208/ 13000 | loss: 1.1322 | ds_loss: 0.0000 | lr: 3.2834e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   4 | Iter:   5209/ 13000 | global iter:   5209/ 13000 | loss: 0.8657 | ds_loss: 0.0000 | lr: 3.2829e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   4 | Iter:   5210/ 13000 | global iter:   5210/ 13000 | loss: 1.2859 | ds_loss: 0.0000 | lr: 3.2823e-05 | scale: 16384.0000 | micro time: 1.836 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5210/ 13000 | global iter:   5210/ 13000 | loss: 1.4205 | ds_loss: 0.0000 | lr: 3.2823e-05 | scale: 16384.0000 | micro time: 1.836 | step time: 1.872
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5211/ 13000 | global iter:   5211/ 13000 | loss: 1.6698 | ds_loss: 0.0000 | lr: 3.2817e-05 | scale: 16384.0000 | micro time: 1.760 | step time: 0.000
train | epoch   4 | Iter:   5212/ 13000 | global iter:   5212/ 13000 | loss: 1.1523 | ds_loss: 0.0000 | lr: 3.2812e-05 | scale: 16384.0000 | micro time: 1.705 | step time: 0.000
train | epoch   4 | Iter:   5213/ 13000 | global iter:   5213/ 13000 | loss: 1.7850 | ds_loss: 0.0000 | lr: 3.2806e-05 | scale: 16384.0000 | micro time: 1.866 | step time: 0.000
train | epoch   4 | Iter:   5214/ 13000 | global iter:   5214/ 13000 | loss: 1.5421 | ds_loss: 0.0000 | lr: 3.2800e-05 | scale: 16384.0000 | micro time: 1.722 | step time: 0.000
train | epoch   4 | Iter:   5215/ 13000 | global iter:   5215/ 13000 | loss: 0.7876 | ds_loss: 0.0000 | lr: 3.2794e-05 | scale: 16384.0000 | micro time: 1.759 | step time: 0.000
train | epoch   4 | Iter:   5216/ 13000 | global iter:   5216/ 13000 | loss: 1.5497 | ds_loss: 0.0000 | lr: 3.2789e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   4 | Iter:   5217/ 13000 | global iter:   5217/ 13000 | loss: 1.2442 | ds_loss: 0.0000 | lr: 3.2783e-05 | scale: 16384.0000 | micro time: 1.828 | step time: 0.000
train | epoch   4 | Iter:   5218/ 13000 | global iter:   5218/ 13000 | loss: 1.5349 | ds_loss: 0.0000 | lr: 3.2777e-05 | scale: 16384.0000 | micro time: 1.824 | step time: 0.000
train | epoch   4 | Iter:   5219/ 13000 | global iter:   5219/ 13000 | loss: 0.5294 | ds_loss: 0.0000 | lr: 3.2771e-05 | scale: 16384.0000 | micro time: 1.836 | step time: 0.000
train | epoch   4 | Iter:   5220/ 13000 | global iter:   5220/ 13000 | loss: 1.5020 | ds_loss: 0.0000 | lr: 3.2766e-05 | scale: 16384.0000 | micro time: 1.850 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5220/ 13000 | global iter:   5220/ 13000 | loss: 1.3297 | ds_loss: 0.0000 | lr: 3.2766e-05 | scale: 16384.0000 | micro time: 1.850 | step time: 1.795
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5221/ 13000 | global iter:   5221/ 13000 | loss: 1.5328 | ds_loss: 0.0000 | lr: 3.2760e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   4 | Iter:   5222/ 13000 | global iter:   5222/ 13000 | loss: 1.6030 | ds_loss: 0.0000 | lr: 3.2754e-05 | scale: 16384.0000 | micro time: 1.821 | step time: 0.000
train | epoch   4 | Iter:   5223/ 13000 | global iter:   5223/ 13000 | loss: 1.8534 | ds_loss: 0.0000 | lr: 3.2749e-05 | scale: 16384.0000 | micro time: 1.897 | step time: 0.000
train | epoch   4 | Iter:   5224/ 13000 | global iter:   5224/ 13000 | loss: 1.4042 | ds_loss: 0.0000 | lr: 3.2743e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   4 | Iter:   5225/ 13000 | global iter:   5225/ 13000 | loss: 1.6881 | ds_loss: 0.0000 | lr: 3.2737e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   4 | Iter:   5226/ 13000 | global iter:   5226/ 13000 | loss: 0.5749 | ds_loss: 0.0000 | lr: 3.2731e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   4 | Iter:   5227/ 13000 | global iter:   5227/ 13000 | loss: 1.3219 | ds_loss: 0.0000 | lr: 3.2726e-05 | scale: 16384.0000 | micro time: 1.816 | step time: 0.000
train | epoch   4 | Iter:   5228/ 13000 | global iter:   5228/ 13000 | loss: 0.8314 | ds_loss: 0.0000 | lr: 3.2720e-05 | scale: 16384.0000 | micro time: 1.841 | step time: 0.000
train | epoch   4 | Iter:   5229/ 13000 | global iter:   5229/ 13000 | loss: 1.1268 | ds_loss: 0.0000 | lr: 3.2714e-05 | scale: 16384.0000 | micro time: 1.756 | step time: 0.000
train | epoch   4 | Iter:   5230/ 13000 | global iter:   5230/ 13000 | loss: 1.2683 | ds_loss: 0.0000 | lr: 3.2708e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5230/ 13000 | global iter:   5230/ 13000 | loss: 1.3205 | ds_loss: 0.0000 | lr: 3.2708e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 1.818
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5231/ 13000 | global iter:   5231/ 13000 | loss: 1.1048 | ds_loss: 0.0000 | lr: 3.2703e-05 | scale: 16384.0000 | micro time: 1.849 | step time: 0.000
train | epoch   4 | Iter:   5232/ 13000 | global iter:   5232/ 13000 | loss: 1.5269 | ds_loss: 0.0000 | lr: 3.2697e-05 | scale: 16384.0000 | micro time: 1.884 | step time: 0.000
train | epoch   4 | Iter:   5233/ 13000 | global iter:   5233/ 13000 | loss: 1.1124 | ds_loss: 0.0000 | lr: 3.2691e-05 | scale: 16384.0000 | micro time: 1.780 | step time: 0.000
train | epoch   4 | Iter:   5234/ 13000 | global iter:   5234/ 13000 | loss: 1.5216 | ds_loss: 0.0000 | lr: 3.2685e-05 | scale: 16384.0000 | micro time: 1.834 | step time: 0.000
train | epoch   4 | Iter:   5235/ 13000 | global iter:   5235/ 13000 | loss: 1.4512 | ds_loss: 0.0000 | lr: 3.2680e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   4 | Iter:   5236/ 13000 | global iter:   5236/ 13000 | loss: 1.5786 | ds_loss: 0.0000 | lr: 3.2674e-05 | scale: 16384.0000 | micro time: 1.707 | step time: 0.000
train | epoch   4 | Iter:   5237/ 13000 | global iter:   5237/ 13000 | loss: 1.3593 | ds_loss: 0.0000 | lr: 3.2668e-05 | scale: 16384.0000 | micro time: 1.735 | step time: 0.000
train | epoch   4 | Iter:   5238/ 13000 | global iter:   5238/ 13000 | loss: 1.1577 | ds_loss: 0.0000 | lr: 3.2662e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   4 | Iter:   5239/ 13000 | global iter:   5239/ 13000 | loss: 1.0231 | ds_loss: 0.0000 | lr: 3.2657e-05 | scale: 16384.0000 | micro time: 1.943 | step time: 0.000
train | epoch   4 | Iter:   5240/ 13000 | global iter:   5240/ 13000 | loss: 1.1731 | ds_loss: 0.0000 | lr: 3.2651e-05 | scale: 16384.0000 | micro time: 1.784 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5240/ 13000 | global iter:   5240/ 13000 | loss: 1.3009 | ds_loss: 0.0000 | lr: 3.2651e-05 | scale: 16384.0000 | micro time: 1.784 | step time: 1.811
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5241/ 13000 | global iter:   5241/ 13000 | loss: 1.1982 | ds_loss: 0.0000 | lr: 3.2645e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   4 | Iter:   5242/ 13000 | global iter:   5242/ 13000 | loss: 1.7929 | ds_loss: 0.0000 | lr: 3.2639e-05 | scale: 16384.0000 | micro time: 1.796 | step time: 0.000
train | epoch   4 | Iter:   5243/ 13000 | global iter:   5243/ 13000 | loss: 1.4414 | ds_loss: 0.0000 | lr: 3.2634e-05 | scale: 16384.0000 | micro time: 1.790 | step time: 0.000
train | epoch   4 | Iter:   5244/ 13000 | global iter:   5244/ 13000 | loss: 0.9764 | ds_loss: 0.0000 | lr: 3.2628e-05 | scale: 16384.0000 | micro time: 1.895 | step time: 0.000
train | epoch   4 | Iter:   5245/ 13000 | global iter:   5245/ 13000 | loss: 1.2096 | ds_loss: 0.0000 | lr: 3.2622e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   4 | Iter:   5246/ 13000 | global iter:   5246/ 13000 | loss: 1.1702 | ds_loss: 0.0000 | lr: 3.2616e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   4 | Iter:   5247/ 13000 | global iter:   5247/ 13000 | loss: 1.1305 | ds_loss: 0.0000 | lr: 3.2611e-05 | scale: 16384.0000 | micro time: 1.792 | step time: 0.000
train | epoch   4 | Iter:   5248/ 13000 | global iter:   5248/ 13000 | loss: 1.2637 | ds_loss: 0.0000 | lr: 3.2605e-05 | scale: 16384.0000 | micro time: 1.858 | step time: 0.000
train | epoch   4 | Iter:   5249/ 13000 | global iter:   5249/ 13000 | loss: 1.0924 | ds_loss: 0.0000 | lr: 3.2599e-05 | scale: 16384.0000 | micro time: 1.743 | step time: 0.000
train | epoch   4 | Iter:   5250/ 13000 | global iter:   5250/ 13000 | loss: 1.2604 | ds_loss: 0.0000 | lr: 3.2593e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5250/ 13000 | global iter:   5250/ 13000 | loss: 1.2536 | ds_loss: 0.0000 | lr: 3.2593e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 1.824
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5251/ 13000 | global iter:   5251/ 13000 | loss: 1.1047 | ds_loss: 0.0000 | lr: 3.2588e-05 | scale: 16384.0000 | micro time: 1.808 | step time: 0.000
train | epoch   4 | Iter:   5252/ 13000 | global iter:   5252/ 13000 | loss: 0.7492 | ds_loss: 0.0000 | lr: 3.2582e-05 | scale: 16384.0000 | micro time: 1.865 | step time: 0.000
train | epoch   4 | Iter:   5253/ 13000 | global iter:   5253/ 13000 | loss: 1.3797 | ds_loss: 0.0000 | lr: 3.2576e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   4 | Iter:   5254/ 13000 | global iter:   5254/ 13000 | loss: 1.3730 | ds_loss: 0.0000 | lr: 3.2570e-05 | scale: 16384.0000 | micro time: 1.748 | step time: 0.000
train | epoch   4 | Iter:   5255/ 13000 | global iter:   5255/ 13000 | loss: 0.9614 | ds_loss: 0.0000 | lr: 3.2565e-05 | scale: 16384.0000 | micro time: 1.770 | step time: 0.000
train | epoch   4 | Iter:   5256/ 13000 | global iter:   5256/ 13000 | loss: 0.8915 | ds_loss: 0.0000 | lr: 3.2559e-05 | scale: 16384.0000 | micro time: 1.735 | step time: 0.000
train | epoch   4 | Iter:   5257/ 13000 | global iter:   5257/ 13000 | loss: 1.6954 | ds_loss: 0.0000 | lr: 3.2553e-05 | scale: 16384.0000 | micro time: 1.822 | step time: 0.000
train | epoch   4 | Iter:   5258/ 13000 | global iter:   5258/ 13000 | loss: 1.1543 | ds_loss: 0.0000 | lr: 3.2547e-05 | scale: 16384.0000 | micro time: 1.754 | step time: 0.000
train | epoch   4 | Iter:   5259/ 13000 | global iter:   5259/ 13000 | loss: 1.5566 | ds_loss: 0.0000 | lr: 3.2542e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   4 | Iter:   5260/ 13000 | global iter:   5260/ 13000 | loss: 1.1914 | ds_loss: 0.0000 | lr: 3.2536e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5260/ 13000 | global iter:   5260/ 13000 | loss: 1.2057 | ds_loss: 0.0000 | lr: 3.2536e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 1.805
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5261/ 13000 | global iter:   5261/ 13000 | loss: 0.4743 | ds_loss: 0.0000 | lr: 3.2530e-05 | scale: 16384.0000 | micro time: 1.834 | step time: 0.000
train | epoch   4 | Iter:   5262/ 13000 | global iter:   5262/ 13000 | loss: 1.2418 | ds_loss: 0.0000 | lr: 3.2524e-05 | scale: 16384.0000 | micro time: 1.804 | step time: 0.000
train | epoch   4 | Iter:   5263/ 13000 | global iter:   5263/ 13000 | loss: 1.3044 | ds_loss: 0.0000 | lr: 3.2519e-05 | scale: 16384.0000 | micro time: 1.758 | step time: 0.000
train | epoch   4 | Iter:   5264/ 13000 | global iter:   5264/ 13000 | loss: 0.9704 | ds_loss: 0.0000 | lr: 3.2513e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   4 | Iter:   5265/ 13000 | global iter:   5265/ 13000 | loss: 1.0215 | ds_loss: 0.0000 | lr: 3.2507e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   4 | Iter:   5266/ 13000 | global iter:   5266/ 13000 | loss: 1.7649 | ds_loss: 0.0000 | lr: 3.2501e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   4 | Iter:   5267/ 13000 | global iter:   5267/ 13000 | loss: 1.6965 | ds_loss: 0.0000 | lr: 3.2496e-05 | scale: 16384.0000 | micro time: 1.707 | step time: 0.000
train | epoch   4 | Iter:   5268/ 13000 | global iter:   5268/ 13000 | loss: 1.4903 | ds_loss: 0.0000 | lr: 3.2490e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   4 | Iter:   5269/ 13000 | global iter:   5269/ 13000 | loss: 1.1561 | ds_loss: 0.0000 | lr: 3.2484e-05 | scale: 16384.0000 | micro time: 1.832 | step time: 0.000
train | epoch   4 | Iter:   5270/ 13000 | global iter:   5270/ 13000 | loss: 1.1524 | ds_loss: 0.0000 | lr: 3.2478e-05 | scale: 16384.0000 | micro time: 1.686 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5270/ 13000 | global iter:   5270/ 13000 | loss: 1.2273 | ds_loss: 0.0000 | lr: 3.2478e-05 | scale: 16384.0000 | micro time: 1.686 | step time: 1.793
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5271/ 13000 | global iter:   5271/ 13000 | loss: 1.2793 | ds_loss: 0.0000 | lr: 3.2473e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   4 | Iter:   5272/ 13000 | global iter:   5272/ 13000 | loss: 1.2795 | ds_loss: 0.0000 | lr: 3.2467e-05 | scale: 16384.0000 | micro time: 1.821 | step time: 0.000
train | epoch   4 | Iter:   5273/ 13000 | global iter:   5273/ 13000 | loss: 1.3994 | ds_loss: 0.0000 | lr: 3.2461e-05 | scale: 16384.0000 | micro time: 1.824 | step time: 0.000
train | epoch   4 | Iter:   5274/ 13000 | global iter:   5274/ 13000 | loss: 1.7776 | ds_loss: 0.0000 | lr: 3.2455e-05 | scale: 16384.0000 | micro time: 1.785 | step time: 0.000
train | epoch   4 | Iter:   5275/ 13000 | global iter:   5275/ 13000 | loss: 1.6048 | ds_loss: 0.0000 | lr: 3.2450e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   4 | Iter:   5276/ 13000 | global iter:   5276/ 13000 | loss: 1.4634 | ds_loss: 0.0000 | lr: 3.2444e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   4 | Iter:   5277/ 13000 | global iter:   5277/ 13000 | loss: 1.4931 | ds_loss: 0.0000 | lr: 3.2438e-05 | scale: 16384.0000 | micro time: 1.853 | step time: 0.000
train | epoch   4 | Iter:   5278/ 13000 | global iter:   5278/ 13000 | loss: 1.1229 | ds_loss: 0.0000 | lr: 3.2432e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   4 | Iter:   5279/ 13000 | global iter:   5279/ 13000 | loss: 1.2698 | ds_loss: 0.0000 | lr: 3.2427e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   4 | Iter:   5280/ 13000 | global iter:   5280/ 13000 | loss: 1.2677 | ds_loss: 0.0000 | lr: 3.2421e-05 | scale: 16384.0000 | micro time: 1.885 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5280/ 13000 | global iter:   5280/ 13000 | loss: 1.3957 | ds_loss: 0.0000 | lr: 3.2421e-05 | scale: 16384.0000 | micro time: 1.885 | step time: 1.821
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5281/ 13000 | global iter:   5281/ 13000 | loss: 1.3103 | ds_loss: 0.0000 | lr: 3.2415e-05 | scale: 16384.0000 | micro time: 1.850 | step time: 0.000
train | epoch   4 | Iter:   5282/ 13000 | global iter:   5282/ 13000 | loss: 1.3307 | ds_loss: 0.0000 | lr: 3.2409e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   4 | Iter:   5283/ 13000 | global iter:   5283/ 13000 | loss: 2.0124 | ds_loss: 0.0000 | lr: 3.2404e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   4 | Iter:   5284/ 13000 | global iter:   5284/ 13000 | loss: 1.7226 | ds_loss: 0.0000 | lr: 3.2398e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   4 | Iter:   5285/ 13000 | global iter:   5285/ 13000 | loss: 1.2880 | ds_loss: 0.0000 | lr: 3.2392e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   4 | Iter:   5286/ 13000 | global iter:   5286/ 13000 | loss: 1.5184 | ds_loss: 0.0000 | lr: 3.2386e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   4 | Iter:   5287/ 13000 | global iter:   5287/ 13000 | loss: 1.3393 | ds_loss: 0.0000 | lr: 3.2381e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   4 | Iter:   5288/ 13000 | global iter:   5288/ 13000 | loss: 1.2173 | ds_loss: 0.0000 | lr: 3.2375e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   4 | Iter:   5289/ 13000 | global iter:   5289/ 13000 | loss: 1.6799 | ds_loss: 0.0000 | lr: 3.2369e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   4 | Iter:   5290/ 13000 | global iter:   5290/ 13000 | loss: 1.2155 | ds_loss: 0.0000 | lr: 3.2363e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5290/ 13000 | global iter:   5290/ 13000 | loss: 1.4634 | ds_loss: 0.0000 | lr: 3.2363e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 1.830
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5291/ 13000 | global iter:   5291/ 13000 | loss: 1.2055 | ds_loss: 0.0000 | lr: 3.2357e-05 | scale: 16384.0000 | micro time: 1.840 | step time: 0.000
train | epoch   4 | Iter:   5292/ 13000 | global iter:   5292/ 13000 | loss: 1.8738 | ds_loss: 0.0000 | lr: 3.2352e-05 | scale: 16384.0000 | micro time: 1.802 | step time: 0.000
train | epoch   4 | Iter:   5293/ 13000 | global iter:   5293/ 13000 | loss: 1.2163 | ds_loss: 0.0000 | lr: 3.2346e-05 | scale: 16384.0000 | micro time: 1.865 | step time: 0.000
train | epoch   4 | Iter:   5294/ 13000 | global iter:   5294/ 13000 | loss: 0.8632 | ds_loss: 0.0000 | lr: 3.2340e-05 | scale: 16384.0000 | micro time: 1.697 | step time: 0.000
train | epoch   4 | Iter:   5295/ 13000 | global iter:   5295/ 13000 | loss: 1.5072 | ds_loss: 0.0000 | lr: 3.2334e-05 | scale: 16384.0000 | micro time: 1.710 | step time: 0.000
train | epoch   4 | Iter:   5296/ 13000 | global iter:   5296/ 13000 | loss: 0.8372 | ds_loss: 0.0000 | lr: 3.2329e-05 | scale: 16384.0000 | micro time: 1.904 | step time: 0.000
train | epoch   4 | Iter:   5297/ 13000 | global iter:   5297/ 13000 | loss: 1.3360 | ds_loss: 0.0000 | lr: 3.2323e-05 | scale: 16384.0000 | micro time: 1.860 | step time: 0.000
train | epoch   4 | Iter:   5298/ 13000 | global iter:   5298/ 13000 | loss: 1.4907 | ds_loss: 0.0000 | lr: 3.2317e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   4 | Iter:   5299/ 13000 | global iter:   5299/ 13000 | loss: 1.0380 | ds_loss: 0.0000 | lr: 3.2311e-05 | scale: 16384.0000 | micro time: 1.865 | step time: 0.000
train | epoch   4 | Iter:   5300/ 13000 | global iter:   5300/ 13000 | loss: 1.5103 | ds_loss: 0.0000 | lr: 3.2306e-05 | scale: 16384.0000 | micro time: 1.862 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5300/ 13000 | global iter:   5300/ 13000 | loss: 1.2878 | ds_loss: 0.0000 | lr: 3.2306e-05 | scale: 16384.0000 | micro time: 1.862 | step time: 1.825
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5301/ 13000 | global iter:   5301/ 13000 | loss: 1.3492 | ds_loss: 0.0000 | lr: 3.2300e-05 | scale: 16384.0000 | micro time: 1.810 | step time: 0.000
train | epoch   4 | Iter:   5302/ 13000 | global iter:   5302/ 13000 | loss: 1.4052 | ds_loss: 0.0000 | lr: 3.2294e-05 | scale: 16384.0000 | micro time: 1.718 | step time: 0.000
train | epoch   4 | Iter:   5303/ 13000 | global iter:   5303/ 13000 | loss: 1.2165 | ds_loss: 0.0000 | lr: 3.2288e-05 | scale: 16384.0000 | micro time: 1.812 | step time: 0.000
train | epoch   4 | Iter:   5304/ 13000 | global iter:   5304/ 13000 | loss: 1.6307 | ds_loss: 0.0000 | lr: 3.2283e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
train | epoch   4 | Iter:   5305/ 13000 | global iter:   5305/ 13000 | loss: 1.0280 | ds_loss: 0.0000 | lr: 3.2277e-05 | scale: 16384.0000 | micro time: 1.740 | step time: 0.000
train | epoch   4 | Iter:   5306/ 13000 | global iter:   5306/ 13000 | loss: 0.7320 | ds_loss: 0.0000 | lr: 3.2271e-05 | scale: 16384.0000 | micro time: 1.772 | step time: 0.000
train | epoch   4 | Iter:   5307/ 13000 | global iter:   5307/ 13000 | loss: 1.2839 | ds_loss: 0.0000 | lr: 3.2265e-05 | scale: 16384.0000 | micro time: 1.772 | step time: 0.000
train | epoch   4 | Iter:   5308/ 13000 | global iter:   5308/ 13000 | loss: 1.3808 | ds_loss: 0.0000 | lr: 3.2259e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 0.000
train | epoch   4 | Iter:   5309/ 13000 | global iter:   5309/ 13000 | loss: 1.5923 | ds_loss: 0.0000 | lr: 3.2254e-05 | scale: 16384.0000 | micro time: 1.736 | step time: 0.000
train | epoch   4 | Iter:   5310/ 13000 | global iter:   5310/ 13000 | loss: 1.3171 | ds_loss: 0.0000 | lr: 3.2248e-05 | scale: 16384.0000 | micro time: 1.776 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5310/ 13000 | global iter:   5310/ 13000 | loss: 1.2936 | ds_loss: 0.0000 | lr: 3.2248e-05 | scale: 16384.0000 | micro time: 1.776 | step time: 1.782
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5311/ 13000 | global iter:   5311/ 13000 | loss: 1.4456 | ds_loss: 0.0000 | lr: 3.2242e-05 | scale: 16384.0000 | micro time: 1.676 | step time: 0.000
train | epoch   4 | Iter:   5312/ 13000 | global iter:   5312/ 13000 | loss: 1.5123 | ds_loss: 0.0000 | lr: 3.2236e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   4 | Iter:   5313/ 13000 | global iter:   5313/ 13000 | loss: 1.3922 | ds_loss: 0.0000 | lr: 3.2231e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   4 | Iter:   5314/ 13000 | global iter:   5314/ 13000 | loss: 1.1318 | ds_loss: 0.0000 | lr: 3.2225e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   4 | Iter:   5315/ 13000 | global iter:   5315/ 13000 | loss: 1.5443 | ds_loss: 0.0000 | lr: 3.2219e-05 | scale: 16384.0000 | micro time: 1.852 | step time: 0.000
train | epoch   4 | Iter:   5316/ 13000 | global iter:   5316/ 13000 | loss: 1.3499 | ds_loss: 0.0000 | lr: 3.2213e-05 | scale: 16384.0000 | micro time: 1.915 | step time: 0.000
train | epoch   4 | Iter:   5317/ 13000 | global iter:   5317/ 13000 | loss: 1.6741 | ds_loss: 0.0000 | lr: 3.2207e-05 | scale: 16384.0000 | micro time: 1.891 | step time: 0.000
train | epoch   4 | Iter:   5318/ 13000 | global iter:   5318/ 13000 | loss: 1.2430 | ds_loss: 0.0000 | lr: 3.2202e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   4 | Iter:   5319/ 13000 | global iter:   5319/ 13000 | loss: 1.2432 | ds_loss: 0.0000 | lr: 3.2196e-05 | scale: 16384.0000 | micro time: 1.897 | step time: 0.000
train | epoch   4 | Iter:   5320/ 13000 | global iter:   5320/ 13000 | loss: 1.8734 | ds_loss: 0.0000 | lr: 3.2190e-05 | scale: 16384.0000 | micro time: 1.853 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5320/ 13000 | global iter:   5320/ 13000 | loss: 1.4410 | ds_loss: 0.0000 | lr: 3.2190e-05 | scale: 16384.0000 | micro time: 1.853 | step time: 1.837
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5321/ 13000 | global iter:   5321/ 13000 | loss: 1.8346 | ds_loss: 0.0000 | lr: 3.2184e-05 | scale: 16384.0000 | micro time: 1.881 | step time: 0.000
train | epoch   4 | Iter:   5322/ 13000 | global iter:   5322/ 13000 | loss: 1.9385 | ds_loss: 0.0000 | lr: 3.2179e-05 | scale: 16384.0000 | micro time: 1.738 | step time: 0.000
train | epoch   4 | Iter:   5323/ 13000 | global iter:   5323/ 13000 | loss: 0.9113 | ds_loss: 0.0000 | lr: 3.2173e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   4 | Iter:   5324/ 13000 | global iter:   5324/ 13000 | loss: 1.5261 | ds_loss: 0.0000 | lr: 3.2167e-05 | scale: 16384.0000 | micro time: 1.886 | step time: 0.000
train | epoch   4 | Iter:   5325/ 13000 | global iter:   5325/ 13000 | loss: 1.1987 | ds_loss: 0.0000 | lr: 3.2161e-05 | scale: 16384.0000 | micro time: 1.715 | step time: 0.000
train | epoch   4 | Iter:   5326/ 13000 | global iter:   5326/ 13000 | loss: 1.1754 | ds_loss: 0.0000 | lr: 3.2155e-05 | scale: 16384.0000 | micro time: 1.809 | step time: 0.000
train | epoch   4 | Iter:   5327/ 13000 | global iter:   5327/ 13000 | loss: 1.7730 | ds_loss: 0.0000 | lr: 3.2150e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   4 | Iter:   5328/ 13000 | global iter:   5328/ 13000 | loss: 1.4372 | ds_loss: 0.0000 | lr: 3.2144e-05 | scale: 16384.0000 | micro time: 1.826 | step time: 0.000
train | epoch   4 | Iter:   5329/ 13000 | global iter:   5329/ 13000 | loss: 1.2687 | ds_loss: 0.0000 | lr: 3.2138e-05 | scale: 16384.0000 | micro time: 1.764 | step time: 0.000
train | epoch   4 | Iter:   5330/ 13000 | global iter:   5330/ 13000 | loss: 1.5156 | ds_loss: 0.0000 | lr: 3.2132e-05 | scale: 16384.0000 | micro time: 1.841 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5330/ 13000 | global iter:   5330/ 13000 | loss: 1.4579 | ds_loss: 0.0000 | lr: 3.2132e-05 | scale: 16384.0000 | micro time: 1.841 | step time: 1.813
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5331/ 13000 | global iter:   5331/ 13000 | loss: 1.4769 | ds_loss: 0.0000 | lr: 3.2127e-05 | scale: 16384.0000 | micro time: 1.706 | step time: 0.000
train | epoch   4 | Iter:   5332/ 13000 | global iter:   5332/ 13000 | loss: 1.6534 | ds_loss: 0.0000 | lr: 3.2121e-05 | scale: 16384.0000 | micro time: 1.817 | step time: 0.000
train | epoch   4 | Iter:   5333/ 13000 | global iter:   5333/ 13000 | loss: 1.5288 | ds_loss: 0.0000 | lr: 3.2115e-05 | scale: 16384.0000 | micro time: 1.699 | step time: 0.000
train | epoch   4 | Iter:   5334/ 13000 | global iter:   5334/ 13000 | loss: 1.3883 | ds_loss: 0.0000 | lr: 3.2109e-05 | scale: 16384.0000 | micro time: 1.790 | step time: 0.000
train | epoch   4 | Iter:   5335/ 13000 | global iter:   5335/ 13000 | loss: 1.6440 | ds_loss: 0.0000 | lr: 3.2103e-05 | scale: 16384.0000 | micro time: 1.861 | step time: 0.000
train | epoch   4 | Iter:   5336/ 13000 | global iter:   5336/ 13000 | loss: 1.1747 | ds_loss: 0.0000 | lr: 3.2098e-05 | scale: 16384.0000 | micro time: 1.762 | step time: 0.000
train | epoch   4 | Iter:   5337/ 13000 | global iter:   5337/ 13000 | loss: 1.3193 | ds_loss: 0.0000 | lr: 3.2092e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   4 | Iter:   5338/ 13000 | global iter:   5338/ 13000 | loss: 1.6467 | ds_loss: 0.0000 | lr: 3.2086e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   4 | Iter:   5339/ 13000 | global iter:   5339/ 13000 | loss: 1.2221 | ds_loss: 0.0000 | lr: 3.2080e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   4 | Iter:   5340/ 13000 | global iter:   5340/ 13000 | loss: 1.7511 | ds_loss: 0.0000 | lr: 3.2074e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5340/ 13000 | global iter:   5340/ 13000 | loss: 1.4805 | ds_loss: 0.0000 | lr: 3.2074e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 1.801
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5341/ 13000 | global iter:   5341/ 13000 | loss: 1.1278 | ds_loss: 0.0000 | lr: 3.2069e-05 | scale: 16384.0000 | micro time: 1.784 | step time: 0.000
train | epoch   4 | Iter:   5342/ 13000 | global iter:   5342/ 13000 | loss: 0.6610 | ds_loss: 0.0000 | lr: 3.2063e-05 | scale: 16384.0000 | micro time: 1.773 | step time: 0.000
train | epoch   4 | Iter:   5343/ 13000 | global iter:   5343/ 13000 | loss: 0.8533 | ds_loss: 0.0000 | lr: 3.2057e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   4 | Iter:   5344/ 13000 | global iter:   5344/ 13000 | loss: 1.1148 | ds_loss: 0.0000 | lr: 3.2051e-05 | scale: 16384.0000 | micro time: 1.826 | step time: 0.000
train | epoch   4 | Iter:   5345/ 13000 | global iter:   5345/ 13000 | loss: 1.2405 | ds_loss: 0.0000 | lr: 3.2046e-05 | scale: 16384.0000 | micro time: 1.760 | step time: 0.000
train | epoch   4 | Iter:   5346/ 13000 | global iter:   5346/ 13000 | loss: 1.6384 | ds_loss: 0.0000 | lr: 3.2040e-05 | scale: 16384.0000 | micro time: 1.900 | step time: 0.000
train | epoch   4 | Iter:   5347/ 13000 | global iter:   5347/ 13000 | loss: 1.3435 | ds_loss: 0.0000 | lr: 3.2034e-05 | scale: 16384.0000 | micro time: 1.750 | step time: 0.000
train | epoch   4 | Iter:   5348/ 13000 | global iter:   5348/ 13000 | loss: 1.3022 | ds_loss: 0.0000 | lr: 3.2028e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   4 | Iter:   5349/ 13000 | global iter:   5349/ 13000 | loss: 1.2939 | ds_loss: 0.0000 | lr: 3.2022e-05 | scale: 16384.0000 | micro time: 1.727 | step time: 0.000
train | epoch   4 | Iter:   5350/ 13000 | global iter:   5350/ 13000 | loss: 1.2348 | ds_loss: 0.0000 | lr: 3.2017e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5350/ 13000 | global iter:   5350/ 13000 | loss: 1.1810 | ds_loss: 0.0000 | lr: 3.2017e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 1.789
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5351/ 13000 | global iter:   5351/ 13000 | loss: 1.5595 | ds_loss: 0.0000 | lr: 3.2011e-05 | scale: 16384.0000 | micro time: 1.706 | step time: 0.000
train | epoch   4 | Iter:   5352/ 13000 | global iter:   5352/ 13000 | loss: 1.1928 | ds_loss: 0.0000 | lr: 3.2005e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   4 | Iter:   5353/ 13000 | global iter:   5353/ 13000 | loss: 1.5430 | ds_loss: 0.0000 | lr: 3.1999e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   4 | Iter:   5354/ 13000 | global iter:   5354/ 13000 | loss: 1.9823 | ds_loss: 0.0000 | lr: 3.1993e-05 | scale: 16384.0000 | micro time: 1.840 | step time: 0.000
train | epoch   4 | Iter:   5355/ 13000 | global iter:   5355/ 13000 | loss: 1.5693 | ds_loss: 0.0000 | lr: 3.1988e-05 | scale: 16384.0000 | micro time: 1.798 | step time: 0.000
train | epoch   4 | Iter:   5356/ 13000 | global iter:   5356/ 13000 | loss: 1.7848 | ds_loss: 0.0000 | lr: 3.1982e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   4 | Iter:   5357/ 13000 | global iter:   5357/ 13000 | loss: 1.3545 | ds_loss: 0.0000 | lr: 3.1976e-05 | scale: 16384.0000 | micro time: 1.866 | step time: 0.000
train | epoch   4 | Iter:   5358/ 13000 | global iter:   5358/ 13000 | loss: 1.2997 | ds_loss: 0.0000 | lr: 3.1970e-05 | scale: 16384.0000 | micro time: 1.792 | step time: 0.000
train | epoch   4 | Iter:   5359/ 13000 | global iter:   5359/ 13000 | loss: 1.3649 | ds_loss: 0.0000 | lr: 3.1964e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   4 | Iter:   5360/ 13000 | global iter:   5360/ 13000 | loss: 1.5299 | ds_loss: 0.0000 | lr: 3.1959e-05 | scale: 16384.0000 | micro time: 1.924 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5360/ 13000 | global iter:   5360/ 13000 | loss: 1.5181 | ds_loss: 0.0000 | lr: 3.1959e-05 | scale: 16384.0000 | micro time: 1.924 | step time: 1.811
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5361/ 13000 | global iter:   5361/ 13000 | loss: 1.6076 | ds_loss: 0.0000 | lr: 3.1953e-05 | scale: 16384.0000 | micro time: 1.818 | step time: 0.000
train | epoch   4 | Iter:   5362/ 13000 | global iter:   5362/ 13000 | loss: 0.9637 | ds_loss: 0.0000 | lr: 3.1947e-05 | scale: 16384.0000 | micro time: 1.735 | step time: 0.000
train | epoch   4 | Iter:   5363/ 13000 | global iter:   5363/ 13000 | loss: 1.2266 | ds_loss: 0.0000 | lr: 3.1941e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   4 | Iter:   5364/ 13000 | global iter:   5364/ 13000 | loss: 1.3628 | ds_loss: 0.0000 | lr: 3.1936e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   4 | Iter:   5365/ 13000 | global iter:   5365/ 13000 | loss: 1.5825 | ds_loss: 0.0000 | lr: 3.1930e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   4 | Iter:   5366/ 13000 | global iter:   5366/ 13000 | loss: 1.2831 | ds_loss: 0.0000 | lr: 3.1924e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   4 | Iter:   5367/ 13000 | global iter:   5367/ 13000 | loss: 1.4908 | ds_loss: 0.0000 | lr: 3.1918e-05 | scale: 16384.0000 | micro time: 1.793 | step time: 0.000
train | epoch   4 | Iter:   5368/ 13000 | global iter:   5368/ 13000 | loss: 1.0312 | ds_loss: 0.0000 | lr: 3.1912e-05 | scale: 16384.0000 | micro time: 1.797 | step time: 0.000
train | epoch   4 | Iter:   5369/ 13000 | global iter:   5369/ 13000 | loss: 1.1507 | ds_loss: 0.0000 | lr: 3.1907e-05 | scale: 16384.0000 | micro time: 1.750 | step time: 0.000
train | epoch   4 | Iter:   5370/ 13000 | global iter:   5370/ 13000 | loss: 0.7079 | ds_loss: 0.0000 | lr: 3.1901e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5370/ 13000 | global iter:   5370/ 13000 | loss: 1.2407 | ds_loss: 0.0000 | lr: 3.1901e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 1.790
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5371/ 13000 | global iter:   5371/ 13000 | loss: 1.4241 | ds_loss: 0.0000 | lr: 3.1895e-05 | scale: 16384.0000 | micro time: 1.854 | step time: 0.000
train | epoch   4 | Iter:   5372/ 13000 | global iter:   5372/ 13000 | loss: 1.4778 | ds_loss: 0.0000 | lr: 3.1889e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   4 | Iter:   5373/ 13000 | global iter:   5373/ 13000 | loss: 0.4911 | ds_loss: 0.0000 | lr: 3.1883e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   4 | Iter:   5374/ 13000 | global iter:   5374/ 13000 | loss: 1.3822 | ds_loss: 0.0000 | lr: 3.1878e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   4 | Iter:   5375/ 13000 | global iter:   5375/ 13000 | loss: 0.7754 | ds_loss: 0.0000 | lr: 3.1872e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
train | epoch   4 | Iter:   5376/ 13000 | global iter:   5376/ 13000 | loss: 1.2982 | ds_loss: 0.0000 | lr: 3.1866e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   4 | Iter:   5377/ 13000 | global iter:   5377/ 13000 | loss: 0.6678 | ds_loss: 0.0000 | lr: 3.1860e-05 | scale: 16384.0000 | micro time: 1.820 | step time: 0.000
train | epoch   4 | Iter:   5378/ 13000 | global iter:   5378/ 13000 | loss: 1.2140 | ds_loss: 0.0000 | lr: 3.1854e-05 | scale: 16384.0000 | micro time: 1.816 | step time: 0.000
train | epoch   4 | Iter:   5379/ 13000 | global iter:   5379/ 13000 | loss: 1.5618 | ds_loss: 0.0000 | lr: 3.1849e-05 | scale: 16384.0000 | micro time: 1.793 | step time: 0.000
train | epoch   4 | Iter:   5380/ 13000 | global iter:   5380/ 13000 | loss: 1.4281 | ds_loss: 0.0000 | lr: 3.1843e-05 | scale: 16384.0000 | micro time: 1.720 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5380/ 13000 | global iter:   5380/ 13000 | loss: 1.1720 | ds_loss: 0.0000 | lr: 3.1843e-05 | scale: 16384.0000 | micro time: 1.720 | step time: 1.811
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5381/ 13000 | global iter:   5381/ 13000 | loss: 1.6955 | ds_loss: 0.0000 | lr: 3.1837e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   4 | Iter:   5382/ 13000 | global iter:   5382/ 13000 | loss: 1.2273 | ds_loss: 0.0000 | lr: 3.1831e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   4 | Iter:   5383/ 13000 | global iter:   5383/ 13000 | loss: 1.2547 | ds_loss: 0.0000 | lr: 3.1825e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   4 | Iter:   5384/ 13000 | global iter:   5384/ 13000 | loss: 1.0931 | ds_loss: 0.0000 | lr: 3.1820e-05 | scale: 16384.0000 | micro time: 1.766 | step time: 0.000
train | epoch   4 | Iter:   5385/ 13000 | global iter:   5385/ 13000 | loss: 1.0872 | ds_loss: 0.0000 | lr: 3.1814e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   4 | Iter:   5386/ 13000 | global iter:   5386/ 13000 | loss: 1.3153 | ds_loss: 0.0000 | lr: 3.1808e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   4 | Iter:   5387/ 13000 | global iter:   5387/ 13000 | loss: 0.6627 | ds_loss: 0.0000 | lr: 3.1802e-05 | scale: 16384.0000 | micro time: 1.916 | step time: 0.000
train | epoch   4 | Iter:   5388/ 13000 | global iter:   5388/ 13000 | loss: 1.2863 | ds_loss: 0.0000 | lr: 3.1796e-05 | scale: 16384.0000 | micro time: 1.918 | step time: 0.000
train | epoch   4 | Iter:   5389/ 13000 | global iter:   5389/ 13000 | loss: 0.5814 | ds_loss: 0.0000 | lr: 3.1791e-05 | scale: 16384.0000 | micro time: 1.741 | step time: 0.000
train | epoch   4 | Iter:   5390/ 13000 | global iter:   5390/ 13000 | loss: 1.4349 | ds_loss: 0.0000 | lr: 3.1785e-05 | scale: 16384.0000 | micro time: 1.929 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5390/ 13000 | global iter:   5390/ 13000 | loss: 1.1638 | ds_loss: 0.0000 | lr: 3.1785e-05 | scale: 16384.0000 | micro time: 1.929 | step time: 1.835
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5391/ 13000 | global iter:   5391/ 13000 | loss: 1.0237 | ds_loss: 0.0000 | lr: 3.1779e-05 | scale: 16384.0000 | micro time: 1.684 | step time: 0.000
train | epoch   4 | Iter:   5392/ 13000 | global iter:   5392/ 13000 | loss: 1.6200 | ds_loss: 0.0000 | lr: 3.1773e-05 | scale: 16384.0000 | micro time: 1.725 | step time: 0.000
train | epoch   4 | Iter:   5393/ 13000 | global iter:   5393/ 13000 | loss: 1.5350 | ds_loss: 0.0000 | lr: 3.1767e-05 | scale: 16384.0000 | micro time: 1.850 | step time: 0.000
train | epoch   4 | Iter:   5394/ 13000 | global iter:   5394/ 13000 | loss: 1.4938 | ds_loss: 0.0000 | lr: 3.1761e-05 | scale: 16384.0000 | micro time: 1.772 | step time: 0.000
train | epoch   4 | Iter:   5395/ 13000 | global iter:   5395/ 13000 | loss: 1.3782 | ds_loss: 0.0000 | lr: 3.1756e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   4 | Iter:   5396/ 13000 | global iter:   5396/ 13000 | loss: 1.9239 | ds_loss: 0.0000 | lr: 3.1750e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   4 | Iter:   5397/ 13000 | global iter:   5397/ 13000 | loss: 0.9102 | ds_loss: 0.0000 | lr: 3.1744e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   4 | Iter:   5398/ 13000 | global iter:   5398/ 13000 | loss: 1.7126 | ds_loss: 0.0000 | lr: 3.1738e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   4 | Iter:   5399/ 13000 | global iter:   5399/ 13000 | loss: 1.7864 | ds_loss: 0.0000 | lr: 3.1732e-05 | scale: 16384.0000 | micro time: 1.759 | step time: 0.000
train | epoch   4 | Iter:   5400/ 13000 | global iter:   5400/ 13000 | loss: 1.6249 | ds_loss: 0.0000 | lr: 3.1727e-05 | scale: 16384.0000 | micro time: 1.772 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5400/ 13000 | global iter:   5400/ 13000 | loss: 1.5009 | ds_loss: 0.0000 | lr: 3.1727e-05 | scale: 16384.0000 | micro time: 1.772 | step time: 1.773
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5401/ 13000 | global iter:   5401/ 13000 | loss: 1.8465 | ds_loss: 0.0000 | lr: 3.1721e-05 | scale: 16384.0000 | micro time: 1.715 | step time: 0.000
train | epoch   4 | Iter:   5402/ 13000 | global iter:   5402/ 13000 | loss: 1.4435 | ds_loss: 0.0000 | lr: 3.1715e-05 | scale: 16384.0000 | micro time: 1.810 | step time: 0.000
train | epoch   4 | Iter:   5403/ 13000 | global iter:   5403/ 13000 | loss: 1.2674 | ds_loss: 0.0000 | lr: 3.1709e-05 | scale: 16384.0000 | micro time: 1.780 | step time: 0.000
train | epoch   4 | Iter:   5404/ 13000 | global iter:   5404/ 13000 | loss: 1.1095 | ds_loss: 0.0000 | lr: 3.1703e-05 | scale: 16384.0000 | micro time: 1.849 | step time: 0.000
train | epoch   4 | Iter:   5405/ 13000 | global iter:   5405/ 13000 | loss: 0.8092 | ds_loss: 0.0000 | lr: 3.1698e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   4 | Iter:   5406/ 13000 | global iter:   5406/ 13000 | loss: 1.3534 | ds_loss: 0.0000 | lr: 3.1692e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   4 | Iter:   5407/ 13000 | global iter:   5407/ 13000 | loss: 0.4731 | ds_loss: 0.0000 | lr: 3.1686e-05 | scale: 16384.0000 | micro time: 1.727 | step time: 0.000
train | epoch   4 | Iter:   5408/ 13000 | global iter:   5408/ 13000 | loss: 1.0314 | ds_loss: 0.0000 | lr: 3.1680e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   4 | Iter:   5409/ 13000 | global iter:   5409/ 13000 | loss: 1.4536 | ds_loss: 0.0000 | lr: 3.1674e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   4 | Iter:   5410/ 13000 | global iter:   5410/ 13000 | loss: 1.3448 | ds_loss: 0.0000 | lr: 3.1669e-05 | scale: 16384.0000 | micro time: 1.711 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5410/ 13000 | global iter:   5410/ 13000 | loss: 1.2132 | ds_loss: 0.0000 | lr: 3.1669e-05 | scale: 16384.0000 | micro time: 1.711 | step time: 1.776
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5411/ 13000 | global iter:   5411/ 13000 | loss: 1.1030 | ds_loss: 0.0000 | lr: 3.1663e-05 | scale: 16384.0000 | micro time: 1.797 | step time: 0.000
train | epoch   4 | Iter:   5412/ 13000 | global iter:   5412/ 13000 | loss: 0.7843 | ds_loss: 0.0000 | lr: 3.1657e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   4 | Iter:   5413/ 13000 | global iter:   5413/ 13000 | loss: 1.1994 | ds_loss: 0.0000 | lr: 3.1651e-05 | scale: 16384.0000 | micro time: 1.744 | step time: 0.000
train | epoch   4 | Iter:   5414/ 13000 | global iter:   5414/ 13000 | loss: 1.3241 | ds_loss: 0.0000 | lr: 3.1645e-05 | scale: 16384.0000 | micro time: 1.746 | step time: 0.000
train | epoch   4 | Iter:   5415/ 13000 | global iter:   5415/ 13000 | loss: 1.8397 | ds_loss: 0.0000 | lr: 3.1639e-05 | scale: 16384.0000 | micro time: 1.677 | step time: 0.000
train | epoch   4 | Iter:   5416/ 13000 | global iter:   5416/ 13000 | loss: 1.2686 | ds_loss: 0.0000 | lr: 3.1634e-05 | scale: 16384.0000 | micro time: 1.744 | step time: 0.000
train | epoch   4 | Iter:   5417/ 13000 | global iter:   5417/ 13000 | loss: 1.0919 | ds_loss: 0.0000 | lr: 3.1628e-05 | scale: 16384.0000 | micro time: 1.828 | step time: 0.000
train | epoch   4 | Iter:   5418/ 13000 | global iter:   5418/ 13000 | loss: 2.3663 | ds_loss: 0.0000 | lr: 3.1622e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   4 | Iter:   5419/ 13000 | global iter:   5419/ 13000 | loss: 1.5048 | ds_loss: 0.0000 | lr: 3.1616e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   4 | Iter:   5420/ 13000 | global iter:   5420/ 13000 | loss: 1.2195 | ds_loss: 0.0000 | lr: 3.1610e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5420/ 13000 | global iter:   5420/ 13000 | loss: 1.3702 | ds_loss: 0.0000 | lr: 3.1610e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 1.776
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5421/ 13000 | global iter:   5421/ 13000 | loss: 1.4611 | ds_loss: 0.0000 | lr: 3.1605e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   4 | Iter:   5422/ 13000 | global iter:   5422/ 13000 | loss: 1.0826 | ds_loss: 0.0000 | lr: 3.1599e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   4 | Iter:   5423/ 13000 | global iter:   5423/ 13000 | loss: 1.2973 | ds_loss: 0.0000 | lr: 3.1593e-05 | scale: 16384.0000 | micro time: 1.790 | step time: 0.000
train | epoch   4 | Iter:   5424/ 13000 | global iter:   5424/ 13000 | loss: 1.2581 | ds_loss: 0.0000 | lr: 3.1587e-05 | scale: 16384.0000 | micro time: 1.672 | step time: 0.000
train | epoch   4 | Iter:   5425/ 13000 | global iter:   5425/ 13000 | loss: 1.6570 | ds_loss: 0.0000 | lr: 3.1581e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   4 | Iter:   5426/ 13000 | global iter:   5426/ 13000 | loss: 1.1155 | ds_loss: 0.0000 | lr: 3.1575e-05 | scale: 16384.0000 | micro time: 1.759 | step time: 0.000
train | epoch   4 | Iter:   5427/ 13000 | global iter:   5427/ 13000 | loss: 1.1521 | ds_loss: 0.0000 | lr: 3.1570e-05 | scale: 16384.0000 | micro time: 1.707 | step time: 0.000
train | epoch   4 | Iter:   5428/ 13000 | global iter:   5428/ 13000 | loss: 1.1085 | ds_loss: 0.0000 | lr: 3.1564e-05 | scale: 16384.0000 | micro time: 1.828 | step time: 0.000
train | epoch   4 | Iter:   5429/ 13000 | global iter:   5429/ 13000 | loss: 1.5537 | ds_loss: 0.0000 | lr: 3.1558e-05 | scale: 16384.0000 | micro time: 1.723 | step time: 0.000
train | epoch   4 | Iter:   5430/ 13000 | global iter:   5430/ 13000 | loss: 1.1543 | ds_loss: 0.0000 | lr: 3.1552e-05 | scale: 16384.0000 | micro time: 1.854 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5430/ 13000 | global iter:   5430/ 13000 | loss: 1.2840 | ds_loss: 0.0000 | lr: 3.1552e-05 | scale: 16384.0000 | micro time: 1.854 | step time: 1.770
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5431/ 13000 | global iter:   5431/ 13000 | loss: 1.9033 | ds_loss: 0.0000 | lr: 3.1546e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   4 | Iter:   5432/ 13000 | global iter:   5432/ 13000 | loss: 1.1866 | ds_loss: 0.0000 | lr: 3.1541e-05 | scale: 16384.0000 | micro time: 1.821 | step time: 0.000
train | epoch   4 | Iter:   5433/ 13000 | global iter:   5433/ 13000 | loss: 2.2041 | ds_loss: 0.0000 | lr: 3.1535e-05 | scale: 16384.0000 | micro time: 1.733 | step time: 0.000
train | epoch   4 | Iter:   5434/ 13000 | global iter:   5434/ 13000 | loss: 1.0403 | ds_loss: 0.0000 | lr: 3.1529e-05 | scale: 16384.0000 | micro time: 1.813 | step time: 0.000
train | epoch   4 | Iter:   5435/ 13000 | global iter:   5435/ 13000 | loss: 1.5814 | ds_loss: 0.0000 | lr: 3.1523e-05 | scale: 16384.0000 | micro time: 1.848 | step time: 0.000
train | epoch   4 | Iter:   5436/ 13000 | global iter:   5436/ 13000 | loss: 0.6196 | ds_loss: 0.0000 | lr: 3.1517e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   4 | Iter:   5437/ 13000 | global iter:   5437/ 13000 | loss: 1.2817 | ds_loss: 0.0000 | lr: 3.1511e-05 | scale: 16384.0000 | micro time: 1.743 | step time: 0.000
train | epoch   4 | Iter:   5438/ 13000 | global iter:   5438/ 13000 | loss: 1.3936 | ds_loss: 0.0000 | lr: 3.1506e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 0.000
train | epoch   4 | Iter:   5439/ 13000 | global iter:   5439/ 13000 | loss: 1.7869 | ds_loss: 0.0000 | lr: 3.1500e-05 | scale: 16384.0000 | micro time: 1.737 | step time: 0.000
train | epoch   4 | Iter:   5440/ 13000 | global iter:   5440/ 13000 | loss: 0.9819 | ds_loss: 0.0000 | lr: 3.1494e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5440/ 13000 | global iter:   5440/ 13000 | loss: 1.3979 | ds_loss: 0.0000 | lr: 3.1494e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 1.788
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5441/ 13000 | global iter:   5441/ 13000 | loss: 0.8030 | ds_loss: 0.0000 | lr: 3.1488e-05 | scale: 16384.0000 | micro time: 1.790 | step time: 0.000
train | epoch   4 | Iter:   5442/ 13000 | global iter:   5442/ 13000 | loss: 0.9001 | ds_loss: 0.0000 | lr: 3.1482e-05 | scale: 16384.0000 | micro time: 1.806 | step time: 0.000
train | epoch   4 | Iter:   5443/ 13000 | global iter:   5443/ 13000 | loss: 1.1942 | ds_loss: 0.0000 | lr: 3.1476e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   4 | Iter:   5444/ 13000 | global iter:   5444/ 13000 | loss: 1.1712 | ds_loss: 0.0000 | lr: 3.1471e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   4 | Iter:   5445/ 13000 | global iter:   5445/ 13000 | loss: 1.2343 | ds_loss: 0.0000 | lr: 3.1465e-05 | scale: 16384.0000 | micro time: 1.903 | step time: 0.000
train | epoch   4 | Iter:   5446/ 13000 | global iter:   5446/ 13000 | loss: 1.5875 | ds_loss: 0.0000 | lr: 3.1459e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   4 | Iter:   5447/ 13000 | global iter:   5447/ 13000 | loss: 1.6962 | ds_loss: 0.0000 | lr: 3.1453e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 0.000
train | epoch   4 | Iter:   5448/ 13000 | global iter:   5448/ 13000 | loss: 1.4304 | ds_loss: 0.0000 | lr: 3.1447e-05 | scale: 16384.0000 | micro time: 1.703 | step time: 0.000
train | epoch   4 | Iter:   5449/ 13000 | global iter:   5449/ 13000 | loss: 1.5570 | ds_loss: 0.0000 | lr: 3.1442e-05 | scale: 16384.0000 | micro time: 1.824 | step time: 0.000
train | epoch   4 | Iter:   5450/ 13000 | global iter:   5450/ 13000 | loss: 1.5024 | ds_loss: 0.0000 | lr: 3.1436e-05 | scale: 16384.0000 | micro time: 1.862 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5450/ 13000 | global iter:   5450/ 13000 | loss: 1.3076 | ds_loss: 0.0000 | lr: 3.1436e-05 | scale: 16384.0000 | micro time: 1.862 | step time: 1.811
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5451/ 13000 | global iter:   5451/ 13000 | loss: 1.5446 | ds_loss: 0.0000 | lr: 3.1430e-05 | scale: 16384.0000 | micro time: 1.854 | step time: 0.000
train | epoch   4 | Iter:   5452/ 13000 | global iter:   5452/ 13000 | loss: 1.3981 | ds_loss: 0.0000 | lr: 3.1424e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   4 | Iter:   5453/ 13000 | global iter:   5453/ 13000 | loss: 1.4149 | ds_loss: 0.0000 | lr: 3.1418e-05 | scale: 16384.0000 | micro time: 1.856 | step time: 0.000
train | epoch   4 | Iter:   5454/ 13000 | global iter:   5454/ 13000 | loss: 1.0900 | ds_loss: 0.0000 | lr: 3.1412e-05 | scale: 16384.0000 | micro time: 1.858 | step time: 0.000
train | epoch   4 | Iter:   5455/ 13000 | global iter:   5455/ 13000 | loss: 1.7960 | ds_loss: 0.0000 | lr: 3.1407e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   4 | Iter:   5456/ 13000 | global iter:   5456/ 13000 | loss: 1.4766 | ds_loss: 0.0000 | lr: 3.1401e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   4 | Iter:   5457/ 13000 | global iter:   5457/ 13000 | loss: 0.9346 | ds_loss: 0.0000 | lr: 3.1395e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   4 | Iter:   5458/ 13000 | global iter:   5458/ 13000 | loss: 1.6387 | ds_loss: 0.0000 | lr: 3.1389e-05 | scale: 16384.0000 | micro time: 1.729 | step time: 0.000
train | epoch   4 | Iter:   5459/ 13000 | global iter:   5459/ 13000 | loss: 1.1084 | ds_loss: 0.0000 | lr: 3.1383e-05 | scale: 16384.0000 | micro time: 1.652 | step time: 0.000
train | epoch   4 | Iter:   5460/ 13000 | global iter:   5460/ 13000 | loss: 1.6508 | ds_loss: 0.0000 | lr: 3.1377e-05 | scale: 16384.0000 | micro time: 1.887 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5460/ 13000 | global iter:   5460/ 13000 | loss: 1.4053 | ds_loss: 0.0000 | lr: 3.1377e-05 | scale: 16384.0000 | micro time: 1.887 | step time: 1.805
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5461/ 13000 | global iter:   5461/ 13000 | loss: 1.9839 | ds_loss: 0.0000 | lr: 3.1372e-05 | scale: 16384.0000 | micro time: 1.774 | step time: 0.000
train | epoch   4 | Iter:   5462/ 13000 | global iter:   5462/ 13000 | loss: 1.2516 | ds_loss: 0.0000 | lr: 3.1366e-05 | scale: 16384.0000 | micro time: 1.813 | step time: 0.000
train | epoch   4 | Iter:   5463/ 13000 | global iter:   5463/ 13000 | loss: 1.4156 | ds_loss: 0.0000 | lr: 3.1360e-05 | scale: 16384.0000 | micro time: 1.832 | step time: 0.000
train | epoch   4 | Iter:   5464/ 13000 | global iter:   5464/ 13000 | loss: 1.4014 | ds_loss: 0.0000 | lr: 3.1354e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   4 | Iter:   5465/ 13000 | global iter:   5465/ 13000 | loss: 1.3641 | ds_loss: 0.0000 | lr: 3.1348e-05 | scale: 16384.0000 | micro time: 1.728 | step time: 0.000
train | epoch   4 | Iter:   5466/ 13000 | global iter:   5466/ 13000 | loss: 1.3221 | ds_loss: 0.0000 | lr: 3.1342e-05 | scale: 16384.0000 | micro time: 1.822 | step time: 0.000
train | epoch   4 | Iter:   5467/ 13000 | global iter:   5467/ 13000 | loss: 1.2890 | ds_loss: 0.0000 | lr: 3.1337e-05 | scale: 16384.0000 | micro time: 1.776 | step time: 0.000
train | epoch   4 | Iter:   5468/ 13000 | global iter:   5468/ 13000 | loss: 1.6923 | ds_loss: 0.0000 | lr: 3.1331e-05 | scale: 16384.0000 | micro time: 1.746 | step time: 0.000
train | epoch   4 | Iter:   5469/ 13000 | global iter:   5469/ 13000 | loss: 1.7825 | ds_loss: 0.0000 | lr: 3.1325e-05 | scale: 16384.0000 | micro time: 1.880 | step time: 0.000
train | epoch   4 | Iter:   5470/ 13000 | global iter:   5470/ 13000 | loss: 1.1409 | ds_loss: 0.0000 | lr: 3.1319e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5470/ 13000 | global iter:   5470/ 13000 | loss: 1.4643 | ds_loss: 0.0000 | lr: 3.1319e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 1.791
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5471/ 13000 | global iter:   5471/ 13000 | loss: 1.1938 | ds_loss: 0.0000 | lr: 3.1313e-05 | scale: 16384.0000 | micro time: 1.797 | step time: 0.000
train | epoch   4 | Iter:   5472/ 13000 | global iter:   5472/ 13000 | loss: 1.7157 | ds_loss: 0.0000 | lr: 3.1307e-05 | scale: 16384.0000 | micro time: 1.825 | step time: 0.000
train | epoch   4 | Iter:   5473/ 13000 | global iter:   5473/ 13000 | loss: 1.3301 | ds_loss: 0.0000 | lr: 3.1302e-05 | scale: 16384.0000 | micro time: 1.888 | step time: 0.000
train | epoch   4 | Iter:   5474/ 13000 | global iter:   5474/ 13000 | loss: 1.7425 | ds_loss: 0.0000 | lr: 3.1296e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   4 | Iter:   5475/ 13000 | global iter:   5475/ 13000 | loss: 1.4174 | ds_loss: 0.0000 | lr: 3.1290e-05 | scale: 16384.0000 | micro time: 1.870 | step time: 0.000
train | epoch   4 | Iter:   5476/ 13000 | global iter:   5476/ 13000 | loss: 0.7518 | ds_loss: 0.0000 | lr: 3.1284e-05 | scale: 16384.0000 | micro time: 1.750 | step time: 0.000
train | epoch   4 | Iter:   5477/ 13000 | global iter:   5477/ 13000 | loss: 0.8218 | ds_loss: 0.0000 | lr: 3.1278e-05 | scale: 16384.0000 | micro time: 1.660 | step time: 0.000
train | epoch   4 | Iter:   5478/ 13000 | global iter:   5478/ 13000 | loss: 1.2839 | ds_loss: 0.0000 | lr: 3.1272e-05 | scale: 16384.0000 | micro time: 1.759 | step time: 0.000
train | epoch   4 | Iter:   5479/ 13000 | global iter:   5479/ 13000 | loss: 0.9412 | ds_loss: 0.0000 | lr: 3.1266e-05 | scale: 16384.0000 | micro time: 1.793 | step time: 0.000
train | epoch   4 | Iter:   5480/ 13000 | global iter:   5480/ 13000 | loss: 1.5636 | ds_loss: 0.0000 | lr: 3.1261e-05 | scale: 16384.0000 | micro time: 1.865 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5480/ 13000 | global iter:   5480/ 13000 | loss: 1.2762 | ds_loss: 0.0000 | lr: 3.1261e-05 | scale: 16384.0000 | micro time: 1.865 | step time: 1.800
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5481/ 13000 | global iter:   5481/ 13000 | loss: 0.6249 | ds_loss: 0.0000 | lr: 3.1255e-05 | scale: 16384.0000 | micro time: 1.877 | step time: 0.000
train | epoch   4 | Iter:   5482/ 13000 | global iter:   5482/ 13000 | loss: 1.6552 | ds_loss: 0.0000 | lr: 3.1249e-05 | scale: 16384.0000 | micro time: 1.833 | step time: 0.000
train | epoch   4 | Iter:   5483/ 13000 | global iter:   5483/ 13000 | loss: 1.4582 | ds_loss: 0.0000 | lr: 3.1243e-05 | scale: 16384.0000 | micro time: 1.692 | step time: 0.000
train | epoch   4 | Iter:   5484/ 13000 | global iter:   5484/ 13000 | loss: 1.1893 | ds_loss: 0.0000 | lr: 3.1237e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   4 | Iter:   5485/ 13000 | global iter:   5485/ 13000 | loss: 1.3031 | ds_loss: 0.0000 | lr: 3.1231e-05 | scale: 16384.0000 | micro time: 1.705 | step time: 0.000
train | epoch   4 | Iter:   5486/ 13000 | global iter:   5486/ 13000 | loss: 1.8416 | ds_loss: 0.0000 | lr: 3.1226e-05 | scale: 16384.0000 | micro time: 1.720 | step time: 0.000
train | epoch   4 | Iter:   5487/ 13000 | global iter:   5487/ 13000 | loss: 1.3342 | ds_loss: 0.0000 | lr: 3.1220e-05 | scale: 16384.0000 | micro time: 1.818 | step time: 0.000
train | epoch   4 | Iter:   5488/ 13000 | global iter:   5488/ 13000 | loss: 1.4986 | ds_loss: 0.0000 | lr: 3.1214e-05 | scale: 16384.0000 | micro time: 1.877 | step time: 0.000
train | epoch   4 | Iter:   5489/ 13000 | global iter:   5489/ 13000 | loss: 1.4181 | ds_loss: 0.0000 | lr: 3.1208e-05 | scale: 16384.0000 | micro time: 1.853 | step time: 0.000
train | epoch   4 | Iter:   5490/ 13000 | global iter:   5490/ 13000 | loss: 1.3059 | ds_loss: 0.0000 | lr: 3.1202e-05 | scale: 16384.0000 | micro time: 1.714 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5490/ 13000 | global iter:   5490/ 13000 | loss: 1.3629 | ds_loss: 0.0000 | lr: 3.1202e-05 | scale: 16384.0000 | micro time: 1.714 | step time: 1.795
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5491/ 13000 | global iter:   5491/ 13000 | loss: 1.2139 | ds_loss: 0.0000 | lr: 3.1196e-05 | scale: 16384.0000 | micro time: 1.794 | step time: 0.000
train | epoch   4 | Iter:   5492/ 13000 | global iter:   5492/ 13000 | loss: 1.2441 | ds_loss: 0.0000 | lr: 3.1191e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   4 | Iter:   5493/ 13000 | global iter:   5493/ 13000 | loss: 2.0068 | ds_loss: 0.0000 | lr: 3.1185e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   4 | Iter:   5494/ 13000 | global iter:   5494/ 13000 | loss: 1.7642 | ds_loss: 0.0000 | lr: 3.1179e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   4 | Iter:   5495/ 13000 | global iter:   5495/ 13000 | loss: 1.3026 | ds_loss: 0.0000 | lr: 3.1173e-05 | scale: 16384.0000 | micro time: 1.731 | step time: 0.000
train | epoch   4 | Iter:   5496/ 13000 | global iter:   5496/ 13000 | loss: 0.9064 | ds_loss: 0.0000 | lr: 3.1167e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   4 | Iter:   5497/ 13000 | global iter:   5497/ 13000 | loss: 1.0918 | ds_loss: 0.0000 | lr: 3.1161e-05 | scale: 16384.0000 | micro time: 1.686 | step time: 0.000
train | epoch   4 | Iter:   5498/ 13000 | global iter:   5498/ 13000 | loss: 0.9314 | ds_loss: 0.0000 | lr: 3.1155e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   4 | Iter:   5499/ 13000 | global iter:   5499/ 13000 | loss: 1.5264 | ds_loss: 0.0000 | lr: 3.1150e-05 | scale: 16384.0000 | micro time: 1.707 | step time: 0.000
train | epoch   4 | Iter:   5500/ 13000 | global iter:   5500/ 13000 | loss: 1.0138 | ds_loss: 0.0000 | lr: 3.1144e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5500/ 13000 | global iter:   5500/ 13000 | loss: 1.3001 | ds_loss: 0.0000 | lr: 3.1144e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 1.777
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5501/ 13000 | global iter:   5501/ 13000 | loss: 1.3923 | ds_loss: 0.0000 | lr: 3.1138e-05 | scale: 16384.0000 | micro time: 1.838 | step time: 0.000
train | epoch   4 | Iter:   5502/ 13000 | global iter:   5502/ 13000 | loss: 1.1783 | ds_loss: 0.0000 | lr: 3.1132e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   4 | Iter:   5503/ 13000 | global iter:   5503/ 13000 | loss: 1.5215 | ds_loss: 0.0000 | lr: 3.1126e-05 | scale: 16384.0000 | micro time: 1.866 | step time: 0.000
train | epoch   4 | Iter:   5504/ 13000 | global iter:   5504/ 13000 | loss: 0.8863 | ds_loss: 0.0000 | lr: 3.1120e-05 | scale: 16384.0000 | micro time: 1.782 | step time: 0.000
train | epoch   4 | Iter:   5505/ 13000 | global iter:   5505/ 13000 | loss: 1.0990 | ds_loss: 0.0000 | lr: 3.1115e-05 | scale: 16384.0000 | micro time: 1.778 | step time: 0.000
train | epoch   4 | Iter:   5506/ 13000 | global iter:   5506/ 13000 | loss: 1.0429 | ds_loss: 0.0000 | lr: 3.1109e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   4 | Iter:   5507/ 13000 | global iter:   5507/ 13000 | loss: 1.0455 | ds_loss: 0.0000 | lr: 3.1103e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   4 | Iter:   5508/ 13000 | global iter:   5508/ 13000 | loss: 1.4032 | ds_loss: 0.0000 | lr: 3.1097e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   4 | Iter:   5509/ 13000 | global iter:   5509/ 13000 | loss: 1.8919 | ds_loss: 0.0000 | lr: 3.1091e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   4 | Iter:   5510/ 13000 | global iter:   5510/ 13000 | loss: 0.7964 | ds_loss: 0.0000 | lr: 3.1085e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5510/ 13000 | global iter:   5510/ 13000 | loss: 1.2257 | ds_loss: 0.0000 | lr: 3.1085e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 1.811
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5511/ 13000 | global iter:   5511/ 13000 | loss: 1.4194 | ds_loss: 0.0000 | lr: 3.1079e-05 | scale: 16384.0000 | micro time: 1.776 | step time: 0.000
train | epoch   4 | Iter:   5512/ 13000 | global iter:   5512/ 13000 | loss: 1.1534 | ds_loss: 0.0000 | lr: 3.1074e-05 | scale: 16384.0000 | micro time: 1.716 | step time: 0.000
train | epoch   4 | Iter:   5513/ 13000 | global iter:   5513/ 13000 | loss: 1.8157 | ds_loss: 0.0000 | lr: 3.1068e-05 | scale: 16384.0000 | micro time: 1.727 | step time: 0.000
train | epoch   4 | Iter:   5514/ 13000 | global iter:   5514/ 13000 | loss: 1.3098 | ds_loss: 0.0000 | lr: 3.1062e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   4 | Iter:   5515/ 13000 | global iter:   5515/ 13000 | loss: 1.6074 | ds_loss: 0.0000 | lr: 3.1056e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   4 | Iter:   5516/ 13000 | global iter:   5516/ 13000 | loss: 1.6080 | ds_loss: 0.0000 | lr: 3.1050e-05 | scale: 16384.0000 | micro time: 1.773 | step time: 0.000
train | epoch   4 | Iter:   5517/ 13000 | global iter:   5517/ 13000 | loss: 1.6804 | ds_loss: 0.0000 | lr: 3.1044e-05 | scale: 16384.0000 | micro time: 1.845 | step time: 0.000
train | epoch   4 | Iter:   5518/ 13000 | global iter:   5518/ 13000 | loss: 1.8515 | ds_loss: 0.0000 | lr: 3.1038e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   4 | Iter:   5519/ 13000 | global iter:   5519/ 13000 | loss: 1.1700 | ds_loss: 0.0000 | lr: 3.1033e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   4 | Iter:   5520/ 13000 | global iter:   5520/ 13000 | loss: 1.4456 | ds_loss: 0.0000 | lr: 3.1027e-05 | scale: 16384.0000 | micro time: 1.873 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5520/ 13000 | global iter:   5520/ 13000 | loss: 1.5061 | ds_loss: 0.0000 | lr: 3.1027e-05 | scale: 16384.0000 | micro time: 1.873 | step time: 1.786
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5521/ 13000 | global iter:   5521/ 13000 | loss: 1.4515 | ds_loss: 0.0000 | lr: 3.1021e-05 | scale: 16384.0000 | micro time: 1.717 | step time: 0.000
train | epoch   4 | Iter:   5522/ 13000 | global iter:   5522/ 13000 | loss: 0.9735 | ds_loss: 0.0000 | lr: 3.1015e-05 | scale: 16384.0000 | micro time: 1.790 | step time: 0.000
train | epoch   4 | Iter:   5523/ 13000 | global iter:   5523/ 13000 | loss: 1.3597 | ds_loss: 0.0000 | lr: 3.1009e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   4 | Iter:   5524/ 13000 | global iter:   5524/ 13000 | loss: 1.3317 | ds_loss: 0.0000 | lr: 3.1003e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   4 | Iter:   5525/ 13000 | global iter:   5525/ 13000 | loss: 1.7236 | ds_loss: 0.0000 | lr: 3.0998e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   4 | Iter:   5526/ 13000 | global iter:   5526/ 13000 | loss: 1.4517 | ds_loss: 0.0000 | lr: 3.0992e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   4 | Iter:   5527/ 13000 | global iter:   5527/ 13000 | loss: 1.0593 | ds_loss: 0.0000 | lr: 3.0986e-05 | scale: 16384.0000 | micro time: 1.884 | step time: 0.000
train | epoch   4 | Iter:   5528/ 13000 | global iter:   5528/ 13000 | loss: 1.5202 | ds_loss: 0.0000 | lr: 3.0980e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   4 | Iter:   5529/ 13000 | global iter:   5529/ 13000 | loss: 1.2421 | ds_loss: 0.0000 | lr: 3.0974e-05 | scale: 16384.0000 | micro time: 1.849 | step time: 0.000
train | epoch   4 | Iter:   5530/ 13000 | global iter:   5530/ 13000 | loss: 1.1020 | ds_loss: 0.0000 | lr: 3.0968e-05 | scale: 16384.0000 | micro time: 1.756 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5530/ 13000 | global iter:   5530/ 13000 | loss: 1.3216 | ds_loss: 0.0000 | lr: 3.0968e-05 | scale: 16384.0000 | micro time: 1.756 | step time: 1.816
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5531/ 13000 | global iter:   5531/ 13000 | loss: 1.5818 | ds_loss: 0.0000 | lr: 3.0962e-05 | scale: 16384.0000 | micro time: 1.821 | step time: 0.000
train | epoch   4 | Iter:   5532/ 13000 | global iter:   5532/ 13000 | loss: 1.1813 | ds_loss: 0.0000 | lr: 3.0957e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 0.000
train | epoch   4 | Iter:   5533/ 13000 | global iter:   5533/ 13000 | loss: 1.1316 | ds_loss: 0.0000 | lr: 3.0951e-05 | scale: 16384.0000 | micro time: 1.703 | step time: 0.000
train | epoch   4 | Iter:   5534/ 13000 | global iter:   5534/ 13000 | loss: 1.0095 | ds_loss: 0.0000 | lr: 3.0945e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   4 | Iter:   5535/ 13000 | global iter:   5535/ 13000 | loss: 1.1181 | ds_loss: 0.0000 | lr: 3.0939e-05 | scale: 16384.0000 | micro time: 1.879 | step time: 0.000
train | epoch   4 | Iter:   5536/ 13000 | global iter:   5536/ 13000 | loss: 1.5195 | ds_loss: 0.0000 | lr: 3.0933e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   4 | Iter:   5537/ 13000 | global iter:   5537/ 13000 | loss: 1.2157 | ds_loss: 0.0000 | lr: 3.0927e-05 | scale: 16384.0000 | micro time: 1.796 | step time: 0.000
train | epoch   4 | Iter:   5538/ 13000 | global iter:   5538/ 13000 | loss: 1.7461 | ds_loss: 0.0000 | lr: 3.0921e-05 | scale: 16384.0000 | micro time: 1.814 | step time: 0.000
train | epoch   4 | Iter:   5539/ 13000 | global iter:   5539/ 13000 | loss: 1.2974 | ds_loss: 0.0000 | lr: 3.0915e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   4 | Iter:   5540/ 13000 | global iter:   5540/ 13000 | loss: 1.2368 | ds_loss: 0.0000 | lr: 3.0910e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5540/ 13000 | global iter:   5540/ 13000 | loss: 1.3038 | ds_loss: 0.0000 | lr: 3.0910e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 1.828
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5541/ 13000 | global iter:   5541/ 13000 | loss: 1.4938 | ds_loss: 0.0000 | lr: 3.0904e-05 | scale: 16384.0000 | micro time: 1.738 | step time: 0.000
train | epoch   4 | Iter:   5542/ 13000 | global iter:   5542/ 13000 | loss: 1.1240 | ds_loss: 0.0000 | lr: 3.0898e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   4 | Iter:   5543/ 13000 | global iter:   5543/ 13000 | loss: 0.9032 | ds_loss: 0.0000 | lr: 3.0892e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   4 | Iter:   5544/ 13000 | global iter:   5544/ 13000 | loss: 0.5968 | ds_loss: 0.0000 | lr: 3.0886e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   4 | Iter:   5545/ 13000 | global iter:   5545/ 13000 | loss: 1.2912 | ds_loss: 0.0000 | lr: 3.0880e-05 | scale: 16384.0000 | micro time: 1.756 | step time: 0.000
train | epoch   4 | Iter:   5546/ 13000 | global iter:   5546/ 13000 | loss: 1.3144 | ds_loss: 0.0000 | lr: 3.0874e-05 | scale: 16384.0000 | micro time: 1.834 | step time: 0.000
train | epoch   4 | Iter:   5547/ 13000 | global iter:   5547/ 13000 | loss: 1.8440 | ds_loss: 0.0000 | lr: 3.0869e-05 | scale: 16384.0000 | micro time: 1.754 | step time: 0.000
train | epoch   4 | Iter:   5548/ 13000 | global iter:   5548/ 13000 | loss: 1.3494 | ds_loss: 0.0000 | lr: 3.0863e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   4 | Iter:   5549/ 13000 | global iter:   5549/ 13000 | loss: 0.8018 | ds_loss: 0.0000 | lr: 3.0857e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   4 | Iter:   5550/ 13000 | global iter:   5550/ 13000 | loss: 1.4012 | ds_loss: 0.0000 | lr: 3.0851e-05 | scale: 16384.0000 | micro time: 1.759 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5550/ 13000 | global iter:   5550/ 13000 | loss: 1.2120 | ds_loss: 0.0000 | lr: 3.0851e-05 | scale: 16384.0000 | micro time: 1.759 | step time: 1.790
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5551/ 13000 | global iter:   5551/ 13000 | loss: 1.3097 | ds_loss: 0.0000 | lr: 3.0845e-05 | scale: 16384.0000 | micro time: 1.813 | step time: 0.000
train | epoch   4 | Iter:   5552/ 13000 | global iter:   5552/ 13000 | loss: 1.7721 | ds_loss: 0.0000 | lr: 3.0839e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   4 | Iter:   5553/ 13000 | global iter:   5553/ 13000 | loss: 1.5411 | ds_loss: 0.0000 | lr: 3.0833e-05 | scale: 16384.0000 | micro time: 1.832 | step time: 0.000
train | epoch   4 | Iter:   5554/ 13000 | global iter:   5554/ 13000 | loss: 1.2064 | ds_loss: 0.0000 | lr: 3.0828e-05 | scale: 16384.0000 | micro time: 1.723 | step time: 0.000
train | epoch   4 | Iter:   5555/ 13000 | global iter:   5555/ 13000 | loss: 1.9032 | ds_loss: 0.0000 | lr: 3.0822e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   4 | Iter:   5556/ 13000 | global iter:   5556/ 13000 | loss: 0.7841 | ds_loss: 0.0000 | lr: 3.0816e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   4 | Iter:   5557/ 13000 | global iter:   5557/ 13000 | loss: 0.9671 | ds_loss: 0.0000 | lr: 3.0810e-05 | scale: 16384.0000 | micro time: 1.903 | step time: 0.000
train | epoch   4 | Iter:   5558/ 13000 | global iter:   5558/ 13000 | loss: 1.4607 | ds_loss: 0.0000 | lr: 3.0804e-05 | scale: 16384.0000 | micro time: 1.768 | step time: 0.000
train | epoch   4 | Iter:   5559/ 13000 | global iter:   5559/ 13000 | loss: 1.2465 | ds_loss: 0.0000 | lr: 3.0798e-05 | scale: 16384.0000 | micro time: 1.890 | step time: 0.000
train | epoch   4 | Iter:   5560/ 13000 | global iter:   5560/ 13000 | loss: 1.7145 | ds_loss: 0.0000 | lr: 3.0792e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5560/ 13000 | global iter:   5560/ 13000 | loss: 1.3905 | ds_loss: 0.0000 | lr: 3.0792e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 1.828
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5561/ 13000 | global iter:   5561/ 13000 | loss: 2.1273 | ds_loss: 0.0000 | lr: 3.0786e-05 | scale: 16384.0000 | micro time: 1.704 | step time: 0.000
train | epoch   4 | Iter:   5562/ 13000 | global iter:   5562/ 13000 | loss: 1.4186 | ds_loss: 0.0000 | lr: 3.0781e-05 | scale: 16384.0000 | micro time: 1.882 | step time: 0.000
train | epoch   4 | Iter:   5563/ 13000 | global iter:   5563/ 13000 | loss: 1.3054 | ds_loss: 0.0000 | lr: 3.0775e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   4 | Iter:   5564/ 13000 | global iter:   5564/ 13000 | loss: 1.0820 | ds_loss: 0.0000 | lr: 3.0769e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   4 | Iter:   5565/ 13000 | global iter:   5565/ 13000 | loss: 1.4350 | ds_loss: 0.0000 | lr: 3.0763e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 0.000
train | epoch   4 | Iter:   5566/ 13000 | global iter:   5566/ 13000 | loss: 1.2406 | ds_loss: 0.0000 | lr: 3.0757e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   4 | Iter:   5567/ 13000 | global iter:   5567/ 13000 | loss: 0.9740 | ds_loss: 0.0000 | lr: 3.0751e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   4 | Iter:   5568/ 13000 | global iter:   5568/ 13000 | loss: 1.1983 | ds_loss: 0.0000 | lr: 3.0745e-05 | scale: 16384.0000 | micro time: 1.862 | step time: 0.000
train | epoch   4 | Iter:   5569/ 13000 | global iter:   5569/ 13000 | loss: 1.1086 | ds_loss: 0.0000 | lr: 3.0740e-05 | scale: 16384.0000 | micro time: 1.876 | step time: 0.000
train | epoch   4 | Iter:   5570/ 13000 | global iter:   5570/ 13000 | loss: 1.9399 | ds_loss: 0.0000 | lr: 3.0734e-05 | scale: 16384.0000 | micro time: 1.838 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5570/ 13000 | global iter:   5570/ 13000 | loss: 1.3830 | ds_loss: 0.0000 | lr: 3.0734e-05 | scale: 16384.0000 | micro time: 1.838 | step time: 1.813
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5571/ 13000 | global iter:   5571/ 13000 | loss: 1.1999 | ds_loss: 0.0000 | lr: 3.0728e-05 | scale: 16384.0000 | micro time: 1.912 | step time: 0.000
train | epoch   4 | Iter:   5572/ 13000 | global iter:   5572/ 13000 | loss: 1.1439 | ds_loss: 0.0000 | lr: 3.0722e-05 | scale: 16384.0000 | micro time: 1.796 | step time: 0.000
train | epoch   4 | Iter:   5573/ 13000 | global iter:   5573/ 13000 | loss: 1.7280 | ds_loss: 0.0000 | lr: 3.0716e-05 | scale: 16384.0000 | micro time: 1.848 | step time: 0.000
train | epoch   4 | Iter:   5574/ 13000 | global iter:   5574/ 13000 | loss: 1.2779 | ds_loss: 0.0000 | lr: 3.0710e-05 | scale: 16384.0000 | micro time: 1.745 | step time: 0.000
train | epoch   4 | Iter:   5575/ 13000 | global iter:   5575/ 13000 | loss: 1.4972 | ds_loss: 0.0000 | lr: 3.0704e-05 | scale: 16384.0000 | micro time: 1.844 | step time: 0.000
train | epoch   4 | Iter:   5576/ 13000 | global iter:   5576/ 13000 | loss: 1.3903 | ds_loss: 0.0000 | lr: 3.0698e-05 | scale: 16384.0000 | micro time: 1.885 | step time: 0.000
train | epoch   4 | Iter:   5577/ 13000 | global iter:   5577/ 13000 | loss: 0.9442 | ds_loss: 0.0000 | lr: 3.0693e-05 | scale: 16384.0000 | micro time: 1.804 | step time: 0.000
train | epoch   4 | Iter:   5578/ 13000 | global iter:   5578/ 13000 | loss: 0.7552 | ds_loss: 0.0000 | lr: 3.0687e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   4 | Iter:   5579/ 13000 | global iter:   5579/ 13000 | loss: 1.2464 | ds_loss: 0.0000 | lr: 3.0681e-05 | scale: 16384.0000 | micro time: 1.817 | step time: 0.000
train | epoch   4 | Iter:   5580/ 13000 | global iter:   5580/ 13000 | loss: 0.9657 | ds_loss: 0.0000 | lr: 3.0675e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5580/ 13000 | global iter:   5580/ 13000 | loss: 1.2149 | ds_loss: 0.0000 | lr: 3.0675e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 1.825
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5581/ 13000 | global iter:   5581/ 13000 | loss: 0.8611 | ds_loss: 0.0000 | lr: 3.0669e-05 | scale: 16384.0000 | micro time: 1.786 | step time: 0.000
train | epoch   4 | Iter:   5582/ 13000 | global iter:   5582/ 13000 | loss: 1.7085 | ds_loss: 0.0000 | lr: 3.0663e-05 | scale: 16384.0000 | micro time: 1.821 | step time: 0.000
train | epoch   4 | Iter:   5583/ 13000 | global iter:   5583/ 13000 | loss: 0.9863 | ds_loss: 0.0000 | lr: 3.0657e-05 | scale: 16384.0000 | micro time: 1.785 | step time: 0.000
train | epoch   4 | Iter:   5584/ 13000 | global iter:   5584/ 13000 | loss: 1.3198 | ds_loss: 0.0000 | lr: 3.0651e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   4 | Iter:   5585/ 13000 | global iter:   5585/ 13000 | loss: 1.0953 | ds_loss: 0.0000 | lr: 3.0646e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   4 | Iter:   5586/ 13000 | global iter:   5586/ 13000 | loss: 1.2194 | ds_loss: 0.0000 | lr: 3.0640e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   4 | Iter:   5587/ 13000 | global iter:   5587/ 13000 | loss: 1.5081 | ds_loss: 0.0000 | lr: 3.0634e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   4 | Iter:   5588/ 13000 | global iter:   5588/ 13000 | loss: 1.7291 | ds_loss: 0.0000 | lr: 3.0628e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   4 | Iter:   5589/ 13000 | global iter:   5589/ 13000 | loss: 1.1707 | ds_loss: 0.0000 | lr: 3.0622e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   4 | Iter:   5590/ 13000 | global iter:   5590/ 13000 | loss: 1.2282 | ds_loss: 0.0000 | lr: 3.0616e-05 | scale: 16384.0000 | micro time: 1.824 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5590/ 13000 | global iter:   5590/ 13000 | loss: 1.2826 | ds_loss: 0.0000 | lr: 3.0616e-05 | scale: 16384.0000 | micro time: 1.824 | step time: 1.805
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5591/ 13000 | global iter:   5591/ 13000 | loss: 1.2479 | ds_loss: 0.0000 | lr: 3.0610e-05 | scale: 16384.0000 | micro time: 1.824 | step time: 0.000
train | epoch   4 | Iter:   5592/ 13000 | global iter:   5592/ 13000 | loss: 1.2263 | ds_loss: 0.0000 | lr: 3.0604e-05 | scale: 16384.0000 | micro time: 1.769 | step time: 0.000
train | epoch   4 | Iter:   5593/ 13000 | global iter:   5593/ 13000 | loss: 1.5862 | ds_loss: 0.0000 | lr: 3.0599e-05 | scale: 16384.0000 | micro time: 1.908 | step time: 0.000
train | epoch   4 | Iter:   5594/ 13000 | global iter:   5594/ 13000 | loss: 1.4299 | ds_loss: 0.0000 | lr: 3.0593e-05 | scale: 16384.0000 | micro time: 1.814 | step time: 0.000
train | epoch   4 | Iter:   5595/ 13000 | global iter:   5595/ 13000 | loss: 1.2590 | ds_loss: 0.0000 | lr: 3.0587e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   4 | Iter:   5596/ 13000 | global iter:   5596/ 13000 | loss: 1.4959 | ds_loss: 0.0000 | lr: 3.0581e-05 | scale: 16384.0000 | micro time: 1.768 | step time: 0.000
train | epoch   4 | Iter:   5597/ 13000 | global iter:   5597/ 13000 | loss: 1.6807 | ds_loss: 0.0000 | lr: 3.0575e-05 | scale: 16384.0000 | micro time: 1.849 | step time: 0.000
train | epoch   4 | Iter:   5598/ 13000 | global iter:   5598/ 13000 | loss: 1.6951 | ds_loss: 0.0000 | lr: 3.0569e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   4 | Iter:   5599/ 13000 | global iter:   5599/ 13000 | loss: 1.2592 | ds_loss: 0.0000 | lr: 3.0563e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   4 | Iter:   5600/ 13000 | global iter:   5600/ 13000 | loss: 1.3826 | ds_loss: 0.0000 | lr: 3.0557e-05 | scale: 16384.0000 | micro time: 1.870 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5600/ 13000 | global iter:   5600/ 13000 | loss: 1.4263 | ds_loss: 0.0000 | lr: 3.0557e-05 | scale: 16384.0000 | micro time: 1.870 | step time: 1.834
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5601/ 13000 | global iter:   5601/ 13000 | loss: 0.8572 | ds_loss: 0.0000 | lr: 3.0552e-05 | scale: 16384.0000 | micro time: 1.778 | step time: 0.000
train | epoch   4 | Iter:   5602/ 13000 | global iter:   5602/ 13000 | loss: 1.3469 | ds_loss: 0.0000 | lr: 3.0546e-05 | scale: 16384.0000 | micro time: 1.809 | step time: 0.000
train | epoch   4 | Iter:   5603/ 13000 | global iter:   5603/ 13000 | loss: 1.4741 | ds_loss: 0.0000 | lr: 3.0540e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   4 | Iter:   5604/ 13000 | global iter:   5604/ 13000 | loss: 1.2985 | ds_loss: 0.0000 | lr: 3.0534e-05 | scale: 16384.0000 | micro time: 1.864 | step time: 0.000
train | epoch   4 | Iter:   5605/ 13000 | global iter:   5605/ 13000 | loss: 1.0532 | ds_loss: 0.0000 | lr: 3.0528e-05 | scale: 16384.0000 | micro time: 1.778 | step time: 0.000
train | epoch   4 | Iter:   5606/ 13000 | global iter:   5606/ 13000 | loss: 1.4770 | ds_loss: 0.0000 | lr: 3.0522e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   4 | Iter:   5607/ 13000 | global iter:   5607/ 13000 | loss: 1.0755 | ds_loss: 0.0000 | lr: 3.0516e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   4 | Iter:   5608/ 13000 | global iter:   5608/ 13000 | loss: 1.6708 | ds_loss: 0.0000 | lr: 3.0510e-05 | scale: 16384.0000 | micro time: 1.935 | step time: 0.000
train | epoch   4 | Iter:   5609/ 13000 | global iter:   5609/ 13000 | loss: 1.7011 | ds_loss: 0.0000 | lr: 3.0504e-05 | scale: 16384.0000 | micro time: 1.731 | step time: 0.000
train | epoch   4 | Iter:   5610/ 13000 | global iter:   5610/ 13000 | loss: 1.1712 | ds_loss: 0.0000 | lr: 3.0499e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5610/ 13000 | global iter:   5610/ 13000 | loss: 1.3125 | ds_loss: 0.0000 | lr: 3.0499e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 1.821
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5611/ 13000 | global iter:   5611/ 13000 | loss: 1.4187 | ds_loss: 0.0000 | lr: 3.0493e-05 | scale: 16384.0000 | micro time: 1.734 | step time: 0.000
train | epoch   4 | Iter:   5612/ 13000 | global iter:   5612/ 13000 | loss: 1.9491 | ds_loss: 0.0000 | lr: 3.0487e-05 | scale: 16384.0000 | micro time: 1.798 | step time: 0.000
train | epoch   4 | Iter:   5613/ 13000 | global iter:   5613/ 13000 | loss: 1.2858 | ds_loss: 0.0000 | lr: 3.0481e-05 | scale: 16384.0000 | micro time: 1.740 | step time: 0.000
train | epoch   4 | Iter:   5614/ 13000 | global iter:   5614/ 13000 | loss: 1.3651 | ds_loss: 0.0000 | lr: 3.0475e-05 | scale: 16384.0000 | micro time: 1.849 | step time: 0.000
train | epoch   4 | Iter:   5615/ 13000 | global iter:   5615/ 13000 | loss: 1.4376 | ds_loss: 0.0000 | lr: 3.0469e-05 | scale: 16384.0000 | micro time: 1.777 | step time: 0.000
train | epoch   4 | Iter:   5616/ 13000 | global iter:   5616/ 13000 | loss: 1.1439 | ds_loss: 0.0000 | lr: 3.0463e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   4 | Iter:   5617/ 13000 | global iter:   5617/ 13000 | loss: 2.0182 | ds_loss: 0.0000 | lr: 3.0457e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   4 | Iter:   5618/ 13000 | global iter:   5618/ 13000 | loss: 1.3483 | ds_loss: 0.0000 | lr: 3.0451e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   4 | Iter:   5619/ 13000 | global iter:   5619/ 13000 | loss: 1.3935 | ds_loss: 0.0000 | lr: 3.0446e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   4 | Iter:   5620/ 13000 | global iter:   5620/ 13000 | loss: 1.1203 | ds_loss: 0.0000 | lr: 3.0440e-05 | scale: 16384.0000 | micro time: 1.761 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5620/ 13000 | global iter:   5620/ 13000 | loss: 1.4480 | ds_loss: 0.0000 | lr: 3.0440e-05 | scale: 16384.0000 | micro time: 1.761 | step time: 1.796
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5621/ 13000 | global iter:   5621/ 13000 | loss: 1.5800 | ds_loss: 0.0000 | lr: 3.0434e-05 | scale: 16384.0000 | micro time: 1.832 | step time: 0.000
train | epoch   4 | Iter:   5622/ 13000 | global iter:   5622/ 13000 | loss: 1.3015 | ds_loss: 0.0000 | lr: 3.0428e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   4 | Iter:   5623/ 13000 | global iter:   5623/ 13000 | loss: 0.9637 | ds_loss: 0.0000 | lr: 3.0422e-05 | scale: 16384.0000 | micro time: 1.726 | step time: 0.000
train | epoch   4 | Iter:   5624/ 13000 | global iter:   5624/ 13000 | loss: 1.1031 | ds_loss: 0.0000 | lr: 3.0416e-05 | scale: 16384.0000 | micro time: 1.740 | step time: 0.000
train | epoch   4 | Iter:   5625/ 13000 | global iter:   5625/ 13000 | loss: 0.8388 | ds_loss: 0.0000 | lr: 3.0410e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   4 | Iter:   5626/ 13000 | global iter:   5626/ 13000 | loss: 1.5373 | ds_loss: 0.0000 | lr: 3.0404e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   4 | Iter:   5627/ 13000 | global iter:   5627/ 13000 | loss: 1.6722 | ds_loss: 0.0000 | lr: 3.0398e-05 | scale: 16384.0000 | micro time: 1.899 | step time: 0.000
train | epoch   4 | Iter:   5628/ 13000 | global iter:   5628/ 13000 | loss: 1.4147 | ds_loss: 0.0000 | lr: 3.0393e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   4 | Iter:   5629/ 13000 | global iter:   5629/ 13000 | loss: 1.4999 | ds_loss: 0.0000 | lr: 3.0387e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   4 | Iter:   5630/ 13000 | global iter:   5630/ 13000 | loss: 1.4665 | ds_loss: 0.0000 | lr: 3.0381e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5630/ 13000 | global iter:   5630/ 13000 | loss: 1.3378 | ds_loss: 0.0000 | lr: 3.0381e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 1.797
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5631/ 13000 | global iter:   5631/ 13000 | loss: 0.9346 | ds_loss: 0.0000 | lr: 3.0375e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 0.000
train | epoch   4 | Iter:   5632/ 13000 | global iter:   5632/ 13000 | loss: 1.5295 | ds_loss: 0.0000 | lr: 3.0369e-05 | scale: 16384.0000 | micro time: 1.816 | step time: 0.000
train | epoch   4 | Iter:   5633/ 13000 | global iter:   5633/ 13000 | loss: 1.6310 | ds_loss: 0.0000 | lr: 3.0363e-05 | scale: 16384.0000 | micro time: 1.830 | step time: 0.000
train | epoch   4 | Iter:   5634/ 13000 | global iter:   5634/ 13000 | loss: 1.5480 | ds_loss: 0.0000 | lr: 3.0357e-05 | scale: 16384.0000 | micro time: 1.891 | step time: 0.000
train | epoch   4 | Iter:   5635/ 13000 | global iter:   5635/ 13000 | loss: 1.6035 | ds_loss: 0.0000 | lr: 3.0351e-05 | scale: 16384.0000 | micro time: 1.778 | step time: 0.000
train | epoch   4 | Iter:   5636/ 13000 | global iter:   5636/ 13000 | loss: 1.4934 | ds_loss: 0.0000 | lr: 3.0345e-05 | scale: 16384.0000 | micro time: 1.768 | step time: 0.000
train | epoch   4 | Iter:   5637/ 13000 | global iter:   5637/ 13000 | loss: 1.2146 | ds_loss: 0.0000 | lr: 3.0340e-05 | scale: 16384.0000 | micro time: 1.923 | step time: 0.000
train | epoch   4 | Iter:   5638/ 13000 | global iter:   5638/ 13000 | loss: 1.4575 | ds_loss: 0.0000 | lr: 3.0334e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   4 | Iter:   5639/ 13000 | global iter:   5639/ 13000 | loss: 1.3545 | ds_loss: 0.0000 | lr: 3.0328e-05 | scale: 16384.0000 | micro time: 1.727 | step time: 0.000
train | epoch   4 | Iter:   5640/ 13000 | global iter:   5640/ 13000 | loss: 1.2857 | ds_loss: 0.0000 | lr: 3.0322e-05 | scale: 16384.0000 | micro time: 1.885 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5640/ 13000 | global iter:   5640/ 13000 | loss: 1.4052 | ds_loss: 0.0000 | lr: 3.0322e-05 | scale: 16384.0000 | micro time: 1.885 | step time: 1.829
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5641/ 13000 | global iter:   5641/ 13000 | loss: 1.5911 | ds_loss: 0.0000 | lr: 3.0316e-05 | scale: 16384.0000 | micro time: 1.768 | step time: 0.000
train | epoch   4 | Iter:   5642/ 13000 | global iter:   5642/ 13000 | loss: 1.0933 | ds_loss: 0.0000 | lr: 3.0310e-05 | scale: 16384.0000 | micro time: 1.822 | step time: 0.000
train | epoch   4 | Iter:   5643/ 13000 | global iter:   5643/ 13000 | loss: 1.2563 | ds_loss: 0.0000 | lr: 3.0304e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   4 | Iter:   5644/ 13000 | global iter:   5644/ 13000 | loss: 1.5602 | ds_loss: 0.0000 | lr: 3.0298e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   4 | Iter:   5645/ 13000 | global iter:   5645/ 13000 | loss: 1.5293 | ds_loss: 0.0000 | lr: 3.0292e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   4 | Iter:   5646/ 13000 | global iter:   5646/ 13000 | loss: 1.2140 | ds_loss: 0.0000 | lr: 3.0287e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   4 | Iter:   5647/ 13000 | global iter:   5647/ 13000 | loss: 1.7262 | ds_loss: 0.0000 | lr: 3.0281e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 0.000
train | epoch   4 | Iter:   5648/ 13000 | global iter:   5648/ 13000 | loss: 1.1042 | ds_loss: 0.0000 | lr: 3.0275e-05 | scale: 16384.0000 | micro time: 1.737 | step time: 0.000
train | epoch   4 | Iter:   5649/ 13000 | global iter:   5649/ 13000 | loss: 1.0343 | ds_loss: 0.0000 | lr: 3.0269e-05 | scale: 16384.0000 | micro time: 1.879 | step time: 0.000
train | epoch   4 | Iter:   5650/ 13000 | global iter:   5650/ 13000 | loss: 0.9798 | ds_loss: 0.0000 | lr: 3.0263e-05 | scale: 16384.0000 | micro time: 1.665 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5650/ 13000 | global iter:   5650/ 13000 | loss: 1.3089 | ds_loss: 0.0000 | lr: 3.0263e-05 | scale: 16384.0000 | micro time: 1.665 | step time: 1.809
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5651/ 13000 | global iter:   5651/ 13000 | loss: 1.5686 | ds_loss: 0.0000 | lr: 3.0257e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   4 | Iter:   5652/ 13000 | global iter:   5652/ 13000 | loss: 0.9000 | ds_loss: 0.0000 | lr: 3.0251e-05 | scale: 16384.0000 | micro time: 1.784 | step time: 0.000
train | epoch   4 | Iter:   5653/ 13000 | global iter:   5653/ 13000 | loss: 1.5550 | ds_loss: 0.0000 | lr: 3.0245e-05 | scale: 16384.0000 | micro time: 1.711 | step time: 0.000
train | epoch   4 | Iter:   5654/ 13000 | global iter:   5654/ 13000 | loss: 1.3665 | ds_loss: 0.0000 | lr: 3.0239e-05 | scale: 16384.0000 | micro time: 1.975 | step time: 0.000
train | epoch   4 | Iter:   5655/ 13000 | global iter:   5655/ 13000 | loss: 1.1486 | ds_loss: 0.0000 | lr: 3.0233e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   4 | Iter:   5656/ 13000 | global iter:   5656/ 13000 | loss: 1.6083 | ds_loss: 0.0000 | lr: 3.0228e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   4 | Iter:   5657/ 13000 | global iter:   5657/ 13000 | loss: 0.8130 | ds_loss: 0.0000 | lr: 3.0222e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   4 | Iter:   5658/ 13000 | global iter:   5658/ 13000 | loss: 1.0968 | ds_loss: 0.0000 | lr: 3.0216e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   4 | Iter:   5659/ 13000 | global iter:   5659/ 13000 | loss: 1.2630 | ds_loss: 0.0000 | lr: 3.0210e-05 | scale: 16384.0000 | micro time: 1.756 | step time: 0.000
train | epoch   4 | Iter:   5660/ 13000 | global iter:   5660/ 13000 | loss: 1.5662 | ds_loss: 0.0000 | lr: 3.0204e-05 | scale: 16384.0000 | micro time: 1.790 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5660/ 13000 | global iter:   5660/ 13000 | loss: 1.2886 | ds_loss: 0.0000 | lr: 3.0204e-05 | scale: 16384.0000 | micro time: 1.790 | step time: 1.815
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5661/ 13000 | global iter:   5661/ 13000 | loss: 2.2261 | ds_loss: 0.0000 | lr: 3.0198e-05 | scale: 16384.0000 | micro time: 1.773 | step time: 0.000
train | epoch   4 | Iter:   5662/ 13000 | global iter:   5662/ 13000 | loss: 1.5624 | ds_loss: 0.0000 | lr: 3.0192e-05 | scale: 16384.0000 | micro time: 1.793 | step time: 0.000
train | epoch   4 | Iter:   5663/ 13000 | global iter:   5663/ 13000 | loss: 1.7889 | ds_loss: 0.0000 | lr: 3.0186e-05 | scale: 16384.0000 | micro time: 1.764 | step time: 0.000
train | epoch   4 | Iter:   5664/ 13000 | global iter:   5664/ 13000 | loss: 1.5174 | ds_loss: 0.0000 | lr: 3.0180e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   4 | Iter:   5665/ 13000 | global iter:   5665/ 13000 | loss: 1.0789 | ds_loss: 0.0000 | lr: 3.0174e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 0.000
train | epoch   4 | Iter:   5666/ 13000 | global iter:   5666/ 13000 | loss: 1.9031 | ds_loss: 0.0000 | lr: 3.0169e-05 | scale: 16384.0000 | micro time: 1.810 | step time: 0.000
train | epoch   4 | Iter:   5667/ 13000 | global iter:   5667/ 13000 | loss: 1.7483 | ds_loss: 0.0000 | lr: 3.0163e-05 | scale: 16384.0000 | micro time: 1.732 | step time: 0.000
train | epoch   4 | Iter:   5668/ 13000 | global iter:   5668/ 13000 | loss: 0.9162 | ds_loss: 0.0000 | lr: 3.0157e-05 | scale: 16384.0000 | micro time: 1.794 | step time: 0.000
train | epoch   4 | Iter:   5669/ 13000 | global iter:   5669/ 13000 | loss: 1.1133 | ds_loss: 0.0000 | lr: 3.0151e-05 | scale: 16384.0000 | micro time: 1.796 | step time: 0.000
train | epoch   4 | Iter:   5670/ 13000 | global iter:   5670/ 13000 | loss: 1.5821 | ds_loss: 0.0000 | lr: 3.0145e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5670/ 13000 | global iter:   5670/ 13000 | loss: 1.5437 | ds_loss: 0.0000 | lr: 3.0145e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 1.788
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5671/ 13000 | global iter:   5671/ 13000 | loss: 2.0151 | ds_loss: 0.0000 | lr: 3.0139e-05 | scale: 16384.0000 | micro time: 1.822 | step time: 0.000
train | epoch   4 | Iter:   5672/ 13000 | global iter:   5672/ 13000 | loss: 1.3598 | ds_loss: 0.0000 | lr: 3.0133e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   4 | Iter:   5673/ 13000 | global iter:   5673/ 13000 | loss: 1.0016 | ds_loss: 0.0000 | lr: 3.0127e-05 | scale: 16384.0000 | micro time: 1.798 | step time: 0.000
train | epoch   4 | Iter:   5674/ 13000 | global iter:   5674/ 13000 | loss: 1.0521 | ds_loss: 0.0000 | lr: 3.0121e-05 | scale: 16384.0000 | micro time: 1.817 | step time: 0.000
train | epoch   4 | Iter:   5675/ 13000 | global iter:   5675/ 13000 | loss: 1.4312 | ds_loss: 0.0000 | lr: 3.0115e-05 | scale: 16384.0000 | micro time: 1.898 | step time: 0.000
train | epoch   4 | Iter:   5676/ 13000 | global iter:   5676/ 13000 | loss: 1.3322 | ds_loss: 0.0000 | lr: 3.0110e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   4 | Iter:   5677/ 13000 | global iter:   5677/ 13000 | loss: 1.2787 | ds_loss: 0.0000 | lr: 3.0104e-05 | scale: 16384.0000 | micro time: 1.879 | step time: 0.000
train | epoch   4 | Iter:   5678/ 13000 | global iter:   5678/ 13000 | loss: 1.7600 | ds_loss: 0.0000 | lr: 3.0098e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   4 | Iter:   5679/ 13000 | global iter:   5679/ 13000 | loss: 1.4721 | ds_loss: 0.0000 | lr: 3.0092e-05 | scale: 16384.0000 | micro time: 1.738 | step time: 0.000
train | epoch   4 | Iter:   5680/ 13000 | global iter:   5680/ 13000 | loss: 0.8562 | ds_loss: 0.0000 | lr: 3.0086e-05 | scale: 16384.0000 | micro time: 1.856 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5680/ 13000 | global iter:   5680/ 13000 | loss: 1.3559 | ds_loss: 0.0000 | lr: 3.0086e-05 | scale: 16384.0000 | micro time: 1.856 | step time: 1.819
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5681/ 13000 | global iter:   5681/ 13000 | loss: 0.8999 | ds_loss: 0.0000 | lr: 3.0080e-05 | scale: 16384.0000 | micro time: 1.762 | step time: 0.000
train | epoch   4 | Iter:   5682/ 13000 | global iter:   5682/ 13000 | loss: 0.9940 | ds_loss: 0.0000 | lr: 3.0074e-05 | scale: 16384.0000 | micro time: 1.774 | step time: 0.000
train | epoch   4 | Iter:   5683/ 13000 | global iter:   5683/ 13000 | loss: 1.5578 | ds_loss: 0.0000 | lr: 3.0068e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   4 | Iter:   5684/ 13000 | global iter:   5684/ 13000 | loss: 1.4914 | ds_loss: 0.0000 | lr: 3.0062e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   4 | Iter:   5685/ 13000 | global iter:   5685/ 13000 | loss: 1.4628 | ds_loss: 0.0000 | lr: 3.0056e-05 | scale: 16384.0000 | micro time: 1.834 | step time: 0.000
train | epoch   4 | Iter:   5686/ 13000 | global iter:   5686/ 13000 | loss: 1.4190 | ds_loss: 0.0000 | lr: 3.0050e-05 | scale: 16384.0000 | micro time: 1.830 | step time: 0.000
train | epoch   4 | Iter:   5687/ 13000 | global iter:   5687/ 13000 | loss: 1.1996 | ds_loss: 0.0000 | lr: 3.0045e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   4 | Iter:   5688/ 13000 | global iter:   5688/ 13000 | loss: 1.1969 | ds_loss: 0.0000 | lr: 3.0039e-05 | scale: 16384.0000 | micro time: 1.901 | step time: 0.000
train | epoch   4 | Iter:   5689/ 13000 | global iter:   5689/ 13000 | loss: 1.5753 | ds_loss: 0.0000 | lr: 3.0033e-05 | scale: 16384.0000 | micro time: 1.736 | step time: 0.000
train | epoch   4 | Iter:   5690/ 13000 | global iter:   5690/ 13000 | loss: 1.0851 | ds_loss: 0.0000 | lr: 3.0027e-05 | scale: 16384.0000 | micro time: 1.820 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5690/ 13000 | global iter:   5690/ 13000 | loss: 1.2882 | ds_loss: 0.0000 | lr: 3.0027e-05 | scale: 16384.0000 | micro time: 1.820 | step time: 1.813
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5691/ 13000 | global iter:   5691/ 13000 | loss: 1.3602 | ds_loss: 0.0000 | lr: 3.0021e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   4 | Iter:   5692/ 13000 | global iter:   5692/ 13000 | loss: 0.9999 | ds_loss: 0.0000 | lr: 3.0015e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   4 | Iter:   5693/ 13000 | global iter:   5693/ 13000 | loss: 1.7952 | ds_loss: 0.0000 | lr: 3.0009e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   4 | Iter:   5694/ 13000 | global iter:   5694/ 13000 | loss: 1.1837 | ds_loss: 0.0000 | lr: 3.0003e-05 | scale: 16384.0000 | micro time: 1.891 | step time: 0.000
train | epoch   4 | Iter:   5695/ 13000 | global iter:   5695/ 13000 | loss: 1.6927 | ds_loss: 0.0000 | lr: 2.9997e-05 | scale: 16384.0000 | micro time: 1.797 | step time: 0.000
train | epoch   4 | Iter:   5696/ 13000 | global iter:   5696/ 13000 | loss: 1.2244 | ds_loss: 0.0000 | lr: 2.9991e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 0.000
train | epoch   4 | Iter:   5697/ 13000 | global iter:   5697/ 13000 | loss: 1.6844 | ds_loss: 0.0000 | lr: 2.9985e-05 | scale: 16384.0000 | micro time: 1.895 | step time: 0.000
train | epoch   4 | Iter:   5698/ 13000 | global iter:   5698/ 13000 | loss: 0.8753 | ds_loss: 0.0000 | lr: 2.9980e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   4 | Iter:   5699/ 13000 | global iter:   5699/ 13000 | loss: 1.4707 | ds_loss: 0.0000 | lr: 2.9974e-05 | scale: 16384.0000 | micro time: 1.746 | step time: 0.000
train | epoch   4 | Iter:   5700/ 13000 | global iter:   5700/ 13000 | loss: 1.5664 | ds_loss: 0.0000 | lr: 2.9968e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5700/ 13000 | global iter:   5700/ 13000 | loss: 1.3853 | ds_loss: 0.0000 | lr: 2.9968e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 1.817
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5701/ 13000 | global iter:   5701/ 13000 | loss: 1.1992 | ds_loss: 0.0000 | lr: 2.9962e-05 | scale: 16384.0000 | micro time: 1.730 | step time: 0.000
train | epoch   4 | Iter:   5702/ 13000 | global iter:   5702/ 13000 | loss: 0.7232 | ds_loss: 0.0000 | lr: 2.9956e-05 | scale: 16384.0000 | micro time: 1.845 | step time: 0.000
train | epoch   4 | Iter:   5703/ 13000 | global iter:   5703/ 13000 | loss: 1.0545 | ds_loss: 0.0000 | lr: 2.9950e-05 | scale: 16384.0000 | micro time: 1.801 | step time: 0.000
train | epoch   4 | Iter:   5704/ 13000 | global iter:   5704/ 13000 | loss: 1.4646 | ds_loss: 0.0000 | lr: 2.9944e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   4 | Iter:   5705/ 13000 | global iter:   5705/ 13000 | loss: 1.0273 | ds_loss: 0.0000 | lr: 2.9938e-05 | scale: 16384.0000 | micro time: 1.942 | step time: 0.000
train | epoch   4 | Iter:   5706/ 13000 | global iter:   5706/ 13000 | loss: 1.3718 | ds_loss: 0.0000 | lr: 2.9932e-05 | scale: 16384.0000 | micro time: 1.769 | step time: 0.000
train | epoch   4 | Iter:   5707/ 13000 | global iter:   5707/ 13000 | loss: 1.9595 | ds_loss: 0.0000 | lr: 2.9926e-05 | scale: 16384.0000 | micro time: 1.876 | step time: 0.000
train | epoch   4 | Iter:   5708/ 13000 | global iter:   5708/ 13000 | loss: 1.3029 | ds_loss: 0.0000 | lr: 2.9920e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   4 | Iter:   5709/ 13000 | global iter:   5709/ 13000 | loss: 1.7178 | ds_loss: 0.0000 | lr: 2.9915e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   4 | Iter:   5710/ 13000 | global iter:   5710/ 13000 | loss: 1.2638 | ds_loss: 0.0000 | lr: 2.9909e-05 | scale: 16384.0000 | micro time: 1.850 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5710/ 13000 | global iter:   5710/ 13000 | loss: 1.3085 | ds_loss: 0.0000 | lr: 2.9909e-05 | scale: 16384.0000 | micro time: 1.850 | step time: 1.832
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5711/ 13000 | global iter:   5711/ 13000 | loss: 1.5767 | ds_loss: 0.0000 | lr: 2.9903e-05 | scale: 16384.0000 | micro time: 1.817 | step time: 0.000
train | epoch   4 | Iter:   5712/ 13000 | global iter:   5712/ 13000 | loss: 1.4581 | ds_loss: 0.0000 | lr: 2.9897e-05 | scale: 16384.0000 | micro time: 1.731 | step time: 0.000
train | epoch   4 | Iter:   5713/ 13000 | global iter:   5713/ 13000 | loss: 1.6550 | ds_loss: 0.0000 | lr: 2.9891e-05 | scale: 16384.0000 | micro time: 1.679 | step time: 0.000
train | epoch   4 | Iter:   5714/ 13000 | global iter:   5714/ 13000 | loss: 0.7209 | ds_loss: 0.0000 | lr: 2.9885e-05 | scale: 16384.0000 | micro time: 1.737 | step time: 0.000
train | epoch   4 | Iter:   5715/ 13000 | global iter:   5715/ 13000 | loss: 1.5256 | ds_loss: 0.0000 | lr: 2.9879e-05 | scale: 16384.0000 | micro time: 1.833 | step time: 0.000
train | epoch   4 | Iter:   5716/ 13000 | global iter:   5716/ 13000 | loss: 1.5269 | ds_loss: 0.0000 | lr: 2.9873e-05 | scale: 16384.0000 | micro time: 1.759 | step time: 0.000
train | epoch   4 | Iter:   5717/ 13000 | global iter:   5717/ 13000 | loss: 0.9023 | ds_loss: 0.0000 | lr: 2.9867e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   4 | Iter:   5718/ 13000 | global iter:   5718/ 13000 | loss: 1.6707 | ds_loss: 0.0000 | lr: 2.9861e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   4 | Iter:   5719/ 13000 | global iter:   5719/ 13000 | loss: 1.0541 | ds_loss: 0.0000 | lr: 2.9855e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   4 | Iter:   5720/ 13000 | global iter:   5720/ 13000 | loss: 1.1356 | ds_loss: 0.0000 | lr: 2.9849e-05 | scale: 16384.0000 | micro time: 1.879 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5720/ 13000 | global iter:   5720/ 13000 | loss: 1.3226 | ds_loss: 0.0000 | lr: 2.9849e-05 | scale: 16384.0000 | micro time: 1.879 | step time: 1.796
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5721/ 13000 | global iter:   5721/ 13000 | loss: 1.3164 | ds_loss: 0.0000 | lr: 2.9844e-05 | scale: 16384.0000 | micro time: 1.809 | step time: 0.000
train | epoch   4 | Iter:   5722/ 13000 | global iter:   5722/ 13000 | loss: 1.9007 | ds_loss: 0.0000 | lr: 2.9838e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   4 | Iter:   5723/ 13000 | global iter:   5723/ 13000 | loss: 1.6952 | ds_loss: 0.0000 | lr: 2.9832e-05 | scale: 16384.0000 | micro time: 1.812 | step time: 0.000
train | epoch   4 | Iter:   5724/ 13000 | global iter:   5724/ 13000 | loss: 1.6333 | ds_loss: 0.0000 | lr: 2.9826e-05 | scale: 16384.0000 | micro time: 1.826 | step time: 0.000
train | epoch   4 | Iter:   5725/ 13000 | global iter:   5725/ 13000 | loss: 1.8253 | ds_loss: 0.0000 | lr: 2.9820e-05 | scale: 16384.0000 | micro time: 1.864 | step time: 0.000
train | epoch   4 | Iter:   5726/ 13000 | global iter:   5726/ 13000 | loss: 1.1872 | ds_loss: 0.0000 | lr: 2.9814e-05 | scale: 16384.0000 | micro time: 1.765 | step time: 0.000
train | epoch   4 | Iter:   5727/ 13000 | global iter:   5727/ 13000 | loss: 0.8577 | ds_loss: 0.0000 | lr: 2.9808e-05 | scale: 16384.0000 | micro time: 1.848 | step time: 0.000
train | epoch   4 | Iter:   5728/ 13000 | global iter:   5728/ 13000 | loss: 1.0263 | ds_loss: 0.0000 | lr: 2.9802e-05 | scale: 16384.0000 | micro time: 1.742 | step time: 0.000
train | epoch   4 | Iter:   5729/ 13000 | global iter:   5729/ 13000 | loss: 0.9473 | ds_loss: 0.0000 | lr: 2.9796e-05 | scale: 16384.0000 | micro time: 1.913 | step time: 0.000
train | epoch   4 | Iter:   5730/ 13000 | global iter:   5730/ 13000 | loss: 1.1369 | ds_loss: 0.0000 | lr: 2.9790e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5730/ 13000 | global iter:   5730/ 13000 | loss: 1.3526 | ds_loss: 0.0000 | lr: 2.9790e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 1.817
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5731/ 13000 | global iter:   5731/ 13000 | loss: 0.8350 | ds_loss: 0.0000 | lr: 2.9784e-05 | scale: 16384.0000 | micro time: 1.734 | step time: 0.000
train | epoch   4 | Iter:   5732/ 13000 | global iter:   5732/ 13000 | loss: 1.1238 | ds_loss: 0.0000 | lr: 2.9778e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   4 | Iter:   5733/ 13000 | global iter:   5733/ 13000 | loss: 1.4115 | ds_loss: 0.0000 | lr: 2.9773e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   4 | Iter:   5734/ 13000 | global iter:   5734/ 13000 | loss: 1.6550 | ds_loss: 0.0000 | lr: 2.9767e-05 | scale: 16384.0000 | micro time: 1.818 | step time: 0.000
train | epoch   4 | Iter:   5735/ 13000 | global iter:   5735/ 13000 | loss: 0.9412 | ds_loss: 0.0000 | lr: 2.9761e-05 | scale: 32768.0000 | micro time: 1.720 | step time: 0.000
train | epoch   4 | Iter:   5736/ 13000 | global iter:   5736/ 13000 | loss: 1.5571 | ds_loss: 0.0000 | lr: 2.9755e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   4 | Iter:   5737/ 13000 | global iter:   5737/ 13000 | loss: 1.3234 | ds_loss: 0.0000 | lr: 2.9749e-05 | scale: 32768.0000 | micro time: 1.887 | step time: 0.000
train | epoch   4 | Iter:   5738/ 13000 | global iter:   5738/ 13000 | loss: 1.0137 | ds_loss: 0.0000 | lr: 2.9743e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 0.000
train | epoch   4 | Iter:   5739/ 13000 | global iter:   5739/ 13000 | loss: 1.2754 | ds_loss: 0.0000 | lr: 2.9737e-05 | scale: 32768.0000 | micro time: 1.672 | step time: 0.000
train | epoch   4 | Iter:   5740/ 13000 | global iter:   5740/ 13000 | loss: 1.1750 | ds_loss: 0.0000 | lr: 2.9731e-05 | scale: 32768.0000 | micro time: 1.794 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5740/ 13000 | global iter:   5740/ 13000 | loss: 1.2311 | ds_loss: 0.0000 | lr: 2.9731e-05 | scale: 32768.0000 | micro time: 1.794 | step time: 1.786
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5741/ 13000 | global iter:   5741/ 13000 | loss: 1.4663 | ds_loss: 0.0000 | lr: 2.9725e-05 | scale: 32768.0000 | micro time: 1.810 | step time: 0.000
train | epoch   4 | Iter:   5742/ 13000 | global iter:   5742/ 13000 | loss: 1.4858 | ds_loss: 0.0000 | lr: 2.9719e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   4 | Iter:   5743/ 13000 | global iter:   5743/ 13000 | loss: 1.3365 | ds_loss: 0.0000 | lr: 2.9713e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   4 | Iter:   5744/ 13000 | global iter:   5744/ 13000 | loss: 1.4976 | ds_loss: 0.0000 | lr: 2.9707e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
[2025-04-19 14:14:01,248] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
train | epoch   4 | Iter:   5745/ 13000 | global iter:   5745/ 13000 | loss: 1.2316 | ds_loss: 0.0000 | lr: 2.9707e-05 | scale: 16384.0000 | micro time: 1.464 | step time: 0.000
train | epoch   4 | Iter:   5746/ 13000 | global iter:   5746/ 13000 | loss: 0.8282 | ds_loss: 0.0000 | lr: 2.9701e-05 | scale: 16384.0000 | micro time: 1.790 | step time: 0.000
train | epoch   4 | Iter:   5747/ 13000 | global iter:   5747/ 13000 | loss: 1.5145 | ds_loss: 0.0000 | lr: 2.9696e-05 | scale: 16384.0000 | micro time: 1.777 | step time: 0.000
train | epoch   4 | Iter:   5748/ 13000 | global iter:   5748/ 13000 | loss: 1.1651 | ds_loss: 0.0000 | lr: 2.9690e-05 | scale: 16384.0000 | micro time: 1.770 | step time: 0.000
train | epoch   4 | Iter:   5749/ 13000 | global iter:   5749/ 13000 | loss: 1.3749 | ds_loss: 0.0000 | lr: 2.9684e-05 | scale: 16384.0000 | micro time: 1.744 | step time: 0.000
train | epoch   4 | Iter:   5750/ 13000 | global iter:   5750/ 13000 | loss: 1.5128 | ds_loss: 0.0000 | lr: 2.9678e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5750/ 13000 | global iter:   5750/ 13000 | loss: 1.3413 | ds_loss: 0.0000 | lr: 2.9678e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 1.764
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5751/ 13000 | global iter:   5751/ 13000 | loss: 1.1450 | ds_loss: 0.0000 | lr: 2.9672e-05 | scale: 16384.0000 | micro time: 1.703 | step time: 0.000
train | epoch   4 | Iter:   5752/ 13000 | global iter:   5752/ 13000 | loss: 1.3731 | ds_loss: 0.0000 | lr: 2.9666e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   4 | Iter:   5753/ 13000 | global iter:   5753/ 13000 | loss: 1.6404 | ds_loss: 0.0000 | lr: 2.9660e-05 | scale: 16384.0000 | micro time: 1.818 | step time: 0.000
train | epoch   4 | Iter:   5754/ 13000 | global iter:   5754/ 13000 | loss: 1.6935 | ds_loss: 0.0000 | lr: 2.9654e-05 | scale: 16384.0000 | micro time: 1.872 | step time: 0.000
train | epoch   4 | Iter:   5755/ 13000 | global iter:   5755/ 13000 | loss: 1.3481 | ds_loss: 0.0000 | lr: 2.9648e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   4 | Iter:   5756/ 13000 | global iter:   5756/ 13000 | loss: 1.7677 | ds_loss: 0.0000 | lr: 2.9642e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   4 | Iter:   5757/ 13000 | global iter:   5757/ 13000 | loss: 0.8204 | ds_loss: 0.0000 | lr: 2.9636e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 0.000
train | epoch   4 | Iter:   5758/ 13000 | global iter:   5758/ 13000 | loss: 1.7772 | ds_loss: 0.0000 | lr: 2.9630e-05 | scale: 16384.0000 | micro time: 1.740 | step time: 0.000
train | epoch   4 | Iter:   5759/ 13000 | global iter:   5759/ 13000 | loss: 1.3840 | ds_loss: 0.0000 | lr: 2.9624e-05 | scale: 16384.0000 | micro time: 1.816 | step time: 0.000
train | epoch   4 | Iter:   5760/ 13000 | global iter:   5760/ 13000 | loss: 1.2245 | ds_loss: 0.0000 | lr: 2.9619e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5760/ 13000 | global iter:   5760/ 13000 | loss: 1.4174 | ds_loss: 0.0000 | lr: 2.9619e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 1.807
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5761/ 13000 | global iter:   5761/ 13000 | loss: 1.1365 | ds_loss: 0.0000 | lr: 2.9613e-05 | scale: 16384.0000 | micro time: 1.841 | step time: 0.000
train | epoch   4 | Iter:   5762/ 13000 | global iter:   5762/ 13000 | loss: 1.5807 | ds_loss: 0.0000 | lr: 2.9607e-05 | scale: 16384.0000 | micro time: 1.639 | step time: 0.000
train | epoch   4 | Iter:   5763/ 13000 | global iter:   5763/ 13000 | loss: 1.6773 | ds_loss: 0.0000 | lr: 2.9601e-05 | scale: 16384.0000 | micro time: 1.743 | step time: 0.000
train | epoch   4 | Iter:   5764/ 13000 | global iter:   5764/ 13000 | loss: 1.5008 | ds_loss: 0.0000 | lr: 2.9595e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   4 | Iter:   5765/ 13000 | global iter:   5765/ 13000 | loss: 1.9828 | ds_loss: 0.0000 | lr: 2.9589e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   4 | Iter:   5766/ 13000 | global iter:   5766/ 13000 | loss: 1.2897 | ds_loss: 0.0000 | lr: 2.9583e-05 | scale: 16384.0000 | micro time: 1.611 | step time: 0.000
train | epoch   4 | Iter:   5767/ 13000 | global iter:   5767/ 13000 | loss: 1.1934 | ds_loss: 0.0000 | lr: 2.9577e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   4 | Iter:   5768/ 13000 | global iter:   5768/ 13000 | loss: 1.0872 | ds_loss: 0.0000 | lr: 2.9571e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   4 | Iter:   5769/ 13000 | global iter:   5769/ 13000 | loss: 1.8003 | ds_loss: 0.0000 | lr: 2.9565e-05 | scale: 16384.0000 | micro time: 1.860 | step time: 0.000
train | epoch   4 | Iter:   5770/ 13000 | global iter:   5770/ 13000 | loss: 1.1225 | ds_loss: 0.0000 | lr: 2.9559e-05 | scale: 16384.0000 | micro time: 1.806 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5770/ 13000 | global iter:   5770/ 13000 | loss: 1.4371 | ds_loss: 0.0000 | lr: 2.9559e-05 | scale: 16384.0000 | micro time: 1.806 | step time: 1.785
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5771/ 13000 | global iter:   5771/ 13000 | loss: 1.2871 | ds_loss: 0.0000 | lr: 2.9553e-05 | scale: 16384.0000 | micro time: 1.810 | step time: 0.000
train | epoch   4 | Iter:   5772/ 13000 | global iter:   5772/ 13000 | loss: 1.8094 | ds_loss: 0.0000 | lr: 2.9547e-05 | scale: 16384.0000 | micro time: 1.711 | step time: 0.000
train | epoch   4 | Iter:   5773/ 13000 | global iter:   5773/ 13000 | loss: 1.3117 | ds_loss: 0.0000 | lr: 2.9541e-05 | scale: 16384.0000 | micro time: 1.845 | step time: 0.000
train | epoch   4 | Iter:   5774/ 13000 | global iter:   5774/ 13000 | loss: 1.1545 | ds_loss: 0.0000 | lr: 2.9536e-05 | scale: 16384.0000 | micro time: 1.940 | step time: 0.000
train | epoch   4 | Iter:   5775/ 13000 | global iter:   5775/ 13000 | loss: 0.9003 | ds_loss: 0.0000 | lr: 2.9530e-05 | scale: 16384.0000 | micro time: 1.816 | step time: 0.000
train | epoch   4 | Iter:   5776/ 13000 | global iter:   5776/ 13000 | loss: 1.2528 | ds_loss: 0.0000 | lr: 2.9524e-05 | scale: 16384.0000 | micro time: 1.794 | step time: 0.000
train | epoch   4 | Iter:   5777/ 13000 | global iter:   5777/ 13000 | loss: 1.4327 | ds_loss: 0.0000 | lr: 2.9518e-05 | scale: 16384.0000 | micro time: 1.798 | step time: 0.000
train | epoch   4 | Iter:   5778/ 13000 | global iter:   5778/ 13000 | loss: 1.5044 | ds_loss: 0.0000 | lr: 2.9512e-05 | scale: 16384.0000 | micro time: 1.848 | step time: 0.000
train | epoch   4 | Iter:   5779/ 13000 | global iter:   5779/ 13000 | loss: 1.6172 | ds_loss: 0.0000 | lr: 2.9506e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   4 | Iter:   5780/ 13000 | global iter:   5780/ 13000 | loss: 1.0863 | ds_loss: 0.0000 | lr: 2.9500e-05 | scale: 16384.0000 | micro time: 1.822 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5780/ 13000 | global iter:   5780/ 13000 | loss: 1.3356 | ds_loss: 0.0000 | lr: 2.9500e-05 | scale: 16384.0000 | micro time: 1.822 | step time: 1.819
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5781/ 13000 | global iter:   5781/ 13000 | loss: 1.1249 | ds_loss: 0.0000 | lr: 2.9494e-05 | scale: 16384.0000 | micro time: 1.798 | step time: 0.000
train | epoch   4 | Iter:   5782/ 13000 | global iter:   5782/ 13000 | loss: 1.4918 | ds_loss: 0.0000 | lr: 2.9488e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   4 | Iter:   5783/ 13000 | global iter:   5783/ 13000 | loss: 1.5977 | ds_loss: 0.0000 | lr: 2.9482e-05 | scale: 16384.0000 | micro time: 1.879 | step time: 0.000
train | epoch   4 | Iter:   5784/ 13000 | global iter:   5784/ 13000 | loss: 0.9617 | ds_loss: 0.0000 | lr: 2.9476e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   4 | Iter:   5785/ 13000 | global iter:   5785/ 13000 | loss: 0.7318 | ds_loss: 0.0000 | lr: 2.9470e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   4 | Iter:   5786/ 13000 | global iter:   5786/ 13000 | loss: 0.8574 | ds_loss: 0.0000 | lr: 2.9464e-05 | scale: 16384.0000 | micro time: 1.895 | step time: 0.000
train | epoch   4 | Iter:   5787/ 13000 | global iter:   5787/ 13000 | loss: 1.3702 | ds_loss: 0.0000 | lr: 2.9458e-05 | scale: 16384.0000 | micro time: 1.903 | step time: 0.000
train | epoch   4 | Iter:   5788/ 13000 | global iter:   5788/ 13000 | loss: 1.1500 | ds_loss: 0.0000 | lr: 2.9452e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   4 | Iter:   5789/ 13000 | global iter:   5789/ 13000 | loss: 1.4964 | ds_loss: 0.0000 | lr: 2.9447e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   4 | Iter:   5790/ 13000 | global iter:   5790/ 13000 | loss: 0.9556 | ds_loss: 0.0000 | lr: 2.9441e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5790/ 13000 | global iter:   5790/ 13000 | loss: 1.1738 | ds_loss: 0.0000 | lr: 2.9441e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 1.835
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5791/ 13000 | global iter:   5791/ 13000 | loss: 1.6310 | ds_loss: 0.0000 | lr: 2.9435e-05 | scale: 16384.0000 | micro time: 1.888 | step time: 0.000
train | epoch   4 | Iter:   5792/ 13000 | global iter:   5792/ 13000 | loss: 1.3555 | ds_loss: 0.0000 | lr: 2.9429e-05 | scale: 16384.0000 | micro time: 1.828 | step time: 0.000
train | epoch   4 | Iter:   5793/ 13000 | global iter:   5793/ 13000 | loss: 1.0607 | ds_loss: 0.0000 | lr: 2.9423e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   4 | Iter:   5794/ 13000 | global iter:   5794/ 13000 | loss: 1.4336 | ds_loss: 0.0000 | lr: 2.9417e-05 | scale: 16384.0000 | micro time: 1.770 | step time: 0.000
train | epoch   4 | Iter:   5795/ 13000 | global iter:   5795/ 13000 | loss: 0.8189 | ds_loss: 0.0000 | lr: 2.9411e-05 | scale: 16384.0000 | micro time: 1.876 | step time: 0.000
train | epoch   4 | Iter:   5796/ 13000 | global iter:   5796/ 13000 | loss: 1.4866 | ds_loss: 0.0000 | lr: 2.9405e-05 | scale: 16384.0000 | micro time: 1.822 | step time: 0.000
train | epoch   4 | Iter:   5797/ 13000 | global iter:   5797/ 13000 | loss: 1.1967 | ds_loss: 0.0000 | lr: 2.9399e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   4 | Iter:   5798/ 13000 | global iter:   5798/ 13000 | loss: 1.4924 | ds_loss: 0.0000 | lr: 2.9393e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   4 | Iter:   5799/ 13000 | global iter:   5799/ 13000 | loss: 1.4299 | ds_loss: 0.0000 | lr: 2.9387e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   4 | Iter:   5800/ 13000 | global iter:   5800/ 13000 | loss: 0.9419 | ds_loss: 0.0000 | lr: 2.9381e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5800/ 13000 | global iter:   5800/ 13000 | loss: 1.2847 | ds_loss: 0.0000 | lr: 2.9381e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 1.827
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5801/ 13000 | global iter:   5801/ 13000 | loss: 1.4497 | ds_loss: 0.0000 | lr: 2.9375e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   4 | Iter:   5802/ 13000 | global iter:   5802/ 13000 | loss: 1.3005 | ds_loss: 0.0000 | lr: 2.9369e-05 | scale: 16384.0000 | micro time: 1.782 | step time: 0.000
train | epoch   4 | Iter:   5803/ 13000 | global iter:   5803/ 13000 | loss: 1.2788 | ds_loss: 0.0000 | lr: 2.9363e-05 | scale: 16384.0000 | micro time: 1.879 | step time: 0.000
train | epoch   4 | Iter:   5804/ 13000 | global iter:   5804/ 13000 | loss: 0.8248 | ds_loss: 0.0000 | lr: 2.9357e-05 | scale: 16384.0000 | micro time: 1.829 | step time: 0.000
train | epoch   4 | Iter:   5805/ 13000 | global iter:   5805/ 13000 | loss: 1.5622 | ds_loss: 0.0000 | lr: 2.9352e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   4 | Iter:   5806/ 13000 | global iter:   5806/ 13000 | loss: 1.0263 | ds_loss: 0.0000 | lr: 2.9346e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   4 | Iter:   5807/ 13000 | global iter:   5807/ 13000 | loss: 1.1458 | ds_loss: 0.0000 | lr: 2.9340e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   4 | Iter:   5808/ 13000 | global iter:   5808/ 13000 | loss: 1.6884 | ds_loss: 0.0000 | lr: 2.9334e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   4 | Iter:   5809/ 13000 | global iter:   5809/ 13000 | loss: 1.2917 | ds_loss: 0.0000 | lr: 2.9328e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   4 | Iter:   5810/ 13000 | global iter:   5810/ 13000 | loss: 2.0423 | ds_loss: 0.0000 | lr: 2.9322e-05 | scale: 16384.0000 | micro time: 1.727 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5810/ 13000 | global iter:   5810/ 13000 | loss: 1.3610 | ds_loss: 0.0000 | lr: 2.9322e-05 | scale: 16384.0000 | micro time: 1.727 | step time: 1.810
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5811/ 13000 | global iter:   5811/ 13000 | loss: 1.4364 | ds_loss: 0.0000 | lr: 2.9316e-05 | scale: 16384.0000 | micro time: 1.814 | step time: 0.000
train | epoch   4 | Iter:   5812/ 13000 | global iter:   5812/ 13000 | loss: 0.9933 | ds_loss: 0.0000 | lr: 2.9310e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   4 | Iter:   5813/ 13000 | global iter:   5813/ 13000 | loss: 1.2993 | ds_loss: 0.0000 | lr: 2.9304e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   4 | Iter:   5814/ 13000 | global iter:   5814/ 13000 | loss: 1.2369 | ds_loss: 0.0000 | lr: 2.9298e-05 | scale: 16384.0000 | micro time: 1.761 | step time: 0.000
train | epoch   4 | Iter:   5815/ 13000 | global iter:   5815/ 13000 | loss: 1.0240 | ds_loss: 0.0000 | lr: 2.9292e-05 | scale: 16384.0000 | micro time: 1.808 | step time: 0.000
train | epoch   4 | Iter:   5816/ 13000 | global iter:   5816/ 13000 | loss: 1.1293 | ds_loss: 0.0000 | lr: 2.9286e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   4 | Iter:   5817/ 13000 | global iter:   5817/ 13000 | loss: 1.4146 | ds_loss: 0.0000 | lr: 2.9280e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   4 | Iter:   5818/ 13000 | global iter:   5818/ 13000 | loss: 1.1212 | ds_loss: 0.0000 | lr: 2.9274e-05 | scale: 16384.0000 | micro time: 1.703 | step time: 0.000
train | epoch   4 | Iter:   5819/ 13000 | global iter:   5819/ 13000 | loss: 1.3838 | ds_loss: 0.0000 | lr: 2.9268e-05 | scale: 16384.0000 | micro time: 1.879 | step time: 0.000
train | epoch   4 | Iter:   5820/ 13000 | global iter:   5820/ 13000 | loss: 1.1427 | ds_loss: 0.0000 | lr: 2.9262e-05 | scale: 16384.0000 | micro time: 1.916 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5820/ 13000 | global iter:   5820/ 13000 | loss: 1.2181 | ds_loss: 0.0000 | lr: 2.9262e-05 | scale: 16384.0000 | micro time: 1.916 | step time: 1.816
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5821/ 13000 | global iter:   5821/ 13000 | loss: 1.5705 | ds_loss: 0.0000 | lr: 2.9256e-05 | scale: 16384.0000 | micro time: 1.873 | step time: 0.000
train | epoch   4 | Iter:   5822/ 13000 | global iter:   5822/ 13000 | loss: 1.2206 | ds_loss: 0.0000 | lr: 2.9251e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   4 | Iter:   5823/ 13000 | global iter:   5823/ 13000 | loss: 1.3922 | ds_loss: 0.0000 | lr: 2.9245e-05 | scale: 16384.0000 | micro time: 1.748 | step time: 0.000
train | epoch   4 | Iter:   5824/ 13000 | global iter:   5824/ 13000 | loss: 1.4698 | ds_loss: 0.0000 | lr: 2.9239e-05 | scale: 16384.0000 | micro time: 1.770 | step time: 0.000
train | epoch   4 | Iter:   5825/ 13000 | global iter:   5825/ 13000 | loss: 0.9250 | ds_loss: 0.0000 | lr: 2.9233e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   4 | Iter:   5826/ 13000 | global iter:   5826/ 13000 | loss: 1.0896 | ds_loss: 0.0000 | lr: 2.9227e-05 | scale: 16384.0000 | micro time: 1.759 | step time: 0.000
train | epoch   4 | Iter:   5827/ 13000 | global iter:   5827/ 13000 | loss: 1.2948 | ds_loss: 0.0000 | lr: 2.9221e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   4 | Iter:   5828/ 13000 | global iter:   5828/ 13000 | loss: 1.1940 | ds_loss: 0.0000 | lr: 2.9215e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   4 | Iter:   5829/ 13000 | global iter:   5829/ 13000 | loss: 1.8932 | ds_loss: 0.0000 | lr: 2.9209e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   4 | Iter:   5830/ 13000 | global iter:   5830/ 13000 | loss: 1.7844 | ds_loss: 0.0000 | lr: 2.9203e-05 | scale: 16384.0000 | micro time: 1.753 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5830/ 13000 | global iter:   5830/ 13000 | loss: 1.3834 | ds_loss: 0.0000 | lr: 2.9203e-05 | scale: 16384.0000 | micro time: 1.753 | step time: 1.805
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5831/ 13000 | global iter:   5831/ 13000 | loss: 1.8341 | ds_loss: 0.0000 | lr: 2.9197e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   4 | Iter:   5832/ 13000 | global iter:   5832/ 13000 | loss: 1.4390 | ds_loss: 0.0000 | lr: 2.9191e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   4 | Iter:   5833/ 13000 | global iter:   5833/ 13000 | loss: 1.1123 | ds_loss: 0.0000 | lr: 2.9185e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   4 | Iter:   5834/ 13000 | global iter:   5834/ 13000 | loss: 0.9770 | ds_loss: 0.0000 | lr: 2.9179e-05 | scale: 16384.0000 | micro time: 1.911 | step time: 0.000
train | epoch   4 | Iter:   5835/ 13000 | global iter:   5835/ 13000 | loss: 1.1779 | ds_loss: 0.0000 | lr: 2.9173e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   4 | Iter:   5836/ 13000 | global iter:   5836/ 13000 | loss: 1.7339 | ds_loss: 0.0000 | lr: 2.9167e-05 | scale: 16384.0000 | micro time: 2.011 | step time: 0.000
train | epoch   4 | Iter:   5837/ 13000 | global iter:   5837/ 13000 | loss: 1.3252 | ds_loss: 0.0000 | lr: 2.9161e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   4 | Iter:   5838/ 13000 | global iter:   5838/ 13000 | loss: 1.5369 | ds_loss: 0.0000 | lr: 2.9155e-05 | scale: 16384.0000 | micro time: 1.743 | step time: 0.000
train | epoch   4 | Iter:   5839/ 13000 | global iter:   5839/ 13000 | loss: 1.4366 | ds_loss: 0.0000 | lr: 2.9149e-05 | scale: 16384.0000 | micro time: 1.699 | step time: 0.000
train | epoch   4 | Iter:   5840/ 13000 | global iter:   5840/ 13000 | loss: 1.2613 | ds_loss: 0.0000 | lr: 2.9143e-05 | scale: 16384.0000 | micro time: 1.776 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5840/ 13000 | global iter:   5840/ 13000 | loss: 1.3834 | ds_loss: 0.0000 | lr: 2.9143e-05 | scale: 16384.0000 | micro time: 1.776 | step time: 1.829
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5841/ 13000 | global iter:   5841/ 13000 | loss: 1.1166 | ds_loss: 0.0000 | lr: 2.9138e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   4 | Iter:   5842/ 13000 | global iter:   5842/ 13000 | loss: 1.4421 | ds_loss: 0.0000 | lr: 2.9132e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   4 | Iter:   5843/ 13000 | global iter:   5843/ 13000 | loss: 1.2618 | ds_loss: 0.0000 | lr: 2.9126e-05 | scale: 16384.0000 | micro time: 1.826 | step time: 0.000
train | epoch   4 | Iter:   5844/ 13000 | global iter:   5844/ 13000 | loss: 0.6205 | ds_loss: 0.0000 | lr: 2.9120e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   4 | Iter:   5845/ 13000 | global iter:   5845/ 13000 | loss: 1.2138 | ds_loss: 0.0000 | lr: 2.9114e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   4 | Iter:   5846/ 13000 | global iter:   5846/ 13000 | loss: 1.1112 | ds_loss: 0.0000 | lr: 2.9108e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   4 | Iter:   5847/ 13000 | global iter:   5847/ 13000 | loss: 1.2267 | ds_loss: 0.0000 | lr: 2.9102e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   4 | Iter:   5848/ 13000 | global iter:   5848/ 13000 | loss: 1.1254 | ds_loss: 0.0000 | lr: 2.9096e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   4 | Iter:   5849/ 13000 | global iter:   5849/ 13000 | loss: 1.3012 | ds_loss: 0.0000 | lr: 2.9090e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   4 | Iter:   5850/ 13000 | global iter:   5850/ 13000 | loss: 1.8701 | ds_loss: 0.0000 | lr: 2.9084e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5850/ 13000 | global iter:   5850/ 13000 | loss: 1.2290 | ds_loss: 0.0000 | lr: 2.9084e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 1.806
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5851/ 13000 | global iter:   5851/ 13000 | loss: 1.2899 | ds_loss: 0.0000 | lr: 2.9078e-05 | scale: 16384.0000 | micro time: 1.798 | step time: 0.000
train | epoch   4 | Iter:   5852/ 13000 | global iter:   5852/ 13000 | loss: 0.7130 | ds_loss: 0.0000 | lr: 2.9072e-05 | scale: 16384.0000 | micro time: 1.810 | step time: 0.000
train | epoch   4 | Iter:   5853/ 13000 | global iter:   5853/ 13000 | loss: 0.9643 | ds_loss: 0.0000 | lr: 2.9066e-05 | scale: 16384.0000 | micro time: 1.832 | step time: 0.000
train | epoch   4 | Iter:   5854/ 13000 | global iter:   5854/ 13000 | loss: 1.4692 | ds_loss: 0.0000 | lr: 2.9060e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   4 | Iter:   5855/ 13000 | global iter:   5855/ 13000 | loss: 1.2602 | ds_loss: 0.0000 | lr: 2.9054e-05 | scale: 16384.0000 | micro time: 1.724 | step time: 0.000
train | epoch   4 | Iter:   5856/ 13000 | global iter:   5856/ 13000 | loss: 1.4622 | ds_loss: 0.0000 | lr: 2.9048e-05 | scale: 16384.0000 | micro time: 1.746 | step time: 0.000
train | epoch   4 | Iter:   5857/ 13000 | global iter:   5857/ 13000 | loss: 1.8470 | ds_loss: 0.0000 | lr: 2.9042e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   4 | Iter:   5858/ 13000 | global iter:   5858/ 13000 | loss: 1.1422 | ds_loss: 0.0000 | lr: 2.9036e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   4 | Iter:   5859/ 13000 | global iter:   5859/ 13000 | loss: 1.5606 | ds_loss: 0.0000 | lr: 2.9030e-05 | scale: 16384.0000 | micro time: 1.816 | step time: 0.000
train | epoch   4 | Iter:   5860/ 13000 | global iter:   5860/ 13000 | loss: 1.7175 | ds_loss: 0.0000 | lr: 2.9024e-05 | scale: 16384.0000 | micro time: 1.711 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5860/ 13000 | global iter:   5860/ 13000 | loss: 1.3426 | ds_loss: 0.0000 | lr: 2.9024e-05 | scale: 16384.0000 | micro time: 1.711 | step time: 1.788
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5861/ 13000 | global iter:   5861/ 13000 | loss: 1.9595 | ds_loss: 0.0000 | lr: 2.9019e-05 | scale: 16384.0000 | micro time: 1.766 | step time: 0.000
train | epoch   4 | Iter:   5862/ 13000 | global iter:   5862/ 13000 | loss: 1.4525 | ds_loss: 0.0000 | lr: 2.9013e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   4 | Iter:   5863/ 13000 | global iter:   5863/ 13000 | loss: 0.9713 | ds_loss: 0.0000 | lr: 2.9007e-05 | scale: 16384.0000 | micro time: 1.748 | step time: 0.000
train | epoch   4 | Iter:   5864/ 13000 | global iter:   5864/ 13000 | loss: 1.4299 | ds_loss: 0.0000 | lr: 2.9001e-05 | scale: 16384.0000 | micro time: 1.817 | step time: 0.000
train | epoch   4 | Iter:   5865/ 13000 | global iter:   5865/ 13000 | loss: 1.3899 | ds_loss: 0.0000 | lr: 2.8995e-05 | scale: 16384.0000 | micro time: 1.814 | step time: 0.000
train | epoch   4 | Iter:   5866/ 13000 | global iter:   5866/ 13000 | loss: 1.2264 | ds_loss: 0.0000 | lr: 2.8989e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   4 | Iter:   5867/ 13000 | global iter:   5867/ 13000 | loss: 0.9949 | ds_loss: 0.0000 | lr: 2.8983e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   4 | Iter:   5868/ 13000 | global iter:   5868/ 13000 | loss: 0.9705 | ds_loss: 0.0000 | lr: 2.8977e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   4 | Iter:   5869/ 13000 | global iter:   5869/ 13000 | loss: 1.9092 | ds_loss: 0.0000 | lr: 2.8971e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   4 | Iter:   5870/ 13000 | global iter:   5870/ 13000 | loss: 1.6911 | ds_loss: 0.0000 | lr: 2.8965e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5870/ 13000 | global iter:   5870/ 13000 | loss: 1.3995 | ds_loss: 0.0000 | lr: 2.8965e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 1.807
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5871/ 13000 | global iter:   5871/ 13000 | loss: 0.7282 | ds_loss: 0.0000 | lr: 2.8959e-05 | scale: 16384.0000 | micro time: 1.750 | step time: 0.000
train | epoch   4 | Iter:   5872/ 13000 | global iter:   5872/ 13000 | loss: 1.2449 | ds_loss: 0.0000 | lr: 2.8953e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   4 | Iter:   5873/ 13000 | global iter:   5873/ 13000 | loss: 1.1860 | ds_loss: 0.0000 | lr: 2.8947e-05 | scale: 16384.0000 | micro time: 1.821 | step time: 0.000
train | epoch   4 | Iter:   5874/ 13000 | global iter:   5874/ 13000 | loss: 1.5700 | ds_loss: 0.0000 | lr: 2.8941e-05 | scale: 16384.0000 | micro time: 1.768 | step time: 0.000
train | epoch   4 | Iter:   5875/ 13000 | global iter:   5875/ 13000 | loss: 0.8601 | ds_loss: 0.0000 | lr: 2.8935e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   4 | Iter:   5876/ 13000 | global iter:   5876/ 13000 | loss: 1.0783 | ds_loss: 0.0000 | lr: 2.8929e-05 | scale: 16384.0000 | micro time: 1.727 | step time: 0.000
train | epoch   4 | Iter:   5877/ 13000 | global iter:   5877/ 13000 | loss: 1.5874 | ds_loss: 0.0000 | lr: 2.8923e-05 | scale: 16384.0000 | micro time: 1.727 | step time: 0.000
train | epoch   4 | Iter:   5878/ 13000 | global iter:   5878/ 13000 | loss: 1.6984 | ds_loss: 0.0000 | lr: 2.8917e-05 | scale: 16384.0000 | micro time: 1.832 | step time: 0.000
train | epoch   4 | Iter:   5879/ 13000 | global iter:   5879/ 13000 | loss: 1.2808 | ds_loss: 0.0000 | lr: 2.8911e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   4 | Iter:   5880/ 13000 | global iter:   5880/ 13000 | loss: 1.3418 | ds_loss: 0.0000 | lr: 2.8905e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5880/ 13000 | global iter:   5880/ 13000 | loss: 1.2576 | ds_loss: 0.0000 | lr: 2.8905e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 1.779
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5881/ 13000 | global iter:   5881/ 13000 | loss: 0.6183 | ds_loss: 0.0000 | lr: 2.8899e-05 | scale: 16384.0000 | micro time: 1.804 | step time: 0.000
train | epoch   4 | Iter:   5882/ 13000 | global iter:   5882/ 13000 | loss: 1.3647 | ds_loss: 0.0000 | lr: 2.8893e-05 | scale: 16384.0000 | micro time: 1.846 | step time: 0.000
train | epoch   4 | Iter:   5883/ 13000 | global iter:   5883/ 13000 | loss: 1.5959 | ds_loss: 0.0000 | lr: 2.8888e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   4 | Iter:   5884/ 13000 | global iter:   5884/ 13000 | loss: 1.7651 | ds_loss: 0.0000 | lr: 2.8882e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   4 | Iter:   5885/ 13000 | global iter:   5885/ 13000 | loss: 1.1492 | ds_loss: 0.0000 | lr: 2.8876e-05 | scale: 16384.0000 | micro time: 1.802 | step time: 0.000
train | epoch   4 | Iter:   5886/ 13000 | global iter:   5886/ 13000 | loss: 1.1404 | ds_loss: 0.0000 | lr: 2.8870e-05 | scale: 16384.0000 | micro time: 1.896 | step time: 0.000
train | epoch   4 | Iter:   5887/ 13000 | global iter:   5887/ 13000 | loss: 2.0932 | ds_loss: 0.0000 | lr: 2.8864e-05 | scale: 16384.0000 | micro time: 1.743 | step time: 0.000
train | epoch   4 | Iter:   5888/ 13000 | global iter:   5888/ 13000 | loss: 1.4155 | ds_loss: 0.0000 | lr: 2.8858e-05 | scale: 16384.0000 | micro time: 1.834 | step time: 0.000
train | epoch   4 | Iter:   5889/ 13000 | global iter:   5889/ 13000 | loss: 0.9690 | ds_loss: 0.0000 | lr: 2.8852e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   4 | Iter:   5890/ 13000 | global iter:   5890/ 13000 | loss: 1.4104 | ds_loss: 0.0000 | lr: 2.8846e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5890/ 13000 | global iter:   5890/ 13000 | loss: 1.3522 | ds_loss: 0.0000 | lr: 2.8846e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 1.822
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5891/ 13000 | global iter:   5891/ 13000 | loss: 1.1129 | ds_loss: 0.0000 | lr: 2.8840e-05 | scale: 16384.0000 | micro time: 1.640 | step time: 0.000
train | epoch   4 | Iter:   5892/ 13000 | global iter:   5892/ 13000 | loss: 1.0235 | ds_loss: 0.0000 | lr: 2.8834e-05 | scale: 16384.0000 | micro time: 1.848 | step time: 0.000
train | epoch   4 | Iter:   5893/ 13000 | global iter:   5893/ 13000 | loss: 1.6605 | ds_loss: 0.0000 | lr: 2.8828e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   4 | Iter:   5894/ 13000 | global iter:   5894/ 13000 | loss: 0.4718 | ds_loss: 0.0000 | lr: 2.8822e-05 | scale: 16384.0000 | micro time: 1.833 | step time: 0.000
train | epoch   4 | Iter:   5895/ 13000 | global iter:   5895/ 13000 | loss: 1.5920 | ds_loss: 0.0000 | lr: 2.8816e-05 | scale: 16384.0000 | micro time: 1.785 | step time: 0.000
train | epoch   4 | Iter:   5896/ 13000 | global iter:   5896/ 13000 | loss: 1.6754 | ds_loss: 0.0000 | lr: 2.8810e-05 | scale: 16384.0000 | micro time: 1.738 | step time: 0.000
train | epoch   4 | Iter:   5897/ 13000 | global iter:   5897/ 13000 | loss: 1.5723 | ds_loss: 0.0000 | lr: 2.8804e-05 | scale: 16384.0000 | micro time: 1.892 | step time: 0.000
train | epoch   4 | Iter:   5898/ 13000 | global iter:   5898/ 13000 | loss: 0.9319 | ds_loss: 0.0000 | lr: 2.8798e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   4 | Iter:   5899/ 13000 | global iter:   5899/ 13000 | loss: 0.8407 | ds_loss: 0.0000 | lr: 2.8792e-05 | scale: 16384.0000 | micro time: 1.865 | step time: 0.000
train | epoch   4 | Iter:   5900/ 13000 | global iter:   5900/ 13000 | loss: 1.0131 | ds_loss: 0.0000 | lr: 2.8786e-05 | scale: 16384.0000 | micro time: 1.809 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5900/ 13000 | global iter:   5900/ 13000 | loss: 1.1894 | ds_loss: 0.0000 | lr: 2.8786e-05 | scale: 16384.0000 | micro time: 1.809 | step time: 1.810
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5901/ 13000 | global iter:   5901/ 13000 | loss: 0.6840 | ds_loss: 0.0000 | lr: 2.8780e-05 | scale: 16384.0000 | micro time: 1.870 | step time: 0.000
train | epoch   4 | Iter:   5902/ 13000 | global iter:   5902/ 13000 | loss: 1.0777 | ds_loss: 0.0000 | lr: 2.8774e-05 | scale: 16384.0000 | micro time: 1.996 | step time: 0.000
train | epoch   4 | Iter:   5903/ 13000 | global iter:   5903/ 13000 | loss: 1.3247 | ds_loss: 0.0000 | lr: 2.8768e-05 | scale: 16384.0000 | micro time: 1.817 | step time: 0.000
train | epoch   4 | Iter:   5904/ 13000 | global iter:   5904/ 13000 | loss: 1.1984 | ds_loss: 0.0000 | lr: 2.8762e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   4 | Iter:   5905/ 13000 | global iter:   5905/ 13000 | loss: 1.1565 | ds_loss: 0.0000 | lr: 2.8756e-05 | scale: 16384.0000 | micro time: 1.891 | step time: 0.000
train | epoch   4 | Iter:   5906/ 13000 | global iter:   5906/ 13000 | loss: 1.7931 | ds_loss: 0.0000 | lr: 2.8750e-05 | scale: 16384.0000 | micro time: 1.856 | step time: 0.000
train | epoch   4 | Iter:   5907/ 13000 | global iter:   5907/ 13000 | loss: 1.0657 | ds_loss: 0.0000 | lr: 2.8744e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   4 | Iter:   5908/ 13000 | global iter:   5908/ 13000 | loss: 1.4344 | ds_loss: 0.0000 | lr: 2.8739e-05 | scale: 16384.0000 | micro time: 1.887 | step time: 0.000
train | epoch   4 | Iter:   5909/ 13000 | global iter:   5909/ 13000 | loss: 1.1963 | ds_loss: 0.0000 | lr: 2.8733e-05 | scale: 16384.0000 | micro time: 1.743 | step time: 0.000
train | epoch   4 | Iter:   5910/ 13000 | global iter:   5910/ 13000 | loss: 1.3814 | ds_loss: 0.0000 | lr: 2.8727e-05 | scale: 16384.0000 | micro time: 1.774 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5910/ 13000 | global iter:   5910/ 13000 | loss: 1.2312 | ds_loss: 0.0000 | lr: 2.8727e-05 | scale: 16384.0000 | micro time: 1.774 | step time: 1.844
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5911/ 13000 | global iter:   5911/ 13000 | loss: 1.1801 | ds_loss: 0.0000 | lr: 2.8721e-05 | scale: 16384.0000 | micro time: 1.901 | step time: 0.000
train | epoch   4 | Iter:   5912/ 13000 | global iter:   5912/ 13000 | loss: 0.7046 | ds_loss: 0.0000 | lr: 2.8715e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   4 | Iter:   5913/ 13000 | global iter:   5913/ 13000 | loss: 1.0994 | ds_loss: 0.0000 | lr: 2.8709e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   4 | Iter:   5914/ 13000 | global iter:   5914/ 13000 | loss: 1.1795 | ds_loss: 0.0000 | lr: 2.8703e-05 | scale: 16384.0000 | micro time: 1.780 | step time: 0.000
train | epoch   4 | Iter:   5915/ 13000 | global iter:   5915/ 13000 | loss: 1.5782 | ds_loss: 0.0000 | lr: 2.8697e-05 | scale: 16384.0000 | micro time: 1.854 | step time: 0.000
train | epoch   4 | Iter:   5916/ 13000 | global iter:   5916/ 13000 | loss: 1.3127 | ds_loss: 0.0000 | lr: 2.8691e-05 | scale: 16384.0000 | micro time: 1.916 | step time: 0.000
train | epoch   4 | Iter:   5917/ 13000 | global iter:   5917/ 13000 | loss: 1.0320 | ds_loss: 0.0000 | lr: 2.8685e-05 | scale: 16384.0000 | micro time: 1.756 | step time: 0.000
train | epoch   4 | Iter:   5918/ 13000 | global iter:   5918/ 13000 | loss: 1.5616 | ds_loss: 0.0000 | lr: 2.8679e-05 | scale: 16384.0000 | micro time: 1.709 | step time: 0.000
train | epoch   4 | Iter:   5919/ 13000 | global iter:   5919/ 13000 | loss: 1.3057 | ds_loss: 0.0000 | lr: 2.8673e-05 | scale: 16384.0000 | micro time: 1.931 | step time: 0.000
train | epoch   4 | Iter:   5920/ 13000 | global iter:   5920/ 13000 | loss: 1.6731 | ds_loss: 0.0000 | lr: 2.8667e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5920/ 13000 | global iter:   5920/ 13000 | loss: 1.2627 | ds_loss: 0.0000 | lr: 2.8667e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 1.828
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5921/ 13000 | global iter:   5921/ 13000 | loss: 1.0584 | ds_loss: 0.0000 | lr: 2.8661e-05 | scale: 16384.0000 | micro time: 1.885 | step time: 0.000
train | epoch   4 | Iter:   5922/ 13000 | global iter:   5922/ 13000 | loss: 1.1372 | ds_loss: 0.0000 | lr: 2.8655e-05 | scale: 16384.0000 | micro time: 1.825 | step time: 0.000
train | epoch   4 | Iter:   5923/ 13000 | global iter:   5923/ 13000 | loss: 1.1932 | ds_loss: 0.0000 | lr: 2.8649e-05 | scale: 16384.0000 | micro time: 1.702 | step time: 0.000
train | epoch   4 | Iter:   5924/ 13000 | global iter:   5924/ 13000 | loss: 1.3871 | ds_loss: 0.0000 | lr: 2.8643e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   4 | Iter:   5925/ 13000 | global iter:   5925/ 13000 | loss: 1.0770 | ds_loss: 0.0000 | lr: 2.8637e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   4 | Iter:   5926/ 13000 | global iter:   5926/ 13000 | loss: 1.4033 | ds_loss: 0.0000 | lr: 2.8631e-05 | scale: 16384.0000 | micro time: 1.695 | step time: 0.000
train | epoch   4 | Iter:   5927/ 13000 | global iter:   5927/ 13000 | loss: 0.9823 | ds_loss: 0.0000 | lr: 2.8625e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   4 | Iter:   5928/ 13000 | global iter:   5928/ 13000 | loss: 1.1321 | ds_loss: 0.0000 | lr: 2.8619e-05 | scale: 16384.0000 | micro time: 1.826 | step time: 0.000
train | epoch   4 | Iter:   5929/ 13000 | global iter:   5929/ 13000 | loss: 1.0440 | ds_loss: 0.0000 | lr: 2.8613e-05 | scale: 16384.0000 | micro time: 1.850 | step time: 0.000
train | epoch   4 | Iter:   5930/ 13000 | global iter:   5930/ 13000 | loss: 0.5089 | ds_loss: 0.0000 | lr: 2.8607e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5930/ 13000 | global iter:   5930/ 13000 | loss: 1.0923 | ds_loss: 0.0000 | lr: 2.8607e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 1.797
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5931/ 13000 | global iter:   5931/ 13000 | loss: 1.2401 | ds_loss: 0.0000 | lr: 2.8601e-05 | scale: 16384.0000 | micro time: 1.802 | step time: 0.000
train | epoch   4 | Iter:   5932/ 13000 | global iter:   5932/ 13000 | loss: 1.9000 | ds_loss: 0.0000 | lr: 2.8595e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   4 | Iter:   5933/ 13000 | global iter:   5933/ 13000 | loss: 1.4812 | ds_loss: 0.0000 | lr: 2.8589e-05 | scale: 16384.0000 | micro time: 1.907 | step time: 0.000
train | epoch   4 | Iter:   5934/ 13000 | global iter:   5934/ 13000 | loss: 1.3613 | ds_loss: 0.0000 | lr: 2.8583e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   4 | Iter:   5935/ 13000 | global iter:   5935/ 13000 | loss: 1.0217 | ds_loss: 0.0000 | lr: 2.8577e-05 | scale: 16384.0000 | micro time: 1.730 | step time: 0.000
train | epoch   4 | Iter:   5936/ 13000 | global iter:   5936/ 13000 | loss: 1.4997 | ds_loss: 0.0000 | lr: 2.8571e-05 | scale: 16384.0000 | micro time: 1.792 | step time: 0.000
train | epoch   4 | Iter:   5937/ 13000 | global iter:   5937/ 13000 | loss: 1.4012 | ds_loss: 0.0000 | lr: 2.8565e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   4 | Iter:   5938/ 13000 | global iter:   5938/ 13000 | loss: 1.3453 | ds_loss: 0.0000 | lr: 2.8560e-05 | scale: 16384.0000 | micro time: 1.734 | step time: 0.000
train | epoch   4 | Iter:   5939/ 13000 | global iter:   5939/ 13000 | loss: 1.7275 | ds_loss: 0.0000 | lr: 2.8554e-05 | scale: 16384.0000 | micro time: 1.800 | step time: 0.000
train | epoch   4 | Iter:   5940/ 13000 | global iter:   5940/ 13000 | loss: 1.1401 | ds_loss: 0.0000 | lr: 2.8548e-05 | scale: 16384.0000 | micro time: 1.759 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5940/ 13000 | global iter:   5940/ 13000 | loss: 1.4118 | ds_loss: 0.0000 | lr: 2.8548e-05 | scale: 16384.0000 | micro time: 1.759 | step time: 1.797
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5941/ 13000 | global iter:   5941/ 13000 | loss: 1.2888 | ds_loss: 0.0000 | lr: 2.8542e-05 | scale: 16384.0000 | micro time: 1.846 | step time: 0.000
train | epoch   4 | Iter:   5942/ 13000 | global iter:   5942/ 13000 | loss: 1.6186 | ds_loss: 0.0000 | lr: 2.8536e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   4 | Iter:   5943/ 13000 | global iter:   5943/ 13000 | loss: 1.1710 | ds_loss: 0.0000 | lr: 2.8530e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   4 | Iter:   5944/ 13000 | global iter:   5944/ 13000 | loss: 0.6439 | ds_loss: 0.0000 | lr: 2.8524e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   4 | Iter:   5945/ 13000 | global iter:   5945/ 13000 | loss: 1.2657 | ds_loss: 0.0000 | lr: 2.8518e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   4 | Iter:   5946/ 13000 | global iter:   5946/ 13000 | loss: 1.6083 | ds_loss: 0.0000 | lr: 2.8512e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   4 | Iter:   5947/ 13000 | global iter:   5947/ 13000 | loss: 1.0082 | ds_loss: 0.0000 | lr: 2.8506e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   4 | Iter:   5948/ 13000 | global iter:   5948/ 13000 | loss: 0.5933 | ds_loss: 0.0000 | lr: 2.8500e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   4 | Iter:   5949/ 13000 | global iter:   5949/ 13000 | loss: 1.7871 | ds_loss: 0.0000 | lr: 2.8494e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   4 | Iter:   5950/ 13000 | global iter:   5950/ 13000 | loss: 1.5796 | ds_loss: 0.0000 | lr: 2.8488e-05 | scale: 16384.0000 | micro time: 1.671 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5950/ 13000 | global iter:   5950/ 13000 | loss: 1.2565 | ds_loss: 0.0000 | lr: 2.8488e-05 | scale: 16384.0000 | micro time: 1.671 | step time: 1.812
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5951/ 13000 | global iter:   5951/ 13000 | loss: 1.3393 | ds_loss: 0.0000 | lr: 2.8482e-05 | scale: 16384.0000 | micro time: 1.818 | step time: 0.000
train | epoch   4 | Iter:   5952/ 13000 | global iter:   5952/ 13000 | loss: 1.2149 | ds_loss: 0.0000 | lr: 2.8476e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 0.000
train | epoch   4 | Iter:   5953/ 13000 | global iter:   5953/ 13000 | loss: 1.9498 | ds_loss: 0.0000 | lr: 2.8470e-05 | scale: 16384.0000 | micro time: 1.728 | step time: 0.000
train | epoch   4 | Iter:   5954/ 13000 | global iter:   5954/ 13000 | loss: 0.9954 | ds_loss: 0.0000 | lr: 2.8464e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   4 | Iter:   5955/ 13000 | global iter:   5955/ 13000 | loss: 1.5599 | ds_loss: 0.0000 | lr: 2.8458e-05 | scale: 16384.0000 | micro time: 1.812 | step time: 0.000
train | epoch   4 | Iter:   5956/ 13000 | global iter:   5956/ 13000 | loss: 1.8878 | ds_loss: 0.0000 | lr: 2.8452e-05 | scale: 16384.0000 | micro time: 1.758 | step time: 0.000
train | epoch   4 | Iter:   5957/ 13000 | global iter:   5957/ 13000 | loss: 1.6988 | ds_loss: 0.0000 | lr: 2.8446e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   4 | Iter:   5958/ 13000 | global iter:   5958/ 13000 | loss: 1.1586 | ds_loss: 0.0000 | lr: 2.8440e-05 | scale: 16384.0000 | micro time: 1.730 | step time: 0.000
train | epoch   4 | Iter:   5959/ 13000 | global iter:   5959/ 13000 | loss: 1.4977 | ds_loss: 0.0000 | lr: 2.8434e-05 | scale: 16384.0000 | micro time: 1.776 | step time: 0.000
train | epoch   4 | Iter:   5960/ 13000 | global iter:   5960/ 13000 | loss: 1.4772 | ds_loss: 0.0000 | lr: 2.8428e-05 | scale: 16384.0000 | micro time: 1.728 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5960/ 13000 | global iter:   5960/ 13000 | loss: 1.4779 | ds_loss: 0.0000 | lr: 2.8428e-05 | scale: 16384.0000 | micro time: 1.728 | step time: 1.769
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5961/ 13000 | global iter:   5961/ 13000 | loss: 1.4807 | ds_loss: 0.0000 | lr: 2.8422e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   4 | Iter:   5962/ 13000 | global iter:   5962/ 13000 | loss: 1.3621 | ds_loss: 0.0000 | lr: 2.8416e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   4 | Iter:   5963/ 13000 | global iter:   5963/ 13000 | loss: 1.0395 | ds_loss: 0.0000 | lr: 2.8410e-05 | scale: 16384.0000 | micro time: 1.759 | step time: 0.000
train | epoch   4 | Iter:   5964/ 13000 | global iter:   5964/ 13000 | loss: 1.8253 | ds_loss: 0.0000 | lr: 2.8404e-05 | scale: 16384.0000 | micro time: 1.719 | step time: 0.000
train | epoch   4 | Iter:   5965/ 13000 | global iter:   5965/ 13000 | loss: 1.2534 | ds_loss: 0.0000 | lr: 2.8398e-05 | scale: 16384.0000 | micro time: 1.785 | step time: 0.000
train | epoch   4 | Iter:   5966/ 13000 | global iter:   5966/ 13000 | loss: 1.7597 | ds_loss: 0.0000 | lr: 2.8392e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   4 | Iter:   5967/ 13000 | global iter:   5967/ 13000 | loss: 1.0721 | ds_loss: 0.0000 | lr: 2.8386e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   4 | Iter:   5968/ 13000 | global iter:   5968/ 13000 | loss: 1.8769 | ds_loss: 0.0000 | lr: 2.8380e-05 | scale: 16384.0000 | micro time: 1.842 | step time: 0.000
train | epoch   4 | Iter:   5969/ 13000 | global iter:   5969/ 13000 | loss: 1.5263 | ds_loss: 0.0000 | lr: 2.8374e-05 | scale: 16384.0000 | micro time: 1.834 | step time: 0.000
train | epoch   4 | Iter:   5970/ 13000 | global iter:   5970/ 13000 | loss: 0.9927 | ds_loss: 0.0000 | lr: 2.8368e-05 | scale: 16384.0000 | micro time: 1.773 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5970/ 13000 | global iter:   5970/ 13000 | loss: 1.4189 | ds_loss: 0.0000 | lr: 2.8368e-05 | scale: 16384.0000 | micro time: 1.773 | step time: 1.790
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5971/ 13000 | global iter:   5971/ 13000 | loss: 0.8182 | ds_loss: 0.0000 | lr: 2.8362e-05 | scale: 16384.0000 | micro time: 1.707 | step time: 0.000
train | epoch   4 | Iter:   5972/ 13000 | global iter:   5972/ 13000 | loss: 1.6247 | ds_loss: 0.0000 | lr: 2.8356e-05 | scale: 16384.0000 | micro time: 1.864 | step time: 0.000
train | epoch   4 | Iter:   5973/ 13000 | global iter:   5973/ 13000 | loss: 1.0775 | ds_loss: 0.0000 | lr: 2.8350e-05 | scale: 16384.0000 | micro time: 1.695 | step time: 0.000
train | epoch   4 | Iter:   5974/ 13000 | global iter:   5974/ 13000 | loss: 1.1370 | ds_loss: 0.0000 | lr: 2.8344e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   4 | Iter:   5975/ 13000 | global iter:   5975/ 13000 | loss: 1.8419 | ds_loss: 0.0000 | lr: 2.8339e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   4 | Iter:   5976/ 13000 | global iter:   5976/ 13000 | loss: 1.3840 | ds_loss: 0.0000 | lr: 2.8333e-05 | scale: 16384.0000 | micro time: 1.879 | step time: 0.000
train | epoch   4 | Iter:   5977/ 13000 | global iter:   5977/ 13000 | loss: 1.3947 | ds_loss: 0.0000 | lr: 2.8327e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   4 | Iter:   5978/ 13000 | global iter:   5978/ 13000 | loss: 1.0957 | ds_loss: 0.0000 | lr: 2.8321e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   4 | Iter:   5979/ 13000 | global iter:   5979/ 13000 | loss: 1.6972 | ds_loss: 0.0000 | lr: 2.8315e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   4 | Iter:   5980/ 13000 | global iter:   5980/ 13000 | loss: 0.5356 | ds_loss: 0.0000 | lr: 2.8309e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5980/ 13000 | global iter:   5980/ 13000 | loss: 1.2606 | ds_loss: 0.0000 | lr: 2.8309e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 1.795
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5981/ 13000 | global iter:   5981/ 13000 | loss: 1.3850 | ds_loss: 0.0000 | lr: 2.8303e-05 | scale: 16384.0000 | micro time: 1.778 | step time: 0.000
train | epoch   4 | Iter:   5982/ 13000 | global iter:   5982/ 13000 | loss: 1.6106 | ds_loss: 0.0000 | lr: 2.8297e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   4 | Iter:   5983/ 13000 | global iter:   5983/ 13000 | loss: 1.8532 | ds_loss: 0.0000 | lr: 2.8291e-05 | scale: 16384.0000 | micro time: 1.812 | step time: 0.000
train | epoch   4 | Iter:   5984/ 13000 | global iter:   5984/ 13000 | loss: 1.4222 | ds_loss: 0.0000 | lr: 2.8285e-05 | scale: 16384.0000 | micro time: 1.903 | step time: 0.000
train | epoch   4 | Iter:   5985/ 13000 | global iter:   5985/ 13000 | loss: 1.0881 | ds_loss: 0.0000 | lr: 2.8279e-05 | scale: 16384.0000 | micro time: 1.748 | step time: 0.000
train | epoch   4 | Iter:   5986/ 13000 | global iter:   5986/ 13000 | loss: 0.9625 | ds_loss: 0.0000 | lr: 2.8273e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   4 | Iter:   5987/ 13000 | global iter:   5987/ 13000 | loss: 0.9721 | ds_loss: 0.0000 | lr: 2.8267e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   4 | Iter:   5988/ 13000 | global iter:   5988/ 13000 | loss: 1.1850 | ds_loss: 0.0000 | lr: 2.8261e-05 | scale: 16384.0000 | micro time: 1.891 | step time: 0.000
train | epoch   4 | Iter:   5989/ 13000 | global iter:   5989/ 13000 | loss: 1.7504 | ds_loss: 0.0000 | lr: 2.8255e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   4 | Iter:   5990/ 13000 | global iter:   5990/ 13000 | loss: 0.5963 | ds_loss: 0.0000 | lr: 2.8249e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   5990/ 13000 | global iter:   5990/ 13000 | loss: 1.2825 | ds_loss: 0.0000 | lr: 2.8249e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 1.814
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   5991/ 13000 | global iter:   5991/ 13000 | loss: 1.7455 | ds_loss: 0.0000 | lr: 2.8243e-05 | scale: 16384.0000 | micro time: 1.884 | step time: 0.000
train | epoch   4 | Iter:   5992/ 13000 | global iter:   5992/ 13000 | loss: 1.3629 | ds_loss: 0.0000 | lr: 2.8237e-05 | scale: 16384.0000 | micro time: 1.804 | step time: 0.000
train | epoch   4 | Iter:   5993/ 13000 | global iter:   5993/ 13000 | loss: 1.5569 | ds_loss: 0.0000 | lr: 2.8231e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   4 | Iter:   5994/ 13000 | global iter:   5994/ 13000 | loss: 1.4327 | ds_loss: 0.0000 | lr: 2.8225e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   4 | Iter:   5995/ 13000 | global iter:   5995/ 13000 | loss: 1.6621 | ds_loss: 0.0000 | lr: 2.8219e-05 | scale: 16384.0000 | micro time: 1.800 | step time: 0.000
train | epoch   4 | Iter:   5996/ 13000 | global iter:   5996/ 13000 | loss: 1.6398 | ds_loss: 0.0000 | lr: 2.8213e-05 | scale: 16384.0000 | micro time: 1.782 | step time: 0.000
train | epoch   4 | Iter:   5997/ 13000 | global iter:   5997/ 13000 | loss: 1.5152 | ds_loss: 0.0000 | lr: 2.8207e-05 | scale: 16384.0000 | micro time: 1.833 | step time: 0.000
train | epoch   4 | Iter:   5998/ 13000 | global iter:   5998/ 13000 | loss: 1.3432 | ds_loss: 0.0000 | lr: 2.8201e-05 | scale: 16384.0000 | micro time: 1.845 | step time: 0.000
train | epoch   4 | Iter:   5999/ 13000 | global iter:   5999/ 13000 | loss: 1.1901 | ds_loss: 0.0000 | lr: 2.8195e-05 | scale: 16384.0000 | micro time: 1.792 | step time: 0.000
train | epoch   4 | Iter:   6000/ 13000 | global iter:   6000/ 13000 | loss: 1.4331 | ds_loss: 0.0000 | lr: 2.8189e-05 | scale: 16384.0000 | micro time: 1.822 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6000/ 13000 | global iter:   6000/ 13000 | loss: 1.4881 | ds_loss: 0.0000 | lr: 2.8189e-05 | scale: 16384.0000 | micro time: 1.822 | step time: 1.820
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6001/ 13000 | global iter:   6001/ 13000 | loss: 1.1155 | ds_loss: 0.0000 | lr: 2.8183e-05 | scale: 16384.0000 | micro time: 1.786 | step time: 0.000
train | epoch   4 | Iter:   6002/ 13000 | global iter:   6002/ 13000 | loss: 2.0214 | ds_loss: 0.0000 | lr: 2.8177e-05 | scale: 16384.0000 | micro time: 1.809 | step time: 0.000
train | epoch   4 | Iter:   6003/ 13000 | global iter:   6003/ 13000 | loss: 1.4520 | ds_loss: 0.0000 | lr: 2.8171e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   4 | Iter:   6004/ 13000 | global iter:   6004/ 13000 | loss: 1.4714 | ds_loss: 0.0000 | lr: 2.8165e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   4 | Iter:   6005/ 13000 | global iter:   6005/ 13000 | loss: 1.8275 | ds_loss: 0.0000 | lr: 2.8159e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   4 | Iter:   6006/ 13000 | global iter:   6006/ 13000 | loss: 0.9705 | ds_loss: 0.0000 | lr: 2.8153e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   4 | Iter:   6007/ 13000 | global iter:   6007/ 13000 | loss: 1.0854 | ds_loss: 0.0000 | lr: 2.8147e-05 | scale: 16384.0000 | micro time: 1.891 | step time: 0.000
train | epoch   4 | Iter:   6008/ 13000 | global iter:   6008/ 13000 | loss: 1.6606 | ds_loss: 0.0000 | lr: 2.8141e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   4 | Iter:   6009/ 13000 | global iter:   6009/ 13000 | loss: 1.6821 | ds_loss: 0.0000 | lr: 2.8135e-05 | scale: 16384.0000 | micro time: 1.793 | step time: 0.000
train | epoch   4 | Iter:   6010/ 13000 | global iter:   6010/ 13000 | loss: 1.5871 | ds_loss: 0.0000 | lr: 2.8129e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6010/ 13000 | global iter:   6010/ 13000 | loss: 1.4874 | ds_loss: 0.0000 | lr: 2.8129e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 1.830
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6011/ 13000 | global iter:   6011/ 13000 | loss: 1.0348 | ds_loss: 0.0000 | lr: 2.8123e-05 | scale: 16384.0000 | micro time: 1.810 | step time: 0.000
train | epoch   4 | Iter:   6012/ 13000 | global iter:   6012/ 13000 | loss: 0.9402 | ds_loss: 0.0000 | lr: 2.8117e-05 | scale: 16384.0000 | micro time: 1.842 | step time: 0.000
train | epoch   4 | Iter:   6013/ 13000 | global iter:   6013/ 13000 | loss: 1.4739 | ds_loss: 0.0000 | lr: 2.8111e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   4 | Iter:   6014/ 13000 | global iter:   6014/ 13000 | loss: 1.1543 | ds_loss: 0.0000 | lr: 2.8105e-05 | scale: 16384.0000 | micro time: 1.679 | step time: 0.000
train | epoch   4 | Iter:   6015/ 13000 | global iter:   6015/ 13000 | loss: 1.6226 | ds_loss: 0.0000 | lr: 2.8099e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   4 | Iter:   6016/ 13000 | global iter:   6016/ 13000 | loss: 0.7122 | ds_loss: 0.0000 | lr: 2.8093e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   4 | Iter:   6017/ 13000 | global iter:   6017/ 13000 | loss: 1.0999 | ds_loss: 0.0000 | lr: 2.8087e-05 | scale: 16384.0000 | micro time: 1.746 | step time: 0.000
train | epoch   4 | Iter:   6018/ 13000 | global iter:   6018/ 13000 | loss: 1.7417 | ds_loss: 0.0000 | lr: 2.8081e-05 | scale: 16384.0000 | micro time: 1.884 | step time: 0.000
train | epoch   4 | Iter:   6019/ 13000 | global iter:   6019/ 13000 | loss: 1.3580 | ds_loss: 0.0000 | lr: 2.8075e-05 | scale: 16384.0000 | micro time: 1.935 | step time: 0.000
train | epoch   4 | Iter:   6020/ 13000 | global iter:   6020/ 13000 | loss: 1.0772 | ds_loss: 0.0000 | lr: 2.8069e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6020/ 13000 | global iter:   6020/ 13000 | loss: 1.2215 | ds_loss: 0.0000 | lr: 2.8069e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 1.820
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6021/ 13000 | global iter:   6021/ 13000 | loss: 1.5824 | ds_loss: 0.0000 | lr: 2.8063e-05 | scale: 16384.0000 | micro time: 1.790 | step time: 0.000
train | epoch   4 | Iter:   6022/ 13000 | global iter:   6022/ 13000 | loss: 1.5096 | ds_loss: 0.0000 | lr: 2.8057e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   4 | Iter:   6023/ 13000 | global iter:   6023/ 13000 | loss: 1.7491 | ds_loss: 0.0000 | lr: 2.8051e-05 | scale: 16384.0000 | micro time: 1.838 | step time: 0.000
train | epoch   4 | Iter:   6024/ 13000 | global iter:   6024/ 13000 | loss: 1.3923 | ds_loss: 0.0000 | lr: 2.8045e-05 | scale: 16384.0000 | micro time: 1.864 | step time: 0.000
train | epoch   4 | Iter:   6025/ 13000 | global iter:   6025/ 13000 | loss: 1.6192 | ds_loss: 0.0000 | lr: 2.8039e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   4 | Iter:   6026/ 13000 | global iter:   6026/ 13000 | loss: 1.3253 | ds_loss: 0.0000 | lr: 2.8033e-05 | scale: 16384.0000 | micro time: 1.930 | step time: 0.000
train | epoch   4 | Iter:   6027/ 13000 | global iter:   6027/ 13000 | loss: 1.4897 | ds_loss: 0.0000 | lr: 2.8027e-05 | scale: 16384.0000 | micro time: 1.845 | step time: 0.000
train | epoch   4 | Iter:   6028/ 13000 | global iter:   6028/ 13000 | loss: 0.8795 | ds_loss: 0.0000 | lr: 2.8021e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 0.000
train | epoch   4 | Iter:   6029/ 13000 | global iter:   6029/ 13000 | loss: 1.5861 | ds_loss: 0.0000 | lr: 2.8015e-05 | scale: 16384.0000 | micro time: 1.879 | step time: 0.000
train | epoch   4 | Iter:   6030/ 13000 | global iter:   6030/ 13000 | loss: 1.2962 | ds_loss: 0.0000 | lr: 2.8010e-05 | scale: 16384.0000 | micro time: 1.887 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6030/ 13000 | global iter:   6030/ 13000 | loss: 1.4429 | ds_loss: 0.0000 | lr: 2.8010e-05 | scale: 16384.0000 | micro time: 1.887 | step time: 1.857
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6031/ 13000 | global iter:   6031/ 13000 | loss: 1.5262 | ds_loss: 0.0000 | lr: 2.8004e-05 | scale: 16384.0000 | micro time: 1.778 | step time: 0.000
train | epoch   4 | Iter:   6032/ 13000 | global iter:   6032/ 13000 | loss: 1.4720 | ds_loss: 0.0000 | lr: 2.7998e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   4 | Iter:   6033/ 13000 | global iter:   6033/ 13000 | loss: 1.8997 | ds_loss: 0.0000 | lr: 2.7992e-05 | scale: 16384.0000 | micro time: 1.731 | step time: 0.000
train | epoch   4 | Iter:   6034/ 13000 | global iter:   6034/ 13000 | loss: 1.8453 | ds_loss: 0.0000 | lr: 2.7986e-05 | scale: 16384.0000 | micro time: 1.825 | step time: 0.000
train | epoch   4 | Iter:   6035/ 13000 | global iter:   6035/ 13000 | loss: 1.5481 | ds_loss: 0.0000 | lr: 2.7980e-05 | scale: 16384.0000 | micro time: 1.877 | step time: 0.000
train | epoch   4 | Iter:   6036/ 13000 | global iter:   6036/ 13000 | loss: 1.6632 | ds_loss: 0.0000 | lr: 2.7974e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   4 | Iter:   6037/ 13000 | global iter:   6037/ 13000 | loss: 1.5376 | ds_loss: 0.0000 | lr: 2.7968e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   4 | Iter:   6038/ 13000 | global iter:   6038/ 13000 | loss: 1.0543 | ds_loss: 0.0000 | lr: 2.7962e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   4 | Iter:   6039/ 13000 | global iter:   6039/ 13000 | loss: 0.4818 | ds_loss: 0.0000 | lr: 2.7956e-05 | scale: 16384.0000 | micro time: 1.754 | step time: 0.000
train | epoch   4 | Iter:   6040/ 13000 | global iter:   6040/ 13000 | loss: 1.8080 | ds_loss: 0.0000 | lr: 2.7950e-05 | scale: 16384.0000 | micro time: 1.854 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6040/ 13000 | global iter:   6040/ 13000 | loss: 1.4836 | ds_loss: 0.0000 | lr: 2.7950e-05 | scale: 16384.0000 | micro time: 1.854 | step time: 1.813
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6041/ 13000 | global iter:   6041/ 13000 | loss: 1.9625 | ds_loss: 0.0000 | lr: 2.7944e-05 | scale: 16384.0000 | micro time: 1.782 | step time: 0.000
train | epoch   4 | Iter:   6042/ 13000 | global iter:   6042/ 13000 | loss: 0.9430 | ds_loss: 0.0000 | lr: 2.7938e-05 | scale: 16384.0000 | micro time: 1.892 | step time: 0.000
train | epoch   4 | Iter:   6043/ 13000 | global iter:   6043/ 13000 | loss: 1.5007 | ds_loss: 0.0000 | lr: 2.7932e-05 | scale: 16384.0000 | micro time: 1.804 | step time: 0.000
train | epoch   4 | Iter:   6044/ 13000 | global iter:   6044/ 13000 | loss: 0.9390 | ds_loss: 0.0000 | lr: 2.7926e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 0.000
train | epoch   4 | Iter:   6045/ 13000 | global iter:   6045/ 13000 | loss: 1.7909 | ds_loss: 0.0000 | lr: 2.7920e-05 | scale: 16384.0000 | micro time: 1.860 | step time: 0.000
train | epoch   4 | Iter:   6046/ 13000 | global iter:   6046/ 13000 | loss: 1.0597 | ds_loss: 0.0000 | lr: 2.7914e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   4 | Iter:   6047/ 13000 | global iter:   6047/ 13000 | loss: 1.3335 | ds_loss: 0.0000 | lr: 2.7908e-05 | scale: 16384.0000 | micro time: 1.671 | step time: 0.000
train | epoch   4 | Iter:   6048/ 13000 | global iter:   6048/ 13000 | loss: 1.3678 | ds_loss: 0.0000 | lr: 2.7902e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   4 | Iter:   6049/ 13000 | global iter:   6049/ 13000 | loss: 1.5525 | ds_loss: 0.0000 | lr: 2.7896e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   4 | Iter:   6050/ 13000 | global iter:   6050/ 13000 | loss: 0.8386 | ds_loss: 0.0000 | lr: 2.7890e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6050/ 13000 | global iter:   6050/ 13000 | loss: 1.3288 | ds_loss: 0.0000 | lr: 2.7890e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 1.816
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6051/ 13000 | global iter:   6051/ 13000 | loss: 1.5150 | ds_loss: 0.0000 | lr: 2.7884e-05 | scale: 16384.0000 | micro time: 1.914 | step time: 0.000
train | epoch   4 | Iter:   6052/ 13000 | global iter:   6052/ 13000 | loss: 1.4183 | ds_loss: 0.0000 | lr: 2.7878e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   4 | Iter:   6053/ 13000 | global iter:   6053/ 13000 | loss: 1.6483 | ds_loss: 0.0000 | lr: 2.7872e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   4 | Iter:   6054/ 13000 | global iter:   6054/ 13000 | loss: 1.3748 | ds_loss: 0.0000 | lr: 2.7866e-05 | scale: 16384.0000 | micro time: 1.760 | step time: 0.000
train | epoch   4 | Iter:   6055/ 13000 | global iter:   6055/ 13000 | loss: 1.2426 | ds_loss: 0.0000 | lr: 2.7860e-05 | scale: 16384.0000 | micro time: 1.770 | step time: 0.000
train | epoch   4 | Iter:   6056/ 13000 | global iter:   6056/ 13000 | loss: 1.1055 | ds_loss: 0.0000 | lr: 2.7854e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   4 | Iter:   6057/ 13000 | global iter:   6057/ 13000 | loss: 1.3397 | ds_loss: 0.0000 | lr: 2.7848e-05 | scale: 16384.0000 | micro time: 1.707 | step time: 0.000
train | epoch   4 | Iter:   6058/ 13000 | global iter:   6058/ 13000 | loss: 1.4807 | ds_loss: 0.0000 | lr: 2.7842e-05 | scale: 16384.0000 | micro time: 1.899 | step time: 0.000
train | epoch   4 | Iter:   6059/ 13000 | global iter:   6059/ 13000 | loss: 1.0935 | ds_loss: 0.0000 | lr: 2.7836e-05 | scale: 16384.0000 | micro time: 1.735 | step time: 0.000
train | epoch   4 | Iter:   6060/ 13000 | global iter:   6060/ 13000 | loss: 1.5241 | ds_loss: 0.0000 | lr: 2.7830e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6060/ 13000 | global iter:   6060/ 13000 | loss: 1.3742 | ds_loss: 0.0000 | lr: 2.7830e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 1.799
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6061/ 13000 | global iter:   6061/ 13000 | loss: 1.4607 | ds_loss: 0.0000 | lr: 2.7824e-05 | scale: 16384.0000 | micro time: 1.756 | step time: 0.000
train | epoch   4 | Iter:   6062/ 13000 | global iter:   6062/ 13000 | loss: 1.7190 | ds_loss: 0.0000 | lr: 2.7818e-05 | scale: 16384.0000 | micro time: 1.808 | step time: 0.000
train | epoch   4 | Iter:   6063/ 13000 | global iter:   6063/ 13000 | loss: 0.7759 | ds_loss: 0.0000 | lr: 2.7812e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   4 | Iter:   6064/ 13000 | global iter:   6064/ 13000 | loss: 1.3536 | ds_loss: 0.0000 | lr: 2.7806e-05 | scale: 16384.0000 | micro time: 1.869 | step time: 0.000
train | epoch   4 | Iter:   6065/ 13000 | global iter:   6065/ 13000 | loss: 1.9265 | ds_loss: 0.0000 | lr: 2.7800e-05 | scale: 16384.0000 | micro time: 1.663 | step time: 0.000
train | epoch   4 | Iter:   6066/ 13000 | global iter:   6066/ 13000 | loss: 1.6933 | ds_loss: 0.0000 | lr: 2.7794e-05 | scale: 16384.0000 | micro time: 1.824 | step time: 0.000
train | epoch   4 | Iter:   6067/ 13000 | global iter:   6067/ 13000 | loss: 1.3302 | ds_loss: 0.0000 | lr: 2.7788e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   4 | Iter:   6068/ 13000 | global iter:   6068/ 13000 | loss: 1.5916 | ds_loss: 0.0000 | lr: 2.7782e-05 | scale: 16384.0000 | micro time: 1.854 | step time: 0.000
train | epoch   4 | Iter:   6069/ 13000 | global iter:   6069/ 13000 | loss: 1.2943 | ds_loss: 0.0000 | lr: 2.7776e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   4 | Iter:   6070/ 13000 | global iter:   6070/ 13000 | loss: 1.4120 | ds_loss: 0.0000 | lr: 2.7770e-05 | scale: 16384.0000 | micro time: 1.756 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6070/ 13000 | global iter:   6070/ 13000 | loss: 1.4557 | ds_loss: 0.0000 | lr: 2.7770e-05 | scale: 16384.0000 | micro time: 1.756 | step time: 1.798
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6071/ 13000 | global iter:   6071/ 13000 | loss: 1.7514 | ds_loss: 0.0000 | lr: 2.7764e-05 | scale: 16384.0000 | micro time: 1.735 | step time: 0.000
train | epoch   4 | Iter:   6072/ 13000 | global iter:   6072/ 13000 | loss: 0.9849 | ds_loss: 0.0000 | lr: 2.7758e-05 | scale: 16384.0000 | micro time: 1.903 | step time: 0.000
train | epoch   4 | Iter:   6073/ 13000 | global iter:   6073/ 13000 | loss: 1.4490 | ds_loss: 0.0000 | lr: 2.7752e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   4 | Iter:   6074/ 13000 | global iter:   6074/ 13000 | loss: 1.3910 | ds_loss: 0.0000 | lr: 2.7746e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   4 | Iter:   6075/ 13000 | global iter:   6075/ 13000 | loss: 1.5608 | ds_loss: 0.0000 | lr: 2.7740e-05 | scale: 16384.0000 | micro time: 1.768 | step time: 0.000
train | epoch   4 | Iter:   6076/ 13000 | global iter:   6076/ 13000 | loss: 1.2934 | ds_loss: 0.0000 | lr: 2.7734e-05 | scale: 16384.0000 | micro time: 1.858 | step time: 0.000
train | epoch   4 | Iter:   6077/ 13000 | global iter:   6077/ 13000 | loss: 1.4872 | ds_loss: 0.0000 | lr: 2.7728e-05 | scale: 16384.0000 | micro time: 1.853 | step time: 0.000
train | epoch   4 | Iter:   6078/ 13000 | global iter:   6078/ 13000 | loss: 1.4664 | ds_loss: 0.0000 | lr: 2.7722e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   4 | Iter:   6079/ 13000 | global iter:   6079/ 13000 | loss: 1.2346 | ds_loss: 0.0000 | lr: 2.7716e-05 | scale: 16384.0000 | micro time: 1.865 | step time: 0.000
train | epoch   4 | Iter:   6080/ 13000 | global iter:   6080/ 13000 | loss: 1.5675 | ds_loss: 0.0000 | lr: 2.7710e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6080/ 13000 | global iter:   6080/ 13000 | loss: 1.4186 | ds_loss: 0.0000 | lr: 2.7710e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 1.824
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6081/ 13000 | global iter:   6081/ 13000 | loss: 1.2134 | ds_loss: 0.0000 | lr: 2.7704e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   4 | Iter:   6082/ 13000 | global iter:   6082/ 13000 | loss: 1.3641 | ds_loss: 0.0000 | lr: 2.7698e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   4 | Iter:   6083/ 13000 | global iter:   6083/ 13000 | loss: 1.4182 | ds_loss: 0.0000 | lr: 2.7692e-05 | scale: 16384.0000 | micro time: 1.810 | step time: 0.000
train | epoch   4 | Iter:   6084/ 13000 | global iter:   6084/ 13000 | loss: 1.3283 | ds_loss: 0.0000 | lr: 2.7686e-05 | scale: 16384.0000 | micro time: 1.852 | step time: 0.000
train | epoch   4 | Iter:   6085/ 13000 | global iter:   6085/ 13000 | loss: 1.3666 | ds_loss: 0.0000 | lr: 2.7680e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   4 | Iter:   6086/ 13000 | global iter:   6086/ 13000 | loss: 1.5526 | ds_loss: 0.0000 | lr: 2.7674e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   4 | Iter:   6087/ 13000 | global iter:   6087/ 13000 | loss: 1.0508 | ds_loss: 0.0000 | lr: 2.7668e-05 | scale: 16384.0000 | micro time: 1.938 | step time: 0.000
train | epoch   4 | Iter:   6088/ 13000 | global iter:   6088/ 13000 | loss: 1.6890 | ds_loss: 0.0000 | lr: 2.7662e-05 | scale: 16384.0000 | micro time: 1.891 | step time: 0.000
train | epoch   4 | Iter:   6089/ 13000 | global iter:   6089/ 13000 | loss: 1.0804 | ds_loss: 0.0000 | lr: 2.7656e-05 | scale: 16384.0000 | micro time: 1.812 | step time: 0.000
train | epoch   4 | Iter:   6090/ 13000 | global iter:   6090/ 13000 | loss: 1.0246 | ds_loss: 0.0000 | lr: 2.7650e-05 | scale: 16384.0000 | micro time: 1.742 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6090/ 13000 | global iter:   6090/ 13000 | loss: 1.3088 | ds_loss: 0.0000 | lr: 2.7650e-05 | scale: 16384.0000 | micro time: 1.742 | step time: 1.828
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6091/ 13000 | global iter:   6091/ 13000 | loss: 1.5534 | ds_loss: 0.0000 | lr: 2.7644e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   4 | Iter:   6092/ 13000 | global iter:   6092/ 13000 | loss: 1.2750 | ds_loss: 0.0000 | lr: 2.7638e-05 | scale: 16384.0000 | micro time: 1.762 | step time: 0.000
train | epoch   4 | Iter:   6093/ 13000 | global iter:   6093/ 13000 | loss: 1.5918 | ds_loss: 0.0000 | lr: 2.7632e-05 | scale: 16384.0000 | micro time: 1.655 | step time: 0.000
train | epoch   4 | Iter:   6094/ 13000 | global iter:   6094/ 13000 | loss: 1.5648 | ds_loss: 0.0000 | lr: 2.7626e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   4 | Iter:   6095/ 13000 | global iter:   6095/ 13000 | loss: 1.3503 | ds_loss: 0.0000 | lr: 2.7620e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   4 | Iter:   6096/ 13000 | global iter:   6096/ 13000 | loss: 1.5057 | ds_loss: 0.0000 | lr: 2.7614e-05 | scale: 16384.0000 | micro time: 1.675 | step time: 0.000
train | epoch   4 | Iter:   6097/ 13000 | global iter:   6097/ 13000 | loss: 1.3240 | ds_loss: 0.0000 | lr: 2.7608e-05 | scale: 16384.0000 | micro time: 1.850 | step time: 0.000
train | epoch   4 | Iter:   6098/ 13000 | global iter:   6098/ 13000 | loss: 0.9584 | ds_loss: 0.0000 | lr: 2.7602e-05 | scale: 16384.0000 | micro time: 1.868 | step time: 0.000
train | epoch   4 | Iter:   6099/ 13000 | global iter:   6099/ 13000 | loss: 1.3898 | ds_loss: 0.0000 | lr: 2.7596e-05 | scale: 16384.0000 | micro time: 1.691 | step time: 0.000
train | epoch   4 | Iter:   6100/ 13000 | global iter:   6100/ 13000 | loss: 0.9016 | ds_loss: 0.0000 | lr: 2.7590e-05 | scale: 16384.0000 | micro time: 1.870 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6100/ 13000 | global iter:   6100/ 13000 | loss: 1.3415 | ds_loss: 0.0000 | lr: 2.7590e-05 | scale: 16384.0000 | micro time: 1.870 | step time: 1.780
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6101/ 13000 | global iter:   6101/ 13000 | loss: 1.4619 | ds_loss: 0.0000 | lr: 2.7584e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   4 | Iter:   6102/ 13000 | global iter:   6102/ 13000 | loss: 1.8437 | ds_loss: 0.0000 | lr: 2.7578e-05 | scale: 16384.0000 | micro time: 1.914 | step time: 0.000
train | epoch   4 | Iter:   6103/ 13000 | global iter:   6103/ 13000 | loss: 1.3980 | ds_loss: 0.0000 | lr: 2.7572e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   4 | Iter:   6104/ 13000 | global iter:   6104/ 13000 | loss: 0.7585 | ds_loss: 0.0000 | lr: 2.7566e-05 | scale: 16384.0000 | micro time: 1.891 | step time: 0.000
train | epoch   4 | Iter:   6105/ 13000 | global iter:   6105/ 13000 | loss: 1.3545 | ds_loss: 0.0000 | lr: 2.7560e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 0.000
train | epoch   4 | Iter:   6106/ 13000 | global iter:   6106/ 13000 | loss: 1.7810 | ds_loss: 0.0000 | lr: 2.7554e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   4 | Iter:   6107/ 13000 | global iter:   6107/ 13000 | loss: 1.6867 | ds_loss: 0.0000 | lr: 2.7548e-05 | scale: 16384.0000 | micro time: 1.789 | step time: 0.000
train | epoch   4 | Iter:   6108/ 13000 | global iter:   6108/ 13000 | loss: 1.3476 | ds_loss: 0.0000 | lr: 2.7542e-05 | scale: 16384.0000 | micro time: 1.765 | step time: 0.000
train | epoch   4 | Iter:   6109/ 13000 | global iter:   6109/ 13000 | loss: 1.7893 | ds_loss: 0.0000 | lr: 2.7536e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   4 | Iter:   6110/ 13000 | global iter:   6110/ 13000 | loss: 0.9537 | ds_loss: 0.0000 | lr: 2.7530e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6110/ 13000 | global iter:   6110/ 13000 | loss: 1.4375 | ds_loss: 0.0000 | lr: 2.7530e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 1.838
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6111/ 13000 | global iter:   6111/ 13000 | loss: 1.2010 | ds_loss: 0.0000 | lr: 2.7524e-05 | scale: 16384.0000 | micro time: 1.770 | step time: 0.000
train | epoch   4 | Iter:   6112/ 13000 | global iter:   6112/ 13000 | loss: 1.2302 | ds_loss: 0.0000 | lr: 2.7518e-05 | scale: 16384.0000 | micro time: 1.671 | step time: 0.000
train | epoch   4 | Iter:   6113/ 13000 | global iter:   6113/ 13000 | loss: 1.2741 | ds_loss: 0.0000 | lr: 2.7512e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   4 | Iter:   6114/ 13000 | global iter:   6114/ 13000 | loss: 1.2376 | ds_loss: 0.0000 | lr: 2.7506e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   4 | Iter:   6115/ 13000 | global iter:   6115/ 13000 | loss: 0.9680 | ds_loss: 0.0000 | lr: 2.7500e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   4 | Iter:   6116/ 13000 | global iter:   6116/ 13000 | loss: 0.6572 | ds_loss: 0.0000 | lr: 2.7494e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   4 | Iter:   6117/ 13000 | global iter:   6117/ 13000 | loss: 1.5761 | ds_loss: 0.0000 | lr: 2.7488e-05 | scale: 16384.0000 | micro time: 1.735 | step time: 0.000
train | epoch   4 | Iter:   6118/ 13000 | global iter:   6118/ 13000 | loss: 1.2808 | ds_loss: 0.0000 | lr: 2.7482e-05 | scale: 16384.0000 | micro time: 1.788 | step time: 0.000
train | epoch   4 | Iter:   6119/ 13000 | global iter:   6119/ 13000 | loss: 1.1914 | ds_loss: 0.0000 | lr: 2.7476e-05 | scale: 16384.0000 | micro time: 1.746 | step time: 0.000
train | epoch   4 | Iter:   6120/ 13000 | global iter:   6120/ 13000 | loss: 1.0182 | ds_loss: 0.0000 | lr: 2.7470e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6120/ 13000 | global iter:   6120/ 13000 | loss: 1.1635 | ds_loss: 0.0000 | lr: 2.7470e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 1.776
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6121/ 13000 | global iter:   6121/ 13000 | loss: 1.6412 | ds_loss: 0.0000 | lr: 2.7464e-05 | scale: 16384.0000 | micro time: 1.881 | step time: 0.000
train | epoch   4 | Iter:   6122/ 13000 | global iter:   6122/ 13000 | loss: 1.1797 | ds_loss: 0.0000 | lr: 2.7458e-05 | scale: 16384.0000 | micro time: 1.731 | step time: 0.000
train | epoch   4 | Iter:   6123/ 13000 | global iter:   6123/ 13000 | loss: 1.1645 | ds_loss: 0.0000 | lr: 2.7452e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   4 | Iter:   6124/ 13000 | global iter:   6124/ 13000 | loss: 1.8356 | ds_loss: 0.0000 | lr: 2.7446e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   4 | Iter:   6125/ 13000 | global iter:   6125/ 13000 | loss: 1.6659 | ds_loss: 0.0000 | lr: 2.7440e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   4 | Iter:   6126/ 13000 | global iter:   6126/ 13000 | loss: 1.7080 | ds_loss: 0.0000 | lr: 2.7434e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   4 | Iter:   6127/ 13000 | global iter:   6127/ 13000 | loss: 0.7046 | ds_loss: 0.0000 | lr: 2.7428e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   4 | Iter:   6128/ 13000 | global iter:   6128/ 13000 | loss: 1.9388 | ds_loss: 0.0000 | lr: 2.7422e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   4 | Iter:   6129/ 13000 | global iter:   6129/ 13000 | loss: 0.6898 | ds_loss: 0.0000 | lr: 2.7416e-05 | scale: 16384.0000 | micro time: 1.727 | step time: 0.000
train | epoch   4 | Iter:   6130/ 13000 | global iter:   6130/ 13000 | loss: 1.8484 | ds_loss: 0.0000 | lr: 2.7410e-05 | scale: 16384.0000 | micro time: 1.690 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6130/ 13000 | global iter:   6130/ 13000 | loss: 1.4376 | ds_loss: 0.0000 | lr: 2.7410e-05 | scale: 16384.0000 | micro time: 1.690 | step time: 1.800
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6131/ 13000 | global iter:   6131/ 13000 | loss: 1.4380 | ds_loss: 0.0000 | lr: 2.7404e-05 | scale: 16384.0000 | micro time: 1.797 | step time: 0.000
train | epoch   4 | Iter:   6132/ 13000 | global iter:   6132/ 13000 | loss: 1.4576 | ds_loss: 0.0000 | lr: 2.7398e-05 | scale: 16384.0000 | micro time: 1.659 | step time: 0.000
train | epoch   4 | Iter:   6133/ 13000 | global iter:   6133/ 13000 | loss: 1.5517 | ds_loss: 0.0000 | lr: 2.7392e-05 | scale: 16384.0000 | micro time: 1.696 | step time: 0.000
train | epoch   4 | Iter:   6134/ 13000 | global iter:   6134/ 13000 | loss: 1.6242 | ds_loss: 0.0000 | lr: 2.7386e-05 | scale: 16384.0000 | micro time: 1.760 | step time: 0.000
train | epoch   4 | Iter:   6135/ 13000 | global iter:   6135/ 13000 | loss: 1.6846 | ds_loss: 0.0000 | lr: 2.7380e-05 | scale: 16384.0000 | micro time: 1.911 | step time: 0.000
train | epoch   4 | Iter:   6136/ 13000 | global iter:   6136/ 13000 | loss: 1.6908 | ds_loss: 0.0000 | lr: 2.7374e-05 | scale: 16384.0000 | micro time: 1.719 | step time: 0.000
train | epoch   4 | Iter:   6137/ 13000 | global iter:   6137/ 13000 | loss: 1.5667 | ds_loss: 0.0000 | lr: 2.7368e-05 | scale: 16384.0000 | micro time: 1.804 | step time: 0.000
train | epoch   4 | Iter:   6138/ 13000 | global iter:   6138/ 13000 | loss: 1.4693 | ds_loss: 0.0000 | lr: 2.7362e-05 | scale: 16384.0000 | micro time: 1.728 | step time: 0.000
train | epoch   4 | Iter:   6139/ 13000 | global iter:   6139/ 13000 | loss: 1.3292 | ds_loss: 0.0000 | lr: 2.7356e-05 | scale: 16384.0000 | micro time: 1.757 | step time: 0.000
train | epoch   4 | Iter:   6140/ 13000 | global iter:   6140/ 13000 | loss: 1.5378 | ds_loss: 0.0000 | lr: 2.7350e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6140/ 13000 | global iter:   6140/ 13000 | loss: 1.5350 | ds_loss: 0.0000 | lr: 2.7350e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 1.769
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6141/ 13000 | global iter:   6141/ 13000 | loss: 1.1379 | ds_loss: 0.0000 | lr: 2.7344e-05 | scale: 16384.0000 | micro time: 1.770 | step time: 0.000
train | epoch   4 | Iter:   6142/ 13000 | global iter:   6142/ 13000 | loss: 0.6049 | ds_loss: 0.0000 | lr: 2.7338e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   4 | Iter:   6143/ 13000 | global iter:   6143/ 13000 | loss: 1.7808 | ds_loss: 0.0000 | lr: 2.7332e-05 | scale: 16384.0000 | micro time: 1.759 | step time: 0.000
train | epoch   4 | Iter:   6144/ 13000 | global iter:   6144/ 13000 | loss: 0.9157 | ds_loss: 0.0000 | lr: 2.7326e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   4 | Iter:   6145/ 13000 | global iter:   6145/ 13000 | loss: 0.9645 | ds_loss: 0.0000 | lr: 2.7320e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   4 | Iter:   6146/ 13000 | global iter:   6146/ 13000 | loss: 1.4591 | ds_loss: 0.0000 | lr: 2.7314e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   4 | Iter:   6147/ 13000 | global iter:   6147/ 13000 | loss: 1.8675 | ds_loss: 0.0000 | lr: 2.7308e-05 | scale: 16384.0000 | micro time: 1.756 | step time: 0.000
train | epoch   4 | Iter:   6148/ 13000 | global iter:   6148/ 13000 | loss: 1.3264 | ds_loss: 0.0000 | lr: 2.7302e-05 | scale: 16384.0000 | micro time: 1.770 | step time: 0.000
train | epoch   4 | Iter:   6149/ 13000 | global iter:   6149/ 13000 | loss: 1.5282 | ds_loss: 0.0000 | lr: 2.7296e-05 | scale: 16384.0000 | micro time: 1.804 | step time: 0.000
train | epoch   4 | Iter:   6150/ 13000 | global iter:   6150/ 13000 | loss: 1.2069 | ds_loss: 0.0000 | lr: 2.7290e-05 | scale: 16384.0000 | micro time: 1.830 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6150/ 13000 | global iter:   6150/ 13000 | loss: 1.2792 | ds_loss: 0.0000 | lr: 2.7290e-05 | scale: 16384.0000 | micro time: 1.830 | step time: 1.793
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6151/ 13000 | global iter:   6151/ 13000 | loss: 0.9674 | ds_loss: 0.0000 | lr: 2.7284e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
train | epoch   4 | Iter:   6152/ 13000 | global iter:   6152/ 13000 | loss: 1.9065 | ds_loss: 0.0000 | lr: 2.7278e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 0.000
train | epoch   4 | Iter:   6153/ 13000 | global iter:   6153/ 13000 | loss: 1.3847 | ds_loss: 0.0000 | lr: 2.7272e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   4 | Iter:   6154/ 13000 | global iter:   6154/ 13000 | loss: 0.5989 | ds_loss: 0.0000 | lr: 2.7266e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   4 | Iter:   6155/ 13000 | global iter:   6155/ 13000 | loss: 1.6086 | ds_loss: 0.0000 | lr: 2.7260e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   4 | Iter:   6156/ 13000 | global iter:   6156/ 13000 | loss: 1.1697 | ds_loss: 0.0000 | lr: 2.7254e-05 | scale: 16384.0000 | micro time: 1.845 | step time: 0.000
train | epoch   4 | Iter:   6157/ 13000 | global iter:   6157/ 13000 | loss: 1.3522 | ds_loss: 0.0000 | lr: 2.7248e-05 | scale: 16384.0000 | micro time: 1.788 | step time: 0.000
train | epoch   4 | Iter:   6158/ 13000 | global iter:   6158/ 13000 | loss: 1.1605 | ds_loss: 0.0000 | lr: 2.7242e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   4 | Iter:   6159/ 13000 | global iter:   6159/ 13000 | loss: 1.5472 | ds_loss: 0.0000 | lr: 2.7236e-05 | scale: 16384.0000 | micro time: 1.663 | step time: 0.000
train | epoch   4 | Iter:   6160/ 13000 | global iter:   6160/ 13000 | loss: 1.5691 | ds_loss: 0.0000 | lr: 2.7230e-05 | scale: 16384.0000 | micro time: 1.661 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6160/ 13000 | global iter:   6160/ 13000 | loss: 1.3265 | ds_loss: 0.0000 | lr: 2.7230e-05 | scale: 16384.0000 | micro time: 1.661 | step time: 1.778
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6161/ 13000 | global iter:   6161/ 13000 | loss: 1.2626 | ds_loss: 0.0000 | lr: 2.7224e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   4 | Iter:   6162/ 13000 | global iter:   6162/ 13000 | loss: 1.5748 | ds_loss: 0.0000 | lr: 2.7218e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   4 | Iter:   6163/ 13000 | global iter:   6163/ 13000 | loss: 1.8905 | ds_loss: 0.0000 | lr: 2.7212e-05 | scale: 16384.0000 | micro time: 1.781 | step time: 0.000
train | epoch   4 | Iter:   6164/ 13000 | global iter:   6164/ 13000 | loss: 1.1808 | ds_loss: 0.0000 | lr: 2.7206e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 0.000
train | epoch   4 | Iter:   6165/ 13000 | global iter:   6165/ 13000 | loss: 1.4777 | ds_loss: 0.0000 | lr: 2.7200e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   4 | Iter:   6166/ 13000 | global iter:   6166/ 13000 | loss: 1.2595 | ds_loss: 0.0000 | lr: 2.7194e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   4 | Iter:   6167/ 13000 | global iter:   6167/ 13000 | loss: 1.4470 | ds_loss: 0.0000 | lr: 2.7188e-05 | scale: 16384.0000 | micro time: 1.918 | step time: 0.000
train | epoch   4 | Iter:   6168/ 13000 | global iter:   6168/ 13000 | loss: 1.4842 | ds_loss: 0.0000 | lr: 2.7182e-05 | scale: 16384.0000 | micro time: 1.844 | step time: 0.000
train | epoch   4 | Iter:   6169/ 13000 | global iter:   6169/ 13000 | loss: 1.3180 | ds_loss: 0.0000 | lr: 2.7176e-05 | scale: 16384.0000 | micro time: 1.659 | step time: 0.000
train | epoch   4 | Iter:   6170/ 13000 | global iter:   6170/ 13000 | loss: 1.0254 | ds_loss: 0.0000 | lr: 2.7170e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6170/ 13000 | global iter:   6170/ 13000 | loss: 1.3921 | ds_loss: 0.0000 | lr: 2.7170e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 1.817
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6171/ 13000 | global iter:   6171/ 13000 | loss: 1.4099 | ds_loss: 0.0000 | lr: 2.7164e-05 | scale: 16384.0000 | micro time: 1.850 | step time: 0.000
train | epoch   4 | Iter:   6172/ 13000 | global iter:   6172/ 13000 | loss: 1.6297 | ds_loss: 0.0000 | lr: 2.7158e-05 | scale: 16384.0000 | micro time: 1.882 | step time: 0.000
train | epoch   4 | Iter:   6173/ 13000 | global iter:   6173/ 13000 | loss: 0.9411 | ds_loss: 0.0000 | lr: 2.7152e-05 | scale: 16384.0000 | micro time: 1.899 | step time: 0.000
train | epoch   4 | Iter:   6174/ 13000 | global iter:   6174/ 13000 | loss: 1.4669 | ds_loss: 0.0000 | lr: 2.7146e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   4 | Iter:   6175/ 13000 | global iter:   6175/ 13000 | loss: 1.5459 | ds_loss: 0.0000 | lr: 2.7140e-05 | scale: 16384.0000 | micro time: 1.848 | step time: 0.000
train | epoch   4 | Iter:   6176/ 13000 | global iter:   6176/ 13000 | loss: 1.5986 | ds_loss: 0.0000 | lr: 2.7134e-05 | scale: 16384.0000 | micro time: 1.905 | step time: 0.000
train | epoch   4 | Iter:   6177/ 13000 | global iter:   6177/ 13000 | loss: 1.3910 | ds_loss: 0.0000 | lr: 2.7128e-05 | scale: 16384.0000 | micro time: 1.728 | step time: 0.000
train | epoch   4 | Iter:   6178/ 13000 | global iter:   6178/ 13000 | loss: 1.2153 | ds_loss: 0.0000 | lr: 2.7122e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   4 | Iter:   6179/ 13000 | global iter:   6179/ 13000 | loss: 1.3290 | ds_loss: 0.0000 | lr: 2.7116e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   4 | Iter:   6180/ 13000 | global iter:   6180/ 13000 | loss: 1.4607 | ds_loss: 0.0000 | lr: 2.7110e-05 | scale: 16384.0000 | micro time: 1.788 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6180/ 13000 | global iter:   6180/ 13000 | loss: 1.3988 | ds_loss: 0.0000 | lr: 2.7110e-05 | scale: 16384.0000 | micro time: 1.788 | step time: 1.828
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6181/ 13000 | global iter:   6181/ 13000 | loss: 1.4521 | ds_loss: 0.0000 | lr: 2.7104e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   4 | Iter:   6182/ 13000 | global iter:   6182/ 13000 | loss: 1.4167 | ds_loss: 0.0000 | lr: 2.7098e-05 | scale: 16384.0000 | micro time: 2.132 | step time: 0.000
train | epoch   4 | Iter:   6183/ 13000 | global iter:   6183/ 13000 | loss: 1.2447 | ds_loss: 0.0000 | lr: 2.7092e-05 | scale: 16384.0000 | micro time: 1.724 | step time: 0.000
train | epoch   4 | Iter:   6184/ 13000 | global iter:   6184/ 13000 | loss: 1.3184 | ds_loss: 0.0000 | lr: 2.7086e-05 | scale: 16384.0000 | micro time: 1.761 | step time: 0.000
train | epoch   4 | Iter:   6185/ 13000 | global iter:   6185/ 13000 | loss: 1.2586 | ds_loss: 0.0000 | lr: 2.7080e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   4 | Iter:   6186/ 13000 | global iter:   6186/ 13000 | loss: 1.6770 | ds_loss: 0.0000 | lr: 2.7074e-05 | scale: 16384.0000 | micro time: 1.832 | step time: 0.000
train | epoch   4 | Iter:   6187/ 13000 | global iter:   6187/ 13000 | loss: 1.6383 | ds_loss: 0.0000 | lr: 2.7068e-05 | scale: 16384.0000 | micro time: 1.715 | step time: 0.000
train | epoch   4 | Iter:   6188/ 13000 | global iter:   6188/ 13000 | loss: 1.4926 | ds_loss: 0.0000 | lr: 2.7062e-05 | scale: 16384.0000 | micro time: 1.862 | step time: 0.000
train | epoch   4 | Iter:   6189/ 13000 | global iter:   6189/ 13000 | loss: 1.1897 | ds_loss: 0.0000 | lr: 2.7056e-05 | scale: 16384.0000 | micro time: 1.801 | step time: 0.000
train | epoch   4 | Iter:   6190/ 13000 | global iter:   6190/ 13000 | loss: 1.7726 | ds_loss: 0.0000 | lr: 2.7050e-05 | scale: 16384.0000 | micro time: 1.848 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6190/ 13000 | global iter:   6190/ 13000 | loss: 1.4461 | ds_loss: 0.0000 | lr: 2.7050e-05 | scale: 16384.0000 | micro time: 1.848 | step time: 1.833
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6191/ 13000 | global iter:   6191/ 13000 | loss: 1.3803 | ds_loss: 0.0000 | lr: 2.7044e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   4 | Iter:   6192/ 13000 | global iter:   6192/ 13000 | loss: 0.8270 | ds_loss: 0.0000 | lr: 2.7038e-05 | scale: 16384.0000 | micro time: 1.778 | step time: 0.000
train | epoch   4 | Iter:   6193/ 13000 | global iter:   6193/ 13000 | loss: 1.7836 | ds_loss: 0.0000 | lr: 2.7032e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
train | epoch   4 | Iter:   6194/ 13000 | global iter:   6194/ 13000 | loss: 0.9280 | ds_loss: 0.0000 | lr: 2.7026e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   4 | Iter:   6195/ 13000 | global iter:   6195/ 13000 | loss: 1.4016 | ds_loss: 0.0000 | lr: 2.7020e-05 | scale: 16384.0000 | micro time: 1.907 | step time: 0.000
train | epoch   4 | Iter:   6196/ 13000 | global iter:   6196/ 13000 | loss: 1.2556 | ds_loss: 0.0000 | lr: 2.7014e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   4 | Iter:   6197/ 13000 | global iter:   6197/ 13000 | loss: 1.3721 | ds_loss: 0.0000 | lr: 2.7008e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   4 | Iter:   6198/ 13000 | global iter:   6198/ 13000 | loss: 1.5926 | ds_loss: 0.0000 | lr: 2.7002e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   4 | Iter:   6199/ 13000 | global iter:   6199/ 13000 | loss: 1.7174 | ds_loss: 0.0000 | lr: 2.6996e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   4 | Iter:   6200/ 13000 | global iter:   6200/ 13000 | loss: 1.1808 | ds_loss: 0.0000 | lr: 2.6990e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6200/ 13000 | global iter:   6200/ 13000 | loss: 1.3439 | ds_loss: 0.0000 | lr: 2.6990e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 1.811
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6201/ 13000 | global iter:   6201/ 13000 | loss: 1.2664 | ds_loss: 0.0000 | lr: 2.6984e-05 | scale: 16384.0000 | micro time: 1.860 | step time: 0.000
train | epoch   4 | Iter:   6202/ 13000 | global iter:   6202/ 13000 | loss: 1.1577 | ds_loss: 0.0000 | lr: 2.6977e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   4 | Iter:   6203/ 13000 | global iter:   6203/ 13000 | loss: 1.0529 | ds_loss: 0.0000 | lr: 2.6971e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 0.000
train | epoch   4 | Iter:   6204/ 13000 | global iter:   6204/ 13000 | loss: 1.3906 | ds_loss: 0.0000 | lr: 2.6965e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   4 | Iter:   6205/ 13000 | global iter:   6205/ 13000 | loss: 1.5726 | ds_loss: 0.0000 | lr: 2.6959e-05 | scale: 16384.0000 | micro time: 1.746 | step time: 0.000
train | epoch   4 | Iter:   6206/ 13000 | global iter:   6206/ 13000 | loss: 1.0071 | ds_loss: 0.0000 | lr: 2.6953e-05 | scale: 16384.0000 | micro time: 1.879 | step time: 0.000
train | epoch   4 | Iter:   6207/ 13000 | global iter:   6207/ 13000 | loss: 1.5867 | ds_loss: 0.0000 | lr: 2.6947e-05 | scale: 16384.0000 | micro time: 1.879 | step time: 0.000
train | epoch   4 | Iter:   6208/ 13000 | global iter:   6208/ 13000 | loss: 1.5706 | ds_loss: 0.0000 | lr: 2.6941e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   4 | Iter:   6209/ 13000 | global iter:   6209/ 13000 | loss: 1.0794 | ds_loss: 0.0000 | lr: 2.6935e-05 | scale: 16384.0000 | micro time: 1.723 | step time: 0.000
train | epoch   4 | Iter:   6210/ 13000 | global iter:   6210/ 13000 | loss: 1.3443 | ds_loss: 0.0000 | lr: 2.6929e-05 | scale: 16384.0000 | micro time: 1.806 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6210/ 13000 | global iter:   6210/ 13000 | loss: 1.3028 | ds_loss: 0.0000 | lr: 2.6929e-05 | scale: 16384.0000 | micro time: 1.806 | step time: 1.808
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6211/ 13000 | global iter:   6211/ 13000 | loss: 1.3510 | ds_loss: 0.0000 | lr: 2.6923e-05 | scale: 16384.0000 | micro time: 1.840 | step time: 0.000
train | epoch   4 | Iter:   6212/ 13000 | global iter:   6212/ 13000 | loss: 0.5469 | ds_loss: 0.0000 | lr: 2.6917e-05 | scale: 16384.0000 | micro time: 1.685 | step time: 0.000
train | epoch   4 | Iter:   6213/ 13000 | global iter:   6213/ 13000 | loss: 1.3947 | ds_loss: 0.0000 | lr: 2.6911e-05 | scale: 16384.0000 | micro time: 1.736 | step time: 0.000
train | epoch   4 | Iter:   6214/ 13000 | global iter:   6214/ 13000 | loss: 0.6864 | ds_loss: 0.0000 | lr: 2.6905e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   4 | Iter:   6215/ 13000 | global iter:   6215/ 13000 | loss: 1.3876 | ds_loss: 0.0000 | lr: 2.6899e-05 | scale: 16384.0000 | micro time: 1.671 | step time: 0.000
train | epoch   4 | Iter:   6216/ 13000 | global iter:   6216/ 13000 | loss: 1.1161 | ds_loss: 0.0000 | lr: 2.6893e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   4 | Iter:   6217/ 13000 | global iter:   6217/ 13000 | loss: 1.6187 | ds_loss: 0.0000 | lr: 2.6887e-05 | scale: 16384.0000 | micro time: 1.834 | step time: 0.000
train | epoch   4 | Iter:   6218/ 13000 | global iter:   6218/ 13000 | loss: 1.8920 | ds_loss: 0.0000 | lr: 2.6881e-05 | scale: 16384.0000 | micro time: 2.300 | step time: 0.000
train | epoch   4 | Iter:   6219/ 13000 | global iter:   6219/ 13000 | loss: 1.4364 | ds_loss: 0.0000 | lr: 2.6875e-05 | scale: 16384.0000 | micro time: 1.710 | step time: 0.000
train | epoch   4 | Iter:   6220/ 13000 | global iter:   6220/ 13000 | loss: 1.1149 | ds_loss: 0.0000 | lr: 2.6869e-05 | scale: 16384.0000 | micro time: 1.764 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6220/ 13000 | global iter:   6220/ 13000 | loss: 1.2545 | ds_loss: 0.0000 | lr: 2.6869e-05 | scale: 16384.0000 | micro time: 1.764 | step time: 1.808
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6221/ 13000 | global iter:   6221/ 13000 | loss: 1.2353 | ds_loss: 0.0000 | lr: 2.6863e-05 | scale: 16384.0000 | micro time: 1.861 | step time: 0.000
train | epoch   4 | Iter:   6222/ 13000 | global iter:   6222/ 13000 | loss: 1.1355 | ds_loss: 0.0000 | lr: 2.6857e-05 | scale: 16384.0000 | micro time: 1.724 | step time: 0.000
train | epoch   4 | Iter:   6223/ 13000 | global iter:   6223/ 13000 | loss: 1.6411 | ds_loss: 0.0000 | lr: 2.6851e-05 | scale: 16384.0000 | micro time: 2.223 | step time: 0.000
train | epoch   4 | Iter:   6224/ 13000 | global iter:   6224/ 13000 | loss: 1.7414 | ds_loss: 0.0000 | lr: 2.6845e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   4 | Iter:   6225/ 13000 | global iter:   6225/ 13000 | loss: 1.4714 | ds_loss: 0.0000 | lr: 2.6839e-05 | scale: 16384.0000 | micro time: 1.711 | step time: 0.000
train | epoch   4 | Iter:   6226/ 13000 | global iter:   6226/ 13000 | loss: 0.9353 | ds_loss: 0.0000 | lr: 2.6833e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   4 | Iter:   6227/ 13000 | global iter:   6227/ 13000 | loss: 1.0625 | ds_loss: 0.0000 | lr: 2.6827e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   4 | Iter:   6228/ 13000 | global iter:   6228/ 13000 | loss: 1.4413 | ds_loss: 0.0000 | lr: 2.6821e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   4 | Iter:   6229/ 13000 | global iter:   6229/ 13000 | loss: 1.5241 | ds_loss: 0.0000 | lr: 2.6815e-05 | scale: 16384.0000 | micro time: 1.840 | step time: 0.000
train | epoch   4 | Iter:   6230/ 13000 | global iter:   6230/ 13000 | loss: 1.3744 | ds_loss: 0.0000 | lr: 2.6809e-05 | scale: 16384.0000 | micro time: 1.769 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6230/ 13000 | global iter:   6230/ 13000 | loss: 1.3562 | ds_loss: 0.0000 | lr: 2.6809e-05 | scale: 16384.0000 | micro time: 1.769 | step time: 1.845
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6231/ 13000 | global iter:   6231/ 13000 | loss: 1.2493 | ds_loss: 0.0000 | lr: 2.6803e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   4 | Iter:   6232/ 13000 | global iter:   6232/ 13000 | loss: 1.5970 | ds_loss: 0.0000 | lr: 2.6797e-05 | scale: 16384.0000 | micro time: 1.865 | step time: 0.000
train | epoch   4 | Iter:   6233/ 13000 | global iter:   6233/ 13000 | loss: 1.1605 | ds_loss: 0.0000 | lr: 2.6791e-05 | scale: 16384.0000 | micro time: 1.869 | step time: 0.000
train | epoch   4 | Iter:   6234/ 13000 | global iter:   6234/ 13000 | loss: 1.7094 | ds_loss: 0.0000 | lr: 2.6785e-05 | scale: 16384.0000 | micro time: 1.836 | step time: 0.000
train | epoch   4 | Iter:   6235/ 13000 | global iter:   6235/ 13000 | loss: 1.3012 | ds_loss: 0.0000 | lr: 2.6779e-05 | scale: 16384.0000 | micro time: 1.870 | step time: 0.000
train | epoch   4 | Iter:   6236/ 13000 | global iter:   6236/ 13000 | loss: 1.0243 | ds_loss: 0.0000 | lr: 2.6773e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   4 | Iter:   6237/ 13000 | global iter:   6237/ 13000 | loss: 1.7798 | ds_loss: 0.0000 | lr: 2.6767e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   4 | Iter:   6238/ 13000 | global iter:   6238/ 13000 | loss: 1.3609 | ds_loss: 0.0000 | lr: 2.6761e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   4 | Iter:   6239/ 13000 | global iter:   6239/ 13000 | loss: 1.3207 | ds_loss: 0.0000 | lr: 2.6755e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   4 | Iter:   6240/ 13000 | global iter:   6240/ 13000 | loss: 1.3522 | ds_loss: 0.0000 | lr: 2.6749e-05 | scale: 16384.0000 | micro time: 1.765 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6240/ 13000 | global iter:   6240/ 13000 | loss: 1.3855 | ds_loss: 0.0000 | lr: 2.6749e-05 | scale: 16384.0000 | micro time: 1.765 | step time: 1.834
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6241/ 13000 | global iter:   6241/ 13000 | loss: 1.5432 | ds_loss: 0.0000 | lr: 2.6743e-05 | scale: 16384.0000 | micro time: 1.794 | step time: 0.000
train | epoch   4 | Iter:   6242/ 13000 | global iter:   6242/ 13000 | loss: 1.4245 | ds_loss: 0.0000 | lr: 2.6737e-05 | scale: 16384.0000 | micro time: 1.876 | step time: 0.000
train | epoch   4 | Iter:   6243/ 13000 | global iter:   6243/ 13000 | loss: 1.4072 | ds_loss: 0.0000 | lr: 2.6731e-05 | scale: 16384.0000 | micro time: 1.760 | step time: 0.000
train | epoch   4 | Iter:   6244/ 13000 | global iter:   6244/ 13000 | loss: 1.3651 | ds_loss: 0.0000 | lr: 2.6725e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 0.000
train | epoch   4 | Iter:   6245/ 13000 | global iter:   6245/ 13000 | loss: 1.5543 | ds_loss: 0.0000 | lr: 2.6719e-05 | scale: 16384.0000 | micro time: 1.810 | step time: 0.000
train | epoch   4 | Iter:   6246/ 13000 | global iter:   6246/ 13000 | loss: 0.9061 | ds_loss: 0.0000 | lr: 2.6713e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   4 | Iter:   6247/ 13000 | global iter:   6247/ 13000 | loss: 1.0416 | ds_loss: 0.0000 | lr: 2.6707e-05 | scale: 16384.0000 | micro time: 1.724 | step time: 0.000
train | epoch   4 | Iter:   6248/ 13000 | global iter:   6248/ 13000 | loss: 1.4447 | ds_loss: 0.0000 | lr: 2.6701e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   4 | Iter:   6249/ 13000 | global iter:   6249/ 13000 | loss: 1.3516 | ds_loss: 0.0000 | lr: 2.6695e-05 | scale: 16384.0000 | micro time: 1.743 | step time: 0.000
train | epoch   4 | Iter:   6250/ 13000 | global iter:   6250/ 13000 | loss: 1.6799 | ds_loss: 0.0000 | lr: 2.6689e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6250/ 13000 | global iter:   6250/ 13000 | loss: 1.3718 | ds_loss: 0.0000 | lr: 2.6689e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 1.792
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6251/ 13000 | global iter:   6251/ 13000 | loss: 1.7172 | ds_loss: 0.0000 | lr: 2.6683e-05 | scale: 16384.0000 | micro time: 1.833 | step time: 0.000
train | epoch   4 | Iter:   6252/ 13000 | global iter:   6252/ 13000 | loss: 0.8743 | ds_loss: 0.0000 | lr: 2.6677e-05 | scale: 16384.0000 | micro time: 1.896 | step time: 0.000
train | epoch   4 | Iter:   6253/ 13000 | global iter:   6253/ 13000 | loss: 1.4629 | ds_loss: 0.0000 | lr: 2.6671e-05 | scale: 16384.0000 | micro time: 1.741 | step time: 0.000
train | epoch   4 | Iter:   6254/ 13000 | global iter:   6254/ 13000 | loss: 1.2586 | ds_loss: 0.0000 | lr: 2.6665e-05 | scale: 16384.0000 | micro time: 1.797 | step time: 0.000
train | epoch   4 | Iter:   6255/ 13000 | global iter:   6255/ 13000 | loss: 1.6722 | ds_loss: 0.0000 | lr: 2.6659e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 0.000
train | epoch   4 | Iter:   6256/ 13000 | global iter:   6256/ 13000 | loss: 1.2360 | ds_loss: 0.0000 | lr: 2.6653e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   4 | Iter:   6257/ 13000 | global iter:   6257/ 13000 | loss: 1.6402 | ds_loss: 0.0000 | lr: 2.6647e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   4 | Iter:   6258/ 13000 | global iter:   6258/ 13000 | loss: 1.2602 | ds_loss: 0.0000 | lr: 2.6641e-05 | scale: 16384.0000 | micro time: 1.846 | step time: 0.000
train | epoch   4 | Iter:   6259/ 13000 | global iter:   6259/ 13000 | loss: 1.2877 | ds_loss: 0.0000 | lr: 2.6635e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   4 | Iter:   6260/ 13000 | global iter:   6260/ 13000 | loss: 0.6842 | ds_loss: 0.0000 | lr: 2.6629e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6260/ 13000 | global iter:   6260/ 13000 | loss: 1.3093 | ds_loss: 0.0000 | lr: 2.6629e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 1.812
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6261/ 13000 | global iter:   6261/ 13000 | loss: 1.5457 | ds_loss: 0.0000 | lr: 2.6623e-05 | scale: 16384.0000 | micro time: 1.797 | step time: 0.000
train | epoch   4 | Iter:   6262/ 13000 | global iter:   6262/ 13000 | loss: 1.6643 | ds_loss: 0.0000 | lr: 2.6617e-05 | scale: 16384.0000 | micro time: 1.818 | step time: 0.000
train | epoch   4 | Iter:   6263/ 13000 | global iter:   6263/ 13000 | loss: 1.2569 | ds_loss: 0.0000 | lr: 2.6611e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   4 | Iter:   6264/ 13000 | global iter:   6264/ 13000 | loss: 1.1613 | ds_loss: 0.0000 | lr: 2.6605e-05 | scale: 16384.0000 | micro time: 1.869 | step time: 0.000
train | epoch   4 | Iter:   6265/ 13000 | global iter:   6265/ 13000 | loss: 1.5126 | ds_loss: 0.0000 | lr: 2.6599e-05 | scale: 16384.0000 | micro time: 1.782 | step time: 0.000
train | epoch   4 | Iter:   6266/ 13000 | global iter:   6266/ 13000 | loss: 1.3883 | ds_loss: 0.0000 | lr: 2.6593e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   4 | Iter:   6267/ 13000 | global iter:   6267/ 13000 | loss: 1.2018 | ds_loss: 0.0000 | lr: 2.6587e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   4 | Iter:   6268/ 13000 | global iter:   6268/ 13000 | loss: 0.9536 | ds_loss: 0.0000 | lr: 2.6581e-05 | scale: 16384.0000 | micro time: 1.705 | step time: 0.000
train | epoch   4 | Iter:   6269/ 13000 | global iter:   6269/ 13000 | loss: 1.2158 | ds_loss: 0.0000 | lr: 2.6574e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   4 | Iter:   6270/ 13000 | global iter:   6270/ 13000 | loss: 0.9342 | ds_loss: 0.0000 | lr: 2.6568e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6270/ 13000 | global iter:   6270/ 13000 | loss: 1.2834 | ds_loss: 0.0000 | lr: 2.6568e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 1.802
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6271/ 13000 | global iter:   6271/ 13000 | loss: 1.0739 | ds_loss: 0.0000 | lr: 2.6562e-05 | scale: 16384.0000 | micro time: 1.777 | step time: 0.000
train | epoch   4 | Iter:   6272/ 13000 | global iter:   6272/ 13000 | loss: 1.0290 | ds_loss: 0.0000 | lr: 2.6556e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   4 | Iter:   6273/ 13000 | global iter:   6273/ 13000 | loss: 1.6689 | ds_loss: 0.0000 | lr: 2.6550e-05 | scale: 16384.0000 | micro time: 1.702 | step time: 0.000
train | epoch   4 | Iter:   6274/ 13000 | global iter:   6274/ 13000 | loss: 1.0002 | ds_loss: 0.0000 | lr: 2.6544e-05 | scale: 16384.0000 | micro time: 1.780 | step time: 0.000
train | epoch   4 | Iter:   6275/ 13000 | global iter:   6275/ 13000 | loss: 1.7005 | ds_loss: 0.0000 | lr: 2.6538e-05 | scale: 16384.0000 | micro time: 1.904 | step time: 0.000
train | epoch   4 | Iter:   6276/ 13000 | global iter:   6276/ 13000 | loss: 1.5746 | ds_loss: 0.0000 | lr: 2.6532e-05 | scale: 16384.0000 | micro time: 1.818 | step time: 0.000
train | epoch   4 | Iter:   6277/ 13000 | global iter:   6277/ 13000 | loss: 1.3704 | ds_loss: 0.0000 | lr: 2.6526e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   4 | Iter:   6278/ 13000 | global iter:   6278/ 13000 | loss: 1.6092 | ds_loss: 0.0000 | lr: 2.6520e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   4 | Iter:   6279/ 13000 | global iter:   6279/ 13000 | loss: 1.2938 | ds_loss: 0.0000 | lr: 2.6514e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   4 | Iter:   6280/ 13000 | global iter:   6280/ 13000 | loss: 1.4359 | ds_loss: 0.0000 | lr: 2.6508e-05 | scale: 16384.0000 | micro time: 1.880 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6280/ 13000 | global iter:   6280/ 13000 | loss: 1.3756 | ds_loss: 0.0000 | lr: 2.6508e-05 | scale: 16384.0000 | micro time: 1.880 | step time: 1.820
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6281/ 13000 | global iter:   6281/ 13000 | loss: 1.8401 | ds_loss: 0.0000 | lr: 2.6502e-05 | scale: 16384.0000 | micro time: 1.777 | step time: 0.000
train | epoch   4 | Iter:   6282/ 13000 | global iter:   6282/ 13000 | loss: 1.3864 | ds_loss: 0.0000 | lr: 2.6496e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   4 | Iter:   6283/ 13000 | global iter:   6283/ 13000 | loss: 0.8515 | ds_loss: 0.0000 | lr: 2.6490e-05 | scale: 16384.0000 | micro time: 1.915 | step time: 0.000
train | epoch   4 | Iter:   6284/ 13000 | global iter:   6284/ 13000 | loss: 1.3848 | ds_loss: 0.0000 | lr: 2.6484e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   4 | Iter:   6285/ 13000 | global iter:   6285/ 13000 | loss: 1.1829 | ds_loss: 0.0000 | lr: 2.6478e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   4 | Iter:   6286/ 13000 | global iter:   6286/ 13000 | loss: 1.4347 | ds_loss: 0.0000 | lr: 2.6472e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   4 | Iter:   6287/ 13000 | global iter:   6287/ 13000 | loss: 1.5612 | ds_loss: 0.0000 | lr: 2.6466e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   4 | Iter:   6288/ 13000 | global iter:   6288/ 13000 | loss: 1.0542 | ds_loss: 0.0000 | lr: 2.6460e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 0.000
train | epoch   4 | Iter:   6289/ 13000 | global iter:   6289/ 13000 | loss: 0.9244 | ds_loss: 0.0000 | lr: 2.6454e-05 | scale: 16384.0000 | micro time: 1.776 | step time: 0.000
train | epoch   4 | Iter:   6290/ 13000 | global iter:   6290/ 13000 | loss: 1.2261 | ds_loss: 0.0000 | lr: 2.6448e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6290/ 13000 | global iter:   6290/ 13000 | loss: 1.2846 | ds_loss: 0.0000 | lr: 2.6448e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 1.824
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6291/ 13000 | global iter:   6291/ 13000 | loss: 1.8794 | ds_loss: 0.0000 | lr: 2.6442e-05 | scale: 16384.0000 | micro time: 1.838 | step time: 0.000
train | epoch   4 | Iter:   6292/ 13000 | global iter:   6292/ 13000 | loss: 0.8986 | ds_loss: 0.0000 | lr: 2.6436e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   4 | Iter:   6293/ 13000 | global iter:   6293/ 13000 | loss: 1.4398 | ds_loss: 0.0000 | lr: 2.6430e-05 | scale: 16384.0000 | micro time: 1.711 | step time: 0.000
train | epoch   4 | Iter:   6294/ 13000 | global iter:   6294/ 13000 | loss: 1.8300 | ds_loss: 0.0000 | lr: 2.6424e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   4 | Iter:   6295/ 13000 | global iter:   6295/ 13000 | loss: 1.3439 | ds_loss: 0.0000 | lr: 2.6418e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   4 | Iter:   6296/ 13000 | global iter:   6296/ 13000 | loss: 1.3420 | ds_loss: 0.0000 | lr: 2.6412e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   4 | Iter:   6297/ 13000 | global iter:   6297/ 13000 | loss: 1.1147 | ds_loss: 0.0000 | lr: 2.6406e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   4 | Iter:   6298/ 13000 | global iter:   6298/ 13000 | loss: 1.1354 | ds_loss: 0.0000 | lr: 2.6400e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   4 | Iter:   6299/ 13000 | global iter:   6299/ 13000 | loss: 1.4005 | ds_loss: 0.0000 | lr: 2.6394e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   4 | Iter:   6300/ 13000 | global iter:   6300/ 13000 | loss: 1.9416 | ds_loss: 0.0000 | lr: 2.6388e-05 | scale: 16384.0000 | micro time: 1.913 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6300/ 13000 | global iter:   6300/ 13000 | loss: 1.4326 | ds_loss: 0.0000 | lr: 2.6388e-05 | scale: 16384.0000 | micro time: 1.913 | step time: 1.802
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6301/ 13000 | global iter:   6301/ 13000 | loss: 0.8917 | ds_loss: 0.0000 | lr: 2.6382e-05 | scale: 16384.0000 | micro time: 1.880 | step time: 0.000
train | epoch   4 | Iter:   6302/ 13000 | global iter:   6302/ 13000 | loss: 1.1607 | ds_loss: 0.0000 | lr: 2.6376e-05 | scale: 16384.0000 | micro time: 1.768 | step time: 0.000
train | epoch   4 | Iter:   6303/ 13000 | global iter:   6303/ 13000 | loss: 1.0801 | ds_loss: 0.0000 | lr: 2.6370e-05 | scale: 16384.0000 | micro time: 1.800 | step time: 0.000
train | epoch   4 | Iter:   6304/ 13000 | global iter:   6304/ 13000 | loss: 1.5925 | ds_loss: 0.0000 | lr: 2.6364e-05 | scale: 16384.0000 | micro time: 1.888 | step time: 0.000
train | epoch   4 | Iter:   6305/ 13000 | global iter:   6305/ 13000 | loss: 1.2505 | ds_loss: 0.0000 | lr: 2.6358e-05 | scale: 16384.0000 | micro time: 1.743 | step time: 0.000
train | epoch   4 | Iter:   6306/ 13000 | global iter:   6306/ 13000 | loss: 1.3210 | ds_loss: 0.0000 | lr: 2.6352e-05 | scale: 16384.0000 | micro time: 1.824 | step time: 0.000
train | epoch   4 | Iter:   6307/ 13000 | global iter:   6307/ 13000 | loss: 0.9640 | ds_loss: 0.0000 | lr: 2.6346e-05 | scale: 16384.0000 | micro time: 1.790 | step time: 0.000
train | epoch   4 | Iter:   6308/ 13000 | global iter:   6308/ 13000 | loss: 1.3123 | ds_loss: 0.0000 | lr: 2.6340e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   4 | Iter:   6309/ 13000 | global iter:   6309/ 13000 | loss: 1.1061 | ds_loss: 0.0000 | lr: 2.6334e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 0.000
train | epoch   4 | Iter:   6310/ 13000 | global iter:   6310/ 13000 | loss: 1.0890 | ds_loss: 0.0000 | lr: 2.6328e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6310/ 13000 | global iter:   6310/ 13000 | loss: 1.1768 | ds_loss: 0.0000 | lr: 2.6328e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 1.803
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6311/ 13000 | global iter:   6311/ 13000 | loss: 1.4829 | ds_loss: 0.0000 | lr: 2.6322e-05 | scale: 16384.0000 | micro time: 1.717 | step time: 0.000
train | epoch   4 | Iter:   6312/ 13000 | global iter:   6312/ 13000 | loss: 1.2469 | ds_loss: 0.0000 | lr: 2.6316e-05 | scale: 16384.0000 | micro time: 1.832 | step time: 0.000
train | epoch   4 | Iter:   6313/ 13000 | global iter:   6313/ 13000 | loss: 1.2386 | ds_loss: 0.0000 | lr: 2.6310e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   4 | Iter:   6314/ 13000 | global iter:   6314/ 13000 | loss: 1.1679 | ds_loss: 0.0000 | lr: 2.6304e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 0.000
train | epoch   4 | Iter:   6315/ 13000 | global iter:   6315/ 13000 | loss: 1.4096 | ds_loss: 0.0000 | lr: 2.6298e-05 | scale: 16384.0000 | micro time: 1.880 | step time: 0.000
train | epoch   4 | Iter:   6316/ 13000 | global iter:   6316/ 13000 | loss: 1.2876 | ds_loss: 0.0000 | lr: 2.6292e-05 | scale: 16384.0000 | micro time: 1.927 | step time: 0.000
train | epoch   4 | Iter:   6317/ 13000 | global iter:   6317/ 13000 | loss: 1.3691 | ds_loss: 0.0000 | lr: 2.6286e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   4 | Iter:   6318/ 13000 | global iter:   6318/ 13000 | loss: 1.2972 | ds_loss: 0.0000 | lr: 2.6280e-05 | scale: 16384.0000 | micro time: 1.778 | step time: 0.000
train | epoch   4 | Iter:   6319/ 13000 | global iter:   6319/ 13000 | loss: 1.1961 | ds_loss: 0.0000 | lr: 2.6273e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   4 | Iter:   6320/ 13000 | global iter:   6320/ 13000 | loss: 1.4684 | ds_loss: 0.0000 | lr: 2.6267e-05 | scale: 16384.0000 | micro time: 1.711 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6320/ 13000 | global iter:   6320/ 13000 | loss: 1.3164 | ds_loss: 0.0000 | lr: 2.6267e-05 | scale: 16384.0000 | micro time: 1.711 | step time: 1.817
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6321/ 13000 | global iter:   6321/ 13000 | loss: 1.9625 | ds_loss: 0.0000 | lr: 2.6261e-05 | scale: 16384.0000 | micro time: 1.845 | step time: 0.000
train | epoch   4 | Iter:   6322/ 13000 | global iter:   6322/ 13000 | loss: 1.5170 | ds_loss: 0.0000 | lr: 2.6255e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   4 | Iter:   6323/ 13000 | global iter:   6323/ 13000 | loss: 1.3397 | ds_loss: 0.0000 | lr: 2.6249e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   4 | Iter:   6324/ 13000 | global iter:   6324/ 13000 | loss: 0.7063 | ds_loss: 0.0000 | lr: 2.6243e-05 | scale: 16384.0000 | micro time: 1.911 | step time: 0.000
train | epoch   4 | Iter:   6325/ 13000 | global iter:   6325/ 13000 | loss: 1.1378 | ds_loss: 0.0000 | lr: 2.6237e-05 | scale: 16384.0000 | micro time: 1.715 | step time: 0.000
train | epoch   4 | Iter:   6326/ 13000 | global iter:   6326/ 13000 | loss: 0.9800 | ds_loss: 0.0000 | lr: 2.6231e-05 | scale: 16384.0000 | micro time: 1.834 | step time: 0.000
train | epoch   4 | Iter:   6327/ 13000 | global iter:   6327/ 13000 | loss: 1.6310 | ds_loss: 0.0000 | lr: 2.6225e-05 | scale: 16384.0000 | micro time: 1.832 | step time: 0.000
train | epoch   4 | Iter:   6328/ 13000 | global iter:   6328/ 13000 | loss: 1.7965 | ds_loss: 0.0000 | lr: 2.6219e-05 | scale: 16384.0000 | micro time: 1.715 | step time: 0.000
train | epoch   4 | Iter:   6329/ 13000 | global iter:   6329/ 13000 | loss: 1.1824 | ds_loss: 0.0000 | lr: 2.6213e-05 | scale: 16384.0000 | micro time: 1.822 | step time: 0.000
train | epoch   4 | Iter:   6330/ 13000 | global iter:   6330/ 13000 | loss: 0.9787 | ds_loss: 0.0000 | lr: 2.6207e-05 | scale: 16384.0000 | micro time: 1.745 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6330/ 13000 | global iter:   6330/ 13000 | loss: 1.3232 | ds_loss: 0.0000 | lr: 2.6207e-05 | scale: 16384.0000 | micro time: 1.745 | step time: 1.810
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6331/ 13000 | global iter:   6331/ 13000 | loss: 1.7306 | ds_loss: 0.0000 | lr: 2.6201e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   4 | Iter:   6332/ 13000 | global iter:   6332/ 13000 | loss: 1.0089 | ds_loss: 0.0000 | lr: 2.6195e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   4 | Iter:   6333/ 13000 | global iter:   6333/ 13000 | loss: 1.1972 | ds_loss: 0.0000 | lr: 2.6189e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   4 | Iter:   6334/ 13000 | global iter:   6334/ 13000 | loss: 1.0774 | ds_loss: 0.0000 | lr: 2.6183e-05 | scale: 16384.0000 | micro time: 1.817 | step time: 0.000
train | epoch   4 | Iter:   6335/ 13000 | global iter:   6335/ 13000 | loss: 1.0512 | ds_loss: 0.0000 | lr: 2.6177e-05 | scale: 16384.0000 | micro time: 1.880 | step time: 0.000
train | epoch   4 | Iter:   6336/ 13000 | global iter:   6336/ 13000 | loss: 1.3982 | ds_loss: 0.0000 | lr: 2.6171e-05 | scale: 16384.0000 | micro time: 1.766 | step time: 0.000
train | epoch   4 | Iter:   6337/ 13000 | global iter:   6337/ 13000 | loss: 1.5873 | ds_loss: 0.0000 | lr: 2.6165e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   4 | Iter:   6338/ 13000 | global iter:   6338/ 13000 | loss: 1.2722 | ds_loss: 0.0000 | lr: 2.6159e-05 | scale: 16384.0000 | micro time: 1.687 | step time: 0.000
train | epoch   4 | Iter:   6339/ 13000 | global iter:   6339/ 13000 | loss: 1.3833 | ds_loss: 0.0000 | lr: 2.6153e-05 | scale: 16384.0000 | micro time: 1.798 | step time: 0.000
train | epoch   4 | Iter:   6340/ 13000 | global iter:   6340/ 13000 | loss: 1.3765 | ds_loss: 0.0000 | lr: 2.6147e-05 | scale: 16384.0000 | micro time: 1.726 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6340/ 13000 | global iter:   6340/ 13000 | loss: 1.3083 | ds_loss: 0.0000 | lr: 2.6147e-05 | scale: 16384.0000 | micro time: 1.726 | step time: 1.787
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6341/ 13000 | global iter:   6341/ 13000 | loss: 1.6402 | ds_loss: 0.0000 | lr: 2.6141e-05 | scale: 16384.0000 | micro time: 1.804 | step time: 0.000
train | epoch   4 | Iter:   6342/ 13000 | global iter:   6342/ 13000 | loss: 1.2770 | ds_loss: 0.0000 | lr: 2.6135e-05 | scale: 16384.0000 | micro time: 1.696 | step time: 0.000
train | epoch   4 | Iter:   6343/ 13000 | global iter:   6343/ 13000 | loss: 1.2801 | ds_loss: 0.0000 | lr: 2.6129e-05 | scale: 16384.0000 | micro time: 1.809 | step time: 0.000
train | epoch   4 | Iter:   6344/ 13000 | global iter:   6344/ 13000 | loss: 0.8636 | ds_loss: 0.0000 | lr: 2.6123e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   4 | Iter:   6345/ 13000 | global iter:   6345/ 13000 | loss: 1.0382 | ds_loss: 0.0000 | lr: 2.6117e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   4 | Iter:   6346/ 13000 | global iter:   6346/ 13000 | loss: 1.4879 | ds_loss: 0.0000 | lr: 2.6111e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   4 | Iter:   6347/ 13000 | global iter:   6347/ 13000 | loss: 1.2454 | ds_loss: 0.0000 | lr: 2.6105e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   4 | Iter:   6348/ 13000 | global iter:   6348/ 13000 | loss: 1.1894 | ds_loss: 0.0000 | lr: 2.6099e-05 | scale: 16384.0000 | micro time: 1.841 | step time: 0.000
train | epoch   4 | Iter:   6349/ 13000 | global iter:   6349/ 13000 | loss: 1.4919 | ds_loss: 0.0000 | lr: 2.6093e-05 | scale: 16384.0000 | micro time: 1.761 | step time: 0.000
train | epoch   4 | Iter:   6350/ 13000 | global iter:   6350/ 13000 | loss: 1.0551 | ds_loss: 0.0000 | lr: 2.6087e-05 | scale: 16384.0000 | micro time: 1.785 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6350/ 13000 | global iter:   6350/ 13000 | loss: 1.2569 | ds_loss: 0.0000 | lr: 2.6087e-05 | scale: 16384.0000 | micro time: 1.785 | step time: 1.792
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6351/ 13000 | global iter:   6351/ 13000 | loss: 1.1808 | ds_loss: 0.0000 | lr: 2.6081e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   4 | Iter:   6352/ 13000 | global iter:   6352/ 13000 | loss: 1.4433 | ds_loss: 0.0000 | lr: 2.6075e-05 | scale: 16384.0000 | micro time: 1.864 | step time: 0.000
train | epoch   4 | Iter:   6353/ 13000 | global iter:   6353/ 13000 | loss: 1.2764 | ds_loss: 0.0000 | lr: 2.6069e-05 | scale: 16384.0000 | micro time: 1.711 | step time: 0.000
train | epoch   4 | Iter:   6354/ 13000 | global iter:   6354/ 13000 | loss: 0.8599 | ds_loss: 0.0000 | lr: 2.6063e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   4 | Iter:   6355/ 13000 | global iter:   6355/ 13000 | loss: 1.1000 | ds_loss: 0.0000 | lr: 2.6057e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   4 | Iter:   6356/ 13000 | global iter:   6356/ 13000 | loss: 1.4519 | ds_loss: 0.0000 | lr: 2.6051e-05 | scale: 16384.0000 | micro time: 1.873 | step time: 0.000
train | epoch   4 | Iter:   6357/ 13000 | global iter:   6357/ 13000 | loss: 1.6000 | ds_loss: 0.0000 | lr: 2.6045e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 0.000
train | epoch   4 | Iter:   6358/ 13000 | global iter:   6358/ 13000 | loss: 1.6277 | ds_loss: 0.0000 | lr: 2.6039e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   4 | Iter:   6359/ 13000 | global iter:   6359/ 13000 | loss: 0.8920 | ds_loss: 0.0000 | lr: 2.6033e-05 | scale: 16384.0000 | micro time: 1.743 | step time: 0.000
train | epoch   4 | Iter:   6360/ 13000 | global iter:   6360/ 13000 | loss: 1.3817 | ds_loss: 0.0000 | lr: 2.6027e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6360/ 13000 | global iter:   6360/ 13000 | loss: 1.2814 | ds_loss: 0.0000 | lr: 2.6027e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 1.799
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6361/ 13000 | global iter:   6361/ 13000 | loss: 1.3538 | ds_loss: 0.0000 | lr: 2.6020e-05 | scale: 16384.0000 | micro time: 1.858 | step time: 0.000
train | epoch   4 | Iter:   6362/ 13000 | global iter:   6362/ 13000 | loss: 1.6166 | ds_loss: 0.0000 | lr: 2.6014e-05 | scale: 16384.0000 | micro time: 1.891 | step time: 0.000
train | epoch   4 | Iter:   6363/ 13000 | global iter:   6363/ 13000 | loss: 1.2321 | ds_loss: 0.0000 | lr: 2.6008e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   4 | Iter:   6364/ 13000 | global iter:   6364/ 13000 | loss: 1.0710 | ds_loss: 0.0000 | lr: 2.6002e-05 | scale: 16384.0000 | micro time: 1.759 | step time: 0.000
train | epoch   4 | Iter:   6365/ 13000 | global iter:   6365/ 13000 | loss: 1.2403 | ds_loss: 0.0000 | lr: 2.5996e-05 | scale: 16384.0000 | micro time: 1.659 | step time: 0.000
train | epoch   4 | Iter:   6366/ 13000 | global iter:   6366/ 13000 | loss: 1.4315 | ds_loss: 0.0000 | lr: 2.5990e-05 | scale: 16384.0000 | micro time: 1.911 | step time: 0.000
train | epoch   4 | Iter:   6367/ 13000 | global iter:   6367/ 13000 | loss: 1.4323 | ds_loss: 0.0000 | lr: 2.5984e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   4 | Iter:   6368/ 13000 | global iter:   6368/ 13000 | loss: 1.5990 | ds_loss: 0.0000 | lr: 2.5978e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   4 | Iter:   6369/ 13000 | global iter:   6369/ 13000 | loss: 1.6618 | ds_loss: 0.0000 | lr: 2.5972e-05 | scale: 16384.0000 | micro time: 1.846 | step time: 0.000
train | epoch   4 | Iter:   6370/ 13000 | global iter:   6370/ 13000 | loss: 1.7291 | ds_loss: 0.0000 | lr: 2.5966e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6370/ 13000 | global iter:   6370/ 13000 | loss: 1.4368 | ds_loss: 0.0000 | lr: 2.5966e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 1.826
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6371/ 13000 | global iter:   6371/ 13000 | loss: 1.6368 | ds_loss: 0.0000 | lr: 2.5960e-05 | scale: 16384.0000 | micro time: 1.810 | step time: 0.000
train | epoch   4 | Iter:   6372/ 13000 | global iter:   6372/ 13000 | loss: 1.1416 | ds_loss: 0.0000 | lr: 2.5954e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   4 | Iter:   6373/ 13000 | global iter:   6373/ 13000 | loss: 1.5362 | ds_loss: 0.0000 | lr: 2.5948e-05 | scale: 16384.0000 | micro time: 1.786 | step time: 0.000
train | epoch   4 | Iter:   6374/ 13000 | global iter:   6374/ 13000 | loss: 1.6246 | ds_loss: 0.0000 | lr: 2.5942e-05 | scale: 16384.0000 | micro time: 1.880 | step time: 0.000
train | epoch   4 | Iter:   6375/ 13000 | global iter:   6375/ 13000 | loss: 1.4617 | ds_loss: 0.0000 | lr: 2.5936e-05 | scale: 16384.0000 | micro time: 1.917 | step time: 0.000
train | epoch   4 | Iter:   6376/ 13000 | global iter:   6376/ 13000 | loss: 0.8924 | ds_loss: 0.0000 | lr: 2.5930e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   4 | Iter:   6377/ 13000 | global iter:   6377/ 13000 | loss: 1.4917 | ds_loss: 0.0000 | lr: 2.5924e-05 | scale: 16384.0000 | micro time: 1.832 | step time: 0.000
train | epoch   4 | Iter:   6378/ 13000 | global iter:   6378/ 13000 | loss: 1.5777 | ds_loss: 0.0000 | lr: 2.5918e-05 | scale: 16384.0000 | micro time: 1.728 | step time: 0.000
train | epoch   4 | Iter:   6379/ 13000 | global iter:   6379/ 13000 | loss: 1.2188 | ds_loss: 0.0000 | lr: 2.5912e-05 | scale: 16384.0000 | micro time: 1.888 | step time: 0.000
train | epoch   4 | Iter:   6380/ 13000 | global iter:   6380/ 13000 | loss: 1.4509 | ds_loss: 0.0000 | lr: 2.5906e-05 | scale: 16384.0000 | micro time: 1.790 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6380/ 13000 | global iter:   6380/ 13000 | loss: 1.4032 | ds_loss: 0.0000 | lr: 2.5906e-05 | scale: 16384.0000 | micro time: 1.790 | step time: 1.829
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6381/ 13000 | global iter:   6381/ 13000 | loss: 1.7545 | ds_loss: 0.0000 | lr: 2.5900e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   4 | Iter:   6382/ 13000 | global iter:   6382/ 13000 | loss: 1.8051 | ds_loss: 0.0000 | lr: 2.5894e-05 | scale: 16384.0000 | micro time: 1.731 | step time: 0.000
train | epoch   4 | Iter:   6383/ 13000 | global iter:   6383/ 13000 | loss: 1.6110 | ds_loss: 0.0000 | lr: 2.5888e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 0.000
train | epoch   4 | Iter:   6384/ 13000 | global iter:   6384/ 13000 | loss: 1.5649 | ds_loss: 0.0000 | lr: 2.5882e-05 | scale: 16384.0000 | micro time: 1.782 | step time: 0.000
train | epoch   4 | Iter:   6385/ 13000 | global iter:   6385/ 13000 | loss: 1.3208 | ds_loss: 0.0000 | lr: 2.5876e-05 | scale: 16384.0000 | micro time: 1.792 | step time: 0.000
train | epoch   4 | Iter:   6386/ 13000 | global iter:   6386/ 13000 | loss: 1.6751 | ds_loss: 0.0000 | lr: 2.5870e-05 | scale: 16384.0000 | micro time: 1.842 | step time: 0.000
train | epoch   4 | Iter:   6387/ 13000 | global iter:   6387/ 13000 | loss: 1.4784 | ds_loss: 0.0000 | lr: 2.5864e-05 | scale: 16384.0000 | micro time: 1.850 | step time: 0.000
train | epoch   4 | Iter:   6388/ 13000 | global iter:   6388/ 13000 | loss: 1.2862 | ds_loss: 0.0000 | lr: 2.5858e-05 | scale: 16384.0000 | micro time: 1.813 | step time: 0.000
train | epoch   4 | Iter:   6389/ 13000 | global iter:   6389/ 13000 | loss: 0.9715 | ds_loss: 0.0000 | lr: 2.5852e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   4 | Iter:   6390/ 13000 | global iter:   6390/ 13000 | loss: 1.2456 | ds_loss: 0.0000 | lr: 2.5846e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6390/ 13000 | global iter:   6390/ 13000 | loss: 1.4713 | ds_loss: 0.0000 | lr: 2.5846e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 1.821
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6391/ 13000 | global iter:   6391/ 13000 | loss: 1.6250 | ds_loss: 0.0000 | lr: 2.5840e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   4 | Iter:   6392/ 13000 | global iter:   6392/ 13000 | loss: 1.5927 | ds_loss: 0.0000 | lr: 2.5834e-05 | scale: 16384.0000 | micro time: 1.785 | step time: 0.000
train | epoch   4 | Iter:   6393/ 13000 | global iter:   6393/ 13000 | loss: 1.4624 | ds_loss: 0.0000 | lr: 2.5828e-05 | scale: 16384.0000 | micro time: 1.786 | step time: 0.000
train | epoch   4 | Iter:   6394/ 13000 | global iter:   6394/ 13000 | loss: 1.2875 | ds_loss: 0.0000 | lr: 2.5822e-05 | scale: 16384.0000 | micro time: 1.842 | step time: 0.000
train | epoch   4 | Iter:   6395/ 13000 | global iter:   6395/ 13000 | loss: 1.3105 | ds_loss: 0.0000 | lr: 2.5816e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   4 | Iter:   6396/ 13000 | global iter:   6396/ 13000 | loss: 1.5637 | ds_loss: 0.0000 | lr: 2.5810e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   4 | Iter:   6397/ 13000 | global iter:   6397/ 13000 | loss: 1.1836 | ds_loss: 0.0000 | lr: 2.5804e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   4 | Iter:   6398/ 13000 | global iter:   6398/ 13000 | loss: 1.0134 | ds_loss: 0.0000 | lr: 2.5798e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   4 | Iter:   6399/ 13000 | global iter:   6399/ 13000 | loss: 1.6669 | ds_loss: 0.0000 | lr: 2.5792e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   4 | Iter:   6400/ 13000 | global iter:   6400/ 13000 | loss: 1.2716 | ds_loss: 0.0000 | lr: 2.5785e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6400/ 13000 | global iter:   6400/ 13000 | loss: 1.3977 | ds_loss: 0.0000 | lr: 2.5785e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 1.817
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6401/ 13000 | global iter:   6401/ 13000 | loss: 1.4025 | ds_loss: 0.0000 | lr: 2.5779e-05 | scale: 16384.0000 | micro time: 1.785 | step time: 0.000
train | epoch   4 | Iter:   6402/ 13000 | global iter:   6402/ 13000 | loss: 1.3732 | ds_loss: 0.0000 | lr: 2.5773e-05 | scale: 16384.0000 | micro time: 1.733 | step time: 0.000
train | epoch   4 | Iter:   6403/ 13000 | global iter:   6403/ 13000 | loss: 1.1931 | ds_loss: 0.0000 | lr: 2.5767e-05 | scale: 16384.0000 | micro time: 1.735 | step time: 0.000
train | epoch   4 | Iter:   6404/ 13000 | global iter:   6404/ 13000 | loss: 1.2681 | ds_loss: 0.0000 | lr: 2.5761e-05 | scale: 16384.0000 | micro time: 1.848 | step time: 0.000
train | epoch   4 | Iter:   6405/ 13000 | global iter:   6405/ 13000 | loss: 1.5034 | ds_loss: 0.0000 | lr: 2.5755e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   4 | Iter:   6406/ 13000 | global iter:   6406/ 13000 | loss: 1.3697 | ds_loss: 0.0000 | lr: 2.5749e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   4 | Iter:   6407/ 13000 | global iter:   6407/ 13000 | loss: 1.2982 | ds_loss: 0.0000 | lr: 2.5743e-05 | scale: 16384.0000 | micro time: 1.877 | step time: 0.000
train | epoch   4 | Iter:   6408/ 13000 | global iter:   6408/ 13000 | loss: 1.4876 | ds_loss: 0.0000 | lr: 2.5737e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 0.000
train | epoch   4 | Iter:   6409/ 13000 | global iter:   6409/ 13000 | loss: 1.4570 | ds_loss: 0.0000 | lr: 2.5731e-05 | scale: 16384.0000 | micro time: 1.824 | step time: 0.000
train | epoch   4 | Iter:   6410/ 13000 | global iter:   6410/ 13000 | loss: 1.5494 | ds_loss: 0.0000 | lr: 2.5725e-05 | scale: 16384.0000 | micro time: 1.742 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6410/ 13000 | global iter:   6410/ 13000 | loss: 1.3902 | ds_loss: 0.0000 | lr: 2.5725e-05 | scale: 16384.0000 | micro time: 1.742 | step time: 1.809
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6411/ 13000 | global iter:   6411/ 13000 | loss: 1.2698 | ds_loss: 0.0000 | lr: 2.5719e-05 | scale: 16384.0000 | micro time: 1.866 | step time: 0.000
train | epoch   4 | Iter:   6412/ 13000 | global iter:   6412/ 13000 | loss: 1.7680 | ds_loss: 0.0000 | lr: 2.5713e-05 | scale: 16384.0000 | micro time: 1.719 | step time: 0.000
train | epoch   4 | Iter:   6413/ 13000 | global iter:   6413/ 13000 | loss: 1.1944 | ds_loss: 0.0000 | lr: 2.5707e-05 | scale: 16384.0000 | micro time: 1.686 | step time: 0.000
train | epoch   4 | Iter:   6414/ 13000 | global iter:   6414/ 13000 | loss: 1.4151 | ds_loss: 0.0000 | lr: 2.5701e-05 | scale: 16384.0000 | micro time: 1.688 | step time: 0.000
train | epoch   4 | Iter:   6415/ 13000 | global iter:   6415/ 13000 | loss: 1.3961 | ds_loss: 0.0000 | lr: 2.5695e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   4 | Iter:   6416/ 13000 | global iter:   6416/ 13000 | loss: 1.6977 | ds_loss: 0.0000 | lr: 2.5689e-05 | scale: 16384.0000 | micro time: 1.806 | step time: 0.000
train | epoch   4 | Iter:   6417/ 13000 | global iter:   6417/ 13000 | loss: 1.4466 | ds_loss: 0.0000 | lr: 2.5683e-05 | scale: 16384.0000 | micro time: 1.744 | step time: 0.000
train | epoch   4 | Iter:   6418/ 13000 | global iter:   6418/ 13000 | loss: 0.9398 | ds_loss: 0.0000 | lr: 2.5677e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   4 | Iter:   6419/ 13000 | global iter:   6419/ 13000 | loss: 0.9219 | ds_loss: 0.0000 | lr: 2.5671e-05 | scale: 16384.0000 | micro time: 1.951 | step time: 0.000
train | epoch   4 | Iter:   6420/ 13000 | global iter:   6420/ 13000 | loss: 1.2566 | ds_loss: 0.0000 | lr: 2.5665e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6420/ 13000 | global iter:   6420/ 13000 | loss: 1.3306 | ds_loss: 0.0000 | lr: 2.5665e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 1.792
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6421/ 13000 | global iter:   6421/ 13000 | loss: 1.5316 | ds_loss: 0.0000 | lr: 2.5659e-05 | scale: 16384.0000 | micro time: 1.765 | step time: 0.000
train | epoch   4 | Iter:   6422/ 13000 | global iter:   6422/ 13000 | loss: 0.7225 | ds_loss: 0.0000 | lr: 2.5653e-05 | scale: 16384.0000 | micro time: 1.744 | step time: 0.000
train | epoch   4 | Iter:   6423/ 13000 | global iter:   6423/ 13000 | loss: 1.1903 | ds_loss: 0.0000 | lr: 2.5647e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   4 | Iter:   6424/ 13000 | global iter:   6424/ 13000 | loss: 1.3460 | ds_loss: 0.0000 | lr: 2.5641e-05 | scale: 16384.0000 | micro time: 1.879 | step time: 0.000
train | epoch   4 | Iter:   6425/ 13000 | global iter:   6425/ 13000 | loss: 1.4513 | ds_loss: 0.0000 | lr: 2.5635e-05 | scale: 16384.0000 | micro time: 1.708 | step time: 0.000
train | epoch   4 | Iter:   6426/ 13000 | global iter:   6426/ 13000 | loss: 0.7877 | ds_loss: 0.0000 | lr: 2.5629e-05 | scale: 16384.0000 | micro time: 1.806 | step time: 0.000
train | epoch   4 | Iter:   6427/ 13000 | global iter:   6427/ 13000 | loss: 1.1411 | ds_loss: 0.0000 | lr: 2.5623e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   4 | Iter:   6428/ 13000 | global iter:   6428/ 13000 | loss: 1.3692 | ds_loss: 0.0000 | lr: 2.5617e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   4 | Iter:   6429/ 13000 | global iter:   6429/ 13000 | loss: 1.1157 | ds_loss: 0.0000 | lr: 2.5611e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   4 | Iter:   6430/ 13000 | global iter:   6430/ 13000 | loss: 1.6984 | ds_loss: 0.0000 | lr: 2.5605e-05 | scale: 16384.0000 | micro time: 1.800 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6430/ 13000 | global iter:   6430/ 13000 | loss: 1.2354 | ds_loss: 0.0000 | lr: 2.5605e-05 | scale: 16384.0000 | micro time: 1.800 | step time: 1.791
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6431/ 13000 | global iter:   6431/ 13000 | loss: 0.9268 | ds_loss: 0.0000 | lr: 2.5599e-05 | scale: 16384.0000 | micro time: 1.832 | step time: 0.000
train | epoch   4 | Iter:   6432/ 13000 | global iter:   6432/ 13000 | loss: 1.6424 | ds_loss: 0.0000 | lr: 2.5593e-05 | scale: 16384.0000 | micro time: 1.789 | step time: 0.000
train | epoch   4 | Iter:   6433/ 13000 | global iter:   6433/ 13000 | loss: 1.4438 | ds_loss: 0.0000 | lr: 2.5587e-05 | scale: 16384.0000 | micro time: 1.800 | step time: 0.000
train | epoch   4 | Iter:   6434/ 13000 | global iter:   6434/ 13000 | loss: 1.0344 | ds_loss: 0.0000 | lr: 2.5581e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   4 | Iter:   6435/ 13000 | global iter:   6435/ 13000 | loss: 1.4716 | ds_loss: 0.0000 | lr: 2.5575e-05 | scale: 16384.0000 | micro time: 1.841 | step time: 0.000
train | epoch   4 | Iter:   6436/ 13000 | global iter:   6436/ 13000 | loss: 1.9496 | ds_loss: 0.0000 | lr: 2.5568e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   4 | Iter:   6437/ 13000 | global iter:   6437/ 13000 | loss: 0.9752 | ds_loss: 0.0000 | lr: 2.5562e-05 | scale: 16384.0000 | micro time: 1.820 | step time: 0.000
train | epoch   4 | Iter:   6438/ 13000 | global iter:   6438/ 13000 | loss: 1.3426 | ds_loss: 0.0000 | lr: 2.5556e-05 | scale: 16384.0000 | micro time: 1.838 | step time: 0.000
train | epoch   4 | Iter:   6439/ 13000 | global iter:   6439/ 13000 | loss: 1.4463 | ds_loss: 0.0000 | lr: 2.5550e-05 | scale: 16384.0000 | micro time: 1.860 | step time: 0.000
train | epoch   4 | Iter:   6440/ 13000 | global iter:   6440/ 13000 | loss: 1.3293 | ds_loss: 0.0000 | lr: 2.5544e-05 | scale: 16384.0000 | micro time: 1.798 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6440/ 13000 | global iter:   6440/ 13000 | loss: 1.3562 | ds_loss: 0.0000 | lr: 2.5544e-05 | scale: 16384.0000 | micro time: 1.798 | step time: 1.823
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6441/ 13000 | global iter:   6441/ 13000 | loss: 1.4414 | ds_loss: 0.0000 | lr: 2.5538e-05 | scale: 16384.0000 | micro time: 1.891 | step time: 0.000
train | epoch   4 | Iter:   6442/ 13000 | global iter:   6442/ 13000 | loss: 1.5686 | ds_loss: 0.0000 | lr: 2.5532e-05 | scale: 16384.0000 | micro time: 1.737 | step time: 0.000
train | epoch   4 | Iter:   6443/ 13000 | global iter:   6443/ 13000 | loss: 1.3370 | ds_loss: 0.0000 | lr: 2.5526e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   4 | Iter:   6444/ 13000 | global iter:   6444/ 13000 | loss: 1.5484 | ds_loss: 0.0000 | lr: 2.5520e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   4 | Iter:   6445/ 13000 | global iter:   6445/ 13000 | loss: 1.6897 | ds_loss: 0.0000 | lr: 2.5514e-05 | scale: 16384.0000 | micro time: 1.687 | step time: 0.000
train | epoch   4 | Iter:   6446/ 13000 | global iter:   6446/ 13000 | loss: 1.0811 | ds_loss: 0.0000 | lr: 2.5508e-05 | scale: 16384.0000 | micro time: 1.711 | step time: 0.000
train | epoch   4 | Iter:   6447/ 13000 | global iter:   6447/ 13000 | loss: 1.3078 | ds_loss: 0.0000 | lr: 2.5502e-05 | scale: 16384.0000 | micro time: 1.682 | step time: 0.000
train | epoch   4 | Iter:   6448/ 13000 | global iter:   6448/ 13000 | loss: 1.2938 | ds_loss: 0.0000 | lr: 2.5496e-05 | scale: 16384.0000 | micro time: 1.773 | step time: 0.000
train | epoch   4 | Iter:   6449/ 13000 | global iter:   6449/ 13000 | loss: 1.0818 | ds_loss: 0.0000 | lr: 2.5490e-05 | scale: 16384.0000 | micro time: 1.865 | step time: 0.000
train | epoch   4 | Iter:   6450/ 13000 | global iter:   6450/ 13000 | loss: 1.3139 | ds_loss: 0.0000 | lr: 2.5484e-05 | scale: 16384.0000 | micro time: 1.864 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6450/ 13000 | global iter:   6450/ 13000 | loss: 1.3664 | ds_loss: 0.0000 | lr: 2.5484e-05 | scale: 16384.0000 | micro time: 1.864 | step time: 1.778
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6451/ 13000 | global iter:   6451/ 13000 | loss: 0.9236 | ds_loss: 0.0000 | lr: 2.5478e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   4 | Iter:   6452/ 13000 | global iter:   6452/ 13000 | loss: 1.3220 | ds_loss: 0.0000 | lr: 2.5472e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   4 | Iter:   6453/ 13000 | global iter:   6453/ 13000 | loss: 1.3534 | ds_loss: 0.0000 | lr: 2.5466e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   4 | Iter:   6454/ 13000 | global iter:   6454/ 13000 | loss: 1.4374 | ds_loss: 0.0000 | lr: 2.5460e-05 | scale: 16384.0000 | micro time: 1.894 | step time: 0.000
train | epoch   4 | Iter:   6455/ 13000 | global iter:   6455/ 13000 | loss: 1.5193 | ds_loss: 0.0000 | lr: 2.5454e-05 | scale: 16384.0000 | micro time: 1.780 | step time: 0.000
train | epoch   4 | Iter:   6456/ 13000 | global iter:   6456/ 13000 | loss: 1.5005 | ds_loss: 0.0000 | lr: 2.5448e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   4 | Iter:   6457/ 13000 | global iter:   6457/ 13000 | loss: 1.9371 | ds_loss: 0.0000 | lr: 2.5442e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   4 | Iter:   6458/ 13000 | global iter:   6458/ 13000 | loss: 0.8943 | ds_loss: 0.0000 | lr: 2.5436e-05 | scale: 16384.0000 | micro time: 1.792 | step time: 0.000
train | epoch   4 | Iter:   6459/ 13000 | global iter:   6459/ 13000 | loss: 0.7516 | ds_loss: 0.0000 | lr: 2.5430e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 0.000
train | epoch   4 | Iter:   6460/ 13000 | global iter:   6460/ 13000 | loss: 1.3730 | ds_loss: 0.0000 | lr: 2.5424e-05 | scale: 16384.0000 | micro time: 1.824 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6460/ 13000 | global iter:   6460/ 13000 | loss: 1.3012 | ds_loss: 0.0000 | lr: 2.5424e-05 | scale: 16384.0000 | micro time: 1.824 | step time: 1.804
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6461/ 13000 | global iter:   6461/ 13000 | loss: 1.7061 | ds_loss: 0.0000 | lr: 2.5418e-05 | scale: 16384.0000 | micro time: 1.797 | step time: 0.000
train | epoch   4 | Iter:   6462/ 13000 | global iter:   6462/ 13000 | loss: 1.1916 | ds_loss: 0.0000 | lr: 2.5412e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   4 | Iter:   6463/ 13000 | global iter:   6463/ 13000 | loss: 1.1720 | ds_loss: 0.0000 | lr: 2.5406e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
train | epoch   4 | Iter:   6464/ 13000 | global iter:   6464/ 13000 | loss: 0.7370 | ds_loss: 0.0000 | lr: 2.5400e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   4 | Iter:   6465/ 13000 | global iter:   6465/ 13000 | loss: 1.7866 | ds_loss: 0.0000 | lr: 2.5394e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   4 | Iter:   6466/ 13000 | global iter:   6466/ 13000 | loss: 1.2394 | ds_loss: 0.0000 | lr: 2.5388e-05 | scale: 16384.0000 | micro time: 1.857 | step time: 0.000
train | epoch   4 | Iter:   6467/ 13000 | global iter:   6467/ 13000 | loss: 1.9201 | ds_loss: 0.0000 | lr: 2.5382e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   4 | Iter:   6468/ 13000 | global iter:   6468/ 13000 | loss: 1.2738 | ds_loss: 0.0000 | lr: 2.5376e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   4 | Iter:   6469/ 13000 | global iter:   6469/ 13000 | loss: 1.7140 | ds_loss: 0.0000 | lr: 2.5370e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   4 | Iter:   6470/ 13000 | global iter:   6470/ 13000 | loss: 0.2729 | ds_loss: 0.0000 | lr: 2.5364e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6470/ 13000 | global iter:   6470/ 13000 | loss: 1.3013 | ds_loss: 0.0000 | lr: 2.5364e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 1.829
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6471/ 13000 | global iter:   6471/ 13000 | loss: 1.3397 | ds_loss: 0.0000 | lr: 2.5357e-05 | scale: 16384.0000 | micro time: 1.866 | step time: 0.000
train | epoch   4 | Iter:   6472/ 13000 | global iter:   6472/ 13000 | loss: 1.5361 | ds_loss: 0.0000 | lr: 2.5351e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   4 | Iter:   6473/ 13000 | global iter:   6473/ 13000 | loss: 1.5896 | ds_loss: 0.0000 | lr: 2.5345e-05 | scale: 16384.0000 | micro time: 1.655 | step time: 0.000
train | epoch   4 | Iter:   6474/ 13000 | global iter:   6474/ 13000 | loss: 1.4009 | ds_loss: 0.0000 | lr: 2.5339e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   4 | Iter:   6475/ 13000 | global iter:   6475/ 13000 | loss: 1.2488 | ds_loss: 0.0000 | lr: 2.5333e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   4 | Iter:   6476/ 13000 | global iter:   6476/ 13000 | loss: 1.4325 | ds_loss: 0.0000 | lr: 2.5327e-05 | scale: 16384.0000 | micro time: 1.939 | step time: 0.000
train | epoch   4 | Iter:   6477/ 13000 | global iter:   6477/ 13000 | loss: 1.2666 | ds_loss: 0.0000 | lr: 2.5321e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   4 | Iter:   6478/ 13000 | global iter:   6478/ 13000 | loss: 1.4764 | ds_loss: 0.0000 | lr: 2.5315e-05 | scale: 16384.0000 | micro time: 1.841 | step time: 0.000
train | epoch   4 | Iter:   6479/ 13000 | global iter:   6479/ 13000 | loss: 1.3974 | ds_loss: 0.0000 | lr: 2.5309e-05 | scale: 16384.0000 | micro time: 1.861 | step time: 0.000
train | epoch   4 | Iter:   6480/ 13000 | global iter:   6480/ 13000 | loss: 1.6341 | ds_loss: 0.0000 | lr: 2.5303e-05 | scale: 16384.0000 | micro time: 1.812 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6480/ 13000 | global iter:   6480/ 13000 | loss: 1.4322 | ds_loss: 0.0000 | lr: 2.5303e-05 | scale: 16384.0000 | micro time: 1.812 | step time: 1.830
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6481/ 13000 | global iter:   6481/ 13000 | loss: 1.1284 | ds_loss: 0.0000 | lr: 2.5297e-05 | scale: 16384.0000 | micro time: 1.880 | step time: 0.000
train | epoch   4 | Iter:   6482/ 13000 | global iter:   6482/ 13000 | loss: 1.7748 | ds_loss: 0.0000 | lr: 2.5291e-05 | scale: 16384.0000 | micro time: 1.903 | step time: 0.000
train | epoch   4 | Iter:   6483/ 13000 | global iter:   6483/ 13000 | loss: 0.8564 | ds_loss: 0.0000 | lr: 2.5285e-05 | scale: 16384.0000 | micro time: 1.903 | step time: 0.000
train | epoch   4 | Iter:   6484/ 13000 | global iter:   6484/ 13000 | loss: 0.7747 | ds_loss: 0.0000 | lr: 2.5279e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   4 | Iter:   6485/ 13000 | global iter:   6485/ 13000 | loss: 1.7573 | ds_loss: 0.0000 | lr: 2.5273e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   4 | Iter:   6486/ 13000 | global iter:   6486/ 13000 | loss: 1.3553 | ds_loss: 0.0000 | lr: 2.5267e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 0.000
train | epoch   4 | Iter:   6487/ 13000 | global iter:   6487/ 13000 | loss: 0.7837 | ds_loss: 0.0000 | lr: 2.5261e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   4 | Iter:   6488/ 13000 | global iter:   6488/ 13000 | loss: 0.9508 | ds_loss: 0.0000 | lr: 2.5255e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   4 | Iter:   6489/ 13000 | global iter:   6489/ 13000 | loss: 1.3451 | ds_loss: 0.0000 | lr: 2.5249e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   4 | Iter:   6490/ 13000 | global iter:   6490/ 13000 | loss: 1.0003 | ds_loss: 0.0000 | lr: 2.5243e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6490/ 13000 | global iter:   6490/ 13000 | loss: 1.1727 | ds_loss: 0.0000 | lr: 2.5243e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 1.829
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6491/ 13000 | global iter:   6491/ 13000 | loss: 1.4786 | ds_loss: 0.0000 | lr: 2.5237e-05 | scale: 16384.0000 | micro time: 1.740 | step time: 0.000
train | epoch   4 | Iter:   6492/ 13000 | global iter:   6492/ 13000 | loss: 1.4712 | ds_loss: 0.0000 | lr: 2.5231e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   4 | Iter:   6493/ 13000 | global iter:   6493/ 13000 | loss: 1.6348 | ds_loss: 0.0000 | lr: 2.5225e-05 | scale: 16384.0000 | micro time: 1.760 | step time: 0.000
train | epoch   4 | Iter:   6494/ 13000 | global iter:   6494/ 13000 | loss: 1.0463 | ds_loss: 0.0000 | lr: 2.5219e-05 | scale: 16384.0000 | micro time: 1.766 | step time: 0.000
train | epoch   4 | Iter:   6495/ 13000 | global iter:   6495/ 13000 | loss: 1.1499 | ds_loss: 0.0000 | lr: 2.5213e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   4 | Iter:   6496/ 13000 | global iter:   6496/ 13000 | loss: 1.0953 | ds_loss: 0.0000 | lr: 2.5207e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   4 | Iter:   6497/ 13000 | global iter:   6497/ 13000 | loss: 1.2406 | ds_loss: 0.0000 | lr: 2.5201e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   4 | Iter:   6498/ 13000 | global iter:   6498/ 13000 | loss: 1.0855 | ds_loss: 0.0000 | lr: 2.5195e-05 | scale: 16384.0000 | micro time: 1.884 | step time: 0.000
train | epoch   4 | Iter:   6499/ 13000 | global iter:   6499/ 13000 | loss: 1.0724 | ds_loss: 0.0000 | lr: 2.5189e-05 | scale: 16384.0000 | micro time: 1.858 | step time: 0.000
train | epoch   4 | Iter:   6500/ 13000 | global iter:   6500/ 13000 | loss: 0.9533 | ds_loss: 0.0000 | lr: 2.5183e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
****************************************************************************************************
train | epoch   4 | Iter:   6500/ 13000 | global iter:   6500/ 13000 | loss: 1.2228 | ds_loss: 0.0000 | lr: 2.5183e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 1.807
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   4 | Iter:   6501/ 13000 | global iter:   6501/ 13000 | loss: 1.5912 | ds_loss: 0.0000 | lr: 2.5177e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   4 | Iter:   6502/ 13000 | global iter:   6502/ 13000 | loss: 1.7969 | ds_loss: 0.0000 | lr: 2.5171e-05 | scale: 16384.0000 | micro time: 1.879 | step time: 0.000
train | epoch   4 | Iter:   6503/ 13000 | global iter:   6503/ 13000 | loss: 1.1116 | ds_loss: 0.0000 | lr: 2.5165e-05 | scale: 16384.0000 | micro time: 1.734 | step time: 0.000
train | epoch   4 | Iter:   6504/ 13000 | global iter:   6504/ 13000 | loss: 1.6077 | ds_loss: 0.0000 | lr: 2.5159e-05 | scale: 16384.0000 | micro time: 1.920 | step time: 0.000
train | epoch   4 | Iter:   6505/ 13000 | global iter:   6505/ 13000 | loss: 0.5669 | ds_loss: 0.0000 | lr: 2.5153e-05 | scale: 16384.0000 | micro time: 1.623 | step time: 0.000
Sat Apr 19 14:36:58 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   40C    P2            105W /  230W |   21581MiB /  23028MiB |     95%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   44C    P2            126W /  230W |   22429MiB /  23028MiB |     90%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   42C    P2            116W /  230W |   21691MiB /  23028MiB |     97%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   40C    P2            122W /  230W |   21059MiB /  23028MiB |    100%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3      21574MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3      22422MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3      21684MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3      21052MiB |
+-----------------------------------------------------------------------------------------+

Sat Apr 19 14:36:58 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   40C    P2            103W /  230W |   21581MiB /  23028MiB |     33%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   44C    P2            122W /  230W |   22429MiB /  23028MiB |     56%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   42C    P2            108W /  230W |   21691MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   40C    P2            114W /  230W |   21059MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3      21574MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3      22422MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3      21684MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3      21052MiB |
+-----------------------------------------------------------------------------------------+

Sat Apr 19 14:36:58 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   42C    P2             97W /  230W |   21581MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   43C    P2            115W /  230W |   22429MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   42C    P2            106W /  230W |   21691MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   39C    P2            114W /  230W |   21059MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3      21574MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3      22422MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3      21684MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3      21052MiB |
+-----------------------------------------------------------------------------------------+

Sat Apr 19 14:36:58 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   42C    P2             97W /  230W |   21581MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   43C    P2            115W /  230W |   22429MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   42C    P2            106W /  230W |   21691MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   39C    P2            114W /  230W |   21059MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3      21574MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3      22422MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3      21684MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3      21052MiB |
+-----------------------------------------------------------------------------------------+

train | epoch   5 | Iter:   6506/ 13000 | global iter:   6506/ 13000 | loss: 1.3659 | ds_loss: 0.0000 | lr: 2.5146e-05 | scale: 16384.0000 | micro time: 2.721 | step time: 0.000
train | epoch   5 | Iter:   6507/ 13000 | global iter:   6507/ 13000 | loss: 1.0482 | ds_loss: 0.0000 | lr: 2.5140e-05 | scale: 16384.0000 | micro time: 1.768 | step time: 0.000
train | epoch   5 | Iter:   6508/ 13000 | global iter:   6508/ 13000 | loss: 1.1287 | ds_loss: 0.0000 | lr: 2.5134e-05 | scale: 16384.0000 | micro time: 1.849 | step time: 0.000
train | epoch   5 | Iter:   6509/ 13000 | global iter:   6509/ 13000 | loss: 1.1868 | ds_loss: 0.0000 | lr: 2.5128e-05 | scale: 16384.0000 | micro time: 1.813 | step time: 0.000
train | epoch   5 | Iter:   6510/ 13000 | global iter:   6510/ 13000 | loss: 1.1092 | ds_loss: 0.0000 | lr: 2.5122e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6510/ 13000 | global iter:   6510/ 13000 | loss: 1.2513 | ds_loss: 0.0000 | lr: 2.5122e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 1.892
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6511/ 13000 | global iter:   6511/ 13000 | loss: 0.7367 | ds_loss: 0.0000 | lr: 2.5116e-05 | scale: 16384.0000 | micro time: 1.794 | step time: 0.000
train | epoch   5 | Iter:   6512/ 13000 | global iter:   6512/ 13000 | loss: 0.8545 | ds_loss: 0.0000 | lr: 2.5110e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   5 | Iter:   6513/ 13000 | global iter:   6513/ 13000 | loss: 1.2299 | ds_loss: 0.0000 | lr: 2.5104e-05 | scale: 16384.0000 | micro time: 1.752 | step time: 0.000
train | epoch   5 | Iter:   6514/ 13000 | global iter:   6514/ 13000 | loss: 1.4106 | ds_loss: 0.0000 | lr: 2.5098e-05 | scale: 16384.0000 | micro time: 1.862 | step time: 0.000
train | epoch   5 | Iter:   6515/ 13000 | global iter:   6515/ 13000 | loss: 1.2688 | ds_loss: 0.0000 | lr: 2.5092e-05 | scale: 16384.0000 | micro time: 1.743 | step time: 0.000
train | epoch   5 | Iter:   6516/ 13000 | global iter:   6516/ 13000 | loss: 1.2160 | ds_loss: 0.0000 | lr: 2.5086e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 0.000
train | epoch   5 | Iter:   6517/ 13000 | global iter:   6517/ 13000 | loss: 1.2482 | ds_loss: 0.0000 | lr: 2.5080e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   5 | Iter:   6518/ 13000 | global iter:   6518/ 13000 | loss: 0.6390 | ds_loss: 0.0000 | lr: 2.5074e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   5 | Iter:   6519/ 13000 | global iter:   6519/ 13000 | loss: 1.3014 | ds_loss: 0.0000 | lr: 2.5068e-05 | scale: 16384.0000 | micro time: 1.744 | step time: 0.000
train | epoch   5 | Iter:   6520/ 13000 | global iter:   6520/ 13000 | loss: 0.5522 | ds_loss: 0.0000 | lr: 2.5062e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6520/ 13000 | global iter:   6520/ 13000 | loss: 1.0457 | ds_loss: 0.0000 | lr: 2.5062e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 1.808
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6521/ 13000 | global iter:   6521/ 13000 | loss: 1.1013 | ds_loss: 0.0000 | lr: 2.5056e-05 | scale: 16384.0000 | micro time: 1.848 | step time: 0.000
train | epoch   5 | Iter:   6522/ 13000 | global iter:   6522/ 13000 | loss: 1.0290 | ds_loss: 0.0000 | lr: 2.5050e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   5 | Iter:   6523/ 13000 | global iter:   6523/ 13000 | loss: 0.6821 | ds_loss: 0.0000 | lr: 2.5044e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   5 | Iter:   6524/ 13000 | global iter:   6524/ 13000 | loss: 0.4787 | ds_loss: 0.0000 | lr: 2.5038e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   5 | Iter:   6525/ 13000 | global iter:   6525/ 13000 | loss: 1.1053 | ds_loss: 0.0000 | lr: 2.5032e-05 | scale: 16384.0000 | micro time: 1.864 | step time: 0.000
train | epoch   5 | Iter:   6526/ 13000 | global iter:   6526/ 13000 | loss: 0.7569 | ds_loss: 0.0000 | lr: 2.5026e-05 | scale: 16384.0000 | micro time: 1.746 | step time: 0.000
train | epoch   5 | Iter:   6527/ 13000 | global iter:   6527/ 13000 | loss: 1.2032 | ds_loss: 0.0000 | lr: 2.5020e-05 | scale: 16384.0000 | micro time: 1.900 | step time: 0.000
train | epoch   5 | Iter:   6528/ 13000 | global iter:   6528/ 13000 | loss: 1.6205 | ds_loss: 0.0000 | lr: 2.5014e-05 | scale: 16384.0000 | micro time: 1.868 | step time: 0.000
train | epoch   5 | Iter:   6529/ 13000 | global iter:   6529/ 13000 | loss: 0.8622 | ds_loss: 0.0000 | lr: 2.5008e-05 | scale: 16384.0000 | micro time: 1.788 | step time: 0.000
train | epoch   5 | Iter:   6530/ 13000 | global iter:   6530/ 13000 | loss: 0.6569 | ds_loss: 0.0000 | lr: 2.5002e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6530/ 13000 | global iter:   6530/ 13000 | loss: 0.9496 | ds_loss: 0.0000 | lr: 2.5002e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 1.817
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6531/ 13000 | global iter:   6531/ 13000 | loss: 1.1468 | ds_loss: 0.0000 | lr: 2.4996e-05 | scale: 16384.0000 | micro time: 1.744 | step time: 0.000
train | epoch   5 | Iter:   6532/ 13000 | global iter:   6532/ 13000 | loss: 1.2920 | ds_loss: 0.0000 | lr: 2.4990e-05 | scale: 16384.0000 | micro time: 1.669 | step time: 0.000
train | epoch   5 | Iter:   6533/ 13000 | global iter:   6533/ 13000 | loss: 0.7060 | ds_loss: 0.0000 | lr: 2.4984e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   5 | Iter:   6534/ 13000 | global iter:   6534/ 13000 | loss: 0.9997 | ds_loss: 0.0000 | lr: 2.4978e-05 | scale: 16384.0000 | micro time: 1.885 | step time: 0.000
train | epoch   5 | Iter:   6535/ 13000 | global iter:   6535/ 13000 | loss: 1.0574 | ds_loss: 0.0000 | lr: 2.4972e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   5 | Iter:   6536/ 13000 | global iter:   6536/ 13000 | loss: 1.2124 | ds_loss: 0.0000 | lr: 2.4966e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   5 | Iter:   6537/ 13000 | global iter:   6537/ 13000 | loss: 1.1829 | ds_loss: 0.0000 | lr: 2.4960e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   5 | Iter:   6538/ 13000 | global iter:   6538/ 13000 | loss: 0.8935 | ds_loss: 0.0000 | lr: 2.4954e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   5 | Iter:   6539/ 13000 | global iter:   6539/ 13000 | loss: 1.2461 | ds_loss: 0.0000 | lr: 2.4947e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   5 | Iter:   6540/ 13000 | global iter:   6540/ 13000 | loss: 0.9571 | ds_loss: 0.0000 | lr: 2.4941e-05 | scale: 16384.0000 | micro time: 1.844 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6540/ 13000 | global iter:   6540/ 13000 | loss: 1.0694 | ds_loss: 0.0000 | lr: 2.4941e-05 | scale: 16384.0000 | micro time: 1.844 | step time: 1.811
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6541/ 13000 | global iter:   6541/ 13000 | loss: 1.4286 | ds_loss: 0.0000 | lr: 2.4935e-05 | scale: 16384.0000 | micro time: 1.808 | step time: 0.000
train | epoch   5 | Iter:   6542/ 13000 | global iter:   6542/ 13000 | loss: 1.2299 | ds_loss: 0.0000 | lr: 2.4929e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
train | epoch   5 | Iter:   6543/ 13000 | global iter:   6543/ 13000 | loss: 1.1047 | ds_loss: 0.0000 | lr: 2.4923e-05 | scale: 16384.0000 | micro time: 1.812 | step time: 0.000
train | epoch   5 | Iter:   6544/ 13000 | global iter:   6544/ 13000 | loss: 1.0709 | ds_loss: 0.0000 | lr: 2.4917e-05 | scale: 16384.0000 | micro time: 1.905 | step time: 0.000
train | epoch   5 | Iter:   6545/ 13000 | global iter:   6545/ 13000 | loss: 1.2495 | ds_loss: 0.0000 | lr: 2.4911e-05 | scale: 16384.0000 | micro time: 1.772 | step time: 0.000
train | epoch   5 | Iter:   6546/ 13000 | global iter:   6546/ 13000 | loss: 1.2158 | ds_loss: 0.0000 | lr: 2.4905e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   5 | Iter:   6547/ 13000 | global iter:   6547/ 13000 | loss: 0.8916 | ds_loss: 0.0000 | lr: 2.4899e-05 | scale: 16384.0000 | micro time: 1.864 | step time: 0.000
train | epoch   5 | Iter:   6548/ 13000 | global iter:   6548/ 13000 | loss: 1.2307 | ds_loss: 0.0000 | lr: 2.4893e-05 | scale: 16384.0000 | micro time: 1.858 | step time: 0.000
train | epoch   5 | Iter:   6549/ 13000 | global iter:   6549/ 13000 | loss: 0.8878 | ds_loss: 0.0000 | lr: 2.4887e-05 | scale: 16384.0000 | micro time: 1.695 | step time: 0.000
train | epoch   5 | Iter:   6550/ 13000 | global iter:   6550/ 13000 | loss: 0.6339 | ds_loss: 0.0000 | lr: 2.4881e-05 | scale: 16384.0000 | micro time: 1.848 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6550/ 13000 | global iter:   6550/ 13000 | loss: 1.0943 | ds_loss: 0.0000 | lr: 2.4881e-05 | scale: 16384.0000 | micro time: 1.848 | step time: 1.823
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6551/ 13000 | global iter:   6551/ 13000 | loss: 0.7629 | ds_loss: 0.0000 | lr: 2.4875e-05 | scale: 16384.0000 | micro time: 1.770 | step time: 0.000
train | epoch   5 | Iter:   6552/ 13000 | global iter:   6552/ 13000 | loss: 1.4091 | ds_loss: 0.0000 | lr: 2.4869e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   5 | Iter:   6553/ 13000 | global iter:   6553/ 13000 | loss: 1.3219 | ds_loss: 0.0000 | lr: 2.4863e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   5 | Iter:   6554/ 13000 | global iter:   6554/ 13000 | loss: 1.3088 | ds_loss: 0.0000 | lr: 2.4857e-05 | scale: 16384.0000 | micro time: 1.850 | step time: 0.000
train | epoch   5 | Iter:   6555/ 13000 | global iter:   6555/ 13000 | loss: 1.5250 | ds_loss: 0.0000 | lr: 2.4851e-05 | scale: 16384.0000 | micro time: 1.788 | step time: 0.000
train | epoch   5 | Iter:   6556/ 13000 | global iter:   6556/ 13000 | loss: 0.9979 | ds_loss: 0.0000 | lr: 2.4845e-05 | scale: 16384.0000 | micro time: 1.757 | step time: 0.000
train | epoch   5 | Iter:   6557/ 13000 | global iter:   6557/ 13000 | loss: 0.9637 | ds_loss: 0.0000 | lr: 2.4839e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   5 | Iter:   6558/ 13000 | global iter:   6558/ 13000 | loss: 1.1236 | ds_loss: 0.0000 | lr: 2.4833e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   5 | Iter:   6559/ 13000 | global iter:   6559/ 13000 | loss: 0.9880 | ds_loss: 0.0000 | lr: 2.4827e-05 | scale: 16384.0000 | micro time: 1.731 | step time: 0.000
train | epoch   5 | Iter:   6560/ 13000 | global iter:   6560/ 13000 | loss: 0.8590 | ds_loss: 0.0000 | lr: 2.4821e-05 | scale: 16384.0000 | micro time: 1.824 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6560/ 13000 | global iter:   6560/ 13000 | loss: 1.1260 | ds_loss: 0.0000 | lr: 2.4821e-05 | scale: 16384.0000 | micro time: 1.824 | step time: 1.802
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6561/ 13000 | global iter:   6561/ 13000 | loss: 1.0173 | ds_loss: 0.0000 | lr: 2.4815e-05 | scale: 16384.0000 | micro time: 1.813 | step time: 0.000
train | epoch   5 | Iter:   6562/ 13000 | global iter:   6562/ 13000 | loss: 0.9633 | ds_loss: 0.0000 | lr: 2.4809e-05 | scale: 16384.0000 | micro time: 1.919 | step time: 0.000
train | epoch   5 | Iter:   6563/ 13000 | global iter:   6563/ 13000 | loss: 1.2490 | ds_loss: 0.0000 | lr: 2.4803e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   5 | Iter:   6564/ 13000 | global iter:   6564/ 13000 | loss: 1.1381 | ds_loss: 0.0000 | lr: 2.4797e-05 | scale: 16384.0000 | micro time: 1.737 | step time: 0.000
train | epoch   5 | Iter:   6565/ 13000 | global iter:   6565/ 13000 | loss: 1.4047 | ds_loss: 0.0000 | lr: 2.4791e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   5 | Iter:   6566/ 13000 | global iter:   6566/ 13000 | loss: 0.7018 | ds_loss: 0.0000 | lr: 2.4785e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   5 | Iter:   6567/ 13000 | global iter:   6567/ 13000 | loss: 1.5033 | ds_loss: 0.0000 | lr: 2.4779e-05 | scale: 16384.0000 | micro time: 1.887 | step time: 0.000
train | epoch   5 | Iter:   6568/ 13000 | global iter:   6568/ 13000 | loss: 1.1487 | ds_loss: 0.0000 | lr: 2.4773e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   5 | Iter:   6569/ 13000 | global iter:   6569/ 13000 | loss: 1.1680 | ds_loss: 0.0000 | lr: 2.4767e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   5 | Iter:   6570/ 13000 | global iter:   6570/ 13000 | loss: 1.2435 | ds_loss: 0.0000 | lr: 2.4761e-05 | scale: 16384.0000 | micro time: 1.886 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6570/ 13000 | global iter:   6570/ 13000 | loss: 1.1538 | ds_loss: 0.0000 | lr: 2.4761e-05 | scale: 16384.0000 | micro time: 1.886 | step time: 1.830
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6571/ 13000 | global iter:   6571/ 13000 | loss: 1.0849 | ds_loss: 0.0000 | lr: 2.4755e-05 | scale: 16384.0000 | micro time: 1.802 | step time: 0.000
train | epoch   5 | Iter:   6572/ 13000 | global iter:   6572/ 13000 | loss: 1.0877 | ds_loss: 0.0000 | lr: 2.4749e-05 | scale: 16384.0000 | micro time: 1.884 | step time: 0.000
train | epoch   5 | Iter:   6573/ 13000 | global iter:   6573/ 13000 | loss: 1.4048 | ds_loss: 0.0000 | lr: 2.4743e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   5 | Iter:   6574/ 13000 | global iter:   6574/ 13000 | loss: 1.1202 | ds_loss: 0.0000 | lr: 2.4736e-05 | scale: 16384.0000 | micro time: 1.880 | step time: 0.000
train | epoch   5 | Iter:   6575/ 13000 | global iter:   6575/ 13000 | loss: 1.4328 | ds_loss: 0.0000 | lr: 2.4730e-05 | scale: 16384.0000 | micro time: 1.868 | step time: 0.000
train | epoch   5 | Iter:   6576/ 13000 | global iter:   6576/ 13000 | loss: 1.5670 | ds_loss: 0.0000 | lr: 2.4724e-05 | scale: 16384.0000 | micro time: 1.873 | step time: 0.000
train | epoch   5 | Iter:   6577/ 13000 | global iter:   6577/ 13000 | loss: 0.5935 | ds_loss: 0.0000 | lr: 2.4718e-05 | scale: 16384.0000 | micro time: 1.778 | step time: 0.000
train | epoch   5 | Iter:   6578/ 13000 | global iter:   6578/ 13000 | loss: 0.9814 | ds_loss: 0.0000 | lr: 2.4712e-05 | scale: 16384.0000 | micro time: 1.814 | step time: 0.000
train | epoch   5 | Iter:   6579/ 13000 | global iter:   6579/ 13000 | loss: 1.3572 | ds_loss: 0.0000 | lr: 2.4706e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   5 | Iter:   6580/ 13000 | global iter:   6580/ 13000 | loss: 0.6822 | ds_loss: 0.0000 | lr: 2.4700e-05 | scale: 16384.0000 | micro time: 1.891 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6580/ 13000 | global iter:   6580/ 13000 | loss: 1.1312 | ds_loss: 0.0000 | lr: 2.4700e-05 | scale: 16384.0000 | micro time: 1.891 | step time: 1.837
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6581/ 13000 | global iter:   6581/ 13000 | loss: 0.8107 | ds_loss: 0.0000 | lr: 2.4694e-05 | scale: 16384.0000 | micro time: 1.914 | step time: 0.000
train | epoch   5 | Iter:   6582/ 13000 | global iter:   6582/ 13000 | loss: 0.8638 | ds_loss: 0.0000 | lr: 2.4688e-05 | scale: 16384.0000 | micro time: 1.825 | step time: 0.000
train | epoch   5 | Iter:   6583/ 13000 | global iter:   6583/ 13000 | loss: 1.4965 | ds_loss: 0.0000 | lr: 2.4682e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 0.000
train | epoch   5 | Iter:   6584/ 13000 | global iter:   6584/ 13000 | loss: 0.5759 | ds_loss: 0.0000 | lr: 2.4676e-05 | scale: 16384.0000 | micro time: 1.854 | step time: 0.000
train | epoch   5 | Iter:   6585/ 13000 | global iter:   6585/ 13000 | loss: 0.7943 | ds_loss: 0.0000 | lr: 2.4670e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   5 | Iter:   6586/ 13000 | global iter:   6586/ 13000 | loss: 1.0700 | ds_loss: 0.0000 | lr: 2.4664e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   5 | Iter:   6587/ 13000 | global iter:   6587/ 13000 | loss: 0.8236 | ds_loss: 0.0000 | lr: 2.4658e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   5 | Iter:   6588/ 13000 | global iter:   6588/ 13000 | loss: 1.0731 | ds_loss: 0.0000 | lr: 2.4652e-05 | scale: 16384.0000 | micro time: 1.797 | step time: 0.000
train | epoch   5 | Iter:   6589/ 13000 | global iter:   6589/ 13000 | loss: 1.2712 | ds_loss: 0.0000 | lr: 2.4646e-05 | scale: 16384.0000 | micro time: 1.829 | step time: 0.000
train | epoch   5 | Iter:   6590/ 13000 | global iter:   6590/ 13000 | loss: 1.3018 | ds_loss: 0.0000 | lr: 2.4640e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6590/ 13000 | global iter:   6590/ 13000 | loss: 1.0081 | ds_loss: 0.0000 | lr: 2.4640e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 1.831
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6591/ 13000 | global iter:   6591/ 13000 | loss: 1.2647 | ds_loss: 0.0000 | lr: 2.4634e-05 | scale: 16384.0000 | micro time: 1.846 | step time: 0.000
train | epoch   5 | Iter:   6592/ 13000 | global iter:   6592/ 13000 | loss: 1.2150 | ds_loss: 0.0000 | lr: 2.4628e-05 | scale: 16384.0000 | micro time: 1.822 | step time: 0.000
train | epoch   5 | Iter:   6593/ 13000 | global iter:   6593/ 13000 | loss: 1.6028 | ds_loss: 0.0000 | lr: 2.4622e-05 | scale: 16384.0000 | micro time: 1.764 | step time: 0.000
train | epoch   5 | Iter:   6594/ 13000 | global iter:   6594/ 13000 | loss: 0.9531 | ds_loss: 0.0000 | lr: 2.4616e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   5 | Iter:   6595/ 13000 | global iter:   6595/ 13000 | loss: 1.3563 | ds_loss: 0.0000 | lr: 2.4610e-05 | scale: 16384.0000 | micro time: 1.754 | step time: 0.000
train | epoch   5 | Iter:   6596/ 13000 | global iter:   6596/ 13000 | loss: 1.0697 | ds_loss: 0.0000 | lr: 2.4604e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   5 | Iter:   6597/ 13000 | global iter:   6597/ 13000 | loss: 1.2693 | ds_loss: 0.0000 | lr: 2.4598e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   5 | Iter:   6598/ 13000 | global iter:   6598/ 13000 | loss: 1.4235 | ds_loss: 0.0000 | lr: 2.4592e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   5 | Iter:   6599/ 13000 | global iter:   6599/ 13000 | loss: 1.1479 | ds_loss: 0.0000 | lr: 2.4586e-05 | scale: 16384.0000 | micro time: 1.679 | step time: 0.000
train | epoch   5 | Iter:   6600/ 13000 | global iter:   6600/ 13000 | loss: 1.0834 | ds_loss: 0.0000 | lr: 2.4580e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6600/ 13000 | global iter:   6600/ 13000 | loss: 1.2386 | ds_loss: 0.0000 | lr: 2.4580e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 1.802
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6601/ 13000 | global iter:   6601/ 13000 | loss: 1.4714 | ds_loss: 0.0000 | lr: 2.4574e-05 | scale: 16384.0000 | micro time: 1.757 | step time: 0.000
train | epoch   5 | Iter:   6602/ 13000 | global iter:   6602/ 13000 | loss: 0.9550 | ds_loss: 0.0000 | lr: 2.4568e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   5 | Iter:   6603/ 13000 | global iter:   6603/ 13000 | loss: 1.1031 | ds_loss: 0.0000 | lr: 2.4562e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   5 | Iter:   6604/ 13000 | global iter:   6604/ 13000 | loss: 1.2607 | ds_loss: 0.0000 | lr: 2.4556e-05 | scale: 16384.0000 | micro time: 1.816 | step time: 0.000
train | epoch   5 | Iter:   6605/ 13000 | global iter:   6605/ 13000 | loss: 1.4997 | ds_loss: 0.0000 | lr: 2.4550e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   5 | Iter:   6606/ 13000 | global iter:   6606/ 13000 | loss: 1.2279 | ds_loss: 0.0000 | lr: 2.4544e-05 | scale: 16384.0000 | micro time: 1.849 | step time: 0.000
train | epoch   5 | Iter:   6607/ 13000 | global iter:   6607/ 13000 | loss: 0.9710 | ds_loss: 0.0000 | lr: 2.4538e-05 | scale: 16384.0000 | micro time: 1.860 | step time: 0.000
train | epoch   5 | Iter:   6608/ 13000 | global iter:   6608/ 13000 | loss: 1.1116 | ds_loss: 0.0000 | lr: 2.4532e-05 | scale: 16384.0000 | micro time: 1.830 | step time: 0.000
train | epoch   5 | Iter:   6609/ 13000 | global iter:   6609/ 13000 | loss: 0.5437 | ds_loss: 0.0000 | lr: 2.4525e-05 | scale: 16384.0000 | micro time: 1.764 | step time: 0.000
train | epoch   5 | Iter:   6610/ 13000 | global iter:   6610/ 13000 | loss: 0.6564 | ds_loss: 0.0000 | lr: 2.4519e-05 | scale: 16384.0000 | micro time: 1.886 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6610/ 13000 | global iter:   6610/ 13000 | loss: 1.0800 | ds_loss: 0.0000 | lr: 2.4519e-05 | scale: 16384.0000 | micro time: 1.886 | step time: 1.823
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6611/ 13000 | global iter:   6611/ 13000 | loss: 1.3067 | ds_loss: 0.0000 | lr: 2.4513e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   5 | Iter:   6612/ 13000 | global iter:   6612/ 13000 | loss: 1.3395 | ds_loss: 0.0000 | lr: 2.4507e-05 | scale: 16384.0000 | micro time: 1.838 | step time: 0.000
train | epoch   5 | Iter:   6613/ 13000 | global iter:   6613/ 13000 | loss: 1.2092 | ds_loss: 0.0000 | lr: 2.4501e-05 | scale: 16384.0000 | micro time: 1.923 | step time: 0.000
train | epoch   5 | Iter:   6614/ 13000 | global iter:   6614/ 13000 | loss: 1.1235 | ds_loss: 0.0000 | lr: 2.4495e-05 | scale: 16384.0000 | micro time: 1.814 | step time: 0.000
train | epoch   5 | Iter:   6615/ 13000 | global iter:   6615/ 13000 | loss: 0.5360 | ds_loss: 0.0000 | lr: 2.4489e-05 | scale: 16384.0000 | micro time: 1.730 | step time: 0.000
train | epoch   5 | Iter:   6616/ 13000 | global iter:   6616/ 13000 | loss: 1.1606 | ds_loss: 0.0000 | lr: 2.4483e-05 | scale: 16384.0000 | micro time: 1.788 | step time: 0.000
train | epoch   5 | Iter:   6617/ 13000 | global iter:   6617/ 13000 | loss: 1.7757 | ds_loss: 0.0000 | lr: 2.4477e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   5 | Iter:   6618/ 13000 | global iter:   6618/ 13000 | loss: 0.8901 | ds_loss: 0.0000 | lr: 2.4471e-05 | scale: 16384.0000 | micro time: 1.911 | step time: 0.000
train | epoch   5 | Iter:   6619/ 13000 | global iter:   6619/ 13000 | loss: 0.7707 | ds_loss: 0.0000 | lr: 2.4465e-05 | scale: 16384.0000 | micro time: 1.856 | step time: 0.000
train | epoch   5 | Iter:   6620/ 13000 | global iter:   6620/ 13000 | loss: 0.7381 | ds_loss: 0.0000 | lr: 2.4459e-05 | scale: 16384.0000 | micro time: 1.842 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6620/ 13000 | global iter:   6620/ 13000 | loss: 1.0850 | ds_loss: 0.0000 | lr: 2.4459e-05 | scale: 16384.0000 | micro time: 1.842 | step time: 1.828
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6621/ 13000 | global iter:   6621/ 13000 | loss: 0.8359 | ds_loss: 0.0000 | lr: 2.4453e-05 | scale: 16384.0000 | micro time: 1.870 | step time: 0.000
train | epoch   5 | Iter:   6622/ 13000 | global iter:   6622/ 13000 | loss: 0.6456 | ds_loss: 0.0000 | lr: 2.4447e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   5 | Iter:   6623/ 13000 | global iter:   6623/ 13000 | loss: 0.9773 | ds_loss: 0.0000 | lr: 2.4441e-05 | scale: 16384.0000 | micro time: 1.727 | step time: 0.000
train | epoch   5 | Iter:   6624/ 13000 | global iter:   6624/ 13000 | loss: 0.6937 | ds_loss: 0.0000 | lr: 2.4435e-05 | scale: 16384.0000 | micro time: 1.691 | step time: 0.000
train | epoch   5 | Iter:   6625/ 13000 | global iter:   6625/ 13000 | loss: 1.1896 | ds_loss: 0.0000 | lr: 2.4429e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   5 | Iter:   6626/ 13000 | global iter:   6626/ 13000 | loss: 1.4503 | ds_loss: 0.0000 | lr: 2.4423e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   5 | Iter:   6627/ 13000 | global iter:   6627/ 13000 | loss: 1.5533 | ds_loss: 0.0000 | lr: 2.4417e-05 | scale: 16384.0000 | micro time: 1.800 | step time: 0.000
train | epoch   5 | Iter:   6628/ 13000 | global iter:   6628/ 13000 | loss: 1.0014 | ds_loss: 0.0000 | lr: 2.4411e-05 | scale: 16384.0000 | micro time: 1.770 | step time: 0.000
train | epoch   5 | Iter:   6629/ 13000 | global iter:   6629/ 13000 | loss: 1.5187 | ds_loss: 0.0000 | lr: 2.4405e-05 | scale: 16384.0000 | micro time: 1.814 | step time: 0.000
train | epoch   5 | Iter:   6630/ 13000 | global iter:   6630/ 13000 | loss: 1.0818 | ds_loss: 0.0000 | lr: 2.4399e-05 | scale: 16384.0000 | micro time: 1.749 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6630/ 13000 | global iter:   6630/ 13000 | loss: 1.0948 | ds_loss: 0.0000 | lr: 2.4399e-05 | scale: 16384.0000 | micro time: 1.749 | step time: 1.783
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6631/ 13000 | global iter:   6631/ 13000 | loss: 1.0929 | ds_loss: 0.0000 | lr: 2.4393e-05 | scale: 16384.0000 | micro time: 1.716 | step time: 0.000
train | epoch   5 | Iter:   6632/ 13000 | global iter:   6632/ 13000 | loss: 0.5974 | ds_loss: 0.0000 | lr: 2.4387e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   5 | Iter:   6633/ 13000 | global iter:   6633/ 13000 | loss: 1.1466 | ds_loss: 0.0000 | lr: 2.4381e-05 | scale: 16384.0000 | micro time: 1.802 | step time: 0.000
train | epoch   5 | Iter:   6634/ 13000 | global iter:   6634/ 13000 | loss: 0.7079 | ds_loss: 0.0000 | lr: 2.4375e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   5 | Iter:   6635/ 13000 | global iter:   6635/ 13000 | loss: 1.1172 | ds_loss: 0.0000 | lr: 2.4369e-05 | scale: 16384.0000 | micro time: 1.717 | step time: 0.000
train | epoch   5 | Iter:   6636/ 13000 | global iter:   6636/ 13000 | loss: 1.1710 | ds_loss: 0.0000 | lr: 2.4363e-05 | scale: 16384.0000 | micro time: 1.797 | step time: 0.000
train | epoch   5 | Iter:   6637/ 13000 | global iter:   6637/ 13000 | loss: 0.8563 | ds_loss: 0.0000 | lr: 2.4357e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   5 | Iter:   6638/ 13000 | global iter:   6638/ 13000 | loss: 1.2900 | ds_loss: 0.0000 | lr: 2.4351e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   5 | Iter:   6639/ 13000 | global iter:   6639/ 13000 | loss: 0.8526 | ds_loss: 0.0000 | lr: 2.4345e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   5 | Iter:   6640/ 13000 | global iter:   6640/ 13000 | loss: 0.8641 | ds_loss: 0.0000 | lr: 2.4339e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6640/ 13000 | global iter:   6640/ 13000 | loss: 0.9696 | ds_loss: 0.0000 | lr: 2.4339e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 1.801
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6641/ 13000 | global iter:   6641/ 13000 | loss: 0.9478 | ds_loss: 0.0000 | lr: 2.4333e-05 | scale: 16384.0000 | micro time: 1.708 | step time: 0.000
train | epoch   5 | Iter:   6642/ 13000 | global iter:   6642/ 13000 | loss: 0.8030 | ds_loss: 0.0000 | lr: 2.4327e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   5 | Iter:   6643/ 13000 | global iter:   6643/ 13000 | loss: 1.0770 | ds_loss: 0.0000 | lr: 2.4321e-05 | scale: 16384.0000 | micro time: 1.808 | step time: 0.000
train | epoch   5 | Iter:   6644/ 13000 | global iter:   6644/ 13000 | loss: 0.7781 | ds_loss: 0.0000 | lr: 2.4315e-05 | scale: 16384.0000 | micro time: 1.732 | step time: 0.000
train | epoch   5 | Iter:   6645/ 13000 | global iter:   6645/ 13000 | loss: 1.2327 | ds_loss: 0.0000 | lr: 2.4308e-05 | scale: 16384.0000 | micro time: 1.741 | step time: 0.000
train | epoch   5 | Iter:   6646/ 13000 | global iter:   6646/ 13000 | loss: 0.5135 | ds_loss: 0.0000 | lr: 2.4302e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 0.000
train | epoch   5 | Iter:   6647/ 13000 | global iter:   6647/ 13000 | loss: 1.2797 | ds_loss: 0.0000 | lr: 2.4296e-05 | scale: 16384.0000 | micro time: 1.673 | step time: 0.000
train | epoch   5 | Iter:   6648/ 13000 | global iter:   6648/ 13000 | loss: 1.1368 | ds_loss: 0.0000 | lr: 2.4290e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   5 | Iter:   6649/ 13000 | global iter:   6649/ 13000 | loss: 1.2204 | ds_loss: 0.0000 | lr: 2.4284e-05 | scale: 16384.0000 | micro time: 1.816 | step time: 0.000
train | epoch   5 | Iter:   6650/ 13000 | global iter:   6650/ 13000 | loss: 1.3207 | ds_loss: 0.0000 | lr: 2.4278e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6650/ 13000 | global iter:   6650/ 13000 | loss: 1.0310 | ds_loss: 0.0000 | lr: 2.4278e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 1.774
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6651/ 13000 | global iter:   6651/ 13000 | loss: 0.9712 | ds_loss: 0.0000 | lr: 2.4272e-05 | scale: 16384.0000 | micro time: 1.754 | step time: 0.000
train | epoch   5 | Iter:   6652/ 13000 | global iter:   6652/ 13000 | loss: 1.1438 | ds_loss: 0.0000 | lr: 2.4266e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   5 | Iter:   6653/ 13000 | global iter:   6653/ 13000 | loss: 1.3471 | ds_loss: 0.0000 | lr: 2.4260e-05 | scale: 16384.0000 | micro time: 1.750 | step time: 0.000
train | epoch   5 | Iter:   6654/ 13000 | global iter:   6654/ 13000 | loss: 1.4785 | ds_loss: 0.0000 | lr: 2.4254e-05 | scale: 16384.0000 | micro time: 1.766 | step time: 0.000
train | epoch   5 | Iter:   6655/ 13000 | global iter:   6655/ 13000 | loss: 1.3164 | ds_loss: 0.0000 | lr: 2.4248e-05 | scale: 16384.0000 | micro time: 1.748 | step time: 0.000
train | epoch   5 | Iter:   6656/ 13000 | global iter:   6656/ 13000 | loss: 0.5667 | ds_loss: 0.0000 | lr: 2.4242e-05 | scale: 16384.0000 | micro time: 1.927 | step time: 0.000
train | epoch   5 | Iter:   6657/ 13000 | global iter:   6657/ 13000 | loss: 1.4811 | ds_loss: 0.0000 | lr: 2.4236e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   5 | Iter:   6658/ 13000 | global iter:   6658/ 13000 | loss: 0.6055 | ds_loss: 0.0000 | lr: 2.4230e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   5 | Iter:   6659/ 13000 | global iter:   6659/ 13000 | loss: 0.4636 | ds_loss: 0.0000 | lr: 2.4224e-05 | scale: 16384.0000 | micro time: 1.798 | step time: 0.000
train | epoch   5 | Iter:   6660/ 13000 | global iter:   6660/ 13000 | loss: 0.8736 | ds_loss: 0.0000 | lr: 2.4218e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6660/ 13000 | global iter:   6660/ 13000 | loss: 1.0248 | ds_loss: 0.0000 | lr: 2.4218e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 1.798
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6661/ 13000 | global iter:   6661/ 13000 | loss: 1.2012 | ds_loss: 0.0000 | lr: 2.4212e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   5 | Iter:   6662/ 13000 | global iter:   6662/ 13000 | loss: 0.8487 | ds_loss: 0.0000 | lr: 2.4206e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   5 | Iter:   6663/ 13000 | global iter:   6663/ 13000 | loss: 1.5477 | ds_loss: 0.0000 | lr: 2.4200e-05 | scale: 16384.0000 | micro time: 1.725 | step time: 0.000
train | epoch   5 | Iter:   6664/ 13000 | global iter:   6664/ 13000 | loss: 1.4532 | ds_loss: 0.0000 | lr: 2.4194e-05 | scale: 16384.0000 | micro time: 1.829 | step time: 0.000
train | epoch   5 | Iter:   6665/ 13000 | global iter:   6665/ 13000 | loss: 1.1391 | ds_loss: 0.0000 | lr: 2.4188e-05 | scale: 16384.0000 | micro time: 1.838 | step time: 0.000
train | epoch   5 | Iter:   6666/ 13000 | global iter:   6666/ 13000 | loss: 1.1402 | ds_loss: 0.0000 | lr: 2.4182e-05 | scale: 16384.0000 | micro time: 1.804 | step time: 0.000
train | epoch   5 | Iter:   6667/ 13000 | global iter:   6667/ 13000 | loss: 0.9336 | ds_loss: 0.0000 | lr: 2.4176e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   5 | Iter:   6668/ 13000 | global iter:   6668/ 13000 | loss: 0.7294 | ds_loss: 0.0000 | lr: 2.4170e-05 | scale: 16384.0000 | micro time: 1.777 | step time: 0.000
train | epoch   5 | Iter:   6669/ 13000 | global iter:   6669/ 13000 | loss: 1.4226 | ds_loss: 0.0000 | lr: 2.4164e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   5 | Iter:   6670/ 13000 | global iter:   6670/ 13000 | loss: 1.1057 | ds_loss: 0.0000 | lr: 2.4158e-05 | scale: 16384.0000 | micro time: 1.727 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6670/ 13000 | global iter:   6670/ 13000 | loss: 1.1521 | ds_loss: 0.0000 | lr: 2.4158e-05 | scale: 16384.0000 | micro time: 1.727 | step time: 1.788
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6671/ 13000 | global iter:   6671/ 13000 | loss: 0.8537 | ds_loss: 0.0000 | lr: 2.4152e-05 | scale: 16384.0000 | micro time: 1.750 | step time: 0.000
train | epoch   5 | Iter:   6672/ 13000 | global iter:   6672/ 13000 | loss: 1.3271 | ds_loss: 0.0000 | lr: 2.4146e-05 | scale: 16384.0000 | micro time: 1.774 | step time: 0.000
train | epoch   5 | Iter:   6673/ 13000 | global iter:   6673/ 13000 | loss: 1.5811 | ds_loss: 0.0000 | lr: 2.4140e-05 | scale: 16384.0000 | micro time: 1.742 | step time: 0.000
train | epoch   5 | Iter:   6674/ 13000 | global iter:   6674/ 13000 | loss: 1.4107 | ds_loss: 0.0000 | lr: 2.4134e-05 | scale: 16384.0000 | micro time: 1.658 | step time: 0.000
train | epoch   5 | Iter:   6675/ 13000 | global iter:   6675/ 13000 | loss: 0.3255 | ds_loss: 0.0000 | lr: 2.4128e-05 | scale: 16384.0000 | micro time: 1.849 | step time: 0.000
train | epoch   5 | Iter:   6676/ 13000 | global iter:   6676/ 13000 | loss: 1.0274 | ds_loss: 0.0000 | lr: 2.4122e-05 | scale: 16384.0000 | micro time: 1.792 | step time: 0.000
train | epoch   5 | Iter:   6677/ 13000 | global iter:   6677/ 13000 | loss: 1.5956 | ds_loss: 0.0000 | lr: 2.4116e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
train | epoch   5 | Iter:   6678/ 13000 | global iter:   6678/ 13000 | loss: 1.1262 | ds_loss: 0.0000 | lr: 2.4110e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   5 | Iter:   6679/ 13000 | global iter:   6679/ 13000 | loss: 0.6901 | ds_loss: 0.0000 | lr: 2.4104e-05 | scale: 16384.0000 | micro time: 1.711 | step time: 0.000
train | epoch   5 | Iter:   6680/ 13000 | global iter:   6680/ 13000 | loss: 1.0523 | ds_loss: 0.0000 | lr: 2.4098e-05 | scale: 16384.0000 | micro time: 1.833 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6680/ 13000 | global iter:   6680/ 13000 | loss: 1.0990 | ds_loss: 0.0000 | lr: 2.4098e-05 | scale: 16384.0000 | micro time: 1.833 | step time: 1.774
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6681/ 13000 | global iter:   6681/ 13000 | loss: 1.8179 | ds_loss: 0.0000 | lr: 2.4092e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   5 | Iter:   6682/ 13000 | global iter:   6682/ 13000 | loss: 1.1629 | ds_loss: 0.0000 | lr: 2.4086e-05 | scale: 16384.0000 | micro time: 1.709 | step time: 0.000
train | epoch   5 | Iter:   6683/ 13000 | global iter:   6683/ 13000 | loss: 0.9344 | ds_loss: 0.0000 | lr: 2.4080e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   5 | Iter:   6684/ 13000 | global iter:   6684/ 13000 | loss: 1.2146 | ds_loss: 0.0000 | lr: 2.4073e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   5 | Iter:   6685/ 13000 | global iter:   6685/ 13000 | loss: 0.6776 | ds_loss: 0.0000 | lr: 2.4067e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 0.000
train | epoch   5 | Iter:   6686/ 13000 | global iter:   6686/ 13000 | loss: 1.4797 | ds_loss: 0.0000 | lr: 2.4061e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   5 | Iter:   6687/ 13000 | global iter:   6687/ 13000 | loss: 1.0849 | ds_loss: 0.0000 | lr: 2.4055e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   5 | Iter:   6688/ 13000 | global iter:   6688/ 13000 | loss: 1.3051 | ds_loss: 0.0000 | lr: 2.4049e-05 | scale: 16384.0000 | micro time: 1.727 | step time: 0.000
train | epoch   5 | Iter:   6689/ 13000 | global iter:   6689/ 13000 | loss: 0.5194 | ds_loss: 0.0000 | lr: 2.4043e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   5 | Iter:   6690/ 13000 | global iter:   6690/ 13000 | loss: 0.9580 | ds_loss: 0.0000 | lr: 2.4037e-05 | scale: 16384.0000 | micro time: 1.715 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6690/ 13000 | global iter:   6690/ 13000 | loss: 1.1155 | ds_loss: 0.0000 | lr: 2.4037e-05 | scale: 16384.0000 | micro time: 1.715 | step time: 1.778
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6691/ 13000 | global iter:   6691/ 13000 | loss: 0.9334 | ds_loss: 0.0000 | lr: 2.4031e-05 | scale: 16384.0000 | micro time: 1.786 | step time: 0.000
train | epoch   5 | Iter:   6692/ 13000 | global iter:   6692/ 13000 | loss: 0.7290 | ds_loss: 0.0000 | lr: 2.4025e-05 | scale: 16384.0000 | micro time: 1.895 | step time: 0.000
train | epoch   5 | Iter:   6693/ 13000 | global iter:   6693/ 13000 | loss: 0.8194 | ds_loss: 0.0000 | lr: 2.4019e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   5 | Iter:   6694/ 13000 | global iter:   6694/ 13000 | loss: 1.4173 | ds_loss: 0.0000 | lr: 2.4013e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
train | epoch   5 | Iter:   6695/ 13000 | global iter:   6695/ 13000 | loss: 1.4244 | ds_loss: 0.0000 | lr: 2.4007e-05 | scale: 16384.0000 | micro time: 1.841 | step time: 0.000
train | epoch   5 | Iter:   6696/ 13000 | global iter:   6696/ 13000 | loss: 1.3677 | ds_loss: 0.0000 | lr: 2.4001e-05 | scale: 16384.0000 | micro time: 1.712 | step time: 0.000
train | epoch   5 | Iter:   6697/ 13000 | global iter:   6697/ 13000 | loss: 1.4247 | ds_loss: 0.0000 | lr: 2.3995e-05 | scale: 16384.0000 | micro time: 1.752 | step time: 0.000
train | epoch   5 | Iter:   6698/ 13000 | global iter:   6698/ 13000 | loss: 0.9931 | ds_loss: 0.0000 | lr: 2.3989e-05 | scale: 16384.0000 | micro time: 1.796 | step time: 0.000
train | epoch   5 | Iter:   6699/ 13000 | global iter:   6699/ 13000 | loss: 1.7454 | ds_loss: 0.0000 | lr: 2.3983e-05 | scale: 16384.0000 | micro time: 1.774 | step time: 0.000
train | epoch   5 | Iter:   6700/ 13000 | global iter:   6700/ 13000 | loss: 1.1665 | ds_loss: 0.0000 | lr: 2.3977e-05 | scale: 16384.0000 | micro time: 1.754 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6700/ 13000 | global iter:   6700/ 13000 | loss: 1.2021 | ds_loss: 0.0000 | lr: 2.3977e-05 | scale: 16384.0000 | micro time: 1.754 | step time: 1.803
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6701/ 13000 | global iter:   6701/ 13000 | loss: 1.5997 | ds_loss: 0.0000 | lr: 2.3971e-05 | scale: 16384.0000 | micro time: 1.792 | step time: 0.000
train | epoch   5 | Iter:   6702/ 13000 | global iter:   6702/ 13000 | loss: 1.6556 | ds_loss: 0.0000 | lr: 2.3965e-05 | scale: 16384.0000 | micro time: 1.845 | step time: 0.000
train | epoch   5 | Iter:   6703/ 13000 | global iter:   6703/ 13000 | loss: 1.2324 | ds_loss: 0.0000 | lr: 2.3959e-05 | scale: 16384.0000 | micro time: 1.858 | step time: 0.000
train | epoch   5 | Iter:   6704/ 13000 | global iter:   6704/ 13000 | loss: 1.0860 | ds_loss: 0.0000 | lr: 2.3953e-05 | scale: 16384.0000 | micro time: 1.842 | step time: 0.000
train | epoch   5 | Iter:   6705/ 13000 | global iter:   6705/ 13000 | loss: 1.5820 | ds_loss: 0.0000 | lr: 2.3947e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   5 | Iter:   6706/ 13000 | global iter:   6706/ 13000 | loss: 1.5054 | ds_loss: 0.0000 | lr: 2.3941e-05 | scale: 16384.0000 | micro time: 1.675 | step time: 0.000
train | epoch   5 | Iter:   6707/ 13000 | global iter:   6707/ 13000 | loss: 1.4587 | ds_loss: 0.0000 | lr: 2.3935e-05 | scale: 16384.0000 | micro time: 1.746 | step time: 0.000
train | epoch   5 | Iter:   6708/ 13000 | global iter:   6708/ 13000 | loss: 1.3990 | ds_loss: 0.0000 | lr: 2.3929e-05 | scale: 16384.0000 | micro time: 1.872 | step time: 0.000
train | epoch   5 | Iter:   6709/ 13000 | global iter:   6709/ 13000 | loss: 0.7451 | ds_loss: 0.0000 | lr: 2.3923e-05 | scale: 16384.0000 | micro time: 1.723 | step time: 0.000
train | epoch   5 | Iter:   6710/ 13000 | global iter:   6710/ 13000 | loss: 0.9499 | ds_loss: 0.0000 | lr: 2.3917e-05 | scale: 16384.0000 | micro time: 1.773 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6710/ 13000 | global iter:   6710/ 13000 | loss: 1.3214 | ds_loss: 0.0000 | lr: 2.3917e-05 | scale: 16384.0000 | micro time: 1.773 | step time: 1.800
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6711/ 13000 | global iter:   6711/ 13000 | loss: 1.3669 | ds_loss: 0.0000 | lr: 2.3911e-05 | scale: 16384.0000 | micro time: 1.804 | step time: 0.000
train | epoch   5 | Iter:   6712/ 13000 | global iter:   6712/ 13000 | loss: 1.1958 | ds_loss: 0.0000 | lr: 2.3905e-05 | scale: 16384.0000 | micro time: 1.838 | step time: 0.000
train | epoch   5 | Iter:   6713/ 13000 | global iter:   6713/ 13000 | loss: 1.7511 | ds_loss: 0.0000 | lr: 2.3899e-05 | scale: 16384.0000 | micro time: 1.901 | step time: 0.000
train | epoch   5 | Iter:   6714/ 13000 | global iter:   6714/ 13000 | loss: 1.1637 | ds_loss: 0.0000 | lr: 2.3893e-05 | scale: 16384.0000 | micro time: 1.866 | step time: 0.000
train | epoch   5 | Iter:   6715/ 13000 | global iter:   6715/ 13000 | loss: 1.2426 | ds_loss: 0.0000 | lr: 2.3887e-05 | scale: 16384.0000 | micro time: 1.908 | step time: 0.000
train | epoch   5 | Iter:   6716/ 13000 | global iter:   6716/ 13000 | loss: 1.1793 | ds_loss: 0.0000 | lr: 2.3881e-05 | scale: 16384.0000 | micro time: 1.846 | step time: 0.000
train | epoch   5 | Iter:   6717/ 13000 | global iter:   6717/ 13000 | loss: 0.6891 | ds_loss: 0.0000 | lr: 2.3875e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
train | epoch   5 | Iter:   6718/ 13000 | global iter:   6718/ 13000 | loss: 0.8220 | ds_loss: 0.0000 | lr: 2.3869e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   5 | Iter:   6719/ 13000 | global iter:   6719/ 13000 | loss: 0.7566 | ds_loss: 0.0000 | lr: 2.3863e-05 | scale: 16384.0000 | micro time: 1.849 | step time: 0.000
train | epoch   5 | Iter:   6720/ 13000 | global iter:   6720/ 13000 | loss: 0.7849 | ds_loss: 0.0000 | lr: 2.3857e-05 | scale: 16384.0000 | micro time: 1.821 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6720/ 13000 | global iter:   6720/ 13000 | loss: 1.0952 | ds_loss: 0.0000 | lr: 2.3857e-05 | scale: 16384.0000 | micro time: 1.821 | step time: 1.848
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6721/ 13000 | global iter:   6721/ 13000 | loss: 1.3982 | ds_loss: 0.0000 | lr: 2.3851e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   5 | Iter:   6722/ 13000 | global iter:   6722/ 13000 | loss: 0.7827 | ds_loss: 0.0000 | lr: 2.3845e-05 | scale: 16384.0000 | micro time: 1.833 | step time: 0.000
train | epoch   5 | Iter:   6723/ 13000 | global iter:   6723/ 13000 | loss: 1.2719 | ds_loss: 0.0000 | lr: 2.3839e-05 | scale: 16384.0000 | micro time: 1.778 | step time: 0.000
train | epoch   5 | Iter:   6724/ 13000 | global iter:   6724/ 13000 | loss: 1.1167 | ds_loss: 0.0000 | lr: 2.3833e-05 | scale: 16384.0000 | micro time: 1.854 | step time: 0.000
train | epoch   5 | Iter:   6725/ 13000 | global iter:   6725/ 13000 | loss: 1.2096 | ds_loss: 0.0000 | lr: 2.3827e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   5 | Iter:   6726/ 13000 | global iter:   6726/ 13000 | loss: 0.8768 | ds_loss: 0.0000 | lr: 2.3820e-05 | scale: 16384.0000 | micro time: 1.834 | step time: 0.000
train | epoch   5 | Iter:   6727/ 13000 | global iter:   6727/ 13000 | loss: 0.9308 | ds_loss: 0.0000 | lr: 2.3814e-05 | scale: 16384.0000 | micro time: 1.792 | step time: 0.000
train | epoch   5 | Iter:   6728/ 13000 | global iter:   6728/ 13000 | loss: 0.9625 | ds_loss: 0.0000 | lr: 2.3808e-05 | scale: 16384.0000 | micro time: 1.707 | step time: 0.000
train | epoch   5 | Iter:   6729/ 13000 | global iter:   6729/ 13000 | loss: 1.1720 | ds_loss: 0.0000 | lr: 2.3802e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   5 | Iter:   6730/ 13000 | global iter:   6730/ 13000 | loss: 0.9896 | ds_loss: 0.0000 | lr: 2.3796e-05 | scale: 16384.0000 | micro time: 1.738 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6730/ 13000 | global iter:   6730/ 13000 | loss: 1.0711 | ds_loss: 0.0000 | lr: 2.3796e-05 | scale: 16384.0000 | micro time: 1.738 | step time: 1.786
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6731/ 13000 | global iter:   6731/ 13000 | loss: 1.4355 | ds_loss: 0.0000 | lr: 2.3790e-05 | scale: 16384.0000 | micro time: 1.790 | step time: 0.000
train | epoch   5 | Iter:   6732/ 13000 | global iter:   6732/ 13000 | loss: 1.4546 | ds_loss: 0.0000 | lr: 2.3784e-05 | scale: 16384.0000 | micro time: 1.812 | step time: 0.000
train | epoch   5 | Iter:   6733/ 13000 | global iter:   6733/ 13000 | loss: 1.5208 | ds_loss: 0.0000 | lr: 2.3778e-05 | scale: 16384.0000 | micro time: 1.718 | step time: 0.000
train | epoch   5 | Iter:   6734/ 13000 | global iter:   6734/ 13000 | loss: 1.4569 | ds_loss: 0.0000 | lr: 2.3772e-05 | scale: 16384.0000 | micro time: 1.711 | step time: 0.000
train | epoch   5 | Iter:   6735/ 13000 | global iter:   6735/ 13000 | loss: 0.9378 | ds_loss: 0.0000 | lr: 2.3766e-05 | scale: 16384.0000 | micro time: 1.724 | step time: 0.000
train | epoch   5 | Iter:   6736/ 13000 | global iter:   6736/ 13000 | loss: 1.1357 | ds_loss: 0.0000 | lr: 2.3760e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   5 | Iter:   6737/ 13000 | global iter:   6737/ 13000 | loss: 0.6926 | ds_loss: 0.0000 | lr: 2.3754e-05 | scale: 16384.0000 | micro time: 1.813 | step time: 0.000
train | epoch   5 | Iter:   6738/ 13000 | global iter:   6738/ 13000 | loss: 1.5316 | ds_loss: 0.0000 | lr: 2.3748e-05 | scale: 16384.0000 | micro time: 1.816 | step time: 0.000
train | epoch   5 | Iter:   6739/ 13000 | global iter:   6739/ 13000 | loss: 0.6280 | ds_loss: 0.0000 | lr: 2.3742e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   5 | Iter:   6740/ 13000 | global iter:   6740/ 13000 | loss: 1.3371 | ds_loss: 0.0000 | lr: 2.3736e-05 | scale: 16384.0000 | micro time: 1.756 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6740/ 13000 | global iter:   6740/ 13000 | loss: 1.2131 | ds_loss: 0.0000 | lr: 2.3736e-05 | scale: 16384.0000 | micro time: 1.756 | step time: 1.775
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6741/ 13000 | global iter:   6741/ 13000 | loss: 1.0132 | ds_loss: 0.0000 | lr: 2.3730e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   5 | Iter:   6742/ 13000 | global iter:   6742/ 13000 | loss: 1.3702 | ds_loss: 0.0000 | lr: 2.3724e-05 | scale: 16384.0000 | micro time: 1.845 | step time: 0.000
train | epoch   5 | Iter:   6743/ 13000 | global iter:   6743/ 13000 | loss: 0.6009 | ds_loss: 0.0000 | lr: 2.3718e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   5 | Iter:   6744/ 13000 | global iter:   6744/ 13000 | loss: 0.9576 | ds_loss: 0.0000 | lr: 2.3712e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   5 | Iter:   6745/ 13000 | global iter:   6745/ 13000 | loss: 0.7988 | ds_loss: 0.0000 | lr: 2.3706e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   5 | Iter:   6746/ 13000 | global iter:   6746/ 13000 | loss: 1.1415 | ds_loss: 0.0000 | lr: 2.3700e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   5 | Iter:   6747/ 13000 | global iter:   6747/ 13000 | loss: 1.2162 | ds_loss: 0.0000 | lr: 2.3694e-05 | scale: 32768.0000 | micro time: 1.793 | step time: 0.000
train | epoch   5 | Iter:   6748/ 13000 | global iter:   6748/ 13000 | loss: 1.2601 | ds_loss: 0.0000 | lr: 2.3688e-05 | scale: 32768.0000 | micro time: 1.897 | step time: 0.000
train | epoch   5 | Iter:   6749/ 13000 | global iter:   6749/ 13000 | loss: 0.9076 | ds_loss: 0.0000 | lr: 2.3682e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   5 | Iter:   6750/ 13000 | global iter:   6750/ 13000 | loss: 1.0240 | ds_loss: 0.0000 | lr: 2.3676e-05 | scale: 32768.0000 | micro time: 1.895 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6750/ 13000 | global iter:   6750/ 13000 | loss: 1.0290 | ds_loss: 0.0000 | lr: 2.3676e-05 | scale: 32768.0000 | micro time: 1.895 | step time: 1.851
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6751/ 13000 | global iter:   6751/ 13000 | loss: 0.8401 | ds_loss: 0.0000 | lr: 2.3670e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 0.000
train | epoch   5 | Iter:   6752/ 13000 | global iter:   6752/ 13000 | loss: 1.1921 | ds_loss: 0.0000 | lr: 2.3664e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   5 | Iter:   6753/ 13000 | global iter:   6753/ 13000 | loss: 1.0350 | ds_loss: 0.0000 | lr: 2.3658e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   5 | Iter:   6754/ 13000 | global iter:   6754/ 13000 | loss: 0.8617 | ds_loss: 0.0000 | lr: 2.3652e-05 | scale: 32768.0000 | micro time: 1.840 | step time: 0.000
train | epoch   5 | Iter:   6755/ 13000 | global iter:   6755/ 13000 | loss: 1.2221 | ds_loss: 0.0000 | lr: 2.3646e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   5 | Iter:   6756/ 13000 | global iter:   6756/ 13000 | loss: 1.2818 | ds_loss: 0.0000 | lr: 2.3640e-05 | scale: 32768.0000 | micro time: 1.841 | step time: 0.000
train | epoch   5 | Iter:   6757/ 13000 | global iter:   6757/ 13000 | loss: 1.4144 | ds_loss: 0.0000 | lr: 2.3634e-05 | scale: 32768.0000 | micro time: 1.686 | step time: 0.000
train | epoch   5 | Iter:   6758/ 13000 | global iter:   6758/ 13000 | loss: 1.3338 | ds_loss: 0.0000 | lr: 2.3628e-05 | scale: 32768.0000 | micro time: 1.834 | step time: 0.000
train | epoch   5 | Iter:   6759/ 13000 | global iter:   6759/ 13000 | loss: 0.9988 | ds_loss: 0.0000 | lr: 2.3622e-05 | scale: 32768.0000 | micro time: 1.841 | step time: 0.000
train | epoch   5 | Iter:   6760/ 13000 | global iter:   6760/ 13000 | loss: 0.3242 | ds_loss: 0.0000 | lr: 2.3616e-05 | scale: 32768.0000 | micro time: 1.844 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6760/ 13000 | global iter:   6760/ 13000 | loss: 1.0504 | ds_loss: 0.0000 | lr: 2.3616e-05 | scale: 32768.0000 | micro time: 1.844 | step time: 1.819
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6761/ 13000 | global iter:   6761/ 13000 | loss: 1.0043 | ds_loss: 0.0000 | lr: 2.3610e-05 | scale: 32768.0000 | micro time: 1.777 | step time: 0.000
train | epoch   5 | Iter:   6762/ 13000 | global iter:   6762/ 13000 | loss: 1.0344 | ds_loss: 0.0000 | lr: 2.3604e-05 | scale: 32768.0000 | micro time: 1.761 | step time: 0.000
train | epoch   5 | Iter:   6763/ 13000 | global iter:   6763/ 13000 | loss: 1.0417 | ds_loss: 0.0000 | lr: 2.3598e-05 | scale: 32768.0000 | micro time: 1.800 | step time: 0.000
train | epoch   5 | Iter:   6764/ 13000 | global iter:   6764/ 13000 | loss: 1.2779 | ds_loss: 0.0000 | lr: 2.3592e-05 | scale: 32768.0000 | micro time: 1.907 | step time: 0.000
train | epoch   5 | Iter:   6765/ 13000 | global iter:   6765/ 13000 | loss: 0.8187 | ds_loss: 0.0000 | lr: 2.3586e-05 | scale: 32768.0000 | micro time: 1.854 | step time: 0.000
train | epoch   5 | Iter:   6766/ 13000 | global iter:   6766/ 13000 | loss: 1.5144 | ds_loss: 0.0000 | lr: 2.3580e-05 | scale: 32768.0000 | micro time: 1.844 | step time: 0.000
train | epoch   5 | Iter:   6767/ 13000 | global iter:   6767/ 13000 | loss: 1.6635 | ds_loss: 0.0000 | lr: 2.3574e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   5 | Iter:   6768/ 13000 | global iter:   6768/ 13000 | loss: 1.1174 | ds_loss: 0.0000 | lr: 2.3568e-05 | scale: 32768.0000 | micro time: 1.785 | step time: 0.000
train | epoch   5 | Iter:   6769/ 13000 | global iter:   6769/ 13000 | loss: 0.9557 | ds_loss: 0.0000 | lr: 2.3562e-05 | scale: 32768.0000 | micro time: 1.773 | step time: 0.000
train | epoch   5 | Iter:   6770/ 13000 | global iter:   6770/ 13000 | loss: 0.9722 | ds_loss: 0.0000 | lr: 2.3556e-05 | scale: 32768.0000 | micro time: 1.891 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6770/ 13000 | global iter:   6770/ 13000 | loss: 1.1400 | ds_loss: 0.0000 | lr: 2.3556e-05 | scale: 32768.0000 | micro time: 1.891 | step time: 1.823
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6771/ 13000 | global iter:   6771/ 13000 | loss: 1.4045 | ds_loss: 0.0000 | lr: 2.3550e-05 | scale: 32768.0000 | micro time: 1.734 | step time: 0.000
train | epoch   5 | Iter:   6772/ 13000 | global iter:   6772/ 13000 | loss: 0.8424 | ds_loss: 0.0000 | lr: 2.3544e-05 | scale: 32768.0000 | micro time: 1.739 | step time: 0.000
train | epoch   5 | Iter:   6773/ 13000 | global iter:   6773/ 13000 | loss: 0.9290 | ds_loss: 0.0000 | lr: 2.3538e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   5 | Iter:   6774/ 13000 | global iter:   6774/ 13000 | loss: 0.4402 | ds_loss: 0.0000 | lr: 2.3532e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
train | epoch   5 | Iter:   6775/ 13000 | global iter:   6775/ 13000 | loss: 1.2732 | ds_loss: 0.0000 | lr: 2.3526e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   5 | Iter:   6776/ 13000 | global iter:   6776/ 13000 | loss: 1.1316 | ds_loss: 0.0000 | lr: 2.3519e-05 | scale: 32768.0000 | micro time: 1.808 | step time: 0.000
train | epoch   5 | Iter:   6777/ 13000 | global iter:   6777/ 13000 | loss: 1.2243 | ds_loss: 0.0000 | lr: 2.3513e-05 | scale: 32768.0000 | micro time: 1.770 | step time: 0.000
train | epoch   5 | Iter:   6778/ 13000 | global iter:   6778/ 13000 | loss: 1.0138 | ds_loss: 0.0000 | lr: 2.3507e-05 | scale: 32768.0000 | micro time: 1.810 | step time: 0.000
train | epoch   5 | Iter:   6779/ 13000 | global iter:   6779/ 13000 | loss: 0.6501 | ds_loss: 0.0000 | lr: 2.3501e-05 | scale: 32768.0000 | micro time: 1.784 | step time: 0.000
train | epoch   5 | Iter:   6780/ 13000 | global iter:   6780/ 13000 | loss: 0.6747 | ds_loss: 0.0000 | lr: 2.3495e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6780/ 13000 | global iter:   6780/ 13000 | loss: 0.9584 | ds_loss: 0.0000 | lr: 2.3495e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 1.779
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6781/ 13000 | global iter:   6781/ 13000 | loss: 0.7067 | ds_loss: 0.0000 | lr: 2.3489e-05 | scale: 32768.0000 | micro time: 1.758 | step time: 0.000
train | epoch   5 | Iter:   6782/ 13000 | global iter:   6782/ 13000 | loss: 1.5715 | ds_loss: 0.0000 | lr: 2.3483e-05 | scale: 32768.0000 | micro time: 1.916 | step time: 0.000
train | epoch   5 | Iter:   6783/ 13000 | global iter:   6783/ 13000 | loss: 0.7392 | ds_loss: 0.0000 | lr: 2.3477e-05 | scale: 32768.0000 | micro time: 1.810 | step time: 0.000
train | epoch   5 | Iter:   6784/ 13000 | global iter:   6784/ 13000 | loss: 1.0493 | ds_loss: 0.0000 | lr: 2.3471e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   5 | Iter:   6785/ 13000 | global iter:   6785/ 13000 | loss: 1.2525 | ds_loss: 0.0000 | lr: 2.3465e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   5 | Iter:   6786/ 13000 | global iter:   6786/ 13000 | loss: 1.3047 | ds_loss: 0.0000 | lr: 2.3459e-05 | scale: 32768.0000 | micro time: 1.814 | step time: 0.000
train | epoch   5 | Iter:   6787/ 13000 | global iter:   6787/ 13000 | loss: 0.7778 | ds_loss: 0.0000 | lr: 2.3453e-05 | scale: 32768.0000 | micro time: 1.879 | step time: 0.000
train | epoch   5 | Iter:   6788/ 13000 | global iter:   6788/ 13000 | loss: 1.3451 | ds_loss: 0.0000 | lr: 2.3447e-05 | scale: 32768.0000 | micro time: 1.817 | step time: 0.000
train | epoch   5 | Iter:   6789/ 13000 | global iter:   6789/ 13000 | loss: 1.3640 | ds_loss: 0.0000 | lr: 2.3441e-05 | scale: 32768.0000 | micro time: 1.887 | step time: 0.000
train | epoch   5 | Iter:   6790/ 13000 | global iter:   6790/ 13000 | loss: 1.6955 | ds_loss: 0.0000 | lr: 2.3435e-05 | scale: 32768.0000 | micro time: 1.741 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6790/ 13000 | global iter:   6790/ 13000 | loss: 1.1806 | ds_loss: 0.0000 | lr: 2.3435e-05 | scale: 32768.0000 | micro time: 1.741 | step time: 1.819
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6791/ 13000 | global iter:   6791/ 13000 | loss: 1.0332 | ds_loss: 0.0000 | lr: 2.3429e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   5 | Iter:   6792/ 13000 | global iter:   6792/ 13000 | loss: 1.3371 | ds_loss: 0.0000 | lr: 2.3423e-05 | scale: 32768.0000 | micro time: 1.795 | step time: 0.000
train | epoch   5 | Iter:   6793/ 13000 | global iter:   6793/ 13000 | loss: 1.0111 | ds_loss: 0.0000 | lr: 2.3417e-05 | scale: 32768.0000 | micro time: 1.751 | step time: 0.000
train | epoch   5 | Iter:   6794/ 13000 | global iter:   6794/ 13000 | loss: 1.0389 | ds_loss: 0.0000 | lr: 2.3411e-05 | scale: 32768.0000 | micro time: 1.735 | step time: 0.000
train | epoch   5 | Iter:   6795/ 13000 | global iter:   6795/ 13000 | loss: 1.0285 | ds_loss: 0.0000 | lr: 2.3405e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 0.000
train | epoch   5 | Iter:   6796/ 13000 | global iter:   6796/ 13000 | loss: 1.2851 | ds_loss: 0.0000 | lr: 2.3399e-05 | scale: 32768.0000 | micro time: 1.881 | step time: 0.000
train | epoch   5 | Iter:   6797/ 13000 | global iter:   6797/ 13000 | loss: 1.5372 | ds_loss: 0.0000 | lr: 2.3393e-05 | scale: 32768.0000 | micro time: 1.917 | step time: 0.000
train | epoch   5 | Iter:   6798/ 13000 | global iter:   6798/ 13000 | loss: 0.9368 | ds_loss: 0.0000 | lr: 2.3387e-05 | scale: 32768.0000 | micro time: 1.743 | step time: 0.000
train | epoch   5 | Iter:   6799/ 13000 | global iter:   6799/ 13000 | loss: 1.2652 | ds_loss: 0.0000 | lr: 2.3381e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   5 | Iter:   6800/ 13000 | global iter:   6800/ 13000 | loss: 1.5043 | ds_loss: 0.0000 | lr: 2.3375e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6800/ 13000 | global iter:   6800/ 13000 | loss: 1.1977 | ds_loss: 0.0000 | lr: 2.3375e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 1.810
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6801/ 13000 | global iter:   6801/ 13000 | loss: 1.2184 | ds_loss: 0.0000 | lr: 2.3369e-05 | scale: 32768.0000 | micro time: 1.785 | step time: 0.000
train | epoch   5 | Iter:   6802/ 13000 | global iter:   6802/ 13000 | loss: 1.0788 | ds_loss: 0.0000 | lr: 2.3363e-05 | scale: 32768.0000 | micro time: 1.853 | step time: 0.000
train | epoch   5 | Iter:   6803/ 13000 | global iter:   6803/ 13000 | loss: 1.6780 | ds_loss: 0.0000 | lr: 2.3357e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   5 | Iter:   6804/ 13000 | global iter:   6804/ 13000 | loss: 1.4481 | ds_loss: 0.0000 | lr: 2.3351e-05 | scale: 32768.0000 | micro time: 1.739 | step time: 0.000
train | epoch   5 | Iter:   6805/ 13000 | global iter:   6805/ 13000 | loss: 0.9991 | ds_loss: 0.0000 | lr: 2.3345e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
train | epoch   5 | Iter:   6806/ 13000 | global iter:   6806/ 13000 | loss: 1.2293 | ds_loss: 0.0000 | lr: 2.3339e-05 | scale: 32768.0000 | micro time: 1.655 | step time: 0.000
train | epoch   5 | Iter:   6807/ 13000 | global iter:   6807/ 13000 | loss: 1.4105 | ds_loss: 0.0000 | lr: 2.3333e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   5 | Iter:   6808/ 13000 | global iter:   6808/ 13000 | loss: 1.4669 | ds_loss: 0.0000 | lr: 2.3327e-05 | scale: 32768.0000 | micro time: 1.797 | step time: 0.000
train | epoch   5 | Iter:   6809/ 13000 | global iter:   6809/ 13000 | loss: 1.0980 | ds_loss: 0.0000 | lr: 2.3321e-05 | scale: 32768.0000 | micro time: 1.805 | step time: 0.000
train | epoch   5 | Iter:   6810/ 13000 | global iter:   6810/ 13000 | loss: 0.9927 | ds_loss: 0.0000 | lr: 2.3315e-05 | scale: 32768.0000 | micro time: 1.822 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6810/ 13000 | global iter:   6810/ 13000 | loss: 1.2620 | ds_loss: 0.0000 | lr: 2.3315e-05 | scale: 32768.0000 | micro time: 1.822 | step time: 1.783
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6811/ 13000 | global iter:   6811/ 13000 | loss: 0.9756 | ds_loss: 0.0000 | lr: 2.3309e-05 | scale: 32768.0000 | micro time: 1.858 | step time: 0.000
train | epoch   5 | Iter:   6812/ 13000 | global iter:   6812/ 13000 | loss: 1.0421 | ds_loss: 0.0000 | lr: 2.3303e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   5 | Iter:   6813/ 13000 | global iter:   6813/ 13000 | loss: 0.7029 | ds_loss: 0.0000 | lr: 2.3297e-05 | scale: 32768.0000 | micro time: 1.887 | step time: 0.000
train | epoch   5 | Iter:   6814/ 13000 | global iter:   6814/ 13000 | loss: 1.3072 | ds_loss: 0.0000 | lr: 2.3291e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   5 | Iter:   6815/ 13000 | global iter:   6815/ 13000 | loss: 0.9052 | ds_loss: 0.0000 | lr: 2.3285e-05 | scale: 32768.0000 | micro time: 1.810 | step time: 0.000
train | epoch   5 | Iter:   6816/ 13000 | global iter:   6816/ 13000 | loss: 1.0550 | ds_loss: 0.0000 | lr: 2.3279e-05 | scale: 32768.0000 | micro time: 1.885 | step time: 0.000
train | epoch   5 | Iter:   6817/ 13000 | global iter:   6817/ 13000 | loss: 1.1402 | ds_loss: 0.0000 | lr: 2.3273e-05 | scale: 32768.0000 | micro time: 1.833 | step time: 0.000
train | epoch   5 | Iter:   6818/ 13000 | global iter:   6818/ 13000 | loss: 0.9531 | ds_loss: 0.0000 | lr: 2.3267e-05 | scale: 32768.0000 | micro time: 1.924 | step time: 0.000
train | epoch   5 | Iter:   6819/ 13000 | global iter:   6819/ 13000 | loss: 0.7741 | ds_loss: 0.0000 | lr: 2.3261e-05 | scale: 32768.0000 | micro time: 1.755 | step time: 0.000
train | epoch   5 | Iter:   6820/ 13000 | global iter:   6820/ 13000 | loss: 0.8786 | ds_loss: 0.0000 | lr: 2.3255e-05 | scale: 32768.0000 | micro time: 1.856 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6820/ 13000 | global iter:   6820/ 13000 | loss: 0.9734 | ds_loss: 0.0000 | lr: 2.3255e-05 | scale: 32768.0000 | micro time: 1.856 | step time: 1.845
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6821/ 13000 | global iter:   6821/ 13000 | loss: 1.0276 | ds_loss: 0.0000 | lr: 2.3249e-05 | scale: 32768.0000 | micro time: 1.735 | step time: 0.000
train | epoch   5 | Iter:   6822/ 13000 | global iter:   6822/ 13000 | loss: 1.0565 | ds_loss: 0.0000 | lr: 2.3243e-05 | scale: 32768.0000 | micro time: 1.750 | step time: 0.000
train | epoch   5 | Iter:   6823/ 13000 | global iter:   6823/ 13000 | loss: 1.4922 | ds_loss: 0.0000 | lr: 2.3237e-05 | scale: 32768.0000 | micro time: 1.748 | step time: 0.000
train | epoch   5 | Iter:   6824/ 13000 | global iter:   6824/ 13000 | loss: 1.3478 | ds_loss: 0.0000 | lr: 2.3231e-05 | scale: 32768.0000 | micro time: 1.830 | step time: 0.000
train | epoch   5 | Iter:   6825/ 13000 | global iter:   6825/ 13000 | loss: 1.5178 | ds_loss: 0.0000 | lr: 2.3225e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   5 | Iter:   6826/ 13000 | global iter:   6826/ 13000 | loss: 0.8818 | ds_loss: 0.0000 | lr: 2.3219e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   5 | Iter:   6827/ 13000 | global iter:   6827/ 13000 | loss: 0.6816 | ds_loss: 0.0000 | lr: 2.3213e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   5 | Iter:   6828/ 13000 | global iter:   6828/ 13000 | loss: 1.3229 | ds_loss: 0.0000 | lr: 2.3207e-05 | scale: 32768.0000 | micro time: 1.804 | step time: 0.000
train | epoch   5 | Iter:   6829/ 13000 | global iter:   6829/ 13000 | loss: 0.9389 | ds_loss: 0.0000 | lr: 2.3201e-05 | scale: 32768.0000 | micro time: 1.912 | step time: 0.000
train | epoch   5 | Iter:   6830/ 13000 | global iter:   6830/ 13000 | loss: 1.8005 | ds_loss: 0.0000 | lr: 2.3195e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6830/ 13000 | global iter:   6830/ 13000 | loss: 1.2068 | ds_loss: 0.0000 | lr: 2.3195e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 1.802
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6831/ 13000 | global iter:   6831/ 13000 | loss: 1.3845 | ds_loss: 0.0000 | lr: 2.3189e-05 | scale: 32768.0000 | micro time: 1.702 | step time: 0.000
train | epoch   5 | Iter:   6832/ 13000 | global iter:   6832/ 13000 | loss: 0.8643 | ds_loss: 0.0000 | lr: 2.3183e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   5 | Iter:   6833/ 13000 | global iter:   6833/ 13000 | loss: 1.0932 | ds_loss: 0.0000 | lr: 2.3177e-05 | scale: 32768.0000 | micro time: 1.793 | step time: 0.000
train | epoch   5 | Iter:   6834/ 13000 | global iter:   6834/ 13000 | loss: 1.2046 | ds_loss: 0.0000 | lr: 2.3171e-05 | scale: 32768.0000 | micro time: 1.795 | step time: 0.000
train | epoch   5 | Iter:   6835/ 13000 | global iter:   6835/ 13000 | loss: 1.5228 | ds_loss: 0.0000 | lr: 2.3165e-05 | scale: 32768.0000 | micro time: 1.806 | step time: 0.000
train | epoch   5 | Iter:   6836/ 13000 | global iter:   6836/ 13000 | loss: 0.9230 | ds_loss: 0.0000 | lr: 2.3159e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
train | epoch   5 | Iter:   6837/ 13000 | global iter:   6837/ 13000 | loss: 1.3083 | ds_loss: 0.0000 | lr: 2.3153e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   5 | Iter:   6838/ 13000 | global iter:   6838/ 13000 | loss: 1.4775 | ds_loss: 0.0000 | lr: 2.3147e-05 | scale: 32768.0000 | micro time: 1.741 | step time: 0.000
train | epoch   5 | Iter:   6839/ 13000 | global iter:   6839/ 13000 | loss: 1.4124 | ds_loss: 0.0000 | lr: 2.3141e-05 | scale: 32768.0000 | micro time: 1.801 | step time: 0.000
train | epoch   5 | Iter:   6840/ 13000 | global iter:   6840/ 13000 | loss: 1.2129 | ds_loss: 0.0000 | lr: 2.3135e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6840/ 13000 | global iter:   6840/ 13000 | loss: 1.2403 | ds_loss: 0.0000 | lr: 2.3135e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 1.783
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6841/ 13000 | global iter:   6841/ 13000 | loss: 1.3036 | ds_loss: 0.0000 | lr: 2.3129e-05 | scale: 32768.0000 | micro time: 1.857 | step time: 0.000
train | epoch   5 | Iter:   6842/ 13000 | global iter:   6842/ 13000 | loss: 1.2730 | ds_loss: 0.0000 | lr: 2.3123e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   5 | Iter:   6843/ 13000 | global iter:   6843/ 13000 | loss: 0.8685 | ds_loss: 0.0000 | lr: 2.3116e-05 | scale: 32768.0000 | micro time: 1.788 | step time: 0.000
train | epoch   5 | Iter:   6844/ 13000 | global iter:   6844/ 13000 | loss: 0.9933 | ds_loss: 0.0000 | lr: 2.3110e-05 | scale: 32768.0000 | micro time: 1.882 | step time: 0.000
train | epoch   5 | Iter:   6845/ 13000 | global iter:   6845/ 13000 | loss: 0.8446 | ds_loss: 0.0000 | lr: 2.3104e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   5 | Iter:   6846/ 13000 | global iter:   6846/ 13000 | loss: 1.0009 | ds_loss: 0.0000 | lr: 2.3098e-05 | scale: 32768.0000 | micro time: 1.765 | step time: 0.000
train | epoch   5 | Iter:   6847/ 13000 | global iter:   6847/ 13000 | loss: 1.0493 | ds_loss: 0.0000 | lr: 2.3092e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   5 | Iter:   6848/ 13000 | global iter:   6848/ 13000 | loss: 0.7327 | ds_loss: 0.0000 | lr: 2.3086e-05 | scale: 32768.0000 | micro time: 1.735 | step time: 0.000
train | epoch   5 | Iter:   6849/ 13000 | global iter:   6849/ 13000 | loss: 1.1294 | ds_loss: 0.0000 | lr: 2.3080e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   5 | Iter:   6850/ 13000 | global iter:   6850/ 13000 | loss: 1.7691 | ds_loss: 0.0000 | lr: 2.3074e-05 | scale: 32768.0000 | micro time: 1.667 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6850/ 13000 | global iter:   6850/ 13000 | loss: 1.0964 | ds_loss: 0.0000 | lr: 2.3074e-05 | scale: 32768.0000 | micro time: 1.667 | step time: 1.802
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6851/ 13000 | global iter:   6851/ 13000 | loss: 1.5873 | ds_loss: 0.0000 | lr: 2.3068e-05 | scale: 32768.0000 | micro time: 1.724 | step time: 0.000
train | epoch   5 | Iter:   6852/ 13000 | global iter:   6852/ 13000 | loss: 0.9517 | ds_loss: 0.0000 | lr: 2.3062e-05 | scale: 32768.0000 | micro time: 1.756 | step time: 0.000
train | epoch   5 | Iter:   6853/ 13000 | global iter:   6853/ 13000 | loss: 1.3272 | ds_loss: 0.0000 | lr: 2.3056e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   5 | Iter:   6854/ 13000 | global iter:   6854/ 13000 | loss: 1.1361 | ds_loss: 0.0000 | lr: 2.3050e-05 | scale: 32768.0000 | micro time: 1.715 | step time: 0.000
train | epoch   5 | Iter:   6855/ 13000 | global iter:   6855/ 13000 | loss: 1.3871 | ds_loss: 0.0000 | lr: 2.3044e-05 | scale: 32768.0000 | micro time: 1.649 | step time: 0.000
train | epoch   5 | Iter:   6856/ 13000 | global iter:   6856/ 13000 | loss: 1.1217 | ds_loss: 0.0000 | lr: 2.3038e-05 | scale: 32768.0000 | micro time: 1.897 | step time: 0.000
train | epoch   5 | Iter:   6857/ 13000 | global iter:   6857/ 13000 | loss: 0.4041 | ds_loss: 0.0000 | lr: 2.3032e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   5 | Iter:   6858/ 13000 | global iter:   6858/ 13000 | loss: 1.1408 | ds_loss: 0.0000 | lr: 2.3026e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   5 | Iter:   6859/ 13000 | global iter:   6859/ 13000 | loss: 1.2201 | ds_loss: 0.0000 | lr: 2.3020e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   5 | Iter:   6860/ 13000 | global iter:   6860/ 13000 | loss: 1.2982 | ds_loss: 0.0000 | lr: 2.3014e-05 | scale: 32768.0000 | micro time: 1.881 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6860/ 13000 | global iter:   6860/ 13000 | loss: 1.1574 | ds_loss: 0.0000 | lr: 2.3014e-05 | scale: 32768.0000 | micro time: 1.881 | step time: 1.785
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6861/ 13000 | global iter:   6861/ 13000 | loss: 0.8887 | ds_loss: 0.0000 | lr: 2.3008e-05 | scale: 32768.0000 | micro time: 1.896 | step time: 0.000
train | epoch   5 | Iter:   6862/ 13000 | global iter:   6862/ 13000 | loss: 0.7049 | ds_loss: 0.0000 | lr: 2.3002e-05 | scale: 32768.0000 | micro time: 1.895 | step time: 0.000
train | epoch   5 | Iter:   6863/ 13000 | global iter:   6863/ 13000 | loss: 1.2547 | ds_loss: 0.0000 | lr: 2.2996e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   5 | Iter:   6864/ 13000 | global iter:   6864/ 13000 | loss: 1.0604 | ds_loss: 0.0000 | lr: 2.2990e-05 | scale: 32768.0000 | micro time: 1.678 | step time: 0.000
train | epoch   5 | Iter:   6865/ 13000 | global iter:   6865/ 13000 | loss: 1.2166 | ds_loss: 0.0000 | lr: 2.2984e-05 | scale: 32768.0000 | micro time: 1.780 | step time: 0.000
train | epoch   5 | Iter:   6866/ 13000 | global iter:   6866/ 13000 | loss: 1.4036 | ds_loss: 0.0000 | lr: 2.2978e-05 | scale: 32768.0000 | micro time: 1.816 | step time: 0.000
train | epoch   5 | Iter:   6867/ 13000 | global iter:   6867/ 13000 | loss: 1.0163 | ds_loss: 0.0000 | lr: 2.2972e-05 | scale: 32768.0000 | micro time: 1.765 | step time: 0.000
train | epoch   5 | Iter:   6868/ 13000 | global iter:   6868/ 13000 | loss: 0.7666 | ds_loss: 0.0000 | lr: 2.2966e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   5 | Iter:   6869/ 13000 | global iter:   6869/ 13000 | loss: 1.3840 | ds_loss: 0.0000 | lr: 2.2960e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   5 | Iter:   6870/ 13000 | global iter:   6870/ 13000 | loss: 0.8681 | ds_loss: 0.0000 | lr: 2.2954e-05 | scale: 32768.0000 | micro time: 1.755 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6870/ 13000 | global iter:   6870/ 13000 | loss: 1.0564 | ds_loss: 0.0000 | lr: 2.2954e-05 | scale: 32768.0000 | micro time: 1.755 | step time: 1.812
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6871/ 13000 | global iter:   6871/ 13000 | loss: 1.4187 | ds_loss: 0.0000 | lr: 2.2948e-05 | scale: 32768.0000 | micro time: 1.683 | step time: 0.000
train | epoch   5 | Iter:   6872/ 13000 | global iter:   6872/ 13000 | loss: 1.1734 | ds_loss: 0.0000 | lr: 2.2942e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   5 | Iter:   6873/ 13000 | global iter:   6873/ 13000 | loss: 1.0911 | ds_loss: 0.0000 | lr: 2.2936e-05 | scale: 32768.0000 | micro time: 1.747 | step time: 0.000
train | epoch   5 | Iter:   6874/ 13000 | global iter:   6874/ 13000 | loss: 1.0276 | ds_loss: 0.0000 | lr: 2.2930e-05 | scale: 32768.0000 | micro time: 1.721 | step time: 0.000
train | epoch   5 | Iter:   6875/ 13000 | global iter:   6875/ 13000 | loss: 1.5031 | ds_loss: 0.0000 | lr: 2.2924e-05 | scale: 32768.0000 | micro time: 1.746 | step time: 0.000
train | epoch   5 | Iter:   6876/ 13000 | global iter:   6876/ 13000 | loss: 0.9683 | ds_loss: 0.0000 | lr: 2.2918e-05 | scale: 32768.0000 | micro time: 1.870 | step time: 0.000
train | epoch   5 | Iter:   6877/ 13000 | global iter:   6877/ 13000 | loss: 1.2217 | ds_loss: 0.0000 | lr: 2.2912e-05 | scale: 32768.0000 | micro time: 1.888 | step time: 0.000
train | epoch   5 | Iter:   6878/ 13000 | global iter:   6878/ 13000 | loss: 1.5144 | ds_loss: 0.0000 | lr: 2.2906e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   5 | Iter:   6879/ 13000 | global iter:   6879/ 13000 | loss: 0.3314 | ds_loss: 0.0000 | lr: 2.2900e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   5 | Iter:   6880/ 13000 | global iter:   6880/ 13000 | loss: 0.9010 | ds_loss: 0.0000 | lr: 2.2894e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6880/ 13000 | global iter:   6880/ 13000 | loss: 1.1151 | ds_loss: 0.0000 | lr: 2.2894e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 1.777
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6881/ 13000 | global iter:   6881/ 13000 | loss: 1.3649 | ds_loss: 0.0000 | lr: 2.2888e-05 | scale: 32768.0000 | micro time: 1.756 | step time: 0.000
train | epoch   5 | Iter:   6882/ 13000 | global iter:   6882/ 13000 | loss: 1.1469 | ds_loss: 0.0000 | lr: 2.2882e-05 | scale: 32768.0000 | micro time: 1.810 | step time: 0.000
train | epoch   5 | Iter:   6883/ 13000 | global iter:   6883/ 13000 | loss: 0.7248 | ds_loss: 0.0000 | lr: 2.2876e-05 | scale: 32768.0000 | micro time: 1.829 | step time: 0.000
train | epoch   5 | Iter:   6884/ 13000 | global iter:   6884/ 13000 | loss: 1.3151 | ds_loss: 0.0000 | lr: 2.2870e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   5 | Iter:   6885/ 13000 | global iter:   6885/ 13000 | loss: 1.4189 | ds_loss: 0.0000 | lr: 2.2864e-05 | scale: 32768.0000 | micro time: 1.741 | step time: 0.000
train | epoch   5 | Iter:   6886/ 13000 | global iter:   6886/ 13000 | loss: 1.0603 | ds_loss: 0.0000 | lr: 2.2858e-05 | scale: 32768.0000 | micro time: 1.781 | step time: 0.000
train | epoch   5 | Iter:   6887/ 13000 | global iter:   6887/ 13000 | loss: 0.8972 | ds_loss: 0.0000 | lr: 2.2852e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   5 | Iter:   6888/ 13000 | global iter:   6888/ 13000 | loss: 1.3314 | ds_loss: 0.0000 | lr: 2.2846e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   5 | Iter:   6889/ 13000 | global iter:   6889/ 13000 | loss: 1.0886 | ds_loss: 0.0000 | lr: 2.2840e-05 | scale: 32768.0000 | micro time: 1.866 | step time: 0.000
train | epoch   5 | Iter:   6890/ 13000 | global iter:   6890/ 13000 | loss: 1.8247 | ds_loss: 0.0000 | lr: 2.2834e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6890/ 13000 | global iter:   6890/ 13000 | loss: 1.2173 | ds_loss: 0.0000 | lr: 2.2834e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 1.791
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6891/ 13000 | global iter:   6891/ 13000 | loss: 0.8916 | ds_loss: 0.0000 | lr: 2.2828e-05 | scale: 32768.0000 | micro time: 1.778 | step time: 0.000
train | epoch   5 | Iter:   6892/ 13000 | global iter:   6892/ 13000 | loss: 1.3822 | ds_loss: 0.0000 | lr: 2.2822e-05 | scale: 32768.0000 | micro time: 1.841 | step time: 0.000
train | epoch   5 | Iter:   6893/ 13000 | global iter:   6893/ 13000 | loss: 0.9242 | ds_loss: 0.0000 | lr: 2.2816e-05 | scale: 32768.0000 | micro time: 1.924 | step time: 0.000
train | epoch   5 | Iter:   6894/ 13000 | global iter:   6894/ 13000 | loss: 0.5335 | ds_loss: 0.0000 | lr: 2.2810e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   5 | Iter:   6895/ 13000 | global iter:   6895/ 13000 | loss: 1.1347 | ds_loss: 0.0000 | lr: 2.2804e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
train | epoch   5 | Iter:   6896/ 13000 | global iter:   6896/ 13000 | loss: 1.1291 | ds_loss: 0.0000 | lr: 2.2798e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 0.000
train | epoch   5 | Iter:   6897/ 13000 | global iter:   6897/ 13000 | loss: 1.4595 | ds_loss: 0.0000 | lr: 2.2792e-05 | scale: 32768.0000 | micro time: 1.781 | step time: 0.000
train | epoch   5 | Iter:   6898/ 13000 | global iter:   6898/ 13000 | loss: 1.1889 | ds_loss: 0.0000 | lr: 2.2786e-05 | scale: 32768.0000 | micro time: 1.962 | step time: 0.000
train | epoch   5 | Iter:   6899/ 13000 | global iter:   6899/ 13000 | loss: 0.9594 | ds_loss: 0.0000 | lr: 2.2780e-05 | scale: 32768.0000 | micro time: 1.738 | step time: 0.000
train | epoch   5 | Iter:   6900/ 13000 | global iter:   6900/ 13000 | loss: 0.7586 | ds_loss: 0.0000 | lr: 2.2774e-05 | scale: 32768.0000 | micro time: 1.891 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6900/ 13000 | global iter:   6900/ 13000 | loss: 1.0362 | ds_loss: 0.0000 | lr: 2.2774e-05 | scale: 32768.0000 | micro time: 1.891 | step time: 1.836
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6901/ 13000 | global iter:   6901/ 13000 | loss: 1.1243 | ds_loss: 0.0000 | lr: 2.2768e-05 | scale: 32768.0000 | micro time: 1.872 | step time: 0.000
train | epoch   5 | Iter:   6902/ 13000 | global iter:   6902/ 13000 | loss: 1.1237 | ds_loss: 0.0000 | lr: 2.2762e-05 | scale: 32768.0000 | micro time: 1.933 | step time: 0.000
train | epoch   5 | Iter:   6903/ 13000 | global iter:   6903/ 13000 | loss: 1.3440 | ds_loss: 0.0000 | lr: 2.2756e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   5 | Iter:   6904/ 13000 | global iter:   6904/ 13000 | loss: 0.4653 | ds_loss: 0.0000 | lr: 2.2750e-05 | scale: 32768.0000 | micro time: 1.841 | step time: 0.000
train | epoch   5 | Iter:   6905/ 13000 | global iter:   6905/ 13000 | loss: 1.0671 | ds_loss: 0.0000 | lr: 2.2744e-05 | scale: 32768.0000 | micro time: 1.838 | step time: 0.000
train | epoch   5 | Iter:   6906/ 13000 | global iter:   6906/ 13000 | loss: 0.4535 | ds_loss: 0.0000 | lr: 2.2738e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   5 | Iter:   6907/ 13000 | global iter:   6907/ 13000 | loss: 1.2024 | ds_loss: 0.0000 | lr: 2.2732e-05 | scale: 32768.0000 | micro time: 1.688 | step time: 0.000
train | epoch   5 | Iter:   6908/ 13000 | global iter:   6908/ 13000 | loss: 1.4456 | ds_loss: 0.0000 | lr: 2.2726e-05 | scale: 32768.0000 | micro time: 1.886 | step time: 0.000
train | epoch   5 | Iter:   6909/ 13000 | global iter:   6909/ 13000 | loss: 1.0752 | ds_loss: 0.0000 | lr: 2.2720e-05 | scale: 32768.0000 | micro time: 1.903 | step time: 0.000
train | epoch   5 | Iter:   6910/ 13000 | global iter:   6910/ 13000 | loss: 0.6967 | ds_loss: 0.0000 | lr: 2.2714e-05 | scale: 32768.0000 | micro time: 1.841 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6910/ 13000 | global iter:   6910/ 13000 | loss: 0.9998 | ds_loss: 0.0000 | lr: 2.2714e-05 | scale: 32768.0000 | micro time: 1.841 | step time: 1.851
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6911/ 13000 | global iter:   6911/ 13000 | loss: 1.2507 | ds_loss: 0.0000 | lr: 2.2708e-05 | scale: 32768.0000 | micro time: 1.784 | step time: 0.000
train | epoch   5 | Iter:   6912/ 13000 | global iter:   6912/ 13000 | loss: 0.8780 | ds_loss: 0.0000 | lr: 2.2702e-05 | scale: 32768.0000 | micro time: 1.864 | step time: 0.000
train | epoch   5 | Iter:   6913/ 13000 | global iter:   6913/ 13000 | loss: 1.6334 | ds_loss: 0.0000 | lr: 2.2696e-05 | scale: 32768.0000 | micro time: 1.794 | step time: 0.000
train | epoch   5 | Iter:   6914/ 13000 | global iter:   6914/ 13000 | loss: 1.3623 | ds_loss: 0.0000 | lr: 2.2690e-05 | scale: 32768.0000 | micro time: 1.864 | step time: 0.000
train | epoch   5 | Iter:   6915/ 13000 | global iter:   6915/ 13000 | loss: 0.8372 | ds_loss: 0.0000 | lr: 2.2684e-05 | scale: 32768.0000 | micro time: 1.738 | step time: 0.000
train | epoch   5 | Iter:   6916/ 13000 | global iter:   6916/ 13000 | loss: 0.7785 | ds_loss: 0.0000 | lr: 2.2678e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   5 | Iter:   6917/ 13000 | global iter:   6917/ 13000 | loss: 1.3750 | ds_loss: 0.0000 | lr: 2.2672e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   5 | Iter:   6918/ 13000 | global iter:   6918/ 13000 | loss: 1.0771 | ds_loss: 0.0000 | lr: 2.2666e-05 | scale: 32768.0000 | micro time: 1.849 | step time: 0.000
train | epoch   5 | Iter:   6919/ 13000 | global iter:   6919/ 13000 | loss: 1.1220 | ds_loss: 0.0000 | lr: 2.2660e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 0.000
train | epoch   5 | Iter:   6920/ 13000 | global iter:   6920/ 13000 | loss: 0.8926 | ds_loss: 0.0000 | lr: 2.2654e-05 | scale: 32768.0000 | micro time: 1.841 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6920/ 13000 | global iter:   6920/ 13000 | loss: 1.1207 | ds_loss: 0.0000 | lr: 2.2654e-05 | scale: 32768.0000 | micro time: 1.841 | step time: 1.824
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6921/ 13000 | global iter:   6921/ 13000 | loss: 1.3932 | ds_loss: 0.0000 | lr: 2.2648e-05 | scale: 32768.0000 | micro time: 1.725 | step time: 0.000
train | epoch   5 | Iter:   6922/ 13000 | global iter:   6922/ 13000 | loss: 0.9815 | ds_loss: 0.0000 | lr: 2.2642e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   5 | Iter:   6923/ 13000 | global iter:   6923/ 13000 | loss: 1.4022 | ds_loss: 0.0000 | lr: 2.2636e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   5 | Iter:   6924/ 13000 | global iter:   6924/ 13000 | loss: 1.3035 | ds_loss: 0.0000 | lr: 2.2630e-05 | scale: 32768.0000 | micro time: 1.935 | step time: 0.000
train | epoch   5 | Iter:   6925/ 13000 | global iter:   6925/ 13000 | loss: 0.9165 | ds_loss: 0.0000 | lr: 2.2624e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   5 | Iter:   6926/ 13000 | global iter:   6926/ 13000 | loss: 1.3761 | ds_loss: 0.0000 | lr: 2.2618e-05 | scale: 32768.0000 | micro time: 1.846 | step time: 0.000
train | epoch   5 | Iter:   6927/ 13000 | global iter:   6927/ 13000 | loss: 0.8891 | ds_loss: 0.0000 | lr: 2.2612e-05 | scale: 32768.0000 | micro time: 1.808 | step time: 0.000
train | epoch   5 | Iter:   6928/ 13000 | global iter:   6928/ 13000 | loss: 1.4856 | ds_loss: 0.0000 | lr: 2.2606e-05 | scale: 32768.0000 | micro time: 1.826 | step time: 0.000
train | epoch   5 | Iter:   6929/ 13000 | global iter:   6929/ 13000 | loss: 0.8089 | ds_loss: 0.0000 | lr: 2.2600e-05 | scale: 32768.0000 | micro time: 1.959 | step time: 0.000
train | epoch   5 | Iter:   6930/ 13000 | global iter:   6930/ 13000 | loss: 0.7802 | ds_loss: 0.0000 | lr: 2.2594e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6930/ 13000 | global iter:   6930/ 13000 | loss: 1.1337 | ds_loss: 0.0000 | lr: 2.2594e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 1.833
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6931/ 13000 | global iter:   6931/ 13000 | loss: 0.9463 | ds_loss: 0.0000 | lr: 2.2588e-05 | scale: 32768.0000 | micro time: 1.792 | step time: 0.000
train | epoch   5 | Iter:   6932/ 13000 | global iter:   6932/ 13000 | loss: 0.8052 | ds_loss: 0.0000 | lr: 2.2582e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   5 | Iter:   6933/ 13000 | global iter:   6933/ 13000 | loss: 0.9121 | ds_loss: 0.0000 | lr: 2.2576e-05 | scale: 32768.0000 | micro time: 1.817 | step time: 0.000
train | epoch   5 | Iter:   6934/ 13000 | global iter:   6934/ 13000 | loss: 1.1364 | ds_loss: 0.0000 | lr: 2.2570e-05 | scale: 32768.0000 | micro time: 1.911 | step time: 0.000
train | epoch   5 | Iter:   6935/ 13000 | global iter:   6935/ 13000 | loss: 0.9722 | ds_loss: 0.0000 | lr: 2.2564e-05 | scale: 32768.0000 | micro time: 1.786 | step time: 0.000
train | epoch   5 | Iter:   6936/ 13000 | global iter:   6936/ 13000 | loss: 1.4696 | ds_loss: 0.0000 | lr: 2.2558e-05 | scale: 32768.0000 | micro time: 1.842 | step time: 0.000
train | epoch   5 | Iter:   6937/ 13000 | global iter:   6937/ 13000 | loss: 1.0059 | ds_loss: 0.0000 | lr: 2.2552e-05 | scale: 32768.0000 | micro time: 1.888 | step time: 0.000
train | epoch   5 | Iter:   6938/ 13000 | global iter:   6938/ 13000 | loss: 0.6433 | ds_loss: 0.0000 | lr: 2.2546e-05 | scale: 32768.0000 | micro time: 1.769 | step time: 0.000
train | epoch   5 | Iter:   6939/ 13000 | global iter:   6939/ 13000 | loss: 0.8264 | ds_loss: 0.0000 | lr: 2.2540e-05 | scale: 32768.0000 | micro time: 1.733 | step time: 0.000
train | epoch   5 | Iter:   6940/ 13000 | global iter:   6940/ 13000 | loss: 1.0928 | ds_loss: 0.0000 | lr: 2.2534e-05 | scale: 32768.0000 | micro time: 1.865 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6940/ 13000 | global iter:   6940/ 13000 | loss: 0.9810 | ds_loss: 0.0000 | lr: 2.2534e-05 | scale: 32768.0000 | micro time: 1.865 | step time: 1.821
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6941/ 13000 | global iter:   6941/ 13000 | loss: 1.2261 | ds_loss: 0.0000 | lr: 2.2528e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   5 | Iter:   6942/ 13000 | global iter:   6942/ 13000 | loss: 1.1902 | ds_loss: 0.0000 | lr: 2.2522e-05 | scale: 32768.0000 | micro time: 1.786 | step time: 0.000
train | epoch   5 | Iter:   6943/ 13000 | global iter:   6943/ 13000 | loss: 0.4463 | ds_loss: 0.0000 | lr: 2.2516e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
train | epoch   5 | Iter:   6944/ 13000 | global iter:   6944/ 13000 | loss: 1.7483 | ds_loss: 0.0000 | lr: 2.2510e-05 | scale: 32768.0000 | micro time: 1.858 | step time: 0.000
train | epoch   5 | Iter:   6945/ 13000 | global iter:   6945/ 13000 | loss: 0.5967 | ds_loss: 0.0000 | lr: 2.2504e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   5 | Iter:   6946/ 13000 | global iter:   6946/ 13000 | loss: 1.0404 | ds_loss: 0.0000 | lr: 2.2498e-05 | scale: 32768.0000 | micro time: 1.751 | step time: 0.000
train | epoch   5 | Iter:   6947/ 13000 | global iter:   6947/ 13000 | loss: 1.0448 | ds_loss: 0.0000 | lr: 2.2492e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 0.000
train | epoch   5 | Iter:   6948/ 13000 | global iter:   6948/ 13000 | loss: 1.4968 | ds_loss: 0.0000 | lr: 2.2486e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   5 | Iter:   6949/ 13000 | global iter:   6949/ 13000 | loss: 1.0525 | ds_loss: 0.0000 | lr: 2.2480e-05 | scale: 32768.0000 | micro time: 1.708 | step time: 0.000
train | epoch   5 | Iter:   6950/ 13000 | global iter:   6950/ 13000 | loss: 0.5636 | ds_loss: 0.0000 | lr: 2.2474e-05 | scale: 32768.0000 | micro time: 1.837 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6950/ 13000 | global iter:   6950/ 13000 | loss: 1.0406 | ds_loss: 0.0000 | lr: 2.2474e-05 | scale: 32768.0000 | micro time: 1.837 | step time: 1.806
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6951/ 13000 | global iter:   6951/ 13000 | loss: 0.8266 | ds_loss: 0.0000 | lr: 2.2468e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   5 | Iter:   6952/ 13000 | global iter:   6952/ 13000 | loss: 1.0444 | ds_loss: 0.0000 | lr: 2.2462e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   5 | Iter:   6953/ 13000 | global iter:   6953/ 13000 | loss: 1.6441 | ds_loss: 0.0000 | lr: 2.2456e-05 | scale: 32768.0000 | micro time: 1.750 | step time: 0.000
train | epoch   5 | Iter:   6954/ 13000 | global iter:   6954/ 13000 | loss: 0.8771 | ds_loss: 0.0000 | lr: 2.2450e-05 | scale: 32768.0000 | micro time: 1.801 | step time: 0.000
train | epoch   5 | Iter:   6955/ 13000 | global iter:   6955/ 13000 | loss: 1.2570 | ds_loss: 0.0000 | lr: 2.2444e-05 | scale: 32768.0000 | micro time: 1.761 | step time: 0.000
train | epoch   5 | Iter:   6956/ 13000 | global iter:   6956/ 13000 | loss: 1.0913 | ds_loss: 0.0000 | lr: 2.2438e-05 | scale: 32768.0000 | micro time: 1.925 | step time: 0.000
train | epoch   5 | Iter:   6957/ 13000 | global iter:   6957/ 13000 | loss: 0.9024 | ds_loss: 0.0000 | lr: 2.2432e-05 | scale: 32768.0000 | micro time: 1.842 | step time: 0.000
train | epoch   5 | Iter:   6958/ 13000 | global iter:   6958/ 13000 | loss: 0.9507 | ds_loss: 0.0000 | lr: 2.2426e-05 | scale: 32768.0000 | micro time: 1.907 | step time: 0.000
train | epoch   5 | Iter:   6959/ 13000 | global iter:   6959/ 13000 | loss: 0.8114 | ds_loss: 0.0000 | lr: 2.2420e-05 | scale: 32768.0000 | micro time: 1.917 | step time: 0.000
train | epoch   5 | Iter:   6960/ 13000 | global iter:   6960/ 13000 | loss: 1.0751 | ds_loss: 0.0000 | lr: 2.2414e-05 | scale: 32768.0000 | micro time: 1.741 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6960/ 13000 | global iter:   6960/ 13000 | loss: 1.0480 | ds_loss: 0.0000 | lr: 2.2414e-05 | scale: 32768.0000 | micro time: 1.741 | step time: 1.831
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6961/ 13000 | global iter:   6961/ 13000 | loss: 1.2159 | ds_loss: 0.0000 | lr: 2.2408e-05 | scale: 32768.0000 | micro time: 1.861 | step time: 0.000
train | epoch   5 | Iter:   6962/ 13000 | global iter:   6962/ 13000 | loss: 1.4775 | ds_loss: 0.0000 | lr: 2.2402e-05 | scale: 32768.0000 | micro time: 1.879 | step time: 0.000
train | epoch   5 | Iter:   6963/ 13000 | global iter:   6963/ 13000 | loss: 0.7480 | ds_loss: 0.0000 | lr: 2.2396e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   5 | Iter:   6964/ 13000 | global iter:   6964/ 13000 | loss: 1.3017 | ds_loss: 0.0000 | lr: 2.2390e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   5 | Iter:   6965/ 13000 | global iter:   6965/ 13000 | loss: 1.0427 | ds_loss: 0.0000 | lr: 2.2384e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   5 | Iter:   6966/ 13000 | global iter:   6966/ 13000 | loss: 1.2785 | ds_loss: 0.0000 | lr: 2.2378e-05 | scale: 32768.0000 | micro time: 1.755 | step time: 0.000
train | epoch   5 | Iter:   6967/ 13000 | global iter:   6967/ 13000 | loss: 1.1620 | ds_loss: 0.0000 | lr: 2.2372e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   5 | Iter:   6968/ 13000 | global iter:   6968/ 13000 | loss: 1.0755 | ds_loss: 0.0000 | lr: 2.2366e-05 | scale: 32768.0000 | micro time: 1.907 | step time: 0.000
train | epoch   5 | Iter:   6969/ 13000 | global iter:   6969/ 13000 | loss: 1.3202 | ds_loss: 0.0000 | lr: 2.2360e-05 | scale: 32768.0000 | micro time: 1.686 | step time: 0.000
train | epoch   5 | Iter:   6970/ 13000 | global iter:   6970/ 13000 | loss: 0.9706 | ds_loss: 0.0000 | lr: 2.2354e-05 | scale: 32768.0000 | micro time: 1.832 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6970/ 13000 | global iter:   6970/ 13000 | loss: 1.1593 | ds_loss: 0.0000 | lr: 2.2354e-05 | scale: 32768.0000 | micro time: 1.832 | step time: 1.808
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6971/ 13000 | global iter:   6971/ 13000 | loss: 1.6705 | ds_loss: 0.0000 | lr: 2.2348e-05 | scale: 32768.0000 | micro time: 1.822 | step time: 0.000
train | epoch   5 | Iter:   6972/ 13000 | global iter:   6972/ 13000 | loss: 1.1910 | ds_loss: 0.0000 | lr: 2.2342e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 0.000
train | epoch   5 | Iter:   6973/ 13000 | global iter:   6973/ 13000 | loss: 1.2281 | ds_loss: 0.0000 | lr: 2.2336e-05 | scale: 32768.0000 | micro time: 1.741 | step time: 0.000
train | epoch   5 | Iter:   6974/ 13000 | global iter:   6974/ 13000 | loss: 0.6872 | ds_loss: 0.0000 | lr: 2.2330e-05 | scale: 32768.0000 | micro time: 1.755 | step time: 0.000
train | epoch   5 | Iter:   6975/ 13000 | global iter:   6975/ 13000 | loss: 1.0668 | ds_loss: 0.0000 | lr: 2.2324e-05 | scale: 32768.0000 | micro time: 1.760 | step time: 0.000
train | epoch   5 | Iter:   6976/ 13000 | global iter:   6976/ 13000 | loss: 1.5032 | ds_loss: 0.0000 | lr: 2.2318e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
train | epoch   5 | Iter:   6977/ 13000 | global iter:   6977/ 13000 | loss: 1.1564 | ds_loss: 0.0000 | lr: 2.2312e-05 | scale: 32768.0000 | micro time: 1.777 | step time: 0.000
train | epoch   5 | Iter:   6978/ 13000 | global iter:   6978/ 13000 | loss: 1.4960 | ds_loss: 0.0000 | lr: 2.2306e-05 | scale: 32768.0000 | micro time: 1.885 | step time: 0.000
train | epoch   5 | Iter:   6979/ 13000 | global iter:   6979/ 13000 | loss: 1.0692 | ds_loss: 0.0000 | lr: 2.2300e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   5 | Iter:   6980/ 13000 | global iter:   6980/ 13000 | loss: 1.1591 | ds_loss: 0.0000 | lr: 2.2294e-05 | scale: 32768.0000 | micro time: 1.788 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6980/ 13000 | global iter:   6980/ 13000 | loss: 1.2228 | ds_loss: 0.0000 | lr: 2.2294e-05 | scale: 32768.0000 | micro time: 1.788 | step time: 1.796
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6981/ 13000 | global iter:   6981/ 13000 | loss: 1.4758 | ds_loss: 0.0000 | lr: 2.2288e-05 | scale: 32768.0000 | micro time: 1.924 | step time: 0.000
train | epoch   5 | Iter:   6982/ 13000 | global iter:   6982/ 13000 | loss: 1.6800 | ds_loss: 0.0000 | lr: 2.2282e-05 | scale: 32768.0000 | micro time: 1.868 | step time: 0.000
train | epoch   5 | Iter:   6983/ 13000 | global iter:   6983/ 13000 | loss: 1.4061 | ds_loss: 0.0000 | lr: 2.2276e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   5 | Iter:   6984/ 13000 | global iter:   6984/ 13000 | loss: 1.0151 | ds_loss: 0.0000 | lr: 2.2270e-05 | scale: 32768.0000 | micro time: 1.832 | step time: 0.000
train | epoch   5 | Iter:   6985/ 13000 | global iter:   6985/ 13000 | loss: 1.4172 | ds_loss: 0.0000 | lr: 2.2264e-05 | scale: 32768.0000 | micro time: 1.686 | step time: 0.000
train | epoch   5 | Iter:   6986/ 13000 | global iter:   6986/ 13000 | loss: 1.1383 | ds_loss: 0.0000 | lr: 2.2258e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   5 | Iter:   6987/ 13000 | global iter:   6987/ 13000 | loss: 1.3854 | ds_loss: 0.0000 | lr: 2.2252e-05 | scale: 32768.0000 | micro time: 1.754 | step time: 0.000
train | epoch   5 | Iter:   6988/ 13000 | global iter:   6988/ 13000 | loss: 1.1921 | ds_loss: 0.0000 | lr: 2.2246e-05 | scale: 32768.0000 | micro time: 1.820 | step time: 0.000
train | epoch   5 | Iter:   6989/ 13000 | global iter:   6989/ 13000 | loss: 1.5232 | ds_loss: 0.0000 | lr: 2.2240e-05 | scale: 32768.0000 | micro time: 1.813 | step time: 0.000
train | epoch   5 | Iter:   6990/ 13000 | global iter:   6990/ 13000 | loss: 0.9074 | ds_loss: 0.0000 | lr: 2.2234e-05 | scale: 32768.0000 | micro time: 1.848 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   6990/ 13000 | global iter:   6990/ 13000 | loss: 1.3141 | ds_loss: 0.0000 | lr: 2.2234e-05 | scale: 32768.0000 | micro time: 1.848 | step time: 1.817
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   6991/ 13000 | global iter:   6991/ 13000 | loss: 1.2072 | ds_loss: 0.0000 | lr: 2.2228e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 0.000
train | epoch   5 | Iter:   6992/ 13000 | global iter:   6992/ 13000 | loss: 1.1306 | ds_loss: 0.0000 | lr: 2.2222e-05 | scale: 32768.0000 | micro time: 1.820 | step time: 0.000
train | epoch   5 | Iter:   6993/ 13000 | global iter:   6993/ 13000 | loss: 1.2641 | ds_loss: 0.0000 | lr: 2.2216e-05 | scale: 32768.0000 | micro time: 1.907 | step time: 0.000
train | epoch   5 | Iter:   6994/ 13000 | global iter:   6994/ 13000 | loss: 1.1645 | ds_loss: 0.0000 | lr: 2.2210e-05 | scale: 32768.0000 | micro time: 1.937 | step time: 0.000
train | epoch   5 | Iter:   6995/ 13000 | global iter:   6995/ 13000 | loss: 1.2278 | ds_loss: 0.0000 | lr: 2.2204e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   5 | Iter:   6996/ 13000 | global iter:   6996/ 13000 | loss: 1.0413 | ds_loss: 0.0000 | lr: 2.2198e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   5 | Iter:   6997/ 13000 | global iter:   6997/ 13000 | loss: 0.7287 | ds_loss: 0.0000 | lr: 2.2192e-05 | scale: 32768.0000 | micro time: 1.834 | step time: 0.000
train | epoch   5 | Iter:   6998/ 13000 | global iter:   6998/ 13000 | loss: 0.9979 | ds_loss: 0.0000 | lr: 2.2186e-05 | scale: 32768.0000 | micro time: 1.888 | step time: 0.000
train | epoch   5 | Iter:   6999/ 13000 | global iter:   6999/ 13000 | loss: 0.8123 | ds_loss: 0.0000 | lr: 2.2180e-05 | scale: 32768.0000 | micro time: 1.708 | step time: 0.000
train | epoch   5 | Iter:   7000/ 13000 | global iter:   7000/ 13000 | loss: 1.7036 | ds_loss: 0.0000 | lr: 2.2174e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7000/ 13000 | global iter:   7000/ 13000 | loss: 1.1278 | ds_loss: 0.0000 | lr: 2.2174e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 1.842
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7001/ 13000 | global iter:   7001/ 13000 | loss: 0.7986 | ds_loss: 0.0000 | lr: 2.2168e-05 | scale: 32768.0000 | micro time: 1.861 | step time: 0.000
train | epoch   5 | Iter:   7002/ 13000 | global iter:   7002/ 13000 | loss: 0.9177 | ds_loss: 0.0000 | lr: 2.2162e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
train | epoch   5 | Iter:   7003/ 13000 | global iter:   7003/ 13000 | loss: 1.5840 | ds_loss: 0.0000 | lr: 2.2156e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   5 | Iter:   7004/ 13000 | global iter:   7004/ 13000 | loss: 1.1145 | ds_loss: 0.0000 | lr: 2.2150e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   5 | Iter:   7005/ 13000 | global iter:   7005/ 13000 | loss: 1.5604 | ds_loss: 0.0000 | lr: 2.2144e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   5 | Iter:   7006/ 13000 | global iter:   7006/ 13000 | loss: 1.3803 | ds_loss: 0.0000 | lr: 2.2138e-05 | scale: 32768.0000 | micro time: 1.741 | step time: 0.000
train | epoch   5 | Iter:   7007/ 13000 | global iter:   7007/ 13000 | loss: 1.3523 | ds_loss: 0.0000 | lr: 2.2132e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   5 | Iter:   7008/ 13000 | global iter:   7008/ 13000 | loss: 1.0333 | ds_loss: 0.0000 | lr: 2.2126e-05 | scale: 32768.0000 | micro time: 1.768 | step time: 0.000
train | epoch   5 | Iter:   7009/ 13000 | global iter:   7009/ 13000 | loss: 1.0735 | ds_loss: 0.0000 | lr: 2.2120e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   5 | Iter:   7010/ 13000 | global iter:   7010/ 13000 | loss: 1.5625 | ds_loss: 0.0000 | lr: 2.2114e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7010/ 13000 | global iter:   7010/ 13000 | loss: 1.2377 | ds_loss: 0.0000 | lr: 2.2114e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 1.805
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7011/ 13000 | global iter:   7011/ 13000 | loss: 1.0936 | ds_loss: 0.0000 | lr: 2.2108e-05 | scale: 32768.0000 | micro time: 1.902 | step time: 0.000
train | epoch   5 | Iter:   7012/ 13000 | global iter:   7012/ 13000 | loss: 1.5232 | ds_loss: 0.0000 | lr: 2.2102e-05 | scale: 32768.0000 | micro time: 1.852 | step time: 0.000
train | epoch   5 | Iter:   7013/ 13000 | global iter:   7013/ 13000 | loss: 0.8534 | ds_loss: 0.0000 | lr: 2.2096e-05 | scale: 32768.0000 | micro time: 1.832 | step time: 0.000
train | epoch   5 | Iter:   7014/ 13000 | global iter:   7014/ 13000 | loss: 1.5684 | ds_loss: 0.0000 | lr: 2.2090e-05 | scale: 32768.0000 | micro time: 1.857 | step time: 0.000
train | epoch   5 | Iter:   7015/ 13000 | global iter:   7015/ 13000 | loss: 1.4929 | ds_loss: 0.0000 | lr: 2.2085e-05 | scale: 32768.0000 | micro time: 1.891 | step time: 0.000
train | epoch   5 | Iter:   7016/ 13000 | global iter:   7016/ 13000 | loss: 1.2214 | ds_loss: 0.0000 | lr: 2.2079e-05 | scale: 32768.0000 | micro time: 1.696 | step time: 0.000
train | epoch   5 | Iter:   7017/ 13000 | global iter:   7017/ 13000 | loss: 0.6030 | ds_loss: 0.0000 | lr: 2.2073e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   5 | Iter:   7018/ 13000 | global iter:   7018/ 13000 | loss: 1.3249 | ds_loss: 0.0000 | lr: 2.2067e-05 | scale: 32768.0000 | micro time: 1.886 | step time: 0.000
train | epoch   5 | Iter:   7019/ 13000 | global iter:   7019/ 13000 | loss: 1.0711 | ds_loss: 0.0000 | lr: 2.2061e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   5 | Iter:   7020/ 13000 | global iter:   7020/ 13000 | loss: 1.5243 | ds_loss: 0.0000 | lr: 2.2055e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7020/ 13000 | global iter:   7020/ 13000 | loss: 1.2276 | ds_loss: 0.0000 | lr: 2.2055e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 1.844
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7021/ 13000 | global iter:   7021/ 13000 | loss: 0.8346 | ds_loss: 0.0000 | lr: 2.2049e-05 | scale: 32768.0000 | micro time: 1.793 | step time: 0.000
train | epoch   5 | Iter:   7022/ 13000 | global iter:   7022/ 13000 | loss: 1.0940 | ds_loss: 0.0000 | lr: 2.2043e-05 | scale: 32768.0000 | micro time: 1.884 | step time: 0.000
train | epoch   5 | Iter:   7023/ 13000 | global iter:   7023/ 13000 | loss: 1.0589 | ds_loss: 0.0000 | lr: 2.2037e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   5 | Iter:   7024/ 13000 | global iter:   7024/ 13000 | loss: 1.0007 | ds_loss: 0.0000 | lr: 2.2031e-05 | scale: 32768.0000 | micro time: 1.740 | step time: 0.000
train | epoch   5 | Iter:   7025/ 13000 | global iter:   7025/ 13000 | loss: 0.7039 | ds_loss: 0.0000 | lr: 2.2025e-05 | scale: 32768.0000 | micro time: 1.688 | step time: 0.000
train | epoch   5 | Iter:   7026/ 13000 | global iter:   7026/ 13000 | loss: 0.7259 | ds_loss: 0.0000 | lr: 2.2019e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   5 | Iter:   7027/ 13000 | global iter:   7027/ 13000 | loss: 0.8176 | ds_loss: 0.0000 | lr: 2.2013e-05 | scale: 32768.0000 | micro time: 1.808 | step time: 0.000
train | epoch   5 | Iter:   7028/ 13000 | global iter:   7028/ 13000 | loss: 1.1355 | ds_loss: 0.0000 | lr: 2.2007e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   5 | Iter:   7029/ 13000 | global iter:   7029/ 13000 | loss: 0.9985 | ds_loss: 0.0000 | lr: 2.2001e-05 | scale: 32768.0000 | micro time: 1.720 | step time: 0.000
train | epoch   5 | Iter:   7030/ 13000 | global iter:   7030/ 13000 | loss: 0.8900 | ds_loss: 0.0000 | lr: 2.1995e-05 | scale: 32768.0000 | micro time: 1.862 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7030/ 13000 | global iter:   7030/ 13000 | loss: 0.9260 | ds_loss: 0.0000 | lr: 2.1995e-05 | scale: 32768.0000 | micro time: 1.862 | step time: 1.794
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7031/ 13000 | global iter:   7031/ 13000 | loss: 0.9836 | ds_loss: 0.0000 | lr: 2.1989e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   5 | Iter:   7032/ 13000 | global iter:   7032/ 13000 | loss: 1.6138 | ds_loss: 0.0000 | lr: 2.1983e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   5 | Iter:   7033/ 13000 | global iter:   7033/ 13000 | loss: 1.0044 | ds_loss: 0.0000 | lr: 2.1977e-05 | scale: 32768.0000 | micro time: 1.821 | step time: 0.000
train | epoch   5 | Iter:   7034/ 13000 | global iter:   7034/ 13000 | loss: 0.9423 | ds_loss: 0.0000 | lr: 2.1971e-05 | scale: 32768.0000 | micro time: 1.828 | step time: 0.000
train | epoch   5 | Iter:   7035/ 13000 | global iter:   7035/ 13000 | loss: 0.9571 | ds_loss: 0.0000 | lr: 2.1965e-05 | scale: 32768.0000 | micro time: 1.880 | step time: 0.000
train | epoch   5 | Iter:   7036/ 13000 | global iter:   7036/ 13000 | loss: 0.6347 | ds_loss: 0.0000 | lr: 2.1959e-05 | scale: 32768.0000 | micro time: 1.895 | step time: 0.000
train | epoch   5 | Iter:   7037/ 13000 | global iter:   7037/ 13000 | loss: 1.1375 | ds_loss: 0.0000 | lr: 2.1953e-05 | scale: 32768.0000 | micro time: 1.794 | step time: 0.000
train | epoch   5 | Iter:   7038/ 13000 | global iter:   7038/ 13000 | loss: 1.0044 | ds_loss: 0.0000 | lr: 2.1947e-05 | scale: 32768.0000 | micro time: 1.737 | step time: 0.000
train | epoch   5 | Iter:   7039/ 13000 | global iter:   7039/ 13000 | loss: 1.1065 | ds_loss: 0.0000 | lr: 2.1941e-05 | scale: 32768.0000 | micro time: 1.857 | step time: 0.000
train | epoch   5 | Iter:   7040/ 13000 | global iter:   7040/ 13000 | loss: 1.2110 | ds_loss: 0.0000 | lr: 2.1935e-05 | scale: 32768.0000 | micro time: 1.899 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7040/ 13000 | global iter:   7040/ 13000 | loss: 1.0595 | ds_loss: 0.0000 | lr: 2.1935e-05 | scale: 32768.0000 | micro time: 1.899 | step time: 1.830
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7041/ 13000 | global iter:   7041/ 13000 | loss: 1.4044 | ds_loss: 0.0000 | lr: 2.1929e-05 | scale: 32768.0000 | micro time: 1.801 | step time: 0.000
train | epoch   5 | Iter:   7042/ 13000 | global iter:   7042/ 13000 | loss: 1.2923 | ds_loss: 0.0000 | lr: 2.1923e-05 | scale: 32768.0000 | micro time: 1.885 | step time: 0.000
train | epoch   5 | Iter:   7043/ 13000 | global iter:   7043/ 13000 | loss: 1.2627 | ds_loss: 0.0000 | lr: 2.1917e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   5 | Iter:   7044/ 13000 | global iter:   7044/ 13000 | loss: 1.9919 | ds_loss: 0.0000 | lr: 2.1911e-05 | scale: 32768.0000 | micro time: 1.889 | step time: 0.000
train | epoch   5 | Iter:   7045/ 13000 | global iter:   7045/ 13000 | loss: 1.1594 | ds_loss: 0.0000 | lr: 2.1905e-05 | scale: 32768.0000 | micro time: 1.768 | step time: 0.000
train | epoch   5 | Iter:   7046/ 13000 | global iter:   7046/ 13000 | loss: 1.2519 | ds_loss: 0.0000 | lr: 2.1899e-05 | scale: 32768.0000 | micro time: 1.826 | step time: 0.000
train | epoch   5 | Iter:   7047/ 13000 | global iter:   7047/ 13000 | loss: 1.3800 | ds_loss: 0.0000 | lr: 2.1893e-05 | scale: 32768.0000 | micro time: 1.915 | step time: 0.000
train | epoch   5 | Iter:   7048/ 13000 | global iter:   7048/ 13000 | loss: 1.2959 | ds_loss: 0.0000 | lr: 2.1887e-05 | scale: 32768.0000 | micro time: 1.852 | step time: 0.000
train | epoch   5 | Iter:   7049/ 13000 | global iter:   7049/ 13000 | loss: 0.9949 | ds_loss: 0.0000 | lr: 2.1881e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   5 | Iter:   7050/ 13000 | global iter:   7050/ 13000 | loss: 1.4402 | ds_loss: 0.0000 | lr: 2.1875e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7050/ 13000 | global iter:   7050/ 13000 | loss: 1.3473 | ds_loss: 0.0000 | lr: 2.1875e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 1.845
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7051/ 13000 | global iter:   7051/ 13000 | loss: 0.7896 | ds_loss: 0.0000 | lr: 2.1869e-05 | scale: 32768.0000 | micro time: 1.806 | step time: 0.000
train | epoch   5 | Iter:   7052/ 13000 | global iter:   7052/ 13000 | loss: 0.8333 | ds_loss: 0.0000 | lr: 2.1863e-05 | scale: 32768.0000 | micro time: 1.688 | step time: 0.000
train | epoch   5 | Iter:   7053/ 13000 | global iter:   7053/ 13000 | loss: 1.1954 | ds_loss: 0.0000 | lr: 2.1857e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   5 | Iter:   7054/ 13000 | global iter:   7054/ 13000 | loss: 0.9779 | ds_loss: 0.0000 | lr: 2.1851e-05 | scale: 32768.0000 | micro time: 1.773 | step time: 0.000
train | epoch   5 | Iter:   7055/ 13000 | global iter:   7055/ 13000 | loss: 1.2865 | ds_loss: 0.0000 | lr: 2.1845e-05 | scale: 32768.0000 | micro time: 1.920 | step time: 0.000
train | epoch   5 | Iter:   7056/ 13000 | global iter:   7056/ 13000 | loss: 1.2275 | ds_loss: 0.0000 | lr: 2.1839e-05 | scale: 32768.0000 | micro time: 1.834 | step time: 0.000
train | epoch   5 | Iter:   7057/ 13000 | global iter:   7057/ 13000 | loss: 1.4691 | ds_loss: 0.0000 | lr: 2.1833e-05 | scale: 32768.0000 | micro time: 1.795 | step time: 0.000
train | epoch   5 | Iter:   7058/ 13000 | global iter:   7058/ 13000 | loss: 1.0799 | ds_loss: 0.0000 | lr: 2.1827e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   5 | Iter:   7059/ 13000 | global iter:   7059/ 13000 | loss: 0.9915 | ds_loss: 0.0000 | lr: 2.1821e-05 | scale: 32768.0000 | micro time: 1.834 | step time: 0.000
train | epoch   5 | Iter:   7060/ 13000 | global iter:   7060/ 13000 | loss: 0.9398 | ds_loss: 0.0000 | lr: 2.1815e-05 | scale: 32768.0000 | micro time: 1.820 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7060/ 13000 | global iter:   7060/ 13000 | loss: 1.0791 | ds_loss: 0.0000 | lr: 2.1815e-05 | scale: 32768.0000 | micro time: 1.820 | step time: 1.810
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7061/ 13000 | global iter:   7061/ 13000 | loss: 1.1880 | ds_loss: 0.0000 | lr: 2.1809e-05 | scale: 32768.0000 | micro time: 1.714 | step time: 0.000
train | epoch   5 | Iter:   7062/ 13000 | global iter:   7062/ 13000 | loss: 1.1983 | ds_loss: 0.0000 | lr: 2.1803e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
train | epoch   5 | Iter:   7063/ 13000 | global iter:   7063/ 13000 | loss: 1.6132 | ds_loss: 0.0000 | lr: 2.1797e-05 | scale: 32768.0000 | micro time: 1.751 | step time: 0.000
train | epoch   5 | Iter:   7064/ 13000 | global iter:   7064/ 13000 | loss: 1.1427 | ds_loss: 0.0000 | lr: 2.1791e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   5 | Iter:   7065/ 13000 | global iter:   7065/ 13000 | loss: 1.2803 | ds_loss: 0.0000 | lr: 2.1785e-05 | scale: 32768.0000 | micro time: 1.868 | step time: 0.000
train | epoch   5 | Iter:   7066/ 13000 | global iter:   7066/ 13000 | loss: 1.0524 | ds_loss: 0.0000 | lr: 2.1779e-05 | scale: 32768.0000 | micro time: 1.862 | step time: 0.000
train | epoch   5 | Iter:   7067/ 13000 | global iter:   7067/ 13000 | loss: 0.8453 | ds_loss: 0.0000 | lr: 2.1773e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   5 | Iter:   7068/ 13000 | global iter:   7068/ 13000 | loss: 0.7752 | ds_loss: 0.0000 | lr: 2.1767e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   5 | Iter:   7069/ 13000 | global iter:   7069/ 13000 | loss: 1.2803 | ds_loss: 0.0000 | lr: 2.1761e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   5 | Iter:   7070/ 13000 | global iter:   7070/ 13000 | loss: 1.3889 | ds_loss: 0.0000 | lr: 2.1756e-05 | scale: 32768.0000 | micro time: 1.805 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7070/ 13000 | global iter:   7070/ 13000 | loss: 1.1764 | ds_loss: 0.0000 | lr: 2.1756e-05 | scale: 32768.0000 | micro time: 1.805 | step time: 1.811
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7071/ 13000 | global iter:   7071/ 13000 | loss: 1.3554 | ds_loss: 0.0000 | lr: 2.1750e-05 | scale: 32768.0000 | micro time: 1.885 | step time: 0.000
train | epoch   5 | Iter:   7072/ 13000 | global iter:   7072/ 13000 | loss: 1.1197 | ds_loss: 0.0000 | lr: 2.1744e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   5 | Iter:   7073/ 13000 | global iter:   7073/ 13000 | loss: 1.5276 | ds_loss: 0.0000 | lr: 2.1738e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
train | epoch   5 | Iter:   7074/ 13000 | global iter:   7074/ 13000 | loss: 0.9020 | ds_loss: 0.0000 | lr: 2.1732e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   5 | Iter:   7075/ 13000 | global iter:   7075/ 13000 | loss: 1.2734 | ds_loss: 0.0000 | lr: 2.1726e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   5 | Iter:   7076/ 13000 | global iter:   7076/ 13000 | loss: 1.9698 | ds_loss: 0.0000 | lr: 2.1720e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   5 | Iter:   7077/ 13000 | global iter:   7077/ 13000 | loss: 1.5731 | ds_loss: 0.0000 | lr: 2.1714e-05 | scale: 32768.0000 | micro time: 1.727 | step time: 0.000
train | epoch   5 | Iter:   7078/ 13000 | global iter:   7078/ 13000 | loss: 0.9433 | ds_loss: 0.0000 | lr: 2.1708e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   5 | Iter:   7079/ 13000 | global iter:   7079/ 13000 | loss: 0.9871 | ds_loss: 0.0000 | lr: 2.1702e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   5 | Iter:   7080/ 13000 | global iter:   7080/ 13000 | loss: 0.8221 | ds_loss: 0.0000 | lr: 2.1696e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7080/ 13000 | global iter:   7080/ 13000 | loss: 1.2474 | ds_loss: 0.0000 | lr: 2.1696e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 1.800
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7081/ 13000 | global iter:   7081/ 13000 | loss: 1.0762 | ds_loss: 0.0000 | lr: 2.1690e-05 | scale: 32768.0000 | micro time: 1.850 | step time: 0.000
train | epoch   5 | Iter:   7082/ 13000 | global iter:   7082/ 13000 | loss: 1.6699 | ds_loss: 0.0000 | lr: 2.1684e-05 | scale: 32768.0000 | micro time: 1.802 | step time: 0.000
train | epoch   5 | Iter:   7083/ 13000 | global iter:   7083/ 13000 | loss: 1.0849 | ds_loss: 0.0000 | lr: 2.1678e-05 | scale: 32768.0000 | micro time: 1.794 | step time: 0.000
train | epoch   5 | Iter:   7084/ 13000 | global iter:   7084/ 13000 | loss: 0.8932 | ds_loss: 0.0000 | lr: 2.1672e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 0.000
train | epoch   5 | Iter:   7085/ 13000 | global iter:   7085/ 13000 | loss: 1.1846 | ds_loss: 0.0000 | lr: 2.1666e-05 | scale: 32768.0000 | micro time: 1.699 | step time: 0.000
train | epoch   5 | Iter:   7086/ 13000 | global iter:   7086/ 13000 | loss: 0.8961 | ds_loss: 0.0000 | lr: 2.1660e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   5 | Iter:   7087/ 13000 | global iter:   7087/ 13000 | loss: 0.7950 | ds_loss: 0.0000 | lr: 2.1654e-05 | scale: 32768.0000 | micro time: 1.736 | step time: 0.000
train | epoch   5 | Iter:   7088/ 13000 | global iter:   7088/ 13000 | loss: 1.1210 | ds_loss: 0.0000 | lr: 2.1648e-05 | scale: 32768.0000 | micro time: 1.885 | step time: 0.000
train | epoch   5 | Iter:   7089/ 13000 | global iter:   7089/ 13000 | loss: 1.5119 | ds_loss: 0.0000 | lr: 2.1642e-05 | scale: 32768.0000 | micro time: 1.748 | step time: 0.000
train | epoch   5 | Iter:   7090/ 13000 | global iter:   7090/ 13000 | loss: 1.2647 | ds_loss: 0.0000 | lr: 2.1636e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7090/ 13000 | global iter:   7090/ 13000 | loss: 1.1497 | ds_loss: 0.0000 | lr: 2.1636e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 1.810
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7091/ 13000 | global iter:   7091/ 13000 | loss: 1.3676 | ds_loss: 0.0000 | lr: 2.1630e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   5 | Iter:   7092/ 13000 | global iter:   7092/ 13000 | loss: 1.3834 | ds_loss: 0.0000 | lr: 2.1624e-05 | scale: 32768.0000 | micro time: 1.754 | step time: 0.000
train | epoch   5 | Iter:   7093/ 13000 | global iter:   7093/ 13000 | loss: 0.8893 | ds_loss: 0.0000 | lr: 2.1618e-05 | scale: 32768.0000 | micro time: 1.777 | step time: 0.000
train | epoch   5 | Iter:   7094/ 13000 | global iter:   7094/ 13000 | loss: 0.6995 | ds_loss: 0.0000 | lr: 2.1612e-05 | scale: 32768.0000 | micro time: 1.726 | step time: 0.000
train | epoch   5 | Iter:   7095/ 13000 | global iter:   7095/ 13000 | loss: 1.2504 | ds_loss: 0.0000 | lr: 2.1606e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   5 | Iter:   7096/ 13000 | global iter:   7096/ 13000 | loss: 1.2683 | ds_loss: 0.0000 | lr: 2.1600e-05 | scale: 32768.0000 | micro time: 1.899 | step time: 0.000
train | epoch   5 | Iter:   7097/ 13000 | global iter:   7097/ 13000 | loss: 0.4994 | ds_loss: 0.0000 | lr: 2.1594e-05 | scale: 32768.0000 | micro time: 1.872 | step time: 0.000
train | epoch   5 | Iter:   7098/ 13000 | global iter:   7098/ 13000 | loss: 1.7482 | ds_loss: 0.0000 | lr: 2.1588e-05 | scale: 32768.0000 | micro time: 1.822 | step time: 0.000
train | epoch   5 | Iter:   7099/ 13000 | global iter:   7099/ 13000 | loss: 1.4282 | ds_loss: 0.0000 | lr: 2.1582e-05 | scale: 32768.0000 | micro time: 1.743 | step time: 0.000
train | epoch   5 | Iter:   7100/ 13000 | global iter:   7100/ 13000 | loss: 1.0538 | ds_loss: 0.0000 | lr: 2.1576e-05 | scale: 32768.0000 | micro time: 1.864 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7100/ 13000 | global iter:   7100/ 13000 | loss: 1.1588 | ds_loss: 0.0000 | lr: 2.1576e-05 | scale: 32768.0000 | micro time: 1.864 | step time: 1.812
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7101/ 13000 | global iter:   7101/ 13000 | loss: 1.1922 | ds_loss: 0.0000 | lr: 2.1570e-05 | scale: 32768.0000 | micro time: 1.751 | step time: 0.000
train | epoch   5 | Iter:   7102/ 13000 | global iter:   7102/ 13000 | loss: 1.1635 | ds_loss: 0.0000 | lr: 2.1564e-05 | scale: 32768.0000 | micro time: 1.822 | step time: 0.000
train | epoch   5 | Iter:   7103/ 13000 | global iter:   7103/ 13000 | loss: 1.3629 | ds_loss: 0.0000 | lr: 2.1558e-05 | scale: 32768.0000 | micro time: 1.792 | step time: 0.000
train | epoch   5 | Iter:   7104/ 13000 | global iter:   7104/ 13000 | loss: 1.3645 | ds_loss: 0.0000 | lr: 2.1552e-05 | scale: 32768.0000 | micro time: 1.748 | step time: 0.000
train | epoch   5 | Iter:   7105/ 13000 | global iter:   7105/ 13000 | loss: 1.3014 | ds_loss: 0.0000 | lr: 2.1546e-05 | scale: 32768.0000 | micro time: 1.850 | step time: 0.000
train | epoch   5 | Iter:   7106/ 13000 | global iter:   7106/ 13000 | loss: 1.1276 | ds_loss: 0.0000 | lr: 2.1540e-05 | scale: 32768.0000 | micro time: 1.765 | step time: 0.000
train | epoch   5 | Iter:   7107/ 13000 | global iter:   7107/ 13000 | loss: 1.8294 | ds_loss: 0.0000 | lr: 2.1535e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   5 | Iter:   7108/ 13000 | global iter:   7108/ 13000 | loss: 1.0903 | ds_loss: 0.0000 | lr: 2.1529e-05 | scale: 32768.0000 | micro time: 1.747 | step time: 0.000
train | epoch   5 | Iter:   7109/ 13000 | global iter:   7109/ 13000 | loss: 1.1799 | ds_loss: 0.0000 | lr: 2.1523e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   5 | Iter:   7110/ 13000 | global iter:   7110/ 13000 | loss: 1.4412 | ds_loss: 0.0000 | lr: 2.1517e-05 | scale: 32768.0000 | micro time: 1.766 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7110/ 13000 | global iter:   7110/ 13000 | loss: 1.3053 | ds_loss: 0.0000 | lr: 2.1517e-05 | scale: 32768.0000 | micro time: 1.766 | step time: 1.779
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7111/ 13000 | global iter:   7111/ 13000 | loss: 1.0303 | ds_loss: 0.0000 | lr: 2.1511e-05 | scale: 32768.0000 | micro time: 1.899 | step time: 0.000
train | epoch   5 | Iter:   7112/ 13000 | global iter:   7112/ 13000 | loss: 1.4453 | ds_loss: 0.0000 | lr: 2.1505e-05 | scale: 32768.0000 | micro time: 1.821 | step time: 0.000
train | epoch   5 | Iter:   7113/ 13000 | global iter:   7113/ 13000 | loss: 1.0492 | ds_loss: 0.0000 | lr: 2.1499e-05 | scale: 32768.0000 | micro time: 1.709 | step time: 0.000
train | epoch   5 | Iter:   7114/ 13000 | global iter:   7114/ 13000 | loss: 0.9000 | ds_loss: 0.0000 | lr: 2.1493e-05 | scale: 32768.0000 | micro time: 1.829 | step time: 0.000
train | epoch   5 | Iter:   7115/ 13000 | global iter:   7115/ 13000 | loss: 1.1130 | ds_loss: 0.0000 | lr: 2.1487e-05 | scale: 32768.0000 | micro time: 1.817 | step time: 0.000
train | epoch   5 | Iter:   7116/ 13000 | global iter:   7116/ 13000 | loss: 1.4067 | ds_loss: 0.0000 | lr: 2.1481e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   5 | Iter:   7117/ 13000 | global iter:   7117/ 13000 | loss: 1.0020 | ds_loss: 0.0000 | lr: 2.1475e-05 | scale: 32768.0000 | micro time: 1.755 | step time: 0.000
train | epoch   5 | Iter:   7118/ 13000 | global iter:   7118/ 13000 | loss: 1.1977 | ds_loss: 0.0000 | lr: 2.1469e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   5 | Iter:   7119/ 13000 | global iter:   7119/ 13000 | loss: 0.7848 | ds_loss: 0.0000 | lr: 2.1463e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   5 | Iter:   7120/ 13000 | global iter:   7120/ 13000 | loss: 1.6145 | ds_loss: 0.0000 | lr: 2.1457e-05 | scale: 32768.0000 | micro time: 1.864 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7120/ 13000 | global iter:   7120/ 13000 | loss: 1.1544 | ds_loss: 0.0000 | lr: 2.1457e-05 | scale: 32768.0000 | micro time: 1.864 | step time: 1.809
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7121/ 13000 | global iter:   7121/ 13000 | loss: 0.7241 | ds_loss: 0.0000 | lr: 2.1451e-05 | scale: 32768.0000 | micro time: 1.735 | step time: 0.000
train | epoch   5 | Iter:   7122/ 13000 | global iter:   7122/ 13000 | loss: 1.2587 | ds_loss: 0.0000 | lr: 2.1445e-05 | scale: 32768.0000 | micro time: 1.790 | step time: 0.000
train | epoch   5 | Iter:   7123/ 13000 | global iter:   7123/ 13000 | loss: 0.9221 | ds_loss: 0.0000 | lr: 2.1439e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   5 | Iter:   7124/ 13000 | global iter:   7124/ 13000 | loss: 1.0017 | ds_loss: 0.0000 | lr: 2.1433e-05 | scale: 32768.0000 | micro time: 1.812 | step time: 0.000
train | epoch   5 | Iter:   7125/ 13000 | global iter:   7125/ 13000 | loss: 0.8852 | ds_loss: 0.0000 | lr: 2.1427e-05 | scale: 32768.0000 | micro time: 1.794 | step time: 0.000
train | epoch   5 | Iter:   7126/ 13000 | global iter:   7126/ 13000 | loss: 0.9970 | ds_loss: 0.0000 | lr: 2.1421e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   5 | Iter:   7127/ 13000 | global iter:   7127/ 13000 | loss: 1.1994 | ds_loss: 0.0000 | lr: 2.1415e-05 | scale: 32768.0000 | micro time: 1.737 | step time: 0.000
train | epoch   5 | Iter:   7128/ 13000 | global iter:   7128/ 13000 | loss: 0.9249 | ds_loss: 0.0000 | lr: 2.1409e-05 | scale: 32768.0000 | micro time: 1.727 | step time: 0.000
train | epoch   5 | Iter:   7129/ 13000 | global iter:   7129/ 13000 | loss: 0.9441 | ds_loss: 0.0000 | lr: 2.1403e-05 | scale: 32768.0000 | micro time: 1.834 | step time: 0.000
train | epoch   5 | Iter:   7130/ 13000 | global iter:   7130/ 13000 | loss: 1.0964 | ds_loss: 0.0000 | lr: 2.1397e-05 | scale: 32768.0000 | micro time: 1.802 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7130/ 13000 | global iter:   7130/ 13000 | loss: 0.9953 | ds_loss: 0.0000 | lr: 2.1397e-05 | scale: 32768.0000 | micro time: 1.802 | step time: 1.788
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7131/ 13000 | global iter:   7131/ 13000 | loss: 1.7016 | ds_loss: 0.0000 | lr: 2.1391e-05 | scale: 32768.0000 | micro time: 1.805 | step time: 0.000
train | epoch   5 | Iter:   7132/ 13000 | global iter:   7132/ 13000 | loss: 0.4668 | ds_loss: 0.0000 | lr: 2.1385e-05 | scale: 32768.0000 | micro time: 1.711 | step time: 0.000
train | epoch   5 | Iter:   7133/ 13000 | global iter:   7133/ 13000 | loss: 1.3408 | ds_loss: 0.0000 | lr: 2.1379e-05 | scale: 32768.0000 | micro time: 1.907 | step time: 0.000
train | epoch   5 | Iter:   7134/ 13000 | global iter:   7134/ 13000 | loss: 1.5434 | ds_loss: 0.0000 | lr: 2.1373e-05 | scale: 32768.0000 | micro time: 1.747 | step time: 0.000
train | epoch   5 | Iter:   7135/ 13000 | global iter:   7135/ 13000 | loss: 0.7146 | ds_loss: 0.0000 | lr: 2.1367e-05 | scale: 32768.0000 | micro time: 1.797 | step time: 0.000
train | epoch   5 | Iter:   7136/ 13000 | global iter:   7136/ 13000 | loss: 1.0169 | ds_loss: 0.0000 | lr: 2.1361e-05 | scale: 32768.0000 | micro time: 1.749 | step time: 0.000
train | epoch   5 | Iter:   7137/ 13000 | global iter:   7137/ 13000 | loss: 1.3772 | ds_loss: 0.0000 | lr: 2.1356e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   5 | Iter:   7138/ 13000 | global iter:   7138/ 13000 | loss: 1.4335 | ds_loss: 0.0000 | lr: 2.1350e-05 | scale: 32768.0000 | micro time: 1.846 | step time: 0.000
train | epoch   5 | Iter:   7139/ 13000 | global iter:   7139/ 13000 | loss: 1.0778 | ds_loss: 0.0000 | lr: 2.1344e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   5 | Iter:   7140/ 13000 | global iter:   7140/ 13000 | loss: 0.9205 | ds_loss: 0.0000 | lr: 2.1338e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7140/ 13000 | global iter:   7140/ 13000 | loss: 1.1593 | ds_loss: 0.0000 | lr: 2.1338e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 1.797
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7141/ 13000 | global iter:   7141/ 13000 | loss: 0.7522 | ds_loss: 0.0000 | lr: 2.1332e-05 | scale: 32768.0000 | micro time: 1.876 | step time: 0.000
train | epoch   5 | Iter:   7142/ 13000 | global iter:   7142/ 13000 | loss: 1.3744 | ds_loss: 0.0000 | lr: 2.1326e-05 | scale: 32768.0000 | micro time: 1.814 | step time: 0.000
train | epoch   5 | Iter:   7143/ 13000 | global iter:   7143/ 13000 | loss: 1.1396 | ds_loss: 0.0000 | lr: 2.1320e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   5 | Iter:   7144/ 13000 | global iter:   7144/ 13000 | loss: 1.2962 | ds_loss: 0.0000 | lr: 2.1314e-05 | scale: 32768.0000 | micro time: 1.682 | step time: 0.000
train | epoch   5 | Iter:   7145/ 13000 | global iter:   7145/ 13000 | loss: 1.1765 | ds_loss: 0.0000 | lr: 2.1308e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   5 | Iter:   7146/ 13000 | global iter:   7146/ 13000 | loss: 1.3530 | ds_loss: 0.0000 | lr: 2.1302e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   5 | Iter:   7147/ 13000 | global iter:   7147/ 13000 | loss: 1.1127 | ds_loss: 0.0000 | lr: 2.1296e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   5 | Iter:   7148/ 13000 | global iter:   7148/ 13000 | loss: 1.4222 | ds_loss: 0.0000 | lr: 2.1290e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   5 | Iter:   7149/ 13000 | global iter:   7149/ 13000 | loss: 0.8668 | ds_loss: 0.0000 | lr: 2.1284e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   5 | Iter:   7150/ 13000 | global iter:   7150/ 13000 | loss: 1.0162 | ds_loss: 0.0000 | lr: 2.1278e-05 | scale: 32768.0000 | micro time: 1.698 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7150/ 13000 | global iter:   7150/ 13000 | loss: 1.1510 | ds_loss: 0.0000 | lr: 2.1278e-05 | scale: 32768.0000 | micro time: 1.698 | step time: 1.793
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7151/ 13000 | global iter:   7151/ 13000 | loss: 0.9963 | ds_loss: 0.0000 | lr: 2.1272e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   5 | Iter:   7152/ 13000 | global iter:   7152/ 13000 | loss: 1.4707 | ds_loss: 0.0000 | lr: 2.1266e-05 | scale: 32768.0000 | micro time: 1.899 | step time: 0.000
train | epoch   5 | Iter:   7153/ 13000 | global iter:   7153/ 13000 | loss: 1.3400 | ds_loss: 0.0000 | lr: 2.1260e-05 | scale: 32768.0000 | micro time: 1.705 | step time: 0.000
train | epoch   5 | Iter:   7154/ 13000 | global iter:   7154/ 13000 | loss: 1.1209 | ds_loss: 0.0000 | lr: 2.1254e-05 | scale: 32768.0000 | micro time: 1.833 | step time: 0.000
train | epoch   5 | Iter:   7155/ 13000 | global iter:   7155/ 13000 | loss: 1.1959 | ds_loss: 0.0000 | lr: 2.1248e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   5 | Iter:   7156/ 13000 | global iter:   7156/ 13000 | loss: 0.8015 | ds_loss: 0.0000 | lr: 2.1242e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   5 | Iter:   7157/ 13000 | global iter:   7157/ 13000 | loss: 0.5928 | ds_loss: 0.0000 | lr: 2.1236e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
train | epoch   5 | Iter:   7158/ 13000 | global iter:   7158/ 13000 | loss: 1.0393 | ds_loss: 0.0000 | lr: 2.1230e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   5 | Iter:   7159/ 13000 | global iter:   7159/ 13000 | loss: 1.2126 | ds_loss: 0.0000 | lr: 2.1224e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 0.000
train | epoch   5 | Iter:   7160/ 13000 | global iter:   7160/ 13000 | loss: 1.4497 | ds_loss: 0.0000 | lr: 2.1218e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7160/ 13000 | global iter:   7160/ 13000 | loss: 1.1219 | ds_loss: 0.0000 | lr: 2.1218e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 1.824
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7161/ 13000 | global iter:   7161/ 13000 | loss: 1.8511 | ds_loss: 0.0000 | lr: 2.1212e-05 | scale: 32768.0000 | micro time: 1.781 | step time: 0.000
train | epoch   5 | Iter:   7162/ 13000 | global iter:   7162/ 13000 | loss: 0.9253 | ds_loss: 0.0000 | lr: 2.1207e-05 | scale: 32768.0000 | micro time: 1.739 | step time: 0.000
train | epoch   5 | Iter:   7163/ 13000 | global iter:   7163/ 13000 | loss: 1.2208 | ds_loss: 0.0000 | lr: 2.1201e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   5 | Iter:   7164/ 13000 | global iter:   7164/ 13000 | loss: 1.5285 | ds_loss: 0.0000 | lr: 2.1195e-05 | scale: 32768.0000 | micro time: 1.816 | step time: 0.000
train | epoch   5 | Iter:   7165/ 13000 | global iter:   7165/ 13000 | loss: 1.3451 | ds_loss: 0.0000 | lr: 2.1189e-05 | scale: 32768.0000 | micro time: 1.732 | step time: 0.000
train | epoch   5 | Iter:   7166/ 13000 | global iter:   7166/ 13000 | loss: 1.6462 | ds_loss: 0.0000 | lr: 2.1183e-05 | scale: 32768.0000 | micro time: 1.749 | step time: 0.000
train | epoch   5 | Iter:   7167/ 13000 | global iter:   7167/ 13000 | loss: 1.0961 | ds_loss: 0.0000 | lr: 2.1177e-05 | scale: 32768.0000 | micro time: 1.719 | step time: 0.000
train | epoch   5 | Iter:   7168/ 13000 | global iter:   7168/ 13000 | loss: 1.1325 | ds_loss: 0.0000 | lr: 2.1171e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   5 | Iter:   7169/ 13000 | global iter:   7169/ 13000 | loss: 1.5761 | ds_loss: 0.0000 | lr: 2.1165e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   5 | Iter:   7170/ 13000 | global iter:   7170/ 13000 | loss: 1.2548 | ds_loss: 0.0000 | lr: 2.1159e-05 | scale: 32768.0000 | micro time: 1.762 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7170/ 13000 | global iter:   7170/ 13000 | loss: 1.3576 | ds_loss: 0.0000 | lr: 2.1159e-05 | scale: 32768.0000 | micro time: 1.762 | step time: 1.769
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7171/ 13000 | global iter:   7171/ 13000 | loss: 0.6708 | ds_loss: 0.0000 | lr: 2.1153e-05 | scale: 32768.0000 | micro time: 1.853 | step time: 0.000
train | epoch   5 | Iter:   7172/ 13000 | global iter:   7172/ 13000 | loss: 1.2077 | ds_loss: 0.0000 | lr: 2.1147e-05 | scale: 32768.0000 | micro time: 1.822 | step time: 0.000
train | epoch   5 | Iter:   7173/ 13000 | global iter:   7173/ 13000 | loss: 1.1133 | ds_loss: 0.0000 | lr: 2.1141e-05 | scale: 32768.0000 | micro time: 1.789 | step time: 0.000
train | epoch   5 | Iter:   7174/ 13000 | global iter:   7174/ 13000 | loss: 1.0688 | ds_loss: 0.0000 | lr: 2.1135e-05 | scale: 32768.0000 | micro time: 1.755 | step time: 0.000
train | epoch   5 | Iter:   7175/ 13000 | global iter:   7175/ 13000 | loss: 1.3141 | ds_loss: 0.0000 | lr: 2.1129e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   5 | Iter:   7176/ 13000 | global iter:   7176/ 13000 | loss: 1.7227 | ds_loss: 0.0000 | lr: 2.1123e-05 | scale: 32768.0000 | micro time: 1.832 | step time: 0.000
train | epoch   5 | Iter:   7177/ 13000 | global iter:   7177/ 13000 | loss: 1.2343 | ds_loss: 0.0000 | lr: 2.1117e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   5 | Iter:   7178/ 13000 | global iter:   7178/ 13000 | loss: 1.5357 | ds_loss: 0.0000 | lr: 2.1111e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   5 | Iter:   7179/ 13000 | global iter:   7179/ 13000 | loss: 1.2038 | ds_loss: 0.0000 | lr: 2.1105e-05 | scale: 32768.0000 | micro time: 1.886 | step time: 0.000
train | epoch   5 | Iter:   7180/ 13000 | global iter:   7180/ 13000 | loss: 0.8644 | ds_loss: 0.0000 | lr: 2.1099e-05 | scale: 32768.0000 | micro time: 1.893 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7180/ 13000 | global iter:   7180/ 13000 | loss: 1.1935 | ds_loss: 0.0000 | lr: 2.1099e-05 | scale: 32768.0000 | micro time: 1.893 | step time: 1.838
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7181/ 13000 | global iter:   7181/ 13000 | loss: 1.4605 | ds_loss: 0.0000 | lr: 2.1093e-05 | scale: 32768.0000 | micro time: 1.889 | step time: 0.000
train | epoch   5 | Iter:   7182/ 13000 | global iter:   7182/ 13000 | loss: 1.3347 | ds_loss: 0.0000 | lr: 2.1087e-05 | scale: 32768.0000 | micro time: 1.861 | step time: 0.000
train | epoch   5 | Iter:   7183/ 13000 | global iter:   7183/ 13000 | loss: 0.5751 | ds_loss: 0.0000 | lr: 2.1081e-05 | scale: 32768.0000 | micro time: 1.838 | step time: 0.000
train | epoch   5 | Iter:   7184/ 13000 | global iter:   7184/ 13000 | loss: 1.0413 | ds_loss: 0.0000 | lr: 2.1076e-05 | scale: 32768.0000 | micro time: 1.887 | step time: 0.000
train | epoch   5 | Iter:   7185/ 13000 | global iter:   7185/ 13000 | loss: 1.2885 | ds_loss: 0.0000 | lr: 2.1070e-05 | scale: 32768.0000 | micro time: 1.820 | step time: 0.000
train | epoch   5 | Iter:   7186/ 13000 | global iter:   7186/ 13000 | loss: 0.6725 | ds_loss: 0.0000 | lr: 2.1064e-05 | scale: 32768.0000 | micro time: 1.802 | step time: 0.000
train | epoch   5 | Iter:   7187/ 13000 | global iter:   7187/ 13000 | loss: 0.9358 | ds_loss: 0.0000 | lr: 2.1058e-05 | scale: 32768.0000 | micro time: 1.702 | step time: 0.000
train | epoch   5 | Iter:   7188/ 13000 | global iter:   7188/ 13000 | loss: 1.2627 | ds_loss: 0.0000 | lr: 2.1052e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   5 | Iter:   7189/ 13000 | global iter:   7189/ 13000 | loss: 1.6642 | ds_loss: 0.0000 | lr: 2.1046e-05 | scale: 32768.0000 | micro time: 1.872 | step time: 0.000
train | epoch   5 | Iter:   7190/ 13000 | global iter:   7190/ 13000 | loss: 1.4286 | ds_loss: 0.0000 | lr: 2.1040e-05 | scale: 32768.0000 | micro time: 1.810 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7190/ 13000 | global iter:   7190/ 13000 | loss: 1.1664 | ds_loss: 0.0000 | lr: 2.1040e-05 | scale: 32768.0000 | micro time: 1.810 | step time: 1.832
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7191/ 13000 | global iter:   7191/ 13000 | loss: 1.3435 | ds_loss: 0.0000 | lr: 2.1034e-05 | scale: 32768.0000 | micro time: 1.778 | step time: 0.000
train | epoch   5 | Iter:   7192/ 13000 | global iter:   7192/ 13000 | loss: 0.5272 | ds_loss: 0.0000 | lr: 2.1028e-05 | scale: 32768.0000 | micro time: 1.828 | step time: 0.000
train | epoch   5 | Iter:   7193/ 13000 | global iter:   7193/ 13000 | loss: 1.1994 | ds_loss: 0.0000 | lr: 2.1022e-05 | scale: 32768.0000 | micro time: 1.769 | step time: 0.000
train | epoch   5 | Iter:   7194/ 13000 | global iter:   7194/ 13000 | loss: 1.2694 | ds_loss: 0.0000 | lr: 2.1016e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   5 | Iter:   7195/ 13000 | global iter:   7195/ 13000 | loss: 1.3883 | ds_loss: 0.0000 | lr: 2.1010e-05 | scale: 32768.0000 | micro time: 1.725 | step time: 0.000
train | epoch   5 | Iter:   7196/ 13000 | global iter:   7196/ 13000 | loss: 0.9069 | ds_loss: 0.0000 | lr: 2.1004e-05 | scale: 32768.0000 | micro time: 1.869 | step time: 0.000
train | epoch   5 | Iter:   7197/ 13000 | global iter:   7197/ 13000 | loss: 1.1216 | ds_loss: 0.0000 | lr: 2.0998e-05 | scale: 32768.0000 | micro time: 1.804 | step time: 0.000
train | epoch   5 | Iter:   7198/ 13000 | global iter:   7198/ 13000 | loss: 1.1279 | ds_loss: 0.0000 | lr: 2.0992e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   5 | Iter:   7199/ 13000 | global iter:   7199/ 13000 | loss: 1.5584 | ds_loss: 0.0000 | lr: 2.0986e-05 | scale: 32768.0000 | micro time: 1.895 | step time: 0.000
train | epoch   5 | Iter:   7200/ 13000 | global iter:   7200/ 13000 | loss: 0.7400 | ds_loss: 0.0000 | lr: 2.0980e-05 | scale: 32768.0000 | micro time: 1.739 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7200/ 13000 | global iter:   7200/ 13000 | loss: 1.1183 | ds_loss: 0.0000 | lr: 2.0980e-05 | scale: 32768.0000 | micro time: 1.739 | step time: 1.810
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7201/ 13000 | global iter:   7201/ 13000 | loss: 1.1520 | ds_loss: 0.0000 | lr: 2.0974e-05 | scale: 32768.0000 | micro time: 1.737 | step time: 0.000
train | epoch   5 | Iter:   7202/ 13000 | global iter:   7202/ 13000 | loss: 0.8611 | ds_loss: 0.0000 | lr: 2.0968e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   5 | Iter:   7203/ 13000 | global iter:   7203/ 13000 | loss: 1.0555 | ds_loss: 0.0000 | lr: 2.0962e-05 | scale: 32768.0000 | micro time: 1.931 | step time: 0.000
train | epoch   5 | Iter:   7204/ 13000 | global iter:   7204/ 13000 | loss: 1.2775 | ds_loss: 0.0000 | lr: 2.0957e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   5 | Iter:   7205/ 13000 | global iter:   7205/ 13000 | loss: 1.1619 | ds_loss: 0.0000 | lr: 2.0951e-05 | scale: 32768.0000 | micro time: 1.900 | step time: 0.000
train | epoch   5 | Iter:   7206/ 13000 | global iter:   7206/ 13000 | loss: 1.5582 | ds_loss: 0.0000 | lr: 2.0945e-05 | scale: 32768.0000 | micro time: 1.826 | step time: 0.000
train | epoch   5 | Iter:   7207/ 13000 | global iter:   7207/ 13000 | loss: 1.3381 | ds_loss: 0.0000 | lr: 2.0939e-05 | scale: 32768.0000 | micro time: 1.792 | step time: 0.000
train | epoch   5 | Iter:   7208/ 13000 | global iter:   7208/ 13000 | loss: 0.8935 | ds_loss: 0.0000 | lr: 2.0933e-05 | scale: 32768.0000 | micro time: 1.726 | step time: 0.000
train | epoch   5 | Iter:   7209/ 13000 | global iter:   7209/ 13000 | loss: 0.7748 | ds_loss: 0.0000 | lr: 2.0927e-05 | scale: 32768.0000 | micro time: 1.904 | step time: 0.000
train | epoch   5 | Iter:   7210/ 13000 | global iter:   7210/ 13000 | loss: 0.9847 | ds_loss: 0.0000 | lr: 2.0921e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7210/ 13000 | global iter:   7210/ 13000 | loss: 1.1057 | ds_loss: 0.0000 | lr: 2.0921e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 1.824
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7211/ 13000 | global iter:   7211/ 13000 | loss: 1.3995 | ds_loss: 0.0000 | lr: 2.0915e-05 | scale: 32768.0000 | micro time: 1.813 | step time: 0.000
train | epoch   5 | Iter:   7212/ 13000 | global iter:   7212/ 13000 | loss: 0.5793 | ds_loss: 0.0000 | lr: 2.0909e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   5 | Iter:   7213/ 13000 | global iter:   7213/ 13000 | loss: 0.8343 | ds_loss: 0.0000 | lr: 2.0903e-05 | scale: 32768.0000 | micro time: 1.885 | step time: 0.000
train | epoch   5 | Iter:   7214/ 13000 | global iter:   7214/ 13000 | loss: 0.6304 | ds_loss: 0.0000 | lr: 2.0897e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   5 | Iter:   7215/ 13000 | global iter:   7215/ 13000 | loss: 1.2749 | ds_loss: 0.0000 | lr: 2.0891e-05 | scale: 32768.0000 | micro time: 1.743 | step time: 0.000
train | epoch   5 | Iter:   7216/ 13000 | global iter:   7216/ 13000 | loss: 1.3012 | ds_loss: 0.0000 | lr: 2.0885e-05 | scale: 32768.0000 | micro time: 1.820 | step time: 0.000
train | epoch   5 | Iter:   7217/ 13000 | global iter:   7217/ 13000 | loss: 1.2619 | ds_loss: 0.0000 | lr: 2.0879e-05 | scale: 32768.0000 | micro time: 1.765 | step time: 0.000
train | epoch   5 | Iter:   7218/ 13000 | global iter:   7218/ 13000 | loss: 0.3683 | ds_loss: 0.0000 | lr: 2.0873e-05 | scale: 32768.0000 | micro time: 1.786 | step time: 0.000
train | epoch   5 | Iter:   7219/ 13000 | global iter:   7219/ 13000 | loss: 1.6199 | ds_loss: 0.0000 | lr: 2.0867e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
train | epoch   5 | Iter:   7220/ 13000 | global iter:   7220/ 13000 | loss: 1.1495 | ds_loss: 0.0000 | lr: 2.0861e-05 | scale: 32768.0000 | micro time: 1.869 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7220/ 13000 | global iter:   7220/ 13000 | loss: 1.0419 | ds_loss: 0.0000 | lr: 2.0861e-05 | scale: 32768.0000 | micro time: 1.869 | step time: 1.804
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7221/ 13000 | global iter:   7221/ 13000 | loss: 1.0136 | ds_loss: 0.0000 | lr: 2.0855e-05 | scale: 32768.0000 | micro time: 1.878 | step time: 0.000
train | epoch   5 | Iter:   7222/ 13000 | global iter:   7222/ 13000 | loss: 0.9720 | ds_loss: 0.0000 | lr: 2.0849e-05 | scale: 32768.0000 | micro time: 1.796 | step time: 0.000
train | epoch   5 | Iter:   7223/ 13000 | global iter:   7223/ 13000 | loss: 1.2947 | ds_loss: 0.0000 | lr: 2.0844e-05 | scale: 32768.0000 | micro time: 1.798 | step time: 0.000
train | epoch   5 | Iter:   7224/ 13000 | global iter:   7224/ 13000 | loss: 0.9994 | ds_loss: 0.0000 | lr: 2.0838e-05 | scale: 32768.0000 | micro time: 1.795 | step time: 0.000
train | epoch   5 | Iter:   7225/ 13000 | global iter:   7225/ 13000 | loss: 1.4817 | ds_loss: 0.0000 | lr: 2.0832e-05 | scale: 32768.0000 | micro time: 1.784 | step time: 0.000
train | epoch   5 | Iter:   7226/ 13000 | global iter:   7226/ 13000 | loss: 1.6261 | ds_loss: 0.0000 | lr: 2.0826e-05 | scale: 32768.0000 | micro time: 1.802 | step time: 0.000
train | epoch   5 | Iter:   7227/ 13000 | global iter:   7227/ 13000 | loss: 1.0047 | ds_loss: 0.0000 | lr: 2.0820e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   5 | Iter:   7228/ 13000 | global iter:   7228/ 13000 | loss: 0.9979 | ds_loss: 0.0000 | lr: 2.0814e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 0.000
train | epoch   5 | Iter:   7229/ 13000 | global iter:   7229/ 13000 | loss: 1.2097 | ds_loss: 0.0000 | lr: 2.0808e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   5 | Iter:   7230/ 13000 | global iter:   7230/ 13000 | loss: 0.7274 | ds_loss: 0.0000 | lr: 2.0802e-05 | scale: 32768.0000 | micro time: 1.911 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7230/ 13000 | global iter:   7230/ 13000 | loss: 1.1327 | ds_loss: 0.0000 | lr: 2.0802e-05 | scale: 32768.0000 | micro time: 1.911 | step time: 1.830
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7231/ 13000 | global iter:   7231/ 13000 | loss: 0.8608 | ds_loss: 0.0000 | lr: 2.0796e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   5 | Iter:   7232/ 13000 | global iter:   7232/ 13000 | loss: 0.9702 | ds_loss: 0.0000 | lr: 2.0790e-05 | scale: 32768.0000 | micro time: 2.200 | step time: 0.000
train | epoch   5 | Iter:   7233/ 13000 | global iter:   7233/ 13000 | loss: 1.4010 | ds_loss: 0.0000 | lr: 2.0784e-05 | scale: 32768.0000 | micro time: 1.764 | step time: 0.000
train | epoch   5 | Iter:   7234/ 13000 | global iter:   7234/ 13000 | loss: 1.5827 | ds_loss: 0.0000 | lr: 2.0778e-05 | scale: 32768.0000 | micro time: 1.730 | step time: 0.000
train | epoch   5 | Iter:   7235/ 13000 | global iter:   7235/ 13000 | loss: 1.2525 | ds_loss: 0.0000 | lr: 2.0772e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   5 | Iter:   7236/ 13000 | global iter:   7236/ 13000 | loss: 1.2430 | ds_loss: 0.0000 | lr: 2.0766e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   5 | Iter:   7237/ 13000 | global iter:   7237/ 13000 | loss: 1.5439 | ds_loss: 0.0000 | lr: 2.0760e-05 | scale: 32768.0000 | micro time: 1.755 | step time: 0.000
train | epoch   5 | Iter:   7238/ 13000 | global iter:   7238/ 13000 | loss: 1.0917 | ds_loss: 0.0000 | lr: 2.0754e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   5 | Iter:   7239/ 13000 | global iter:   7239/ 13000 | loss: 1.2937 | ds_loss: 0.0000 | lr: 2.0748e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   5 | Iter:   7240/ 13000 | global iter:   7240/ 13000 | loss: 1.4459 | ds_loss: 0.0000 | lr: 2.0743e-05 | scale: 32768.0000 | micro time: 1.900 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7240/ 13000 | global iter:   7240/ 13000 | loss: 1.2685 | ds_loss: 0.0000 | lr: 2.0743e-05 | scale: 32768.0000 | micro time: 1.900 | step time: 1.849
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7241/ 13000 | global iter:   7241/ 13000 | loss: 1.2353 | ds_loss: 0.0000 | lr: 2.0737e-05 | scale: 32768.0000 | micro time: 1.773 | step time: 0.000
train | epoch   5 | Iter:   7242/ 13000 | global iter:   7242/ 13000 | loss: 1.3258 | ds_loss: 0.0000 | lr: 2.0731e-05 | scale: 32768.0000 | micro time: 1.900 | step time: 0.000
train | epoch   5 | Iter:   7243/ 13000 | global iter:   7243/ 13000 | loss: 1.3197 | ds_loss: 0.0000 | lr: 2.0725e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   5 | Iter:   7244/ 13000 | global iter:   7244/ 13000 | loss: 1.3301 | ds_loss: 0.0000 | lr: 2.0719e-05 | scale: 32768.0000 | micro time: 1.809 | step time: 0.000
train | epoch   5 | Iter:   7245/ 13000 | global iter:   7245/ 13000 | loss: 1.5276 | ds_loss: 0.0000 | lr: 2.0713e-05 | scale: 32768.0000 | micro time: 1.813 | step time: 0.000
train | epoch   5 | Iter:   7246/ 13000 | global iter:   7246/ 13000 | loss: 0.6951 | ds_loss: 0.0000 | lr: 2.0707e-05 | scale: 32768.0000 | micro time: 1.782 | step time: 0.000
train | epoch   5 | Iter:   7247/ 13000 | global iter:   7247/ 13000 | loss: 1.3524 | ds_loss: 0.0000 | lr: 2.0701e-05 | scale: 32768.0000 | micro time: 1.768 | step time: 0.000
train | epoch   5 | Iter:   7248/ 13000 | global iter:   7248/ 13000 | loss: 1.6890 | ds_loss: 0.0000 | lr: 2.0695e-05 | scale: 32768.0000 | micro time: 1.810 | step time: 0.000
train | epoch   5 | Iter:   7249/ 13000 | global iter:   7249/ 13000 | loss: 0.3224 | ds_loss: 0.0000 | lr: 2.0689e-05 | scale: 32768.0000 | micro time: 1.829 | step time: 0.000
train | epoch   5 | Iter:   7250/ 13000 | global iter:   7250/ 13000 | loss: 1.1688 | ds_loss: 0.0000 | lr: 2.0683e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7250/ 13000 | global iter:   7250/ 13000 | loss: 1.1966 | ds_loss: 0.0000 | lr: 2.0683e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 1.818
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7251/ 13000 | global iter:   7251/ 13000 | loss: 1.3710 | ds_loss: 0.0000 | lr: 2.0677e-05 | scale: 32768.0000 | micro time: 1.700 | step time: 0.000
train | epoch   5 | Iter:   7252/ 13000 | global iter:   7252/ 13000 | loss: 0.8933 | ds_loss: 0.0000 | lr: 2.0671e-05 | scale: 32768.0000 | micro time: 1.738 | step time: 0.000
train | epoch   5 | Iter:   7253/ 13000 | global iter:   7253/ 13000 | loss: 1.6154 | ds_loss: 0.0000 | lr: 2.0665e-05 | scale: 32768.0000 | micro time: 1.926 | step time: 0.000
train | epoch   5 | Iter:   7254/ 13000 | global iter:   7254/ 13000 | loss: 0.7017 | ds_loss: 0.0000 | lr: 2.0659e-05 | scale: 32768.0000 | micro time: 1.915 | step time: 0.000
train | epoch   5 | Iter:   7255/ 13000 | global iter:   7255/ 13000 | loss: 1.1308 | ds_loss: 0.0000 | lr: 2.0653e-05 | scale: 32768.0000 | micro time: 1.693 | step time: 0.000
train | epoch   5 | Iter:   7256/ 13000 | global iter:   7256/ 13000 | loss: 0.9645 | ds_loss: 0.0000 | lr: 2.0648e-05 | scale: 32768.0000 | micro time: 1.797 | step time: 0.000
train | epoch   5 | Iter:   7257/ 13000 | global iter:   7257/ 13000 | loss: 1.3989 | ds_loss: 0.0000 | lr: 2.0642e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   5 | Iter:   7258/ 13000 | global iter:   7258/ 13000 | loss: 1.0530 | ds_loss: 0.0000 | lr: 2.0636e-05 | scale: 32768.0000 | micro time: 1.846 | step time: 0.000
train | epoch   5 | Iter:   7259/ 13000 | global iter:   7259/ 13000 | loss: 0.9590 | ds_loss: 0.0000 | lr: 2.0630e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   5 | Iter:   7260/ 13000 | global iter:   7260/ 13000 | loss: 0.9968 | ds_loss: 0.0000 | lr: 2.0624e-05 | scale: 32768.0000 | micro time: 1.837 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7260/ 13000 | global iter:   7260/ 13000 | loss: 1.1084 | ds_loss: 0.0000 | lr: 2.0624e-05 | scale: 32768.0000 | micro time: 1.837 | step time: 1.805
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7261/ 13000 | global iter:   7261/ 13000 | loss: 1.5467 | ds_loss: 0.0000 | lr: 2.0618e-05 | scale: 32768.0000 | micro time: 1.804 | step time: 0.000
train | epoch   5 | Iter:   7262/ 13000 | global iter:   7262/ 13000 | loss: 1.4080 | ds_loss: 0.0000 | lr: 2.0612e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   5 | Iter:   7263/ 13000 | global iter:   7263/ 13000 | loss: 0.5424 | ds_loss: 0.0000 | lr: 2.0606e-05 | scale: 32768.0000 | micro time: 1.677 | step time: 0.000
train | epoch   5 | Iter:   7264/ 13000 | global iter:   7264/ 13000 | loss: 1.7104 | ds_loss: 0.0000 | lr: 2.0600e-05 | scale: 32768.0000 | micro time: 1.825 | step time: 0.000
train | epoch   5 | Iter:   7265/ 13000 | global iter:   7265/ 13000 | loss: 1.2463 | ds_loss: 0.0000 | lr: 2.0594e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   5 | Iter:   7266/ 13000 | global iter:   7266/ 13000 | loss: 1.4405 | ds_loss: 0.0000 | lr: 2.0588e-05 | scale: 32768.0000 | micro time: 1.667 | step time: 0.000
train | epoch   5 | Iter:   7267/ 13000 | global iter:   7267/ 13000 | loss: 1.2045 | ds_loss: 0.0000 | lr: 2.0582e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
train | epoch   5 | Iter:   7268/ 13000 | global iter:   7268/ 13000 | loss: 1.2406 | ds_loss: 0.0000 | lr: 2.0576e-05 | scale: 32768.0000 | micro time: 2.295 | step time: 0.000
train | epoch   5 | Iter:   7269/ 13000 | global iter:   7269/ 13000 | loss: 1.3676 | ds_loss: 0.0000 | lr: 2.0570e-05 | scale: 32768.0000 | micro time: 1.751 | step time: 0.000
train | epoch   5 | Iter:   7270/ 13000 | global iter:   7270/ 13000 | loss: 1.1936 | ds_loss: 0.0000 | lr: 2.0564e-05 | scale: 32768.0000 | micro time: 1.895 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7270/ 13000 | global iter:   7270/ 13000 | loss: 1.2901 | ds_loss: 0.0000 | lr: 2.0564e-05 | scale: 32768.0000 | micro time: 1.895 | step time: 1.832
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7271/ 13000 | global iter:   7271/ 13000 | loss: 1.3821 | ds_loss: 0.0000 | lr: 2.0559e-05 | scale: 32768.0000 | micro time: 1.826 | step time: 0.000
train | epoch   5 | Iter:   7272/ 13000 | global iter:   7272/ 13000 | loss: 0.9528 | ds_loss: 0.0000 | lr: 2.0553e-05 | scale: 32768.0000 | micro time: 1.801 | step time: 0.000
train | epoch   5 | Iter:   7273/ 13000 | global iter:   7273/ 13000 | loss: 1.4026 | ds_loss: 0.0000 | lr: 2.0547e-05 | scale: 32768.0000 | micro time: 2.179 | step time: 0.000
train | epoch   5 | Iter:   7274/ 13000 | global iter:   7274/ 13000 | loss: 1.3333 | ds_loss: 0.0000 | lr: 2.0541e-05 | scale: 32768.0000 | micro time: 1.911 | step time: 0.000
train | epoch   5 | Iter:   7275/ 13000 | global iter:   7275/ 13000 | loss: 1.0927 | ds_loss: 0.0000 | lr: 2.0535e-05 | scale: 32768.0000 | micro time: 1.947 | step time: 0.000
train | epoch   5 | Iter:   7276/ 13000 | global iter:   7276/ 13000 | loss: 0.9649 | ds_loss: 0.0000 | lr: 2.0529e-05 | scale: 32768.0000 | micro time: 1.889 | step time: 0.000
train | epoch   5 | Iter:   7277/ 13000 | global iter:   7277/ 13000 | loss: 0.9940 | ds_loss: 0.0000 | lr: 2.0523e-05 | scale: 32768.0000 | micro time: 1.755 | step time: 0.000
train | epoch   5 | Iter:   7278/ 13000 | global iter:   7278/ 13000 | loss: 1.2329 | ds_loss: 0.0000 | lr: 2.0517e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   5 | Iter:   7279/ 13000 | global iter:   7279/ 13000 | loss: 1.2552 | ds_loss: 0.0000 | lr: 2.0511e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   5 | Iter:   7280/ 13000 | global iter:   7280/ 13000 | loss: 0.7952 | ds_loss: 0.0000 | lr: 2.0505e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7280/ 13000 | global iter:   7280/ 13000 | loss: 1.1406 | ds_loss: 0.0000 | lr: 2.0505e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 1.872
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7281/ 13000 | global iter:   7281/ 13000 | loss: 0.9214 | ds_loss: 0.0000 | lr: 2.0499e-05 | scale: 32768.0000 | micro time: 1.830 | step time: 0.000
train | epoch   5 | Iter:   7282/ 13000 | global iter:   7282/ 13000 | loss: 1.1693 | ds_loss: 0.0000 | lr: 2.0493e-05 | scale: 32768.0000 | micro time: 1.785 | step time: 0.000
train | epoch   5 | Iter:   7283/ 13000 | global iter:   7283/ 13000 | loss: 0.9083 | ds_loss: 0.0000 | lr: 2.0487e-05 | scale: 32768.0000 | micro time: 1.866 | step time: 0.000
train | epoch   5 | Iter:   7284/ 13000 | global iter:   7284/ 13000 | loss: 1.0734 | ds_loss: 0.0000 | lr: 2.0481e-05 | scale: 32768.0000 | micro time: 1.746 | step time: 0.000
train | epoch   5 | Iter:   7285/ 13000 | global iter:   7285/ 13000 | loss: 0.9975 | ds_loss: 0.0000 | lr: 2.0476e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   5 | Iter:   7286/ 13000 | global iter:   7286/ 13000 | loss: 0.9239 | ds_loss: 0.0000 | lr: 2.0470e-05 | scale: 32768.0000 | micro time: 1.828 | step time: 0.000
train | epoch   5 | Iter:   7287/ 13000 | global iter:   7287/ 13000 | loss: 1.3224 | ds_loss: 0.0000 | lr: 2.0464e-05 | scale: 32768.0000 | micro time: 1.748 | step time: 0.000
train | epoch   5 | Iter:   7288/ 13000 | global iter:   7288/ 13000 | loss: 1.3210 | ds_loss: 0.0000 | lr: 2.0458e-05 | scale: 32768.0000 | micro time: 1.753 | step time: 0.000
train | epoch   5 | Iter:   7289/ 13000 | global iter:   7289/ 13000 | loss: 1.4902 | ds_loss: 0.0000 | lr: 2.0452e-05 | scale: 32768.0000 | micro time: 1.728 | step time: 0.000
train | epoch   5 | Iter:   7290/ 13000 | global iter:   7290/ 13000 | loss: 1.4879 | ds_loss: 0.0000 | lr: 2.0446e-05 | scale: 32768.0000 | micro time: 1.761 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7290/ 13000 | global iter:   7290/ 13000 | loss: 1.1615 | ds_loss: 0.0000 | lr: 2.0446e-05 | scale: 32768.0000 | micro time: 1.761 | step time: 1.786
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7291/ 13000 | global iter:   7291/ 13000 | loss: 0.7862 | ds_loss: 0.0000 | lr: 2.0440e-05 | scale: 32768.0000 | micro time: 1.754 | step time: 0.000
train | epoch   5 | Iter:   7292/ 13000 | global iter:   7292/ 13000 | loss: 1.0829 | ds_loss: 0.0000 | lr: 2.0434e-05 | scale: 32768.0000 | micro time: 1.872 | step time: 0.000
train | epoch   5 | Iter:   7293/ 13000 | global iter:   7293/ 13000 | loss: 1.0858 | ds_loss: 0.0000 | lr: 2.0428e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   5 | Iter:   7294/ 13000 | global iter:   7294/ 13000 | loss: 1.3033 | ds_loss: 0.0000 | lr: 2.0422e-05 | scale: 32768.0000 | micro time: 1.747 | step time: 0.000
train | epoch   5 | Iter:   7295/ 13000 | global iter:   7295/ 13000 | loss: 0.7926 | ds_loss: 0.0000 | lr: 2.0416e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   5 | Iter:   7296/ 13000 | global iter:   7296/ 13000 | loss: 1.7998 | ds_loss: 0.0000 | lr: 2.0410e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   5 | Iter:   7297/ 13000 | global iter:   7297/ 13000 | loss: 0.4092 | ds_loss: 0.0000 | lr: 2.0404e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   5 | Iter:   7298/ 13000 | global iter:   7298/ 13000 | loss: 0.6534 | ds_loss: 0.0000 | lr: 2.0399e-05 | scale: 32768.0000 | micro time: 1.797 | step time: 0.000
train | epoch   5 | Iter:   7299/ 13000 | global iter:   7299/ 13000 | loss: 0.3593 | ds_loss: 0.0000 | lr: 2.0393e-05 | scale: 32768.0000 | micro time: 1.816 | step time: 0.000
train | epoch   5 | Iter:   7300/ 13000 | global iter:   7300/ 13000 | loss: 0.2027 | ds_loss: 0.0000 | lr: 2.0387e-05 | scale: 32768.0000 | micro time: 1.878 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7300/ 13000 | global iter:   7300/ 13000 | loss: 0.8475 | ds_loss: 0.0000 | lr: 2.0387e-05 | scale: 32768.0000 | micro time: 1.878 | step time: 1.806
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7301/ 13000 | global iter:   7301/ 13000 | loss: 0.9789 | ds_loss: 0.0000 | lr: 2.0381e-05 | scale: 32768.0000 | micro time: 1.874 | step time: 0.000
train | epoch   5 | Iter:   7302/ 13000 | global iter:   7302/ 13000 | loss: 0.9514 | ds_loss: 0.0000 | lr: 2.0375e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   5 | Iter:   7303/ 13000 | global iter:   7303/ 13000 | loss: 1.0353 | ds_loss: 0.0000 | lr: 2.0369e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   5 | Iter:   7304/ 13000 | global iter:   7304/ 13000 | loss: 0.8844 | ds_loss: 0.0000 | lr: 2.0363e-05 | scale: 32768.0000 | micro time: 1.879 | step time: 0.000
train | epoch   5 | Iter:   7305/ 13000 | global iter:   7305/ 13000 | loss: 1.6050 | ds_loss: 0.0000 | lr: 2.0357e-05 | scale: 32768.0000 | micro time: 1.830 | step time: 0.000
train | epoch   5 | Iter:   7306/ 13000 | global iter:   7306/ 13000 | loss: 0.9957 | ds_loss: 0.0000 | lr: 2.0351e-05 | scale: 32768.0000 | micro time: 1.848 | step time: 0.000
train | epoch   5 | Iter:   7307/ 13000 | global iter:   7307/ 13000 | loss: 0.7575 | ds_loss: 0.0000 | lr: 2.0345e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   5 | Iter:   7308/ 13000 | global iter:   7308/ 13000 | loss: 1.4612 | ds_loss: 0.0000 | lr: 2.0339e-05 | scale: 32768.0000 | micro time: 1.813 | step time: 0.000
train | epoch   5 | Iter:   7309/ 13000 | global iter:   7309/ 13000 | loss: 1.0406 | ds_loss: 0.0000 | lr: 2.0333e-05 | scale: 32768.0000 | micro time: 1.840 | step time: 0.000
train | epoch   5 | Iter:   7310/ 13000 | global iter:   7310/ 13000 | loss: 1.2540 | ds_loss: 0.0000 | lr: 2.0327e-05 | scale: 32768.0000 | micro time: 1.896 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7310/ 13000 | global iter:   7310/ 13000 | loss: 1.0964 | ds_loss: 0.0000 | lr: 2.0327e-05 | scale: 32768.0000 | micro time: 1.896 | step time: 1.849
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7311/ 13000 | global iter:   7311/ 13000 | loss: 1.4432 | ds_loss: 0.0000 | lr: 2.0322e-05 | scale: 32768.0000 | micro time: 1.780 | step time: 0.000
train | epoch   5 | Iter:   7312/ 13000 | global iter:   7312/ 13000 | loss: 1.2704 | ds_loss: 0.0000 | lr: 2.0316e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   5 | Iter:   7313/ 13000 | global iter:   7313/ 13000 | loss: 0.9897 | ds_loss: 0.0000 | lr: 2.0310e-05 | scale: 32768.0000 | micro time: 1.738 | step time: 0.000
train | epoch   5 | Iter:   7314/ 13000 | global iter:   7314/ 13000 | loss: 1.6069 | ds_loss: 0.0000 | lr: 2.0304e-05 | scale: 32768.0000 | micro time: 1.806 | step time: 0.000
train | epoch   5 | Iter:   7315/ 13000 | global iter:   7315/ 13000 | loss: 1.2343 | ds_loss: 0.0000 | lr: 2.0298e-05 | scale: 32768.0000 | micro time: 1.852 | step time: 0.000
train | epoch   5 | Iter:   7316/ 13000 | global iter:   7316/ 13000 | loss: 1.2425 | ds_loss: 0.0000 | lr: 2.0292e-05 | scale: 32768.0000 | micro time: 1.752 | step time: 0.000
train | epoch   5 | Iter:   7317/ 13000 | global iter:   7317/ 13000 | loss: 1.3381 | ds_loss: 0.0000 | lr: 2.0286e-05 | scale: 32768.0000 | micro time: 1.846 | step time: 0.000
train | epoch   5 | Iter:   7318/ 13000 | global iter:   7318/ 13000 | loss: 0.9201 | ds_loss: 0.0000 | lr: 2.0280e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   5 | Iter:   7319/ 13000 | global iter:   7319/ 13000 | loss: 1.1552 | ds_loss: 0.0000 | lr: 2.0274e-05 | scale: 32768.0000 | micro time: 1.798 | step time: 0.000
train | epoch   5 | Iter:   7320/ 13000 | global iter:   7320/ 13000 | loss: 0.8742 | ds_loss: 0.0000 | lr: 2.0268e-05 | scale: 32768.0000 | micro time: 1.876 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7320/ 13000 | global iter:   7320/ 13000 | loss: 1.2075 | ds_loss: 0.0000 | lr: 2.0268e-05 | scale: 32768.0000 | micro time: 1.876 | step time: 1.815
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7321/ 13000 | global iter:   7321/ 13000 | loss: 1.0812 | ds_loss: 0.0000 | lr: 2.0262e-05 | scale: 32768.0000 | micro time: 1.681 | step time: 0.000
train | epoch   5 | Iter:   7322/ 13000 | global iter:   7322/ 13000 | loss: 1.2870 | ds_loss: 0.0000 | lr: 2.0256e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   5 | Iter:   7323/ 13000 | global iter:   7323/ 13000 | loss: 0.8056 | ds_loss: 0.0000 | lr: 2.0251e-05 | scale: 32768.0000 | micro time: 1.886 | step time: 0.000
train | epoch   5 | Iter:   7324/ 13000 | global iter:   7324/ 13000 | loss: 0.9781 | ds_loss: 0.0000 | lr: 2.0245e-05 | scale: 32768.0000 | micro time: 1.924 | step time: 0.000
train | epoch   5 | Iter:   7325/ 13000 | global iter:   7325/ 13000 | loss: 1.2013 | ds_loss: 0.0000 | lr: 2.0239e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   5 | Iter:   7326/ 13000 | global iter:   7326/ 13000 | loss: 1.1542 | ds_loss: 0.0000 | lr: 2.0233e-05 | scale: 32768.0000 | micro time: 1.887 | step time: 0.000
train | epoch   5 | Iter:   7327/ 13000 | global iter:   7327/ 13000 | loss: 1.2268 | ds_loss: 0.0000 | lr: 2.0227e-05 | scale: 32768.0000 | micro time: 1.895 | step time: 0.000
train | epoch   5 | Iter:   7328/ 13000 | global iter:   7328/ 13000 | loss: 1.0410 | ds_loss: 0.0000 | lr: 2.0221e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   5 | Iter:   7329/ 13000 | global iter:   7329/ 13000 | loss: 1.3125 | ds_loss: 0.0000 | lr: 2.0215e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   5 | Iter:   7330/ 13000 | global iter:   7330/ 13000 | loss: 0.5521 | ds_loss: 0.0000 | lr: 2.0209e-05 | scale: 32768.0000 | micro time: 1.703 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7330/ 13000 | global iter:   7330/ 13000 | loss: 1.0640 | ds_loss: 0.0000 | lr: 2.0209e-05 | scale: 32768.0000 | micro time: 1.703 | step time: 1.829
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7331/ 13000 | global iter:   7331/ 13000 | loss: 1.1959 | ds_loss: 0.0000 | lr: 2.0203e-05 | scale: 32768.0000 | micro time: 1.784 | step time: 0.000
train | epoch   5 | Iter:   7332/ 13000 | global iter:   7332/ 13000 | loss: 1.3067 | ds_loss: 0.0000 | lr: 2.0197e-05 | scale: 32768.0000 | micro time: 1.854 | step time: 0.000
train | epoch   5 | Iter:   7333/ 13000 | global iter:   7333/ 13000 | loss: 1.0886 | ds_loss: 0.0000 | lr: 2.0191e-05 | scale: 32768.0000 | micro time: 1.744 | step time: 0.000
train | epoch   5 | Iter:   7334/ 13000 | global iter:   7334/ 13000 | loss: 1.0448 | ds_loss: 0.0000 | lr: 2.0185e-05 | scale: 32768.0000 | micro time: 1.796 | step time: 0.000
train | epoch   5 | Iter:   7335/ 13000 | global iter:   7335/ 13000 | loss: 1.2967 | ds_loss: 0.0000 | lr: 2.0180e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   5 | Iter:   7336/ 13000 | global iter:   7336/ 13000 | loss: 1.4054 | ds_loss: 0.0000 | lr: 2.0174e-05 | scale: 32768.0000 | micro time: 1.805 | step time: 0.000
train | epoch   5 | Iter:   7337/ 13000 | global iter:   7337/ 13000 | loss: 1.4405 | ds_loss: 0.0000 | lr: 2.0168e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   5 | Iter:   7338/ 13000 | global iter:   7338/ 13000 | loss: 1.1989 | ds_loss: 0.0000 | lr: 2.0162e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 0.000
train | epoch   5 | Iter:   7339/ 13000 | global iter:   7339/ 13000 | loss: 0.8036 | ds_loss: 0.0000 | lr: 2.0156e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   5 | Iter:   7340/ 13000 | global iter:   7340/ 13000 | loss: 1.1276 | ds_loss: 0.0000 | lr: 2.0150e-05 | scale: 32768.0000 | micro time: 1.727 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7340/ 13000 | global iter:   7340/ 13000 | loss: 1.1909 | ds_loss: 0.0000 | lr: 2.0150e-05 | scale: 32768.0000 | micro time: 1.727 | step time: 1.806
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7341/ 13000 | global iter:   7341/ 13000 | loss: 1.3135 | ds_loss: 0.0000 | lr: 2.0144e-05 | scale: 32768.0000 | micro time: 1.748 | step time: 0.000
train | epoch   5 | Iter:   7342/ 13000 | global iter:   7342/ 13000 | loss: 1.1946 | ds_loss: 0.0000 | lr: 2.0138e-05 | scale: 32768.0000 | micro time: 1.794 | step time: 0.000
train | epoch   5 | Iter:   7343/ 13000 | global iter:   7343/ 13000 | loss: 1.1301 | ds_loss: 0.0000 | lr: 2.0132e-05 | scale: 32768.0000 | micro time: 1.806 | step time: 0.000
train | epoch   5 | Iter:   7344/ 13000 | global iter:   7344/ 13000 | loss: 0.8250 | ds_loss: 0.0000 | lr: 2.0126e-05 | scale: 32768.0000 | micro time: 1.806 | step time: 0.000
train | epoch   5 | Iter:   7345/ 13000 | global iter:   7345/ 13000 | loss: 0.7033 | ds_loss: 0.0000 | lr: 2.0120e-05 | scale: 32768.0000 | micro time: 1.753 | step time: 0.000
train | epoch   5 | Iter:   7346/ 13000 | global iter:   7346/ 13000 | loss: 1.0250 | ds_loss: 0.0000 | lr: 2.0115e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   5 | Iter:   7347/ 13000 | global iter:   7347/ 13000 | loss: 1.3120 | ds_loss: 0.0000 | lr: 2.0109e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   5 | Iter:   7348/ 13000 | global iter:   7348/ 13000 | loss: 1.5221 | ds_loss: 0.0000 | lr: 2.0103e-05 | scale: 32768.0000 | micro time: 1.765 | step time: 0.000
train | epoch   5 | Iter:   7349/ 13000 | global iter:   7349/ 13000 | loss: 1.5416 | ds_loss: 0.0000 | lr: 2.0097e-05 | scale: 32768.0000 | micro time: 1.774 | step time: 0.000
train | epoch   5 | Iter:   7350/ 13000 | global iter:   7350/ 13000 | loss: 0.7793 | ds_loss: 0.0000 | lr: 2.0091e-05 | scale: 32768.0000 | micro time: 1.742 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7350/ 13000 | global iter:   7350/ 13000 | loss: 1.1346 | ds_loss: 0.0000 | lr: 2.0091e-05 | scale: 32768.0000 | micro time: 1.742 | step time: 1.778
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7351/ 13000 | global iter:   7351/ 13000 | loss: 1.2415 | ds_loss: 0.0000 | lr: 2.0085e-05 | scale: 32768.0000 | micro time: 1.842 | step time: 0.000
train | epoch   5 | Iter:   7352/ 13000 | global iter:   7352/ 13000 | loss: 1.2338 | ds_loss: 0.0000 | lr: 2.0079e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   5 | Iter:   7353/ 13000 | global iter:   7353/ 13000 | loss: 0.8934 | ds_loss: 0.0000 | lr: 2.0073e-05 | scale: 32768.0000 | micro time: 1.755 | step time: 0.000
train | epoch   5 | Iter:   7354/ 13000 | global iter:   7354/ 13000 | loss: 0.9668 | ds_loss: 0.0000 | lr: 2.0067e-05 | scale: 32768.0000 | micro time: 1.792 | step time: 0.000
train | epoch   5 | Iter:   7355/ 13000 | global iter:   7355/ 13000 | loss: 1.2088 | ds_loss: 0.0000 | lr: 2.0061e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   5 | Iter:   7356/ 13000 | global iter:   7356/ 13000 | loss: 0.4244 | ds_loss: 0.0000 | lr: 2.0055e-05 | scale: 32768.0000 | micro time: 1.828 | step time: 0.000
train | epoch   5 | Iter:   7357/ 13000 | global iter:   7357/ 13000 | loss: 1.5910 | ds_loss: 0.0000 | lr: 2.0050e-05 | scale: 32768.0000 | micro time: 1.870 | step time: 0.000
train | epoch   5 | Iter:   7358/ 13000 | global iter:   7358/ 13000 | loss: 1.3223 | ds_loss: 0.0000 | lr: 2.0044e-05 | scale: 32768.0000 | micro time: 1.955 | step time: 0.000
train | epoch   5 | Iter:   7359/ 13000 | global iter:   7359/ 13000 | loss: 1.0378 | ds_loss: 0.0000 | lr: 2.0038e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   5 | Iter:   7360/ 13000 | global iter:   7360/ 13000 | loss: 1.2928 | ds_loss: 0.0000 | lr: 2.0032e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7360/ 13000 | global iter:   7360/ 13000 | loss: 1.1213 | ds_loss: 0.0000 | lr: 2.0032e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 1.835
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7361/ 13000 | global iter:   7361/ 13000 | loss: 0.8129 | ds_loss: 0.0000 | lr: 2.0026e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   5 | Iter:   7362/ 13000 | global iter:   7362/ 13000 | loss: 1.3929 | ds_loss: 0.0000 | lr: 2.0020e-05 | scale: 32768.0000 | micro time: 1.901 | step time: 0.000
train | epoch   5 | Iter:   7363/ 13000 | global iter:   7363/ 13000 | loss: 1.4145 | ds_loss: 0.0000 | lr: 2.0014e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   5 | Iter:   7364/ 13000 | global iter:   7364/ 13000 | loss: 1.1663 | ds_loss: 0.0000 | lr: 2.0008e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   5 | Iter:   7365/ 13000 | global iter:   7365/ 13000 | loss: 1.3630 | ds_loss: 0.0000 | lr: 2.0002e-05 | scale: 32768.0000 | micro time: 1.828 | step time: 0.000
train | epoch   5 | Iter:   7366/ 13000 | global iter:   7366/ 13000 | loss: 0.9600 | ds_loss: 0.0000 | lr: 1.9996e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   5 | Iter:   7367/ 13000 | global iter:   7367/ 13000 | loss: 1.4307 | ds_loss: 0.0000 | lr: 1.9990e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   5 | Iter:   7368/ 13000 | global iter:   7368/ 13000 | loss: 1.3388 | ds_loss: 0.0000 | lr: 1.9985e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   5 | Iter:   7369/ 13000 | global iter:   7369/ 13000 | loss: 0.9382 | ds_loss: 0.0000 | lr: 1.9979e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   5 | Iter:   7370/ 13000 | global iter:   7370/ 13000 | loss: 0.7646 | ds_loss: 0.0000 | lr: 1.9973e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7370/ 13000 | global iter:   7370/ 13000 | loss: 1.1582 | ds_loss: 0.0000 | lr: 1.9973e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 1.824
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7371/ 13000 | global iter:   7371/ 13000 | loss: 1.2159 | ds_loss: 0.0000 | lr: 1.9967e-05 | scale: 32768.0000 | micro time: 1.754 | step time: 0.000
train | epoch   5 | Iter:   7372/ 13000 | global iter:   7372/ 13000 | loss: 1.2814 | ds_loss: 0.0000 | lr: 1.9961e-05 | scale: 32768.0000 | micro time: 1.766 | step time: 0.000
train | epoch   5 | Iter:   7373/ 13000 | global iter:   7373/ 13000 | loss: 1.4112 | ds_loss: 0.0000 | lr: 1.9955e-05 | scale: 32768.0000 | micro time: 1.822 | step time: 0.000
train | epoch   5 | Iter:   7374/ 13000 | global iter:   7374/ 13000 | loss: 1.2599 | ds_loss: 0.0000 | lr: 1.9949e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   5 | Iter:   7375/ 13000 | global iter:   7375/ 13000 | loss: 1.1184 | ds_loss: 0.0000 | lr: 1.9943e-05 | scale: 32768.0000 | micro time: 1.769 | step time: 0.000
train | epoch   5 | Iter:   7376/ 13000 | global iter:   7376/ 13000 | loss: 1.2975 | ds_loss: 0.0000 | lr: 1.9937e-05 | scale: 32768.0000 | micro time: 1.753 | step time: 0.000
train | epoch   5 | Iter:   7377/ 13000 | global iter:   7377/ 13000 | loss: 0.8440 | ds_loss: 0.0000 | lr: 1.9931e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   5 | Iter:   7378/ 13000 | global iter:   7378/ 13000 | loss: 1.0956 | ds_loss: 0.0000 | lr: 1.9926e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   5 | Iter:   7379/ 13000 | global iter:   7379/ 13000 | loss: 1.1934 | ds_loss: 0.0000 | lr: 1.9920e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   5 | Iter:   7380/ 13000 | global iter:   7380/ 13000 | loss: 1.3648 | ds_loss: 0.0000 | lr: 1.9914e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7380/ 13000 | global iter:   7380/ 13000 | loss: 1.2082 | ds_loss: 0.0000 | lr: 1.9914e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 1.799
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7381/ 13000 | global iter:   7381/ 13000 | loss: 1.3373 | ds_loss: 0.0000 | lr: 1.9908e-05 | scale: 32768.0000 | micro time: 1.798 | step time: 0.000
train | epoch   5 | Iter:   7382/ 13000 | global iter:   7382/ 13000 | loss: 1.2726 | ds_loss: 0.0000 | lr: 1.9902e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   5 | Iter:   7383/ 13000 | global iter:   7383/ 13000 | loss: 0.9461 | ds_loss: 0.0000 | lr: 1.9896e-05 | scale: 32768.0000 | micro time: 1.808 | step time: 0.000
train | epoch   5 | Iter:   7384/ 13000 | global iter:   7384/ 13000 | loss: 1.2156 | ds_loss: 0.0000 | lr: 1.9890e-05 | scale: 32768.0000 | micro time: 1.837 | step time: 0.000
train | epoch   5 | Iter:   7385/ 13000 | global iter:   7385/ 13000 | loss: 1.4078 | ds_loss: 0.0000 | lr: 1.9884e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   5 | Iter:   7386/ 13000 | global iter:   7386/ 13000 | loss: 1.0297 | ds_loss: 0.0000 | lr: 1.9878e-05 | scale: 32768.0000 | micro time: 1.903 | step time: 0.000
train | epoch   5 | Iter:   7387/ 13000 | global iter:   7387/ 13000 | loss: 1.1236 | ds_loss: 0.0000 | lr: 1.9872e-05 | scale: 32768.0000 | micro time: 1.792 | step time: 0.000
train | epoch   5 | Iter:   7388/ 13000 | global iter:   7388/ 13000 | loss: 0.7125 | ds_loss: 0.0000 | lr: 1.9867e-05 | scale: 32768.0000 | micro time: 1.834 | step time: 0.000
train | epoch   5 | Iter:   7389/ 13000 | global iter:   7389/ 13000 | loss: 1.2283 | ds_loss: 0.0000 | lr: 1.9861e-05 | scale: 32768.0000 | micro time: 1.822 | step time: 0.000
train | epoch   5 | Iter:   7390/ 13000 | global iter:   7390/ 13000 | loss: 1.2318 | ds_loss: 0.0000 | lr: 1.9855e-05 | scale: 32768.0000 | micro time: 1.777 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7390/ 13000 | global iter:   7390/ 13000 | loss: 1.1505 | ds_loss: 0.0000 | lr: 1.9855e-05 | scale: 32768.0000 | micro time: 1.777 | step time: 1.826
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7391/ 13000 | global iter:   7391/ 13000 | loss: 0.9677 | ds_loss: 0.0000 | lr: 1.9849e-05 | scale: 32768.0000 | micro time: 1.879 | step time: 0.000
train | epoch   5 | Iter:   7392/ 13000 | global iter:   7392/ 13000 | loss: 0.6779 | ds_loss: 0.0000 | lr: 1.9843e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   5 | Iter:   7393/ 13000 | global iter:   7393/ 13000 | loss: 1.4332 | ds_loss: 0.0000 | lr: 1.9837e-05 | scale: 32768.0000 | micro time: 1.751 | step time: 0.000
train | epoch   5 | Iter:   7394/ 13000 | global iter:   7394/ 13000 | loss: 1.3008 | ds_loss: 0.0000 | lr: 1.9831e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   5 | Iter:   7395/ 13000 | global iter:   7395/ 13000 | loss: 1.5037 | ds_loss: 0.0000 | lr: 1.9825e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   5 | Iter:   7396/ 13000 | global iter:   7396/ 13000 | loss: 0.9606 | ds_loss: 0.0000 | lr: 1.9819e-05 | scale: 32768.0000 | micro time: 1.728 | step time: 0.000
train | epoch   5 | Iter:   7397/ 13000 | global iter:   7397/ 13000 | loss: 0.7867 | ds_loss: 0.0000 | lr: 1.9813e-05 | scale: 32768.0000 | micro time: 1.663 | step time: 0.000
train | epoch   5 | Iter:   7398/ 13000 | global iter:   7398/ 13000 | loss: 1.3112 | ds_loss: 0.0000 | lr: 1.9808e-05 | scale: 32768.0000 | micro time: 1.878 | step time: 0.000
train | epoch   5 | Iter:   7399/ 13000 | global iter:   7399/ 13000 | loss: 0.9799 | ds_loss: 0.0000 | lr: 1.9802e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   5 | Iter:   7400/ 13000 | global iter:   7400/ 13000 | loss: 0.9728 | ds_loss: 0.0000 | lr: 1.9796e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7400/ 13000 | global iter:   7400/ 13000 | loss: 1.0895 | ds_loss: 0.0000 | lr: 1.9796e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 1.814
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7401/ 13000 | global iter:   7401/ 13000 | loss: 1.2263 | ds_loss: 0.0000 | lr: 1.9790e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   5 | Iter:   7402/ 13000 | global iter:   7402/ 13000 | loss: 1.4060 | ds_loss: 0.0000 | lr: 1.9784e-05 | scale: 32768.0000 | micro time: 1.833 | step time: 0.000
train | epoch   5 | Iter:   7403/ 13000 | global iter:   7403/ 13000 | loss: 0.9806 | ds_loss: 0.0000 | lr: 1.9778e-05 | scale: 32768.0000 | micro time: 1.699 | step time: 0.000
train | epoch   5 | Iter:   7404/ 13000 | global iter:   7404/ 13000 | loss: 0.8710 | ds_loss: 0.0000 | lr: 1.9772e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   5 | Iter:   7405/ 13000 | global iter:   7405/ 13000 | loss: 0.8380 | ds_loss: 0.0000 | lr: 1.9766e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   5 | Iter:   7406/ 13000 | global iter:   7406/ 13000 | loss: 1.2540 | ds_loss: 0.0000 | lr: 1.9760e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   5 | Iter:   7407/ 13000 | global iter:   7407/ 13000 | loss: 1.4210 | ds_loss: 0.0000 | lr: 1.9755e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   5 | Iter:   7408/ 13000 | global iter:   7408/ 13000 | loss: 0.9536 | ds_loss: 0.0000 | lr: 1.9749e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
train | epoch   5 | Iter:   7409/ 13000 | global iter:   7409/ 13000 | loss: 0.8687 | ds_loss: 0.0000 | lr: 1.9743e-05 | scale: 32768.0000 | micro time: 1.706 | step time: 0.000
train | epoch   5 | Iter:   7410/ 13000 | global iter:   7410/ 13000 | loss: 1.0631 | ds_loss: 0.0000 | lr: 1.9737e-05 | scale: 32768.0000 | micro time: 1.755 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7410/ 13000 | global iter:   7410/ 13000 | loss: 1.0882 | ds_loss: 0.0000 | lr: 1.9737e-05 | scale: 32768.0000 | micro time: 1.755 | step time: 1.786
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7411/ 13000 | global iter:   7411/ 13000 | loss: 0.9875 | ds_loss: 0.0000 | lr: 1.9731e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   5 | Iter:   7412/ 13000 | global iter:   7412/ 13000 | loss: 0.7226 | ds_loss: 0.0000 | lr: 1.9725e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   5 | Iter:   7413/ 13000 | global iter:   7413/ 13000 | loss: 1.0408 | ds_loss: 0.0000 | lr: 1.9719e-05 | scale: 32768.0000 | micro time: 1.860 | step time: 0.000
train | epoch   5 | Iter:   7414/ 13000 | global iter:   7414/ 13000 | loss: 1.0330 | ds_loss: 0.0000 | lr: 1.9713e-05 | scale: 32768.0000 | micro time: 1.842 | step time: 0.000
train | epoch   5 | Iter:   7415/ 13000 | global iter:   7415/ 13000 | loss: 1.1300 | ds_loss: 0.0000 | lr: 1.9707e-05 | scale: 32768.0000 | micro time: 1.879 | step time: 0.000
train | epoch   5 | Iter:   7416/ 13000 | global iter:   7416/ 13000 | loss: 0.7399 | ds_loss: 0.0000 | lr: 1.9702e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 0.000
train | epoch   5 | Iter:   7417/ 13000 | global iter:   7417/ 13000 | loss: 1.2737 | ds_loss: 0.0000 | lr: 1.9696e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   5 | Iter:   7418/ 13000 | global iter:   7418/ 13000 | loss: 1.2600 | ds_loss: 0.0000 | lr: 1.9690e-05 | scale: 32768.0000 | micro time: 1.715 | step time: 0.000
train | epoch   5 | Iter:   7419/ 13000 | global iter:   7419/ 13000 | loss: 1.3395 | ds_loss: 0.0000 | lr: 1.9684e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   5 | Iter:   7420/ 13000 | global iter:   7420/ 13000 | loss: 1.3084 | ds_loss: 0.0000 | lr: 1.9678e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7420/ 13000 | global iter:   7420/ 13000 | loss: 1.0835 | ds_loss: 0.0000 | lr: 1.9678e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 1.824
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7421/ 13000 | global iter:   7421/ 13000 | loss: 0.7510 | ds_loss: 0.0000 | lr: 1.9672e-05 | scale: 32768.0000 | micro time: 1.760 | step time: 0.000
train | epoch   5 | Iter:   7422/ 13000 | global iter:   7422/ 13000 | loss: 1.2150 | ds_loss: 0.0000 | lr: 1.9666e-05 | scale: 32768.0000 | micro time: 1.781 | step time: 0.000
train | epoch   5 | Iter:   7423/ 13000 | global iter:   7423/ 13000 | loss: 1.1435 | ds_loss: 0.0000 | lr: 1.9660e-05 | scale: 32768.0000 | micro time: 1.858 | step time: 0.000
train | epoch   5 | Iter:   7424/ 13000 | global iter:   7424/ 13000 | loss: 1.6725 | ds_loss: 0.0000 | lr: 1.9654e-05 | scale: 32768.0000 | micro time: 1.832 | step time: 0.000
train | epoch   5 | Iter:   7425/ 13000 | global iter:   7425/ 13000 | loss: 1.3037 | ds_loss: 0.0000 | lr: 1.9649e-05 | scale: 32768.0000 | micro time: 1.758 | step time: 0.000
train | epoch   5 | Iter:   7426/ 13000 | global iter:   7426/ 13000 | loss: 0.6650 | ds_loss: 0.0000 | lr: 1.9643e-05 | scale: 32768.0000 | micro time: 1.887 | step time: 0.000
train | epoch   5 | Iter:   7427/ 13000 | global iter:   7427/ 13000 | loss: 0.7757 | ds_loss: 0.0000 | lr: 1.9637e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
train | epoch   5 | Iter:   7428/ 13000 | global iter:   7428/ 13000 | loss: 1.6891 | ds_loss: 0.0000 | lr: 1.9631e-05 | scale: 32768.0000 | micro time: 1.755 | step time: 0.000
train | epoch   5 | Iter:   7429/ 13000 | global iter:   7429/ 13000 | loss: 1.2771 | ds_loss: 0.0000 | lr: 1.9625e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 0.000
train | epoch   5 | Iter:   7430/ 13000 | global iter:   7430/ 13000 | loss: 0.8335 | ds_loss: 0.0000 | lr: 1.9619e-05 | scale: 32768.0000 | micro time: 1.739 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7430/ 13000 | global iter:   7430/ 13000 | loss: 1.1326 | ds_loss: 0.0000 | lr: 1.9619e-05 | scale: 32768.0000 | micro time: 1.739 | step time: 1.800
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7431/ 13000 | global iter:   7431/ 13000 | loss: 0.8777 | ds_loss: 0.0000 | lr: 1.9613e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   5 | Iter:   7432/ 13000 | global iter:   7432/ 13000 | loss: 1.1192 | ds_loss: 0.0000 | lr: 1.9607e-05 | scale: 32768.0000 | micro time: 1.695 | step time: 0.000
train | epoch   5 | Iter:   7433/ 13000 | global iter:   7433/ 13000 | loss: 1.0979 | ds_loss: 0.0000 | lr: 1.9601e-05 | scale: 32768.0000 | micro time: 1.874 | step time: 0.000
train | epoch   5 | Iter:   7434/ 13000 | global iter:   7434/ 13000 | loss: 0.7331 | ds_loss: 0.0000 | lr: 1.9596e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   5 | Iter:   7435/ 13000 | global iter:   7435/ 13000 | loss: 1.5303 | ds_loss: 0.0000 | lr: 1.9590e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   5 | Iter:   7436/ 13000 | global iter:   7436/ 13000 | loss: 1.1686 | ds_loss: 0.0000 | lr: 1.9584e-05 | scale: 32768.0000 | micro time: 1.695 | step time: 0.000
train | epoch   5 | Iter:   7437/ 13000 | global iter:   7437/ 13000 | loss: 1.1042 | ds_loss: 0.0000 | lr: 1.9578e-05 | scale: 32768.0000 | micro time: 1.844 | step time: 0.000
train | epoch   5 | Iter:   7438/ 13000 | global iter:   7438/ 13000 | loss: 0.8753 | ds_loss: 0.0000 | lr: 1.9572e-05 | scale: 32768.0000 | micro time: 1.906 | step time: 0.000
train | epoch   5 | Iter:   7439/ 13000 | global iter:   7439/ 13000 | loss: 1.0686 | ds_loss: 0.0000 | lr: 1.9566e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   5 | Iter:   7440/ 13000 | global iter:   7440/ 13000 | loss: 1.2749 | ds_loss: 0.0000 | lr: 1.9560e-05 | scale: 32768.0000 | micro time: 1.955 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7440/ 13000 | global iter:   7440/ 13000 | loss: 1.0850 | ds_loss: 0.0000 | lr: 1.9560e-05 | scale: 32768.0000 | micro time: 1.955 | step time: 1.814
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7441/ 13000 | global iter:   7441/ 13000 | loss: 0.9182 | ds_loss: 0.0000 | lr: 1.9554e-05 | scale: 32768.0000 | micro time: 1.794 | step time: 0.000
train | epoch   5 | Iter:   7442/ 13000 | global iter:   7442/ 13000 | loss: 1.5333 | ds_loss: 0.0000 | lr: 1.9548e-05 | scale: 32768.0000 | micro time: 1.822 | step time: 0.000
train | epoch   5 | Iter:   7443/ 13000 | global iter:   7443/ 13000 | loss: 1.1811 | ds_loss: 0.0000 | lr: 1.9543e-05 | scale: 32768.0000 | micro time: 1.743 | step time: 0.000
train | epoch   5 | Iter:   7444/ 13000 | global iter:   7444/ 13000 | loss: 0.9701 | ds_loss: 0.0000 | lr: 1.9537e-05 | scale: 32768.0000 | micro time: 1.899 | step time: 0.000
train | epoch   5 | Iter:   7445/ 13000 | global iter:   7445/ 13000 | loss: 1.1492 | ds_loss: 0.0000 | lr: 1.9531e-05 | scale: 32768.0000 | micro time: 1.755 | step time: 0.000
train | epoch   5 | Iter:   7446/ 13000 | global iter:   7446/ 13000 | loss: 0.8907 | ds_loss: 0.0000 | lr: 1.9525e-05 | scale: 32768.0000 | micro time: 1.758 | step time: 0.000
train | epoch   5 | Iter:   7447/ 13000 | global iter:   7447/ 13000 | loss: 1.3788 | ds_loss: 0.0000 | lr: 1.9519e-05 | scale: 32768.0000 | micro time: 1.756 | step time: 0.000
train | epoch   5 | Iter:   7448/ 13000 | global iter:   7448/ 13000 | loss: 1.4399 | ds_loss: 0.0000 | lr: 1.9513e-05 | scale: 32768.0000 | micro time: 1.683 | step time: 0.000
train | epoch   5 | Iter:   7449/ 13000 | global iter:   7449/ 13000 | loss: 1.0315 | ds_loss: 0.0000 | lr: 1.9507e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   5 | Iter:   7450/ 13000 | global iter:   7450/ 13000 | loss: 1.3463 | ds_loss: 0.0000 | lr: 1.9501e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7450/ 13000 | global iter:   7450/ 13000 | loss: 1.1839 | ds_loss: 0.0000 | lr: 1.9501e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 1.783
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7451/ 13000 | global iter:   7451/ 13000 | loss: 1.3669 | ds_loss: 0.0000 | lr: 1.9496e-05 | scale: 32768.0000 | micro time: 1.825 | step time: 0.000
train | epoch   5 | Iter:   7452/ 13000 | global iter:   7452/ 13000 | loss: 1.5142 | ds_loss: 0.0000 | lr: 1.9490e-05 | scale: 32768.0000 | micro time: 1.804 | step time: 0.000
train | epoch   5 | Iter:   7453/ 13000 | global iter:   7453/ 13000 | loss: 1.3807 | ds_loss: 0.0000 | lr: 1.9484e-05 | scale: 32768.0000 | micro time: 1.826 | step time: 0.000
train | epoch   5 | Iter:   7454/ 13000 | global iter:   7454/ 13000 | loss: 1.2093 | ds_loss: 0.0000 | lr: 1.9478e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
train | epoch   5 | Iter:   7455/ 13000 | global iter:   7455/ 13000 | loss: 1.2288 | ds_loss: 0.0000 | lr: 1.9472e-05 | scale: 32768.0000 | micro time: 1.719 | step time: 0.000
train | epoch   5 | Iter:   7456/ 13000 | global iter:   7456/ 13000 | loss: 1.4445 | ds_loss: 0.0000 | lr: 1.9466e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   5 | Iter:   7457/ 13000 | global iter:   7457/ 13000 | loss: 0.9803 | ds_loss: 0.0000 | lr: 1.9460e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   5 | Iter:   7458/ 13000 | global iter:   7458/ 13000 | loss: 1.3732 | ds_loss: 0.0000 | lr: 1.9454e-05 | scale: 32768.0000 | micro time: 1.751 | step time: 0.000
train | epoch   5 | Iter:   7459/ 13000 | global iter:   7459/ 13000 | loss: 0.6589 | ds_loss: 0.0000 | lr: 1.9449e-05 | scale: 32768.0000 | micro time: 1.800 | step time: 0.000
train | epoch   5 | Iter:   7460/ 13000 | global iter:   7460/ 13000 | loss: 1.0481 | ds_loss: 0.0000 | lr: 1.9443e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7460/ 13000 | global iter:   7460/ 13000 | loss: 1.2205 | ds_loss: 0.0000 | lr: 1.9443e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 1.797
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7461/ 13000 | global iter:   7461/ 13000 | loss: 1.3604 | ds_loss: 0.0000 | lr: 1.9437e-05 | scale: 32768.0000 | micro time: 1.829 | step time: 0.000
train | epoch   5 | Iter:   7462/ 13000 | global iter:   7462/ 13000 | loss: 1.2081 | ds_loss: 0.0000 | lr: 1.9431e-05 | scale: 32768.0000 | micro time: 1.879 | step time: 0.000
train | epoch   5 | Iter:   7463/ 13000 | global iter:   7463/ 13000 | loss: 0.4789 | ds_loss: 0.0000 | lr: 1.9425e-05 | scale: 32768.0000 | micro time: 1.747 | step time: 0.000
train | epoch   5 | Iter:   7464/ 13000 | global iter:   7464/ 13000 | loss: 1.5258 | ds_loss: 0.0000 | lr: 1.9419e-05 | scale: 32768.0000 | micro time: 1.846 | step time: 0.000
train | epoch   5 | Iter:   7465/ 13000 | global iter:   7465/ 13000 | loss: 0.8824 | ds_loss: 0.0000 | lr: 1.9413e-05 | scale: 32768.0000 | micro time: 1.832 | step time: 0.000
train | epoch   5 | Iter:   7466/ 13000 | global iter:   7466/ 13000 | loss: 1.1441 | ds_loss: 0.0000 | lr: 1.9407e-05 | scale: 32768.0000 | micro time: 1.853 | step time: 0.000
train | epoch   5 | Iter:   7467/ 13000 | global iter:   7467/ 13000 | loss: 1.1853 | ds_loss: 0.0000 | lr: 1.9402e-05 | scale: 32768.0000 | micro time: 1.781 | step time: 0.000
train | epoch   5 | Iter:   7468/ 13000 | global iter:   7468/ 13000 | loss: 1.1712 | ds_loss: 0.0000 | lr: 1.9396e-05 | scale: 32768.0000 | micro time: 1.795 | step time: 0.000
train | epoch   5 | Iter:   7469/ 13000 | global iter:   7469/ 13000 | loss: 1.1286 | ds_loss: 0.0000 | lr: 1.9390e-05 | scale: 32768.0000 | micro time: 1.899 | step time: 0.000
train | epoch   5 | Iter:   7470/ 13000 | global iter:   7470/ 13000 | loss: 1.2903 | ds_loss: 0.0000 | lr: 1.9384e-05 | scale: 32768.0000 | micro time: 1.840 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7470/ 13000 | global iter:   7470/ 13000 | loss: 1.1375 | ds_loss: 0.0000 | lr: 1.9384e-05 | scale: 32768.0000 | micro time: 1.840 | step time: 1.830
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7471/ 13000 | global iter:   7471/ 13000 | loss: 0.5163 | ds_loss: 0.0000 | lr: 1.9378e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   5 | Iter:   7472/ 13000 | global iter:   7472/ 13000 | loss: 1.1905 | ds_loss: 0.0000 | lr: 1.9372e-05 | scale: 32768.0000 | micro time: 1.813 | step time: 0.000
train | epoch   5 | Iter:   7473/ 13000 | global iter:   7473/ 13000 | loss: 1.2213 | ds_loss: 0.0000 | lr: 1.9366e-05 | scale: 32768.0000 | micro time: 1.846 | step time: 0.000
train | epoch   5 | Iter:   7474/ 13000 | global iter:   7474/ 13000 | loss: 1.2130 | ds_loss: 0.0000 | lr: 1.9360e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   5 | Iter:   7475/ 13000 | global iter:   7475/ 13000 | loss: 0.8086 | ds_loss: 0.0000 | lr: 1.9355e-05 | scale: 32768.0000 | micro time: 1.792 | step time: 0.000
train | epoch   5 | Iter:   7476/ 13000 | global iter:   7476/ 13000 | loss: 1.0118 | ds_loss: 0.0000 | lr: 1.9349e-05 | scale: 32768.0000 | micro time: 1.890 | step time: 0.000
train | epoch   5 | Iter:   7477/ 13000 | global iter:   7477/ 13000 | loss: 1.3817 | ds_loss: 0.0000 | lr: 1.9343e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   5 | Iter:   7478/ 13000 | global iter:   7478/ 13000 | loss: 1.0752 | ds_loss: 0.0000 | lr: 1.9337e-05 | scale: 32768.0000 | micro time: 1.818 | step time: 0.000
train | epoch   5 | Iter:   7479/ 13000 | global iter:   7479/ 13000 | loss: 1.0644 | ds_loss: 0.0000 | lr: 1.9331e-05 | scale: 32768.0000 | micro time: 1.888 | step time: 0.000
train | epoch   5 | Iter:   7480/ 13000 | global iter:   7480/ 13000 | loss: 0.7010 | ds_loss: 0.0000 | lr: 1.9325e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7480/ 13000 | global iter:   7480/ 13000 | loss: 1.0184 | ds_loss: 0.0000 | lr: 1.9325e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 1.824
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7481/ 13000 | global iter:   7481/ 13000 | loss: 1.2788 | ds_loss: 0.0000 | lr: 1.9319e-05 | scale: 32768.0000 | micro time: 1.797 | step time: 0.000
train | epoch   5 | Iter:   7482/ 13000 | global iter:   7482/ 13000 | loss: 0.5128 | ds_loss: 0.0000 | lr: 1.9314e-05 | scale: 32768.0000 | micro time: 1.802 | step time: 0.000
train | epoch   5 | Iter:   7483/ 13000 | global iter:   7483/ 13000 | loss: 1.3487 | ds_loss: 0.0000 | lr: 1.9308e-05 | scale: 32768.0000 | micro time: 1.760 | step time: 0.000
train | epoch   5 | Iter:   7484/ 13000 | global iter:   7484/ 13000 | loss: 1.4028 | ds_loss: 0.0000 | lr: 1.9302e-05 | scale: 32768.0000 | micro time: 1.853 | step time: 0.000
train | epoch   5 | Iter:   7485/ 13000 | global iter:   7485/ 13000 | loss: 1.8867 | ds_loss: 0.0000 | lr: 1.9296e-05 | scale: 32768.0000 | micro time: 1.756 | step time: 0.000
train | epoch   5 | Iter:   7486/ 13000 | global iter:   7486/ 13000 | loss: 1.2216 | ds_loss: 0.0000 | lr: 1.9290e-05 | scale: 32768.0000 | micro time: 1.805 | step time: 0.000
train | epoch   5 | Iter:   7487/ 13000 | global iter:   7487/ 13000 | loss: 0.9767 | ds_loss: 0.0000 | lr: 1.9284e-05 | scale: 32768.0000 | micro time: 1.842 | step time: 0.000
train | epoch   5 | Iter:   7488/ 13000 | global iter:   7488/ 13000 | loss: 0.7868 | ds_loss: 0.0000 | lr: 1.9278e-05 | scale: 32768.0000 | micro time: 1.797 | step time: 0.000
train | epoch   5 | Iter:   7489/ 13000 | global iter:   7489/ 13000 | loss: 1.6547 | ds_loss: 0.0000 | lr: 1.9272e-05 | scale: 32768.0000 | micro time: 1.837 | step time: 0.000
train | epoch   5 | Iter:   7490/ 13000 | global iter:   7490/ 13000 | loss: 1.0215 | ds_loss: 0.0000 | lr: 1.9267e-05 | scale: 32768.0000 | micro time: 1.805 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7490/ 13000 | global iter:   7490/ 13000 | loss: 1.2091 | ds_loss: 0.0000 | lr: 1.9267e-05 | scale: 32768.0000 | micro time: 1.805 | step time: 1.806
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7491/ 13000 | global iter:   7491/ 13000 | loss: 1.0338 | ds_loss: 0.0000 | lr: 1.9261e-05 | scale: 32768.0000 | micro time: 1.802 | step time: 0.000
train | epoch   5 | Iter:   7492/ 13000 | global iter:   7492/ 13000 | loss: 0.6379 | ds_loss: 0.0000 | lr: 1.9255e-05 | scale: 32768.0000 | micro time: 1.808 | step time: 0.000
train | epoch   5 | Iter:   7493/ 13000 | global iter:   7493/ 13000 | loss: 1.8944 | ds_loss: 0.0000 | lr: 1.9249e-05 | scale: 32768.0000 | micro time: 1.838 | step time: 0.000
train | epoch   5 | Iter:   7494/ 13000 | global iter:   7494/ 13000 | loss: 0.9874 | ds_loss: 0.0000 | lr: 1.9243e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 0.000
train | epoch   5 | Iter:   7495/ 13000 | global iter:   7495/ 13000 | loss: 1.1764 | ds_loss: 0.0000 | lr: 1.9237e-05 | scale: 32768.0000 | micro time: 1.872 | step time: 0.000
train | epoch   5 | Iter:   7496/ 13000 | global iter:   7496/ 13000 | loss: 1.0373 | ds_loss: 0.0000 | lr: 1.9231e-05 | scale: 32768.0000 | micro time: 1.726 | step time: 0.000
train | epoch   5 | Iter:   7497/ 13000 | global iter:   7497/ 13000 | loss: 1.3616 | ds_loss: 0.0000 | lr: 1.9226e-05 | scale: 32768.0000 | micro time: 1.687 | step time: 0.000
train | epoch   5 | Iter:   7498/ 13000 | global iter:   7498/ 13000 | loss: 0.9404 | ds_loss: 0.0000 | lr: 1.9220e-05 | scale: 32768.0000 | micro time: 1.784 | step time: 0.000
train | epoch   5 | Iter:   7499/ 13000 | global iter:   7499/ 13000 | loss: 0.9647 | ds_loss: 0.0000 | lr: 1.9214e-05 | scale: 32768.0000 | micro time: 1.754 | step time: 0.000
train | epoch   5 | Iter:   7500/ 13000 | global iter:   7500/ 13000 | loss: 1.6879 | ds_loss: 0.0000 | lr: 1.9208e-05 | scale: 32768.0000 | micro time: 1.874 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7500/ 13000 | global iter:   7500/ 13000 | loss: 1.1722 | ds_loss: 0.0000 | lr: 1.9208e-05 | scale: 32768.0000 | micro time: 1.874 | step time: 1.801
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7501/ 13000 | global iter:   7501/ 13000 | loss: 1.5832 | ds_loss: 0.0000 | lr: 1.9202e-05 | scale: 32768.0000 | micro time: 1.842 | step time: 0.000
train | epoch   5 | Iter:   7502/ 13000 | global iter:   7502/ 13000 | loss: 1.0498 | ds_loss: 0.0000 | lr: 1.9196e-05 | scale: 32768.0000 | micro time: 1.751 | step time: 0.000
train | epoch   5 | Iter:   7503/ 13000 | global iter:   7503/ 13000 | loss: 1.2227 | ds_loss: 0.0000 | lr: 1.9190e-05 | scale: 32768.0000 | micro time: 1.892 | step time: 0.000
train | epoch   5 | Iter:   7504/ 13000 | global iter:   7504/ 13000 | loss: 0.9192 | ds_loss: 0.0000 | lr: 1.9185e-05 | scale: 32768.0000 | micro time: 1.734 | step time: 0.000
train | epoch   5 | Iter:   7505/ 13000 | global iter:   7505/ 13000 | loss: 1.7676 | ds_loss: 0.0000 | lr: 1.9179e-05 | scale: 32768.0000 | micro time: 1.784 | step time: 0.000
train | epoch   5 | Iter:   7506/ 13000 | global iter:   7506/ 13000 | loss: 0.7598 | ds_loss: 0.0000 | lr: 1.9173e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   5 | Iter:   7507/ 13000 | global iter:   7507/ 13000 | loss: 1.7713 | ds_loss: 0.0000 | lr: 1.9167e-05 | scale: 32768.0000 | micro time: 1.895 | step time: 0.000
train | epoch   5 | Iter:   7508/ 13000 | global iter:   7508/ 13000 | loss: 1.0937 | ds_loss: 0.0000 | lr: 1.9161e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
train | epoch   5 | Iter:   7509/ 13000 | global iter:   7509/ 13000 | loss: 1.5556 | ds_loss: 0.0000 | lr: 1.9155e-05 | scale: 32768.0000 | micro time: 1.821 | step time: 0.000
train | epoch   5 | Iter:   7510/ 13000 | global iter:   7510/ 13000 | loss: 0.8959 | ds_loss: 0.0000 | lr: 1.9149e-05 | scale: 32768.0000 | micro time: 1.805 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7510/ 13000 | global iter:   7510/ 13000 | loss: 1.2619 | ds_loss: 0.0000 | lr: 1.9149e-05 | scale: 32768.0000 | micro time: 1.805 | step time: 1.809
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7511/ 13000 | global iter:   7511/ 13000 | loss: 1.1007 | ds_loss: 0.0000 | lr: 1.9143e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   5 | Iter:   7512/ 13000 | global iter:   7512/ 13000 | loss: 0.9405 | ds_loss: 0.0000 | lr: 1.9138e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   5 | Iter:   7513/ 13000 | global iter:   7513/ 13000 | loss: 0.9472 | ds_loss: 0.0000 | lr: 1.9132e-05 | scale: 32768.0000 | micro time: 1.914 | step time: 0.000
train | epoch   5 | Iter:   7514/ 13000 | global iter:   7514/ 13000 | loss: 0.9599 | ds_loss: 0.0000 | lr: 1.9126e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
train | epoch   5 | Iter:   7515/ 13000 | global iter:   7515/ 13000 | loss: 1.1970 | ds_loss: 0.0000 | lr: 1.9120e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   5 | Iter:   7516/ 13000 | global iter:   7516/ 13000 | loss: 0.8943 | ds_loss: 0.0000 | lr: 1.9114e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   5 | Iter:   7517/ 13000 | global iter:   7517/ 13000 | loss: 1.1256 | ds_loss: 0.0000 | lr: 1.9108e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   5 | Iter:   7518/ 13000 | global iter:   7518/ 13000 | loss: 1.4085 | ds_loss: 0.0000 | lr: 1.9102e-05 | scale: 32768.0000 | micro time: 1.749 | step time: 0.000
train | epoch   5 | Iter:   7519/ 13000 | global iter:   7519/ 13000 | loss: 1.0715 | ds_loss: 0.0000 | lr: 1.9097e-05 | scale: 32768.0000 | micro time: 1.762 | step time: 0.000
train | epoch   5 | Iter:   7520/ 13000 | global iter:   7520/ 13000 | loss: 1.0993 | ds_loss: 0.0000 | lr: 1.9091e-05 | scale: 32768.0000 | micro time: 1.786 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7520/ 13000 | global iter:   7520/ 13000 | loss: 1.0744 | ds_loss: 0.0000 | lr: 1.9091e-05 | scale: 32768.0000 | micro time: 1.786 | step time: 1.795
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7521/ 13000 | global iter:   7521/ 13000 | loss: 0.9332 | ds_loss: 0.0000 | lr: 1.9085e-05 | scale: 32768.0000 | micro time: 1.750 | step time: 0.000
train | epoch   5 | Iter:   7522/ 13000 | global iter:   7522/ 13000 | loss: 1.0749 | ds_loss: 0.0000 | lr: 1.9079e-05 | scale: 32768.0000 | micro time: 1.878 | step time: 0.000
train | epoch   5 | Iter:   7523/ 13000 | global iter:   7523/ 13000 | loss: 0.7608 | ds_loss: 0.0000 | lr: 1.9073e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   5 | Iter:   7524/ 13000 | global iter:   7524/ 13000 | loss: 0.6853 | ds_loss: 0.0000 | lr: 1.9067e-05 | scale: 32768.0000 | micro time: 1.782 | step time: 0.000
train | epoch   5 | Iter:   7525/ 13000 | global iter:   7525/ 13000 | loss: 1.0881 | ds_loss: 0.0000 | lr: 1.9062e-05 | scale: 32768.0000 | micro time: 1.796 | step time: 0.000
train | epoch   5 | Iter:   7526/ 13000 | global iter:   7526/ 13000 | loss: 1.3077 | ds_loss: 0.0000 | lr: 1.9056e-05 | scale: 32768.0000 | micro time: 1.778 | step time: 0.000
train | epoch   5 | Iter:   7527/ 13000 | global iter:   7527/ 13000 | loss: 1.5348 | ds_loss: 0.0000 | lr: 1.9050e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   5 | Iter:   7528/ 13000 | global iter:   7528/ 13000 | loss: 1.1737 | ds_loss: 0.0000 | lr: 1.9044e-05 | scale: 32768.0000 | micro time: 1.887 | step time: 0.000
train | epoch   5 | Iter:   7529/ 13000 | global iter:   7529/ 13000 | loss: 1.0999 | ds_loss: 0.0000 | lr: 1.9038e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   5 | Iter:   7530/ 13000 | global iter:   7530/ 13000 | loss: 1.1535 | ds_loss: 0.0000 | lr: 1.9032e-05 | scale: 32768.0000 | micro time: 1.731 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7530/ 13000 | global iter:   7530/ 13000 | loss: 1.0812 | ds_loss: 0.0000 | lr: 1.9032e-05 | scale: 32768.0000 | micro time: 1.731 | step time: 1.813
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7531/ 13000 | global iter:   7531/ 13000 | loss: 1.3482 | ds_loss: 0.0000 | lr: 1.9026e-05 | scale: 32768.0000 | micro time: 1.833 | step time: 0.000
train | epoch   5 | Iter:   7532/ 13000 | global iter:   7532/ 13000 | loss: 1.2680 | ds_loss: 0.0000 | lr: 1.9021e-05 | scale: 32768.0000 | micro time: 1.773 | step time: 0.000
train | epoch   5 | Iter:   7533/ 13000 | global iter:   7533/ 13000 | loss: 1.0194 | ds_loss: 0.0000 | lr: 1.9015e-05 | scale: 32768.0000 | micro time: 1.801 | step time: 0.000
train | epoch   5 | Iter:   7534/ 13000 | global iter:   7534/ 13000 | loss: 1.2672 | ds_loss: 0.0000 | lr: 1.9009e-05 | scale: 32768.0000 | micro time: 1.813 | step time: 0.000
train | epoch   5 | Iter:   7535/ 13000 | global iter:   7535/ 13000 | loss: 0.9289 | ds_loss: 0.0000 | lr: 1.9003e-05 | scale: 32768.0000 | micro time: 1.719 | step time: 0.000
train | epoch   5 | Iter:   7536/ 13000 | global iter:   7536/ 13000 | loss: 1.1980 | ds_loss: 0.0000 | lr: 1.8997e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   5 | Iter:   7537/ 13000 | global iter:   7537/ 13000 | loss: 1.0133 | ds_loss: 0.0000 | lr: 1.8991e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   5 | Iter:   7538/ 13000 | global iter:   7538/ 13000 | loss: 1.7358 | ds_loss: 0.0000 | lr: 1.8985e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   5 | Iter:   7539/ 13000 | global iter:   7539/ 13000 | loss: 1.0402 | ds_loss: 0.0000 | lr: 1.8980e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   5 | Iter:   7540/ 13000 | global iter:   7540/ 13000 | loss: 1.0981 | ds_loss: 0.0000 | lr: 1.8974e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7540/ 13000 | global iter:   7540/ 13000 | loss: 1.1917 | ds_loss: 0.0000 | lr: 1.8974e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 1.805
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7541/ 13000 | global iter:   7541/ 13000 | loss: 0.5049 | ds_loss: 0.0000 | lr: 1.8968e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   5 | Iter:   7542/ 13000 | global iter:   7542/ 13000 | loss: 1.1682 | ds_loss: 0.0000 | lr: 1.8962e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   5 | Iter:   7543/ 13000 | global iter:   7543/ 13000 | loss: 1.6448 | ds_loss: 0.0000 | lr: 1.8956e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   5 | Iter:   7544/ 13000 | global iter:   7544/ 13000 | loss: 1.1883 | ds_loss: 0.0000 | lr: 1.8950e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   5 | Iter:   7545/ 13000 | global iter:   7545/ 13000 | loss: 1.6078 | ds_loss: 0.0000 | lr: 1.8945e-05 | scale: 32768.0000 | micro time: 1.788 | step time: 0.000
train | epoch   5 | Iter:   7546/ 13000 | global iter:   7546/ 13000 | loss: 0.5031 | ds_loss: 0.0000 | lr: 1.8939e-05 | scale: 32768.0000 | micro time: 1.790 | step time: 0.000
train | epoch   5 | Iter:   7547/ 13000 | global iter:   7547/ 13000 | loss: 1.1422 | ds_loss: 0.0000 | lr: 1.8933e-05 | scale: 32768.0000 | micro time: 1.856 | step time: 0.000
train | epoch   5 | Iter:   7548/ 13000 | global iter:   7548/ 13000 | loss: 1.1956 | ds_loss: 0.0000 | lr: 1.8927e-05 | scale: 32768.0000 | micro time: 1.746 | step time: 0.000
train | epoch   5 | Iter:   7549/ 13000 | global iter:   7549/ 13000 | loss: 0.9864 | ds_loss: 0.0000 | lr: 1.8921e-05 | scale: 32768.0000 | micro time: 1.891 | step time: 0.000
train | epoch   5 | Iter:   7550/ 13000 | global iter:   7550/ 13000 | loss: 1.0010 | ds_loss: 0.0000 | lr: 1.8915e-05 | scale: 32768.0000 | micro time: 1.731 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7550/ 13000 | global iter:   7550/ 13000 | loss: 1.0942 | ds_loss: 0.0000 | lr: 1.8915e-05 | scale: 32768.0000 | micro time: 1.731 | step time: 1.805
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7551/ 13000 | global iter:   7551/ 13000 | loss: 0.8424 | ds_loss: 0.0000 | lr: 1.8909e-05 | scale: 32768.0000 | micro time: 1.810 | step time: 0.000
train | epoch   5 | Iter:   7552/ 13000 | global iter:   7552/ 13000 | loss: 1.1815 | ds_loss: 0.0000 | lr: 1.8904e-05 | scale: 32768.0000 | micro time: 1.896 | step time: 0.000
train | epoch   5 | Iter:   7553/ 13000 | global iter:   7553/ 13000 | loss: 1.0060 | ds_loss: 0.0000 | lr: 1.8898e-05 | scale: 32768.0000 | micro time: 1.826 | step time: 0.000
train | epoch   5 | Iter:   7554/ 13000 | global iter:   7554/ 13000 | loss: 1.2345 | ds_loss: 0.0000 | lr: 1.8892e-05 | scale: 32768.0000 | micro time: 1.912 | step time: 0.000
train | epoch   5 | Iter:   7555/ 13000 | global iter:   7555/ 13000 | loss: 1.0658 | ds_loss: 0.0000 | lr: 1.8886e-05 | scale: 32768.0000 | micro time: 1.730 | step time: 0.000
train | epoch   5 | Iter:   7556/ 13000 | global iter:   7556/ 13000 | loss: 1.1234 | ds_loss: 0.0000 | lr: 1.8880e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   5 | Iter:   7557/ 13000 | global iter:   7557/ 13000 | loss: 1.6711 | ds_loss: 0.0000 | lr: 1.8874e-05 | scale: 32768.0000 | micro time: 1.899 | step time: 0.000
train | epoch   5 | Iter:   7558/ 13000 | global iter:   7558/ 13000 | loss: 1.3470 | ds_loss: 0.0000 | lr: 1.8869e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   5 | Iter:   7559/ 13000 | global iter:   7559/ 13000 | loss: 1.1351 | ds_loss: 0.0000 | lr: 1.8863e-05 | scale: 32768.0000 | micro time: 1.790 | step time: 0.000
train | epoch   5 | Iter:   7560/ 13000 | global iter:   7560/ 13000 | loss: 0.9248 | ds_loss: 0.0000 | lr: 1.8857e-05 | scale: 32768.0000 | micro time: 1.845 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7560/ 13000 | global iter:   7560/ 13000 | loss: 1.1532 | ds_loss: 0.0000 | lr: 1.8857e-05 | scale: 32768.0000 | micro time: 1.845 | step time: 1.835
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7561/ 13000 | global iter:   7561/ 13000 | loss: 1.1837 | ds_loss: 0.0000 | lr: 1.8851e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   5 | Iter:   7562/ 13000 | global iter:   7562/ 13000 | loss: 1.5366 | ds_loss: 0.0000 | lr: 1.8845e-05 | scale: 32768.0000 | micro time: 1.743 | step time: 0.000
train | epoch   5 | Iter:   7563/ 13000 | global iter:   7563/ 13000 | loss: 0.8181 | ds_loss: 0.0000 | lr: 1.8839e-05 | scale: 32768.0000 | micro time: 1.844 | step time: 0.000
train | epoch   5 | Iter:   7564/ 13000 | global iter:   7564/ 13000 | loss: 0.8000 | ds_loss: 0.0000 | lr: 1.8834e-05 | scale: 32768.0000 | micro time: 1.861 | step time: 0.000
train | epoch   5 | Iter:   7565/ 13000 | global iter:   7565/ 13000 | loss: 1.3639 | ds_loss: 0.0000 | lr: 1.8828e-05 | scale: 32768.0000 | micro time: 1.796 | step time: 0.000
train | epoch   5 | Iter:   7566/ 13000 | global iter:   7566/ 13000 | loss: 0.8013 | ds_loss: 0.0000 | lr: 1.8822e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   5 | Iter:   7567/ 13000 | global iter:   7567/ 13000 | loss: 0.9485 | ds_loss: 0.0000 | lr: 1.8816e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   5 | Iter:   7568/ 13000 | global iter:   7568/ 13000 | loss: 1.5409 | ds_loss: 0.0000 | lr: 1.8810e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   5 | Iter:   7569/ 13000 | global iter:   7569/ 13000 | loss: 1.0062 | ds_loss: 0.0000 | lr: 1.8804e-05 | scale: 32768.0000 | micro time: 1.808 | step time: 0.000
train | epoch   5 | Iter:   7570/ 13000 | global iter:   7570/ 13000 | loss: 1.2119 | ds_loss: 0.0000 | lr: 1.8798e-05 | scale: 32768.0000 | micro time: 1.834 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7570/ 13000 | global iter:   7570/ 13000 | loss: 1.1211 | ds_loss: 0.0000 | lr: 1.8798e-05 | scale: 32768.0000 | micro time: 1.834 | step time: 1.817
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7571/ 13000 | global iter:   7571/ 13000 | loss: 1.2616 | ds_loss: 0.0000 | lr: 1.8793e-05 | scale: 32768.0000 | micro time: 1.790 | step time: 0.000
train | epoch   5 | Iter:   7572/ 13000 | global iter:   7572/ 13000 | loss: 1.2046 | ds_loss: 0.0000 | lr: 1.8787e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   5 | Iter:   7573/ 13000 | global iter:   7573/ 13000 | loss: 0.9820 | ds_loss: 0.0000 | lr: 1.8781e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   5 | Iter:   7574/ 13000 | global iter:   7574/ 13000 | loss: 1.4094 | ds_loss: 0.0000 | lr: 1.8775e-05 | scale: 32768.0000 | micro time: 1.796 | step time: 0.000
train | epoch   5 | Iter:   7575/ 13000 | global iter:   7575/ 13000 | loss: 1.2432 | ds_loss: 0.0000 | lr: 1.8769e-05 | scale: 32768.0000 | micro time: 1.837 | step time: 0.000
train | epoch   5 | Iter:   7576/ 13000 | global iter:   7576/ 13000 | loss: 0.7117 | ds_loss: 0.0000 | lr: 1.8763e-05 | scale: 32768.0000 | micro time: 1.851 | step time: 0.000
train | epoch   5 | Iter:   7577/ 13000 | global iter:   7577/ 13000 | loss: 1.0893 | ds_loss: 0.0000 | lr: 1.8758e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   5 | Iter:   7578/ 13000 | global iter:   7578/ 13000 | loss: 0.9744 | ds_loss: 0.0000 | lr: 1.8752e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   5 | Iter:   7579/ 13000 | global iter:   7579/ 13000 | loss: 1.2521 | ds_loss: 0.0000 | lr: 1.8746e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   5 | Iter:   7580/ 13000 | global iter:   7580/ 13000 | loss: 1.2475 | ds_loss: 0.0000 | lr: 1.8740e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7580/ 13000 | global iter:   7580/ 13000 | loss: 1.1376 | ds_loss: 0.0000 | lr: 1.8740e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 1.829
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7581/ 13000 | global iter:   7581/ 13000 | loss: 0.9889 | ds_loss: 0.0000 | lr: 1.8734e-05 | scale: 32768.0000 | micro time: 1.774 | step time: 0.000
train | epoch   5 | Iter:   7582/ 13000 | global iter:   7582/ 13000 | loss: 1.2819 | ds_loss: 0.0000 | lr: 1.8728e-05 | scale: 32768.0000 | micro time: 1.814 | step time: 0.000
train | epoch   5 | Iter:   7583/ 13000 | global iter:   7583/ 13000 | loss: 1.1254 | ds_loss: 0.0000 | lr: 1.8723e-05 | scale: 32768.0000 | micro time: 1.826 | step time: 0.000
train | epoch   5 | Iter:   7584/ 13000 | global iter:   7584/ 13000 | loss: 0.4819 | ds_loss: 0.0000 | lr: 1.8717e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   5 | Iter:   7585/ 13000 | global iter:   7585/ 13000 | loss: 1.0686 | ds_loss: 0.0000 | lr: 1.8711e-05 | scale: 32768.0000 | micro time: 1.772 | step time: 0.000
train | epoch   5 | Iter:   7586/ 13000 | global iter:   7586/ 13000 | loss: 1.2718 | ds_loss: 0.0000 | lr: 1.8705e-05 | scale: 32768.0000 | micro time: 1.790 | step time: 0.000
train | epoch   5 | Iter:   7587/ 13000 | global iter:   7587/ 13000 | loss: 1.4104 | ds_loss: 0.0000 | lr: 1.8699e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   5 | Iter:   7588/ 13000 | global iter:   7588/ 13000 | loss: 1.4528 | ds_loss: 0.0000 | lr: 1.8693e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 0.000
train | epoch   5 | Iter:   7589/ 13000 | global iter:   7589/ 13000 | loss: 1.2427 | ds_loss: 0.0000 | lr: 1.8688e-05 | scale: 32768.0000 | micro time: 1.931 | step time: 0.000
train | epoch   5 | Iter:   7590/ 13000 | global iter:   7590/ 13000 | loss: 1.3547 | ds_loss: 0.0000 | lr: 1.8682e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7590/ 13000 | global iter:   7590/ 13000 | loss: 1.1679 | ds_loss: 0.0000 | lr: 1.8682e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 1.823
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7591/ 13000 | global iter:   7591/ 13000 | loss: 1.1218 | ds_loss: 0.0000 | lr: 1.8676e-05 | scale: 32768.0000 | micro time: 1.745 | step time: 0.000
train | epoch   5 | Iter:   7592/ 13000 | global iter:   7592/ 13000 | loss: 1.5525 | ds_loss: 0.0000 | lr: 1.8670e-05 | scale: 32768.0000 | micro time: 1.743 | step time: 0.000
train | epoch   5 | Iter:   7593/ 13000 | global iter:   7593/ 13000 | loss: 0.7360 | ds_loss: 0.0000 | lr: 1.8664e-05 | scale: 32768.0000 | micro time: 1.846 | step time: 0.000
train | epoch   5 | Iter:   7594/ 13000 | global iter:   7594/ 13000 | loss: 1.6283 | ds_loss: 0.0000 | lr: 1.8658e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   5 | Iter:   7595/ 13000 | global iter:   7595/ 13000 | loss: 0.8100 | ds_loss: 0.0000 | lr: 1.8653e-05 | scale: 32768.0000 | micro time: 1.769 | step time: 0.000
train | epoch   5 | Iter:   7596/ 13000 | global iter:   7596/ 13000 | loss: 1.3035 | ds_loss: 0.0000 | lr: 1.8647e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   5 | Iter:   7597/ 13000 | global iter:   7597/ 13000 | loss: 0.6860 | ds_loss: 0.0000 | lr: 1.8641e-05 | scale: 32768.0000 | micro time: 1.766 | step time: 0.000
train | epoch   5 | Iter:   7598/ 13000 | global iter:   7598/ 13000 | loss: 0.5529 | ds_loss: 0.0000 | lr: 1.8635e-05 | scale: 32768.0000 | micro time: 1.846 | step time: 0.000
train | epoch   5 | Iter:   7599/ 13000 | global iter:   7599/ 13000 | loss: 1.2459 | ds_loss: 0.0000 | lr: 1.8629e-05 | scale: 32768.0000 | micro time: 1.753 | step time: 0.000
train | epoch   5 | Iter:   7600/ 13000 | global iter:   7600/ 13000 | loss: 1.6307 | ds_loss: 0.0000 | lr: 1.8624e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7600/ 13000 | global iter:   7600/ 13000 | loss: 1.1268 | ds_loss: 0.0000 | lr: 1.8624e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 1.806
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7601/ 13000 | global iter:   7601/ 13000 | loss: 1.4029 | ds_loss: 0.0000 | lr: 1.8618e-05 | scale: 32768.0000 | micro time: 1.725 | step time: 0.000
train | epoch   5 | Iter:   7602/ 13000 | global iter:   7602/ 13000 | loss: 1.3807 | ds_loss: 0.0000 | lr: 1.8612e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   5 | Iter:   7603/ 13000 | global iter:   7603/ 13000 | loss: 1.0331 | ds_loss: 0.0000 | lr: 1.8606e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   5 | Iter:   7604/ 13000 | global iter:   7604/ 13000 | loss: 0.5549 | ds_loss: 0.0000 | lr: 1.8600e-05 | scale: 32768.0000 | micro time: 1.687 | step time: 0.000
train | epoch   5 | Iter:   7605/ 13000 | global iter:   7605/ 13000 | loss: 0.8688 | ds_loss: 0.0000 | lr: 1.8594e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   5 | Iter:   7606/ 13000 | global iter:   7606/ 13000 | loss: 1.3120 | ds_loss: 0.0000 | lr: 1.8589e-05 | scale: 32768.0000 | micro time: 1.899 | step time: 0.000
train | epoch   5 | Iter:   7607/ 13000 | global iter:   7607/ 13000 | loss: 0.6405 | ds_loss: 0.0000 | lr: 1.8583e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   5 | Iter:   7608/ 13000 | global iter:   7608/ 13000 | loss: 0.8161 | ds_loss: 0.0000 | lr: 1.8577e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   5 | Iter:   7609/ 13000 | global iter:   7609/ 13000 | loss: 0.8415 | ds_loss: 0.0000 | lr: 1.8571e-05 | scale: 32768.0000 | micro time: 1.896 | step time: 0.000
train | epoch   5 | Iter:   7610/ 13000 | global iter:   7610/ 13000 | loss: 1.4257 | ds_loss: 0.0000 | lr: 1.8565e-05 | scale: 32768.0000 | micro time: 1.713 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7610/ 13000 | global iter:   7610/ 13000 | loss: 1.0276 | ds_loss: 0.0000 | lr: 1.8565e-05 | scale: 32768.0000 | micro time: 1.713 | step time: 1.811
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7611/ 13000 | global iter:   7611/ 13000 | loss: 0.9861 | ds_loss: 0.0000 | lr: 1.8559e-05 | scale: 32768.0000 | micro time: 1.826 | step time: 0.000
train | epoch   5 | Iter:   7612/ 13000 | global iter:   7612/ 13000 | loss: 0.9599 | ds_loss: 0.0000 | lr: 1.8554e-05 | scale: 32768.0000 | micro time: 1.712 | step time: 0.000
train | epoch   5 | Iter:   7613/ 13000 | global iter:   7613/ 13000 | loss: 1.0688 | ds_loss: 0.0000 | lr: 1.8548e-05 | scale: 32768.0000 | micro time: 1.828 | step time: 0.000
train | epoch   5 | Iter:   7614/ 13000 | global iter:   7614/ 13000 | loss: 1.8987 | ds_loss: 0.0000 | lr: 1.8542e-05 | scale: 32768.0000 | micro time: 1.768 | step time: 0.000
train | epoch   5 | Iter:   7615/ 13000 | global iter:   7615/ 13000 | loss: 0.7787 | ds_loss: 0.0000 | lr: 1.8536e-05 | scale: 32768.0000 | micro time: 1.840 | step time: 0.000
train | epoch   5 | Iter:   7616/ 13000 | global iter:   7616/ 13000 | loss: 0.9015 | ds_loss: 0.0000 | lr: 1.8530e-05 | scale: 32768.0000 | micro time: 1.720 | step time: 0.000
train | epoch   5 | Iter:   7617/ 13000 | global iter:   7617/ 13000 | loss: 1.4034 | ds_loss: 0.0000 | lr: 1.8525e-05 | scale: 32768.0000 | micro time: 1.786 | step time: 0.000
train | epoch   5 | Iter:   7618/ 13000 | global iter:   7618/ 13000 | loss: 1.0971 | ds_loss: 0.0000 | lr: 1.8519e-05 | scale: 32768.0000 | micro time: 1.824 | step time: 0.000
train | epoch   5 | Iter:   7619/ 13000 | global iter:   7619/ 13000 | loss: 1.0622 | ds_loss: 0.0000 | lr: 1.8513e-05 | scale: 32768.0000 | micro time: 1.722 | step time: 0.000
train | epoch   5 | Iter:   7620/ 13000 | global iter:   7620/ 13000 | loss: 1.5930 | ds_loss: 0.0000 | lr: 1.8507e-05 | scale: 32768.0000 | micro time: 1.824 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7620/ 13000 | global iter:   7620/ 13000 | loss: 1.1749 | ds_loss: 0.0000 | lr: 1.8507e-05 | scale: 32768.0000 | micro time: 1.824 | step time: 1.785
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7621/ 13000 | global iter:   7621/ 13000 | loss: 0.9740 | ds_loss: 0.0000 | lr: 1.8501e-05 | scale: 32768.0000 | micro time: 1.673 | step time: 0.000
train | epoch   5 | Iter:   7622/ 13000 | global iter:   7622/ 13000 | loss: 1.3068 | ds_loss: 0.0000 | lr: 1.8495e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   5 | Iter:   7623/ 13000 | global iter:   7623/ 13000 | loss: 0.9075 | ds_loss: 0.0000 | lr: 1.8490e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   5 | Iter:   7624/ 13000 | global iter:   7624/ 13000 | loss: 1.2645 | ds_loss: 0.0000 | lr: 1.8484e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   5 | Iter:   7625/ 13000 | global iter:   7625/ 13000 | loss: 1.5506 | ds_loss: 0.0000 | lr: 1.8478e-05 | scale: 32768.0000 | micro time: 1.895 | step time: 0.000
train | epoch   5 | Iter:   7626/ 13000 | global iter:   7626/ 13000 | loss: 0.8328 | ds_loss: 0.0000 | lr: 1.8472e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   5 | Iter:   7627/ 13000 | global iter:   7627/ 13000 | loss: 0.5062 | ds_loss: 0.0000 | lr: 1.8466e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   5 | Iter:   7628/ 13000 | global iter:   7628/ 13000 | loss: 1.3562 | ds_loss: 0.0000 | lr: 1.8461e-05 | scale: 32768.0000 | micro time: 1.816 | step time: 0.000
train | epoch   5 | Iter:   7629/ 13000 | global iter:   7629/ 13000 | loss: 0.6625 | ds_loss: 0.0000 | lr: 1.8455e-05 | scale: 32768.0000 | micro time: 1.786 | step time: 0.000
train | epoch   5 | Iter:   7630/ 13000 | global iter:   7630/ 13000 | loss: 1.3313 | ds_loss: 0.0000 | lr: 1.8449e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7630/ 13000 | global iter:   7630/ 13000 | loss: 1.0692 | ds_loss: 0.0000 | lr: 1.8449e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 1.815
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7631/ 13000 | global iter:   7631/ 13000 | loss: 0.5233 | ds_loss: 0.0000 | lr: 1.8443e-05 | scale: 32768.0000 | micro time: 1.840 | step time: 0.000
train | epoch   5 | Iter:   7632/ 13000 | global iter:   7632/ 13000 | loss: 0.9435 | ds_loss: 0.0000 | lr: 1.8437e-05 | scale: 32768.0000 | micro time: 1.773 | step time: 0.000
train | epoch   5 | Iter:   7633/ 13000 | global iter:   7633/ 13000 | loss: 1.0556 | ds_loss: 0.0000 | lr: 1.8431e-05 | scale: 32768.0000 | micro time: 1.727 | step time: 0.000
train | epoch   5 | Iter:   7634/ 13000 | global iter:   7634/ 13000 | loss: 1.3396 | ds_loss: 0.0000 | lr: 1.8426e-05 | scale: 32768.0000 | micro time: 1.731 | step time: 0.000
train | epoch   5 | Iter:   7635/ 13000 | global iter:   7635/ 13000 | loss: 1.4460 | ds_loss: 0.0000 | lr: 1.8420e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   5 | Iter:   7636/ 13000 | global iter:   7636/ 13000 | loss: 0.9970 | ds_loss: 0.0000 | lr: 1.8414e-05 | scale: 32768.0000 | micro time: 1.877 | step time: 0.000
train | epoch   5 | Iter:   7637/ 13000 | global iter:   7637/ 13000 | loss: 0.6809 | ds_loss: 0.0000 | lr: 1.8408e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   5 | Iter:   7638/ 13000 | global iter:   7638/ 13000 | loss: 1.4964 | ds_loss: 0.0000 | lr: 1.8402e-05 | scale: 32768.0000 | micro time: 1.907 | step time: 0.000
train | epoch   5 | Iter:   7639/ 13000 | global iter:   7639/ 13000 | loss: 1.0116 | ds_loss: 0.0000 | lr: 1.8397e-05 | scale: 32768.0000 | micro time: 1.767 | step time: 0.000
train | epoch   5 | Iter:   7640/ 13000 | global iter:   7640/ 13000 | loss: 0.8233 | ds_loss: 0.0000 | lr: 1.8391e-05 | scale: 32768.0000 | micro time: 1.872 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7640/ 13000 | global iter:   7640/ 13000 | loss: 1.0317 | ds_loss: 0.0000 | lr: 1.8391e-05 | scale: 32768.0000 | micro time: 1.872 | step time: 1.814
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7641/ 13000 | global iter:   7641/ 13000 | loss: 0.9186 | ds_loss: 0.0000 | lr: 1.8385e-05 | scale: 32768.0000 | micro time: 1.868 | step time: 0.000
train | epoch   5 | Iter:   7642/ 13000 | global iter:   7642/ 13000 | loss: 1.0038 | ds_loss: 0.0000 | lr: 1.8379e-05 | scale: 32768.0000 | micro time: 1.739 | step time: 0.000
train | epoch   5 | Iter:   7643/ 13000 | global iter:   7643/ 13000 | loss: 0.8622 | ds_loss: 0.0000 | lr: 1.8373e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   5 | Iter:   7644/ 13000 | global iter:   7644/ 13000 | loss: 0.9749 | ds_loss: 0.0000 | lr: 1.8368e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   5 | Iter:   7645/ 13000 | global iter:   7645/ 13000 | loss: 1.3889 | ds_loss: 0.0000 | lr: 1.8362e-05 | scale: 32768.0000 | micro time: 1.907 | step time: 0.000
train | epoch   5 | Iter:   7646/ 13000 | global iter:   7646/ 13000 | loss: 1.2340 | ds_loss: 0.0000 | lr: 1.8356e-05 | scale: 32768.0000 | micro time: 1.731 | step time: 0.000
train | epoch   5 | Iter:   7647/ 13000 | global iter:   7647/ 13000 | loss: 0.8577 | ds_loss: 0.0000 | lr: 1.8350e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   5 | Iter:   7648/ 13000 | global iter:   7648/ 13000 | loss: 1.1980 | ds_loss: 0.0000 | lr: 1.8344e-05 | scale: 32768.0000 | micro time: 1.683 | step time: 0.000
train | epoch   5 | Iter:   7649/ 13000 | global iter:   7649/ 13000 | loss: 0.7790 | ds_loss: 0.0000 | lr: 1.8339e-05 | scale: 32768.0000 | micro time: 1.817 | step time: 0.000
train | epoch   5 | Iter:   7650/ 13000 | global iter:   7650/ 13000 | loss: 1.4132 | ds_loss: 0.0000 | lr: 1.8333e-05 | scale: 32768.0000 | micro time: 1.841 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7650/ 13000 | global iter:   7650/ 13000 | loss: 1.0630 | ds_loss: 0.0000 | lr: 1.8333e-05 | scale: 32768.0000 | micro time: 1.841 | step time: 1.795
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7651/ 13000 | global iter:   7651/ 13000 | loss: 1.3121 | ds_loss: 0.0000 | lr: 1.8327e-05 | scale: 32768.0000 | micro time: 1.797 | step time: 0.000
train | epoch   5 | Iter:   7652/ 13000 | global iter:   7652/ 13000 | loss: 0.9959 | ds_loss: 0.0000 | lr: 1.8321e-05 | scale: 32768.0000 | micro time: 1.812 | step time: 0.000
train | epoch   5 | Iter:   7653/ 13000 | global iter:   7653/ 13000 | loss: 1.1997 | ds_loss: 0.0000 | lr: 1.8315e-05 | scale: 32768.0000 | micro time: 1.866 | step time: 0.000
train | epoch   5 | Iter:   7654/ 13000 | global iter:   7654/ 13000 | loss: 1.4275 | ds_loss: 0.0000 | lr: 1.8309e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   5 | Iter:   7655/ 13000 | global iter:   7655/ 13000 | loss: 1.2678 | ds_loss: 0.0000 | lr: 1.8304e-05 | scale: 32768.0000 | micro time: 1.870 | step time: 0.000
train | epoch   5 | Iter:   7656/ 13000 | global iter:   7656/ 13000 | loss: 1.2180 | ds_loss: 0.0000 | lr: 1.8298e-05 | scale: 32768.0000 | micro time: 1.820 | step time: 0.000
train | epoch   5 | Iter:   7657/ 13000 | global iter:   7657/ 13000 | loss: 1.4912 | ds_loss: 0.0000 | lr: 1.8292e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   5 | Iter:   7658/ 13000 | global iter:   7658/ 13000 | loss: 1.3703 | ds_loss: 0.0000 | lr: 1.8286e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   5 | Iter:   7659/ 13000 | global iter:   7659/ 13000 | loss: 0.9910 | ds_loss: 0.0000 | lr: 1.8280e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   5 | Iter:   7660/ 13000 | global iter:   7660/ 13000 | loss: 0.8449 | ds_loss: 0.0000 | lr: 1.8275e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7660/ 13000 | global iter:   7660/ 13000 | loss: 1.2118 | ds_loss: 0.0000 | lr: 1.8275e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 1.844
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7661/ 13000 | global iter:   7661/ 13000 | loss: 1.2044 | ds_loss: 0.0000 | lr: 1.8269e-05 | scale: 32768.0000 | micro time: 1.812 | step time: 0.000
train | epoch   5 | Iter:   7662/ 13000 | global iter:   7662/ 13000 | loss: 0.4785 | ds_loss: 0.0000 | lr: 1.8263e-05 | scale: 32768.0000 | micro time: 1.876 | step time: 0.000
train | epoch   5 | Iter:   7663/ 13000 | global iter:   7663/ 13000 | loss: 1.2133 | ds_loss: 0.0000 | lr: 1.8257e-05 | scale: 32768.0000 | micro time: 1.826 | step time: 0.000
train | epoch   5 | Iter:   7664/ 13000 | global iter:   7664/ 13000 | loss: 1.5387 | ds_loss: 0.0000 | lr: 1.8251e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   5 | Iter:   7665/ 13000 | global iter:   7665/ 13000 | loss: 1.0829 | ds_loss: 0.0000 | lr: 1.8246e-05 | scale: 32768.0000 | micro time: 1.732 | step time: 0.000
train | epoch   5 | Iter:   7666/ 13000 | global iter:   7666/ 13000 | loss: 1.2334 | ds_loss: 0.0000 | lr: 1.8240e-05 | scale: 32768.0000 | micro time: 1.834 | step time: 0.000
train | epoch   5 | Iter:   7667/ 13000 | global iter:   7667/ 13000 | loss: 0.9210 | ds_loss: 0.0000 | lr: 1.8234e-05 | scale: 32768.0000 | micro time: 1.800 | step time: 0.000
train | epoch   5 | Iter:   7668/ 13000 | global iter:   7668/ 13000 | loss: 0.9347 | ds_loss: 0.0000 | lr: 1.8228e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   5 | Iter:   7669/ 13000 | global iter:   7669/ 13000 | loss: 0.7536 | ds_loss: 0.0000 | lr: 1.8222e-05 | scale: 32768.0000 | micro time: 1.915 | step time: 0.000
train | epoch   5 | Iter:   7670/ 13000 | global iter:   7670/ 13000 | loss: 1.6034 | ds_loss: 0.0000 | lr: 1.8217e-05 | scale: 32768.0000 | micro time: 1.910 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7670/ 13000 | global iter:   7670/ 13000 | loss: 1.0964 | ds_loss: 0.0000 | lr: 1.8217e-05 | scale: 32768.0000 | micro time: 1.910 | step time: 1.836
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7671/ 13000 | global iter:   7671/ 13000 | loss: 0.7000 | ds_loss: 0.0000 | lr: 1.8211e-05 | scale: 32768.0000 | micro time: 1.849 | step time: 0.000
train | epoch   5 | Iter:   7672/ 13000 | global iter:   7672/ 13000 | loss: 0.6041 | ds_loss: 0.0000 | lr: 1.8205e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   5 | Iter:   7673/ 13000 | global iter:   7673/ 13000 | loss: 1.0617 | ds_loss: 0.0000 | lr: 1.8199e-05 | scale: 32768.0000 | micro time: 1.849 | step time: 0.000
train | epoch   5 | Iter:   7674/ 13000 | global iter:   7674/ 13000 | loss: 1.4368 | ds_loss: 0.0000 | lr: 1.8193e-05 | scale: 32768.0000 | micro time: 1.891 | step time: 0.000
train | epoch   5 | Iter:   7675/ 13000 | global iter:   7675/ 13000 | loss: 1.6448 | ds_loss: 0.0000 | lr: 1.8188e-05 | scale: 32768.0000 | micro time: 1.769 | step time: 0.000
train | epoch   5 | Iter:   7676/ 13000 | global iter:   7676/ 13000 | loss: 0.9142 | ds_loss: 0.0000 | lr: 1.8182e-05 | scale: 32768.0000 | micro time: 1.773 | step time: 0.000
train | epoch   5 | Iter:   7677/ 13000 | global iter:   7677/ 13000 | loss: 1.2103 | ds_loss: 0.0000 | lr: 1.8176e-05 | scale: 32768.0000 | micro time: 1.859 | step time: 0.000
train | epoch   5 | Iter:   7678/ 13000 | global iter:   7678/ 13000 | loss: 1.3026 | ds_loss: 0.0000 | lr: 1.8170e-05 | scale: 32768.0000 | micro time: 1.895 | step time: 0.000
train | epoch   5 | Iter:   7679/ 13000 | global iter:   7679/ 13000 | loss: 1.5751 | ds_loss: 0.0000 | lr: 1.8164e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   5 | Iter:   7680/ 13000 | global iter:   7680/ 13000 | loss: 1.1772 | ds_loss: 0.0000 | lr: 1.8159e-05 | scale: 32768.0000 | micro time: 1.873 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7680/ 13000 | global iter:   7680/ 13000 | loss: 1.1627 | ds_loss: 0.0000 | lr: 1.8159e-05 | scale: 32768.0000 | micro time: 1.873 | step time: 1.843
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7681/ 13000 | global iter:   7681/ 13000 | loss: 1.6952 | ds_loss: 0.0000 | lr: 1.8153e-05 | scale: 32768.0000 | micro time: 1.770 | step time: 0.000
train | epoch   5 | Iter:   7682/ 13000 | global iter:   7682/ 13000 | loss: 1.6870 | ds_loss: 0.0000 | lr: 1.8147e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   5 | Iter:   7683/ 13000 | global iter:   7683/ 13000 | loss: 1.2772 | ds_loss: 0.0000 | lr: 1.8141e-05 | scale: 32768.0000 | micro time: 1.751 | step time: 0.000
train | epoch   5 | Iter:   7684/ 13000 | global iter:   7684/ 13000 | loss: 0.8469 | ds_loss: 0.0000 | lr: 1.8136e-05 | scale: 32768.0000 | micro time: 1.795 | step time: 0.000
train | epoch   5 | Iter:   7685/ 13000 | global iter:   7685/ 13000 | loss: 0.6922 | ds_loss: 0.0000 | lr: 1.8130e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   5 | Iter:   7686/ 13000 | global iter:   7686/ 13000 | loss: 1.4794 | ds_loss: 0.0000 | lr: 1.8124e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   5 | Iter:   7687/ 13000 | global iter:   7687/ 13000 | loss: 1.6386 | ds_loss: 0.0000 | lr: 1.8118e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   5 | Iter:   7688/ 13000 | global iter:   7688/ 13000 | loss: 1.4102 | ds_loss: 0.0000 | lr: 1.8112e-05 | scale: 32768.0000 | micro time: 1.955 | step time: 0.000
train | epoch   5 | Iter:   7689/ 13000 | global iter:   7689/ 13000 | loss: 1.2956 | ds_loss: 0.0000 | lr: 1.8107e-05 | scale: 32768.0000 | micro time: 1.811 | step time: 0.000
train | epoch   5 | Iter:   7690/ 13000 | global iter:   7690/ 13000 | loss: 1.1792 | ds_loss: 0.0000 | lr: 1.8101e-05 | scale: 32768.0000 | micro time: 1.842 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7690/ 13000 | global iter:   7690/ 13000 | loss: 1.3202 | ds_loss: 0.0000 | lr: 1.8101e-05 | scale: 32768.0000 | micro time: 1.842 | step time: 1.821
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7691/ 13000 | global iter:   7691/ 13000 | loss: 1.0857 | ds_loss: 0.0000 | lr: 1.8095e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   5 | Iter:   7692/ 13000 | global iter:   7692/ 13000 | loss: 0.5308 | ds_loss: 0.0000 | lr: 1.8089e-05 | scale: 32768.0000 | micro time: 1.808 | step time: 0.000
train | epoch   5 | Iter:   7693/ 13000 | global iter:   7693/ 13000 | loss: 1.5671 | ds_loss: 0.0000 | lr: 1.8083e-05 | scale: 32768.0000 | micro time: 1.801 | step time: 0.000
train | epoch   5 | Iter:   7694/ 13000 | global iter:   7694/ 13000 | loss: 0.7973 | ds_loss: 0.0000 | lr: 1.8078e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 0.000
train | epoch   5 | Iter:   7695/ 13000 | global iter:   7695/ 13000 | loss: 0.9932 | ds_loss: 0.0000 | lr: 1.8072e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   5 | Iter:   7696/ 13000 | global iter:   7696/ 13000 | loss: 1.3181 | ds_loss: 0.0000 | lr: 1.8066e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   5 | Iter:   7697/ 13000 | global iter:   7697/ 13000 | loss: 1.0239 | ds_loss: 0.0000 | lr: 1.8060e-05 | scale: 32768.0000 | micro time: 1.845 | step time: 0.000
train | epoch   5 | Iter:   7698/ 13000 | global iter:   7698/ 13000 | loss: 0.6409 | ds_loss: 0.0000 | lr: 1.8054e-05 | scale: 32768.0000 | micro time: 1.900 | step time: 0.000
train | epoch   5 | Iter:   7699/ 13000 | global iter:   7699/ 13000 | loss: 1.1705 | ds_loss: 0.0000 | lr: 1.8049e-05 | scale: 32768.0000 | micro time: 1.688 | step time: 0.000
train | epoch   5 | Iter:   7700/ 13000 | global iter:   7700/ 13000 | loss: 1.2368 | ds_loss: 0.0000 | lr: 1.8043e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7700/ 13000 | global iter:   7700/ 13000 | loss: 1.0364 | ds_loss: 0.0000 | lr: 1.8043e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 1.821
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7701/ 13000 | global iter:   7701/ 13000 | loss: 0.8440 | ds_loss: 0.0000 | lr: 1.8037e-05 | scale: 32768.0000 | micro time: 1.758 | step time: 0.000
train | epoch   5 | Iter:   7702/ 13000 | global iter:   7702/ 13000 | loss: 1.7512 | ds_loss: 0.0000 | lr: 1.8031e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
train | epoch   5 | Iter:   7703/ 13000 | global iter:   7703/ 13000 | loss: 1.0869 | ds_loss: 0.0000 | lr: 1.8026e-05 | scale: 32768.0000 | micro time: 1.786 | step time: 0.000
train | epoch   5 | Iter:   7704/ 13000 | global iter:   7704/ 13000 | loss: 0.8285 | ds_loss: 0.0000 | lr: 1.8020e-05 | scale: 32768.0000 | micro time: 1.776 | step time: 0.000
train | epoch   5 | Iter:   7705/ 13000 | global iter:   7705/ 13000 | loss: 0.5542 | ds_loss: 0.0000 | lr: 1.8014e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   5 | Iter:   7706/ 13000 | global iter:   7706/ 13000 | loss: 1.1699 | ds_loss: 0.0000 | lr: 1.8008e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
train | epoch   5 | Iter:   7707/ 13000 | global iter:   7707/ 13000 | loss: 1.0077 | ds_loss: 0.0000 | lr: 1.8002e-05 | scale: 32768.0000 | micro time: 1.695 | step time: 0.000
train | epoch   5 | Iter:   7708/ 13000 | global iter:   7708/ 13000 | loss: 1.3740 | ds_loss: 0.0000 | lr: 1.7997e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   5 | Iter:   7709/ 13000 | global iter:   7709/ 13000 | loss: 0.9659 | ds_loss: 0.0000 | lr: 1.7991e-05 | scale: 32768.0000 | micro time: 1.721 | step time: 0.000
train | epoch   5 | Iter:   7710/ 13000 | global iter:   7710/ 13000 | loss: 1.1549 | ds_loss: 0.0000 | lr: 1.7985e-05 | scale: 32768.0000 | micro time: 1.801 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7710/ 13000 | global iter:   7710/ 13000 | loss: 1.0737 | ds_loss: 0.0000 | lr: 1.7985e-05 | scale: 32768.0000 | micro time: 1.801 | step time: 1.775
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7711/ 13000 | global iter:   7711/ 13000 | loss: 1.2307 | ds_loss: 0.0000 | lr: 1.7979e-05 | scale: 32768.0000 | micro time: 1.842 | step time: 0.000
train | epoch   5 | Iter:   7712/ 13000 | global iter:   7712/ 13000 | loss: 0.7795 | ds_loss: 0.0000 | lr: 1.7973e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   5 | Iter:   7713/ 13000 | global iter:   7713/ 13000 | loss: 0.7375 | ds_loss: 0.0000 | lr: 1.7968e-05 | scale: 32768.0000 | micro time: 1.740 | step time: 0.000
train | epoch   5 | Iter:   7714/ 13000 | global iter:   7714/ 13000 | loss: 1.1488 | ds_loss: 0.0000 | lr: 1.7962e-05 | scale: 32768.0000 | micro time: 1.762 | step time: 0.000
train | epoch   5 | Iter:   7715/ 13000 | global iter:   7715/ 13000 | loss: 1.2759 | ds_loss: 0.0000 | lr: 1.7956e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   5 | Iter:   7716/ 13000 | global iter:   7716/ 13000 | loss: 0.8814 | ds_loss: 0.0000 | lr: 1.7950e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   5 | Iter:   7717/ 13000 | global iter:   7717/ 13000 | loss: 1.5087 | ds_loss: 0.0000 | lr: 1.7945e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   5 | Iter:   7718/ 13000 | global iter:   7718/ 13000 | loss: 1.1420 | ds_loss: 0.0000 | lr: 1.7939e-05 | scale: 32768.0000 | micro time: 1.721 | step time: 0.000
train | epoch   5 | Iter:   7719/ 13000 | global iter:   7719/ 13000 | loss: 0.7512 | ds_loss: 0.0000 | lr: 1.7933e-05 | scale: 32768.0000 | micro time: 1.793 | step time: 0.000
train | epoch   5 | Iter:   7720/ 13000 | global iter:   7720/ 13000 | loss: 1.3348 | ds_loss: 0.0000 | lr: 1.7927e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7720/ 13000 | global iter:   7720/ 13000 | loss: 1.0790 | ds_loss: 0.0000 | lr: 1.7927e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 1.809
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7721/ 13000 | global iter:   7721/ 13000 | loss: 0.9831 | ds_loss: 0.0000 | lr: 1.7921e-05 | scale: 32768.0000 | micro time: 1.813 | step time: 0.000
train | epoch   5 | Iter:   7722/ 13000 | global iter:   7722/ 13000 | loss: 1.6223 | ds_loss: 0.0000 | lr: 1.7916e-05 | scale: 32768.0000 | micro time: 1.894 | step time: 0.000
train | epoch   5 | Iter:   7723/ 13000 | global iter:   7723/ 13000 | loss: 1.0631 | ds_loss: 0.0000 | lr: 1.7910e-05 | scale: 32768.0000 | micro time: 1.836 | step time: 0.000
train | epoch   5 | Iter:   7724/ 13000 | global iter:   7724/ 13000 | loss: 1.1572 | ds_loss: 0.0000 | lr: 1.7904e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   5 | Iter:   7725/ 13000 | global iter:   7725/ 13000 | loss: 1.4511 | ds_loss: 0.0000 | lr: 1.7898e-05 | scale: 32768.0000 | micro time: 1.806 | step time: 0.000
train | epoch   5 | Iter:   7726/ 13000 | global iter:   7726/ 13000 | loss: 1.5634 | ds_loss: 0.0000 | lr: 1.7893e-05 | scale: 32768.0000 | micro time: 1.720 | step time: 0.000
train | epoch   5 | Iter:   7727/ 13000 | global iter:   7727/ 13000 | loss: 0.8092 | ds_loss: 0.0000 | lr: 1.7887e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   5 | Iter:   7728/ 13000 | global iter:   7728/ 13000 | loss: 1.1883 | ds_loss: 0.0000 | lr: 1.7881e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   5 | Iter:   7729/ 13000 | global iter:   7729/ 13000 | loss: 1.1927 | ds_loss: 0.0000 | lr: 1.7875e-05 | scale: 32768.0000 | micro time: 1.755 | step time: 0.000
train | epoch   5 | Iter:   7730/ 13000 | global iter:   7730/ 13000 | loss: 0.8333 | ds_loss: 0.0000 | lr: 1.7869e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7730/ 13000 | global iter:   7730/ 13000 | loss: 1.1864 | ds_loss: 0.0000 | lr: 1.7869e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 1.793
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7731/ 13000 | global iter:   7731/ 13000 | loss: 1.0556 | ds_loss: 0.0000 | lr: 1.7864e-05 | scale: 32768.0000 | micro time: 1.878 | step time: 0.000
train | epoch   5 | Iter:   7732/ 13000 | global iter:   7732/ 13000 | loss: 0.6878 | ds_loss: 0.0000 | lr: 1.7858e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   5 | Iter:   7733/ 13000 | global iter:   7733/ 13000 | loss: 1.2758 | ds_loss: 0.0000 | lr: 1.7852e-05 | scale: 32768.0000 | micro time: 1.738 | step time: 0.000
train | epoch   5 | Iter:   7734/ 13000 | global iter:   7734/ 13000 | loss: 0.6647 | ds_loss: 0.0000 | lr: 1.7846e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   5 | Iter:   7735/ 13000 | global iter:   7735/ 13000 | loss: 1.5424 | ds_loss: 0.0000 | lr: 1.7841e-05 | scale: 32768.0000 | micro time: 1.796 | step time: 0.000
train | epoch   5 | Iter:   7736/ 13000 | global iter:   7736/ 13000 | loss: 1.6824 | ds_loss: 0.0000 | lr: 1.7835e-05 | scale: 32768.0000 | micro time: 1.826 | step time: 0.000
train | epoch   5 | Iter:   7737/ 13000 | global iter:   7737/ 13000 | loss: 1.0560 | ds_loss: 0.0000 | lr: 1.7829e-05 | scale: 32768.0000 | micro time: 1.739 | step time: 0.000
train | epoch   5 | Iter:   7738/ 13000 | global iter:   7738/ 13000 | loss: 1.2342 | ds_loss: 0.0000 | lr: 1.7823e-05 | scale: 32768.0000 | micro time: 1.751 | step time: 0.000
train | epoch   5 | Iter:   7739/ 13000 | global iter:   7739/ 13000 | loss: 0.9316 | ds_loss: 0.0000 | lr: 1.7817e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   5 | Iter:   7740/ 13000 | global iter:   7740/ 13000 | loss: 1.1494 | ds_loss: 0.0000 | lr: 1.7812e-05 | scale: 32768.0000 | micro time: 1.719 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7740/ 13000 | global iter:   7740/ 13000 | loss: 1.1280 | ds_loss: 0.0000 | lr: 1.7812e-05 | scale: 32768.0000 | micro time: 1.719 | step time: 1.785
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7741/ 13000 | global iter:   7741/ 13000 | loss: 0.9617 | ds_loss: 0.0000 | lr: 1.7806e-05 | scale: 32768.0000 | micro time: 1.883 | step time: 0.000
train | epoch   5 | Iter:   7742/ 13000 | global iter:   7742/ 13000 | loss: 0.7765 | ds_loss: 0.0000 | lr: 1.7800e-05 | scale: 32768.0000 | micro time: 1.820 | step time: 0.000
train | epoch   5 | Iter:   7743/ 13000 | global iter:   7743/ 13000 | loss: 1.0664 | ds_loss: 0.0000 | lr: 1.7794e-05 | scale: 32768.0000 | micro time: 1.794 | step time: 0.000
train | epoch   5 | Iter:   7744/ 13000 | global iter:   7744/ 13000 | loss: 0.9934 | ds_loss: 0.0000 | lr: 1.7789e-05 | scale: 32768.0000 | micro time: 1.751 | step time: 0.000
train | epoch   5 | Iter:   7745/ 13000 | global iter:   7745/ 13000 | loss: 1.0814 | ds_loss: 0.0000 | lr: 1.7783e-05 | scale: 65536.0000 | micro time: 1.815 | step time: 0.000
train | epoch   5 | Iter:   7746/ 13000 | global iter:   7746/ 13000 | loss: 0.6906 | ds_loss: 0.0000 | lr: 1.7777e-05 | scale: 65536.0000 | micro time: 1.827 | step time: 0.000
train | epoch   5 | Iter:   7747/ 13000 | global iter:   7747/ 13000 | loss: 1.4577 | ds_loss: 0.0000 | lr: 1.7771e-05 | scale: 65536.0000 | micro time: 1.828 | step time: 0.000
train | epoch   5 | Iter:   7748/ 13000 | global iter:   7748/ 13000 | loss: 0.8665 | ds_loss: 0.0000 | lr: 1.7766e-05 | scale: 65536.0000 | micro time: 1.866 | step time: 0.000
train | epoch   5 | Iter:   7749/ 13000 | global iter:   7749/ 13000 | loss: 0.9409 | ds_loss: 0.0000 | lr: 1.7760e-05 | scale: 65536.0000 | micro time: 1.758 | step time: 0.000
train | epoch   5 | Iter:   7750/ 13000 | global iter:   7750/ 13000 | loss: 1.0781 | ds_loss: 0.0000 | lr: 1.7754e-05 | scale: 65536.0000 | micro time: 1.833 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7750/ 13000 | global iter:   7750/ 13000 | loss: 0.9913 | ds_loss: 0.0000 | lr: 1.7754e-05 | scale: 65536.0000 | micro time: 1.833 | step time: 1.818
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7751/ 13000 | global iter:   7751/ 13000 | loss: 0.5480 | ds_loss: 0.0000 | lr: 1.7748e-05 | scale: 65536.0000 | micro time: 1.715 | step time: 0.000
train | epoch   5 | Iter:   7752/ 13000 | global iter:   7752/ 13000 | loss: 1.8011 | ds_loss: 0.0000 | lr: 1.7743e-05 | scale: 65536.0000 | micro time: 1.787 | step time: 0.000
train | epoch   5 | Iter:   7753/ 13000 | global iter:   7753/ 13000 | loss: 1.3561 | ds_loss: 0.0000 | lr: 1.7737e-05 | scale: 65536.0000 | micro time: 1.859 | step time: 0.000
train | epoch   5 | Iter:   7754/ 13000 | global iter:   7754/ 13000 | loss: 0.8156 | ds_loss: 0.0000 | lr: 1.7731e-05 | scale: 65536.0000 | micro time: 1.883 | step time: 0.000
train | epoch   5 | Iter:   7755/ 13000 | global iter:   7755/ 13000 | loss: 0.9652 | ds_loss: 0.0000 | lr: 1.7725e-05 | scale: 65536.0000 | micro time: 1.910 | step time: 0.000
train | epoch   5 | Iter:   7756/ 13000 | global iter:   7756/ 13000 | loss: 0.9831 | ds_loss: 0.0000 | lr: 1.7719e-05 | scale: 65536.0000 | micro time: 1.744 | step time: 0.000
train | epoch   5 | Iter:   7757/ 13000 | global iter:   7757/ 13000 | loss: 1.4241 | ds_loss: 0.0000 | lr: 1.7714e-05 | scale: 65536.0000 | micro time: 1.831 | step time: 0.000
train | epoch   5 | Iter:   7758/ 13000 | global iter:   7758/ 13000 | loss: 0.8409 | ds_loss: 0.0000 | lr: 1.7708e-05 | scale: 65536.0000 | micro time: 1.806 | step time: 0.000
train | epoch   5 | Iter:   7759/ 13000 | global iter:   7759/ 13000 | loss: 1.4316 | ds_loss: 0.0000 | lr: 1.7702e-05 | scale: 65536.0000 | micro time: 1.816 | step time: 0.000
train | epoch   5 | Iter:   7760/ 13000 | global iter:   7760/ 13000 | loss: 0.9840 | ds_loss: 0.0000 | lr: 1.7696e-05 | scale: 65536.0000 | micro time: 1.823 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7760/ 13000 | global iter:   7760/ 13000 | loss: 1.1150 | ds_loss: 0.0000 | lr: 1.7696e-05 | scale: 65536.0000 | micro time: 1.823 | step time: 1.817
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7761/ 13000 | global iter:   7761/ 13000 | loss: 0.9772 | ds_loss: 0.0000 | lr: 1.7691e-05 | scale: 65536.0000 | micro time: 1.729 | step time: 0.000
train | epoch   5 | Iter:   7762/ 13000 | global iter:   7762/ 13000 | loss: 0.7888 | ds_loss: 0.0000 | lr: 1.7685e-05 | scale: 65536.0000 | micro time: 1.651 | step time: 0.000
train | epoch   5 | Iter:   7763/ 13000 | global iter:   7763/ 13000 | loss: 1.5105 | ds_loss: 0.0000 | lr: 1.7679e-05 | scale: 65536.0000 | micro time: 1.715 | step time: 0.000
train | epoch   5 | Iter:   7764/ 13000 | global iter:   7764/ 13000 | loss: 0.5761 | ds_loss: 0.0000 | lr: 1.7673e-05 | scale: 65536.0000 | micro time: 1.823 | step time: 0.000
train | epoch   5 | Iter:   7765/ 13000 | global iter:   7765/ 13000 | loss: 1.4243 | ds_loss: 0.0000 | lr: 1.7668e-05 | scale: 65536.0000 | micro time: 1.775 | step time: 0.000
train | epoch   5 | Iter:   7766/ 13000 | global iter:   7766/ 13000 | loss: 0.7425 | ds_loss: 0.0000 | lr: 1.7662e-05 | scale: 65536.0000 | micro time: 1.851 | step time: 0.000
train | epoch   5 | Iter:   7767/ 13000 | global iter:   7767/ 13000 | loss: 0.9002 | ds_loss: 0.0000 | lr: 1.7656e-05 | scale: 65536.0000 | micro time: 1.767 | step time: 0.000
train | epoch   5 | Iter:   7768/ 13000 | global iter:   7768/ 13000 | loss: 1.1175 | ds_loss: 0.0000 | lr: 1.7650e-05 | scale: 65536.0000 | micro time: 1.871 | step time: 0.000
train | epoch   5 | Iter:   7769/ 13000 | global iter:   7769/ 13000 | loss: 0.8891 | ds_loss: 0.0000 | lr: 1.7645e-05 | scale: 65536.0000 | micro time: 1.747 | step time: 0.000
train | epoch   5 | Iter:   7770/ 13000 | global iter:   7770/ 13000 | loss: 1.5338 | ds_loss: 0.0000 | lr: 1.7639e-05 | scale: 65536.0000 | micro time: 1.823 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7770/ 13000 | global iter:   7770/ 13000 | loss: 1.0460 | ds_loss: 0.0000 | lr: 1.7639e-05 | scale: 65536.0000 | micro time: 1.823 | step time: 1.775
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7771/ 13000 | global iter:   7771/ 13000 | loss: 1.0874 | ds_loss: 0.0000 | lr: 1.7633e-05 | scale: 65536.0000 | micro time: 1.782 | step time: 0.000
train | epoch   5 | Iter:   7772/ 13000 | global iter:   7772/ 13000 | loss: 1.0691 | ds_loss: 0.0000 | lr: 1.7627e-05 | scale: 65536.0000 | micro time: 1.839 | step time: 0.000
train | epoch   5 | Iter:   7773/ 13000 | global iter:   7773/ 13000 | loss: 1.1127 | ds_loss: 0.0000 | lr: 1.7622e-05 | scale: 65536.0000 | micro time: 1.843 | step time: 0.000
train | epoch   5 | Iter:   7774/ 13000 | global iter:   7774/ 13000 | loss: 1.2725 | ds_loss: 0.0000 | lr: 1.7616e-05 | scale: 65536.0000 | micro time: 1.823 | step time: 0.000
train | epoch   5 | Iter:   7775/ 13000 | global iter:   7775/ 13000 | loss: 1.3416 | ds_loss: 0.0000 | lr: 1.7610e-05 | scale: 65536.0000 | micro time: 1.850 | step time: 0.000
train | epoch   5 | Iter:   7776/ 13000 | global iter:   7776/ 13000 | loss: 0.7858 | ds_loss: 0.0000 | lr: 1.7604e-05 | scale: 65536.0000 | micro time: 1.711 | step time: 0.000
train | epoch   5 | Iter:   7777/ 13000 | global iter:   7777/ 13000 | loss: 0.9857 | ds_loss: 0.0000 | lr: 1.7599e-05 | scale: 65536.0000 | micro time: 1.773 | step time: 0.000
train | epoch   5 | Iter:   7778/ 13000 | global iter:   7778/ 13000 | loss: 1.0712 | ds_loss: 0.0000 | lr: 1.7593e-05 | scale: 65536.0000 | micro time: 1.809 | step time: 0.000
train | epoch   5 | Iter:   7779/ 13000 | global iter:   7779/ 13000 | loss: 0.6989 | ds_loss: 0.0000 | lr: 1.7587e-05 | scale: 65536.0000 | micro time: 1.747 | step time: 0.000
train | epoch   5 | Iter:   7780/ 13000 | global iter:   7780/ 13000 | loss: 1.2882 | ds_loss: 0.0000 | lr: 1.7581e-05 | scale: 65536.0000 | micro time: 1.823 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7780/ 13000 | global iter:   7780/ 13000 | loss: 1.0713 | ds_loss: 0.0000 | lr: 1.7581e-05 | scale: 65536.0000 | micro time: 1.823 | step time: 1.800
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7781/ 13000 | global iter:   7781/ 13000 | loss: 0.7538 | ds_loss: 0.0000 | lr: 1.7576e-05 | scale: 65536.0000 | micro time: 1.742 | step time: 0.000
train | epoch   5 | Iter:   7782/ 13000 | global iter:   7782/ 13000 | loss: 1.1903 | ds_loss: 0.0000 | lr: 1.7570e-05 | scale: 65536.0000 | micro time: 1.799 | step time: 0.000
train | epoch   5 | Iter:   7783/ 13000 | global iter:   7783/ 13000 | loss: 1.0430 | ds_loss: 0.0000 | lr: 1.7564e-05 | scale: 65536.0000 | micro time: 1.851 | step time: 0.000
train | epoch   5 | Iter:   7784/ 13000 | global iter:   7784/ 13000 | loss: 1.1576 | ds_loss: 0.0000 | lr: 1.7558e-05 | scale: 65536.0000 | micro time: 1.787 | step time: 0.000
train | epoch   5 | Iter:   7785/ 13000 | global iter:   7785/ 13000 | loss: 0.7084 | ds_loss: 0.0000 | lr: 1.7553e-05 | scale: 65536.0000 | micro time: 1.809 | step time: 0.000
train | epoch   5 | Iter:   7786/ 13000 | global iter:   7786/ 13000 | loss: 0.7935 | ds_loss: 0.0000 | lr: 1.7547e-05 | scale: 65536.0000 | micro time: 1.736 | step time: 0.000
train | epoch   5 | Iter:   7787/ 13000 | global iter:   7787/ 13000 | loss: 0.9866 | ds_loss: 0.0000 | lr: 1.7541e-05 | scale: 65536.0000 | micro time: 1.789 | step time: 0.000
train | epoch   5 | Iter:   7788/ 13000 | global iter:   7788/ 13000 | loss: 0.7538 | ds_loss: 0.0000 | lr: 1.7535e-05 | scale: 65536.0000 | micro time: 1.724 | step time: 0.000
train | epoch   5 | Iter:   7789/ 13000 | global iter:   7789/ 13000 | loss: 0.9241 | ds_loss: 0.0000 | lr: 1.7530e-05 | scale: 65536.0000 | micro time: 1.817 | step time: 0.000
train | epoch   5 | Iter:   7790/ 13000 | global iter:   7790/ 13000 | loss: 1.0101 | ds_loss: 0.0000 | lr: 1.7524e-05 | scale: 65536.0000 | micro time: 1.699 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7790/ 13000 | global iter:   7790/ 13000 | loss: 0.9321 | ds_loss: 0.0000 | lr: 1.7524e-05 | scale: 65536.0000 | micro time: 1.699 | step time: 1.775
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7791/ 13000 | global iter:   7791/ 13000 | loss: 1.3456 | ds_loss: 0.0000 | lr: 1.7518e-05 | scale: 65536.0000 | micro time: 1.746 | step time: 0.000
train | epoch   5 | Iter:   7792/ 13000 | global iter:   7792/ 13000 | loss: 1.0698 | ds_loss: 0.0000 | lr: 1.7512e-05 | scale: 65536.0000 | micro time: 1.875 | step time: 0.000
train | epoch   5 | Iter:   7793/ 13000 | global iter:   7793/ 13000 | loss: 1.3605 | ds_loss: 0.0000 | lr: 1.7507e-05 | scale: 65536.0000 | micro time: 1.727 | step time: 0.000
train | epoch   5 | Iter:   7794/ 13000 | global iter:   7794/ 13000 | loss: 1.2074 | ds_loss: 0.0000 | lr: 1.7501e-05 | scale: 65536.0000 | micro time: 1.795 | step time: 0.000
train | epoch   5 | Iter:   7795/ 13000 | global iter:   7795/ 13000 | loss: 1.0112 | ds_loss: 0.0000 | lr: 1.7495e-05 | scale: 65536.0000 | micro time: 1.745 | step time: 0.000
train | epoch   5 | Iter:   7796/ 13000 | global iter:   7796/ 13000 | loss: 1.1838 | ds_loss: 0.0000 | lr: 1.7489e-05 | scale: 65536.0000 | micro time: 1.876 | step time: 0.000
train | epoch   5 | Iter:   7797/ 13000 | global iter:   7797/ 13000 | loss: 0.7150 | ds_loss: 0.0000 | lr: 1.7484e-05 | scale: 65536.0000 | micro time: 1.755 | step time: 0.000
train | epoch   5 | Iter:   7798/ 13000 | global iter:   7798/ 13000 | loss: 1.1254 | ds_loss: 0.0000 | lr: 1.7478e-05 | scale: 65536.0000 | micro time: 1.751 | step time: 0.000
train | epoch   5 | Iter:   7799/ 13000 | global iter:   7799/ 13000 | loss: 0.4569 | ds_loss: 0.0000 | lr: 1.7472e-05 | scale: 65536.0000 | micro time: 1.835 | step time: 0.000
train | epoch   5 | Iter:   7800/ 13000 | global iter:   7800/ 13000 | loss: 0.8853 | ds_loss: 0.0000 | lr: 1.7466e-05 | scale: 65536.0000 | micro time: 1.855 | step time: 0.000
****************************************************************************************************
train | epoch   5 | Iter:   7800/ 13000 | global iter:   7800/ 13000 | loss: 1.0361 | ds_loss: 0.0000 | lr: 1.7466e-05 | scale: 65536.0000 | micro time: 1.855 | step time: 1.796
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   5 | Iter:   7801/ 13000 | global iter:   7801/ 13000 | loss: 1.6180 | ds_loss: 0.0000 | lr: 1.7461e-05 | scale: 65536.0000 | micro time: 1.830 | step time: 0.000
train | epoch   5 | Iter:   7802/ 13000 | global iter:   7802/ 13000 | loss: 0.8048 | ds_loss: 0.0000 | lr: 1.7455e-05 | scale: 65536.0000 | micro time: 1.851 | step time: 0.000
train | epoch   5 | Iter:   7803/ 13000 | global iter:   7803/ 13000 | loss: 1.0770 | ds_loss: 0.0000 | lr: 1.7449e-05 | scale: 65536.0000 | micro time: 1.703 | step time: 0.000
train | epoch   5 | Iter:   7804/ 13000 | global iter:   7804/ 13000 | loss: 1.3091 | ds_loss: 0.0000 | lr: 1.7443e-05 | scale: 65536.0000 | micro time: 1.767 | step time: 0.000
train | epoch   5 | Iter:   7805/ 13000 | global iter:   7805/ 13000 | loss: 1.3741 | ds_loss: 0.0000 | lr: 1.7438e-05 | scale: 65536.0000 | micro time: 1.747 | step time: 0.000
[2025-04-19 15:16:18,501] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
train | epoch   5 | Iter:   7806/ 13000 | global iter:   7806/ 13000 | loss: 0.6196 | ds_loss: 0.0000 | lr: 1.7438e-05 | scale: 32768.0000 | micro time: 1.344 | step time: 0.000
Sat Apr 19 15:16:18 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   40C    P2            102W /  230W |   21581MiB /  23028MiB |     98%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   44C    P2            117W /  230W |   20943MiB /  23028MiB |     56%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   42C    P2            110W /  230W |   21691MiB /  23028MiB |     95%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   40C    P2            117W /  230W |   21059MiB /  23028MiB |     96%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3      21574MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3      20936MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3      21684MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3      21052MiB |
+-----------------------------------------------------------------------------------------+

Sat Apr 19 15:16:18 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   40C    P2            102W /  230W |   21581MiB /  23028MiB |     98%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   44C    P2            117W /  230W |   20943MiB /  23028MiB |     56%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   42C    P2            110W /  230W |   21691MiB /  23028MiB |     95%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   40C    P2            117W /  230W |   21059MiB /  23028MiB |     96%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3      21574MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3      20936MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3      21684MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3      21052MiB |
+-----------------------------------------------------------------------------------------+

Sat Apr 19 15:16:18 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   40C    P2            102W /  230W |   21581MiB /  23028MiB |     98%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   44C    P2            122W /  230W |   20943MiB /  23028MiB |     89%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   42C    P2            110W /  230W |   21691MiB /  23028MiB |     95%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   40C    P2            117W /  230W |   21059MiB /  23028MiB |     96%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3      21574MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3      20936MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3      21684MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3      21052MiB |
+-----------------------------------------------------------------------------------------+

Sat Apr 19 15:16:18 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               Off |   00000000:17:00.0 Off |                    0 |
| 30%   40C    P2            102W /  230W |   21581MiB /  23028MiB |     98%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA RTX A5000               Off |   00000000:31:00.0 Off |                    0 |
| 30%   44C    P2            117W /  230W |   20943MiB /  23028MiB |     56%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA RTX A5000               Off |   00000000:B1:00.0 Off |                    0 |
| 30%   42C    P2            108W /  230W |   21691MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA RTX A5000               Off |   00000000:CA:00.0 Off |                    0 |
| 30%   39C    P2             94W /  230W |   21059MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   4164992      C   ...project_distillLLM/venv/bin/python3      21574MiB |
|    1   N/A  N/A   4164993      C   ...project_distillLLM/venv/bin/python3      20936MiB |
|    2   N/A  N/A   4164994      C   ...project_distillLLM/venv/bin/python3      21684MiB |
|    3   N/A  N/A   4164995      C   ...project_distillLLM/venv/bin/python3      21052MiB |
+-----------------------------------------------------------------------------------------+

train | epoch   6 | Iter:   7807/ 13000 | global iter:   7807/ 13000 | loss: 1.3019 | ds_loss: 0.0000 | lr: 1.7432e-05 | scale: 32768.0000 | micro time: 1.703 | step time: 0.000
train | epoch   6 | Iter:   7808/ 13000 | global iter:   7808/ 13000 | loss: 1.5182 | ds_loss: 0.0000 | lr: 1.7426e-05 | scale: 32768.0000 | micro time: 1.864 | step time: 0.000
train | epoch   6 | Iter:   7809/ 13000 | global iter:   7809/ 13000 | loss: 1.3420 | ds_loss: 0.0000 | lr: 1.7420e-05 | scale: 32768.0000 | micro time: 1.895 | step time: 0.000
train | epoch   6 | Iter:   7810/ 13000 | global iter:   7810/ 13000 | loss: 1.3675 | ds_loss: 0.0000 | lr: 1.7415e-05 | scale: 32768.0000 | micro time: 1.747 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   7810/ 13000 | global iter:   7810/ 13000 | loss: 1.2332 | ds_loss: 0.0000 | lr: 1.7415e-05 | scale: 32768.0000 | micro time: 1.747 | step time: 1.745
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   7811/ 13000 | global iter:   7811/ 13000 | loss: 0.8048 | ds_loss: 0.0000 | lr: 1.7409e-05 | scale: 32768.0000 | micro time: 1.850 | step time: 0.000
train | epoch   6 | Iter:   7812/ 13000 | global iter:   7812/ 13000 | loss: 0.4452 | ds_loss: 0.0000 | lr: 1.7403e-05 | scale: 32768.0000 | micro time: 1.845 | step time: 0.000
train | epoch   6 | Iter:   7813/ 13000 | global iter:   7813/ 13000 | loss: 0.9235 | ds_loss: 0.0000 | lr: 1.7397e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 0.000
train | epoch   6 | Iter:   7814/ 13000 | global iter:   7814/ 13000 | loss: 0.8403 | ds_loss: 0.0000 | lr: 1.7392e-05 | scale: 32768.0000 | micro time: 1.733 | step time: 0.000
train | epoch   6 | Iter:   7815/ 13000 | global iter:   7815/ 13000 | loss: 1.3676 | ds_loss: 0.0000 | lr: 1.7386e-05 | scale: 32768.0000 | micro time: 1.768 | step time: 0.000
train | epoch   6 | Iter:   7816/ 13000 | global iter:   7816/ 13000 | loss: 1.0843 | ds_loss: 0.0000 | lr: 1.7380e-05 | scale: 32768.0000 | micro time: 1.738 | step time: 0.000
train | epoch   6 | Iter:   7817/ 13000 | global iter:   7817/ 13000 | loss: 1.1887 | ds_loss: 0.0000 | lr: 1.7374e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
train | epoch   6 | Iter:   7818/ 13000 | global iter:   7818/ 13000 | loss: 0.6888 | ds_loss: 0.0000 | lr: 1.7369e-05 | scale: 32768.0000 | micro time: 1.823 | step time: 0.000
train | epoch   6 | Iter:   7819/ 13000 | global iter:   7819/ 13000 | loss: 0.4603 | ds_loss: 0.0000 | lr: 1.7363e-05 | scale: 32768.0000 | micro time: 1.723 | step time: 0.000
train | epoch   6 | Iter:   7820/ 13000 | global iter:   7820/ 13000 | loss: 1.0748 | ds_loss: 0.0000 | lr: 1.7357e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   7820/ 13000 | global iter:   7820/ 13000 | loss: 0.8878 | ds_loss: 0.0000 | lr: 1.7357e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 1.793
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   7821/ 13000 | global iter:   7821/ 13000 | loss: 0.7319 | ds_loss: 0.0000 | lr: 1.7351e-05 | scale: 32768.0000 | micro time: 1.893 | step time: 0.000
train | epoch   6 | Iter:   7822/ 13000 | global iter:   7822/ 13000 | loss: 1.1078 | ds_loss: 0.0000 | lr: 1.7346e-05 | scale: 32768.0000 | micro time: 1.813 | step time: 0.000
train | epoch   6 | Iter:   7823/ 13000 | global iter:   7823/ 13000 | loss: 0.9912 | ds_loss: 0.0000 | lr: 1.7340e-05 | scale: 32768.0000 | micro time: 1.810 | step time: 0.000
train | epoch   6 | Iter:   7824/ 13000 | global iter:   7824/ 13000 | loss: 0.8663 | ds_loss: 0.0000 | lr: 1.7334e-05 | scale: 32768.0000 | micro time: 1.828 | step time: 0.000
train | epoch   6 | Iter:   7825/ 13000 | global iter:   7825/ 13000 | loss: 0.6576 | ds_loss: 0.0000 | lr: 1.7329e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   6 | Iter:   7826/ 13000 | global iter:   7826/ 13000 | loss: 1.1300 | ds_loss: 0.0000 | lr: 1.7323e-05 | scale: 32768.0000 | micro time: 1.699 | step time: 0.000
train | epoch   6 | Iter:   7827/ 13000 | global iter:   7827/ 13000 | loss: 1.2224 | ds_loss: 0.0000 | lr: 1.7317e-05 | scale: 32768.0000 | micro time: 1.769 | step time: 0.000
train | epoch   6 | Iter:   7828/ 13000 | global iter:   7828/ 13000 | loss: 0.9568 | ds_loss: 0.0000 | lr: 1.7311e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   6 | Iter:   7829/ 13000 | global iter:   7829/ 13000 | loss: 1.0243 | ds_loss: 0.0000 | lr: 1.7306e-05 | scale: 32768.0000 | micro time: 1.727 | step time: 0.000
train | epoch   6 | Iter:   7830/ 13000 | global iter:   7830/ 13000 | loss: 1.1007 | ds_loss: 0.0000 | lr: 1.7300e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   7830/ 13000 | global iter:   7830/ 13000 | loss: 0.9789 | ds_loss: 0.0000 | lr: 1.7300e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 1.796
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   7831/ 13000 | global iter:   7831/ 13000 | loss: 1.2151 | ds_loss: 0.0000 | lr: 1.7294e-05 | scale: 32768.0000 | micro time: 1.870 | step time: 0.000
train | epoch   6 | Iter:   7832/ 13000 | global iter:   7832/ 13000 | loss: 0.9952 | ds_loss: 0.0000 | lr: 1.7288e-05 | scale: 32768.0000 | micro time: 1.751 | step time: 0.000
train | epoch   6 | Iter:   7833/ 13000 | global iter:   7833/ 13000 | loss: 0.7528 | ds_loss: 0.0000 | lr: 1.7283e-05 | scale: 32768.0000 | micro time: 1.766 | step time: 0.000
train | epoch   6 | Iter:   7834/ 13000 | global iter:   7834/ 13000 | loss: 1.1647 | ds_loss: 0.0000 | lr: 1.7277e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   6 | Iter:   7835/ 13000 | global iter:   7835/ 13000 | loss: 1.1567 | ds_loss: 0.0000 | lr: 1.7271e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   6 | Iter:   7836/ 13000 | global iter:   7836/ 13000 | loss: 1.0984 | ds_loss: 0.0000 | lr: 1.7266e-05 | scale: 32768.0000 | micro time: 1.731 | step time: 0.000
train | epoch   6 | Iter:   7837/ 13000 | global iter:   7837/ 13000 | loss: 1.2957 | ds_loss: 0.0000 | lr: 1.7260e-05 | scale: 32768.0000 | micro time: 1.795 | step time: 0.000
train | epoch   6 | Iter:   7838/ 13000 | global iter:   7838/ 13000 | loss: 0.6712 | ds_loss: 0.0000 | lr: 1.7254e-05 | scale: 32768.0000 | micro time: 1.818 | step time: 0.000
train | epoch   6 | Iter:   7839/ 13000 | global iter:   7839/ 13000 | loss: 1.3662 | ds_loss: 0.0000 | lr: 1.7248e-05 | scale: 32768.0000 | micro time: 1.812 | step time: 0.000
train | epoch   6 | Iter:   7840/ 13000 | global iter:   7840/ 13000 | loss: 0.7619 | ds_loss: 0.0000 | lr: 1.7243e-05 | scale: 32768.0000 | micro time: 1.731 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   7840/ 13000 | global iter:   7840/ 13000 | loss: 1.0478 | ds_loss: 0.0000 | lr: 1.7243e-05 | scale: 32768.0000 | micro time: 1.731 | step time: 1.788
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   7841/ 13000 | global iter:   7841/ 13000 | loss: 0.8842 | ds_loss: 0.0000 | lr: 1.7237e-05 | scale: 32768.0000 | micro time: 1.825 | step time: 0.000
train | epoch   6 | Iter:   7842/ 13000 | global iter:   7842/ 13000 | loss: 1.0529 | ds_loss: 0.0000 | lr: 1.7231e-05 | scale: 32768.0000 | micro time: 1.743 | step time: 0.000
train | epoch   6 | Iter:   7843/ 13000 | global iter:   7843/ 13000 | loss: 0.5493 | ds_loss: 0.0000 | lr: 1.7225e-05 | scale: 32768.0000 | micro time: 1.888 | step time: 0.000
train | epoch   6 | Iter:   7844/ 13000 | global iter:   7844/ 13000 | loss: 1.2787 | ds_loss: 0.0000 | lr: 1.7220e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   6 | Iter:   7845/ 13000 | global iter:   7845/ 13000 | loss: 0.8289 | ds_loss: 0.0000 | lr: 1.7214e-05 | scale: 32768.0000 | micro time: 1.753 | step time: 0.000
train | epoch   6 | Iter:   7846/ 13000 | global iter:   7846/ 13000 | loss: 1.0646 | ds_loss: 0.0000 | lr: 1.7208e-05 | scale: 32768.0000 | micro time: 1.888 | step time: 0.000
train | epoch   6 | Iter:   7847/ 13000 | global iter:   7847/ 13000 | loss: 1.6338 | ds_loss: 0.0000 | lr: 1.7203e-05 | scale: 32768.0000 | micro time: 1.857 | step time: 0.000
train | epoch   6 | Iter:   7848/ 13000 | global iter:   7848/ 13000 | loss: 1.0014 | ds_loss: 0.0000 | lr: 1.7197e-05 | scale: 32768.0000 | micro time: 1.802 | step time: 0.000
train | epoch   6 | Iter:   7849/ 13000 | global iter:   7849/ 13000 | loss: 1.0563 | ds_loss: 0.0000 | lr: 1.7191e-05 | scale: 32768.0000 | micro time: 1.833 | step time: 0.000
train | epoch   6 | Iter:   7850/ 13000 | global iter:   7850/ 13000 | loss: 1.0419 | ds_loss: 0.0000 | lr: 1.7185e-05 | scale: 32768.0000 | micro time: 1.739 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   7850/ 13000 | global iter:   7850/ 13000 | loss: 1.0392 | ds_loss: 0.0000 | lr: 1.7185e-05 | scale: 32768.0000 | micro time: 1.739 | step time: 1.816
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   7851/ 13000 | global iter:   7851/ 13000 | loss: 1.2580 | ds_loss: 0.0000 | lr: 1.7180e-05 | scale: 32768.0000 | micro time: 1.858 | step time: 0.000
train | epoch   6 | Iter:   7852/ 13000 | global iter:   7852/ 13000 | loss: 0.7647 | ds_loss: 0.0000 | lr: 1.7174e-05 | scale: 32768.0000 | micro time: 1.857 | step time: 0.000
train | epoch   6 | Iter:   7853/ 13000 | global iter:   7853/ 13000 | loss: 0.5859 | ds_loss: 0.0000 | lr: 1.7168e-05 | scale: 32768.0000 | micro time: 1.840 | step time: 0.000
train | epoch   6 | Iter:   7854/ 13000 | global iter:   7854/ 13000 | loss: 0.9365 | ds_loss: 0.0000 | lr: 1.7162e-05 | scale: 32768.0000 | micro time: 1.886 | step time: 0.000
train | epoch   6 | Iter:   7855/ 13000 | global iter:   7855/ 13000 | loss: 0.6434 | ds_loss: 0.0000 | lr: 1.7157e-05 | scale: 32768.0000 | micro time: 1.772 | step time: 0.000
train | epoch   6 | Iter:   7856/ 13000 | global iter:   7856/ 13000 | loss: 1.1456 | ds_loss: 0.0000 | lr: 1.7151e-05 | scale: 32768.0000 | micro time: 1.771 | step time: 0.000
train | epoch   6 | Iter:   7857/ 13000 | global iter:   7857/ 13000 | loss: 0.8764 | ds_loss: 0.0000 | lr: 1.7145e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   6 | Iter:   7858/ 13000 | global iter:   7858/ 13000 | loss: 1.2946 | ds_loss: 0.0000 | lr: 1.7140e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   6 | Iter:   7859/ 13000 | global iter:   7859/ 13000 | loss: 0.5653 | ds_loss: 0.0000 | lr: 1.7134e-05 | scale: 32768.0000 | micro time: 1.699 | step time: 0.000
train | epoch   6 | Iter:   7860/ 13000 | global iter:   7860/ 13000 | loss: 0.5953 | ds_loss: 0.0000 | lr: 1.7128e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   7860/ 13000 | global iter:   7860/ 13000 | loss: 0.8666 | ds_loss: 0.0000 | lr: 1.7128e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 1.814
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   7861/ 13000 | global iter:   7861/ 13000 | loss: 0.7232 | ds_loss: 0.0000 | lr: 1.7122e-05 | scale: 32768.0000 | micro time: 1.871 | step time: 0.000
train | epoch   6 | Iter:   7862/ 13000 | global iter:   7862/ 13000 | loss: 0.9761 | ds_loss: 0.0000 | lr: 1.7117e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   6 | Iter:   7863/ 13000 | global iter:   7863/ 13000 | loss: 0.7558 | ds_loss: 0.0000 | lr: 1.7111e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   6 | Iter:   7864/ 13000 | global iter:   7864/ 13000 | loss: 1.3446 | ds_loss: 0.0000 | lr: 1.7105e-05 | scale: 32768.0000 | micro time: 1.648 | step time: 0.000
train | epoch   6 | Iter:   7865/ 13000 | global iter:   7865/ 13000 | loss: 1.0361 | ds_loss: 0.0000 | lr: 1.7100e-05 | scale: 32768.0000 | micro time: 1.845 | step time: 0.000
train | epoch   6 | Iter:   7866/ 13000 | global iter:   7866/ 13000 | loss: 0.7404 | ds_loss: 0.0000 | lr: 1.7094e-05 | scale: 32768.0000 | micro time: 1.860 | step time: 0.000
train | epoch   6 | Iter:   7867/ 13000 | global iter:   7867/ 13000 | loss: 1.0752 | ds_loss: 0.0000 | lr: 1.7088e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   6 | Iter:   7868/ 13000 | global iter:   7868/ 13000 | loss: 0.4070 | ds_loss: 0.0000 | lr: 1.7082e-05 | scale: 32768.0000 | micro time: 1.799 | step time: 0.000
train | epoch   6 | Iter:   7869/ 13000 | global iter:   7869/ 13000 | loss: 0.6987 | ds_loss: 0.0000 | lr: 1.7077e-05 | scale: 32768.0000 | micro time: 1.834 | step time: 0.000
train | epoch   6 | Iter:   7870/ 13000 | global iter:   7870/ 13000 | loss: 1.2016 | ds_loss: 0.0000 | lr: 1.7071e-05 | scale: 32768.0000 | micro time: 1.963 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   7870/ 13000 | global iter:   7870/ 13000 | loss: 0.8959 | ds_loss: 0.0000 | lr: 1.7071e-05 | scale: 32768.0000 | micro time: 1.963 | step time: 1.822
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   7871/ 13000 | global iter:   7871/ 13000 | loss: 0.2699 | ds_loss: 0.0000 | lr: 1.7065e-05 | scale: 32768.0000 | micro time: 1.813 | step time: 0.000
train | epoch   6 | Iter:   7872/ 13000 | global iter:   7872/ 13000 | loss: 1.0353 | ds_loss: 0.0000 | lr: 1.7060e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   6 | Iter:   7873/ 13000 | global iter:   7873/ 13000 | loss: 0.8760 | ds_loss: 0.0000 | lr: 1.7054e-05 | scale: 32768.0000 | micro time: 1.792 | step time: 0.000
train | epoch   6 | Iter:   7874/ 13000 | global iter:   7874/ 13000 | loss: 1.0904 | ds_loss: 0.0000 | lr: 1.7048e-05 | scale: 32768.0000 | micro time: 1.747 | step time: 0.000
train | epoch   6 | Iter:   7875/ 13000 | global iter:   7875/ 13000 | loss: 0.9747 | ds_loss: 0.0000 | lr: 1.7042e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   6 | Iter:   7876/ 13000 | global iter:   7876/ 13000 | loss: 0.5080 | ds_loss: 0.0000 | lr: 1.7037e-05 | scale: 32768.0000 | micro time: 1.772 | step time: 0.000
train | epoch   6 | Iter:   7877/ 13000 | global iter:   7877/ 13000 | loss: 0.7930 | ds_loss: 0.0000 | lr: 1.7031e-05 | scale: 32768.0000 | micro time: 1.854 | step time: 0.000
train | epoch   6 | Iter:   7878/ 13000 | global iter:   7878/ 13000 | loss: 0.9842 | ds_loss: 0.0000 | lr: 1.7025e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   6 | Iter:   7879/ 13000 | global iter:   7879/ 13000 | loss: 0.4009 | ds_loss: 0.0000 | lr: 1.7020e-05 | scale: 32768.0000 | micro time: 1.800 | step time: 0.000
train | epoch   6 | Iter:   7880/ 13000 | global iter:   7880/ 13000 | loss: 0.6230 | ds_loss: 0.0000 | lr: 1.7014e-05 | scale: 32768.0000 | micro time: 1.861 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   7880/ 13000 | global iter:   7880/ 13000 | loss: 0.7555 | ds_loss: 0.0000 | lr: 1.7014e-05 | scale: 32768.0000 | micro time: 1.861 | step time: 1.795
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   7881/ 13000 | global iter:   7881/ 13000 | loss: 0.9606 | ds_loss: 0.0000 | lr: 1.7008e-05 | scale: 32768.0000 | micro time: 1.868 | step time: 0.000
train | epoch   6 | Iter:   7882/ 13000 | global iter:   7882/ 13000 | loss: 1.0921 | ds_loss: 0.0000 | lr: 1.7002e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   6 | Iter:   7883/ 13000 | global iter:   7883/ 13000 | loss: 0.8800 | ds_loss: 0.0000 | lr: 1.6997e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   6 | Iter:   7884/ 13000 | global iter:   7884/ 13000 | loss: 1.2193 | ds_loss: 0.0000 | lr: 1.6991e-05 | scale: 32768.0000 | micro time: 1.935 | step time: 0.000
train | epoch   6 | Iter:   7885/ 13000 | global iter:   7885/ 13000 | loss: 0.5928 | ds_loss: 0.0000 | lr: 1.6985e-05 | scale: 32768.0000 | micro time: 1.726 | step time: 0.000
train | epoch   6 | Iter:   7886/ 13000 | global iter:   7886/ 13000 | loss: 1.0848 | ds_loss: 0.0000 | lr: 1.6980e-05 | scale: 32768.0000 | micro time: 1.783 | step time: 0.000
train | epoch   6 | Iter:   7887/ 13000 | global iter:   7887/ 13000 | loss: 1.0504 | ds_loss: 0.0000 | lr: 1.6974e-05 | scale: 32768.0000 | micro time: 1.860 | step time: 0.000
train | epoch   6 | Iter:   7888/ 13000 | global iter:   7888/ 13000 | loss: 0.9266 | ds_loss: 0.0000 | lr: 1.6968e-05 | scale: 32768.0000 | micro time: 1.877 | step time: 0.000
train | epoch   6 | Iter:   7889/ 13000 | global iter:   7889/ 13000 | loss: 1.3816 | ds_loss: 0.0000 | lr: 1.6963e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   6 | Iter:   7890/ 13000 | global iter:   7890/ 13000 | loss: 1.3790 | ds_loss: 0.0000 | lr: 1.6957e-05 | scale: 32768.0000 | micro time: 1.769 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   7890/ 13000 | global iter:   7890/ 13000 | loss: 1.0567 | ds_loss: 0.0000 | lr: 1.6957e-05 | scale: 32768.0000 | micro time: 1.769 | step time: 1.835
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   7891/ 13000 | global iter:   7891/ 13000 | loss: 1.4840 | ds_loss: 0.0000 | lr: 1.6951e-05 | scale: 32768.0000 | micro time: 1.790 | step time: 0.000
train | epoch   6 | Iter:   7892/ 13000 | global iter:   7892/ 13000 | loss: 1.1921 | ds_loss: 0.0000 | lr: 1.6945e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   6 | Iter:   7893/ 13000 | global iter:   7893/ 13000 | loss: 0.2896 | ds_loss: 0.0000 | lr: 1.6940e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
train | epoch   6 | Iter:   7894/ 13000 | global iter:   7894/ 13000 | loss: 0.9548 | ds_loss: 0.0000 | lr: 1.6934e-05 | scale: 32768.0000 | micro time: 1.779 | step time: 0.000
train | epoch   6 | Iter:   7895/ 13000 | global iter:   7895/ 13000 | loss: 0.9527 | ds_loss: 0.0000 | lr: 1.6928e-05 | scale: 32768.0000 | micro time: 1.765 | step time: 0.000
train | epoch   6 | Iter:   7896/ 13000 | global iter:   7896/ 13000 | loss: 0.8146 | ds_loss: 0.0000 | lr: 1.6923e-05 | scale: 32768.0000 | micro time: 1.821 | step time: 0.000
train | epoch   6 | Iter:   7897/ 13000 | global iter:   7897/ 13000 | loss: 0.9345 | ds_loss: 0.0000 | lr: 1.6917e-05 | scale: 32768.0000 | micro time: 1.793 | step time: 0.000
train | epoch   6 | Iter:   7898/ 13000 | global iter:   7898/ 13000 | loss: 1.5129 | ds_loss: 0.0000 | lr: 1.6911e-05 | scale: 32768.0000 | micro time: 1.780 | step time: 0.000
train | epoch   6 | Iter:   7899/ 13000 | global iter:   7899/ 13000 | loss: 0.8455 | ds_loss: 0.0000 | lr: 1.6906e-05 | scale: 32768.0000 | micro time: 1.868 | step time: 0.000
train | epoch   6 | Iter:   7900/ 13000 | global iter:   7900/ 13000 | loss: 0.9562 | ds_loss: 0.0000 | lr: 1.6900e-05 | scale: 32768.0000 | micro time: 1.822 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   7900/ 13000 | global iter:   7900/ 13000 | loss: 0.9937 | ds_loss: 0.0000 | lr: 1.6900e-05 | scale: 32768.0000 | micro time: 1.822 | step time: 1.804
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   7901/ 13000 | global iter:   7901/ 13000 | loss: 0.7253 | ds_loss: 0.0000 | lr: 1.6894e-05 | scale: 32768.0000 | micro time: 1.761 | step time: 0.000
train | epoch   6 | Iter:   7902/ 13000 | global iter:   7902/ 13000 | loss: 0.6443 | ds_loss: 0.0000 | lr: 1.6888e-05 | scale: 32768.0000 | micro time: 1.731 | step time: 0.000
train | epoch   6 | Iter:   7903/ 13000 | global iter:   7903/ 13000 | loss: 1.7740 | ds_loss: 0.0000 | lr: 1.6883e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   6 | Iter:   7904/ 13000 | global iter:   7904/ 13000 | loss: 0.5504 | ds_loss: 0.0000 | lr: 1.6877e-05 | scale: 32768.0000 | micro time: 1.819 | step time: 0.000
train | epoch   6 | Iter:   7905/ 13000 | global iter:   7905/ 13000 | loss: 0.8225 | ds_loss: 0.0000 | lr: 1.6871e-05 | scale: 32768.0000 | micro time: 1.915 | step time: 0.000
train | epoch   6 | Iter:   7906/ 13000 | global iter:   7906/ 13000 | loss: 1.1320 | ds_loss: 0.0000 | lr: 1.6866e-05 | scale: 32768.0000 | micro time: 1.912 | step time: 0.000
train | epoch   6 | Iter:   7907/ 13000 | global iter:   7907/ 13000 | loss: 0.6608 | ds_loss: 0.0000 | lr: 1.6860e-05 | scale: 32768.0000 | micro time: 1.834 | step time: 0.000
train | epoch   6 | Iter:   7908/ 13000 | global iter:   7908/ 13000 | loss: 0.9319 | ds_loss: 0.0000 | lr: 1.6854e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
train | epoch   6 | Iter:   7909/ 13000 | global iter:   7909/ 13000 | loss: 1.0432 | ds_loss: 0.0000 | lr: 1.6849e-05 | scale: 32768.0000 | micro time: 1.873 | step time: 0.000
train | epoch   6 | Iter:   7910/ 13000 | global iter:   7910/ 13000 | loss: 0.9442 | ds_loss: 0.0000 | lr: 1.6843e-05 | scale: 32768.0000 | micro time: 1.806 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   7910/ 13000 | global iter:   7910/ 13000 | loss: 0.9229 | ds_loss: 0.0000 | lr: 1.6843e-05 | scale: 32768.0000 | micro time: 1.806 | step time: 1.830
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   7911/ 13000 | global iter:   7911/ 13000 | loss: 1.0515 | ds_loss: 0.0000 | lr: 1.6837e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   6 | Iter:   7912/ 13000 | global iter:   7912/ 13000 | loss: 1.2494 | ds_loss: 0.0000 | lr: 1.6831e-05 | scale: 32768.0000 | micro time: 1.812 | step time: 0.000
train | epoch   6 | Iter:   7913/ 13000 | global iter:   7913/ 13000 | loss: 0.5504 | ds_loss: 0.0000 | lr: 1.6826e-05 | scale: 32768.0000 | micro time: 1.855 | step time: 0.000
train | epoch   6 | Iter:   7914/ 13000 | global iter:   7914/ 13000 | loss: 1.0576 | ds_loss: 0.0000 | lr: 1.6820e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
train | epoch   6 | Iter:   7915/ 13000 | global iter:   7915/ 13000 | loss: 1.2454 | ds_loss: 0.0000 | lr: 1.6814e-05 | scale: 32768.0000 | micro time: 1.673 | step time: 0.000
train | epoch   6 | Iter:   7916/ 13000 | global iter:   7916/ 13000 | loss: 1.2905 | ds_loss: 0.0000 | lr: 1.6809e-05 | scale: 32768.0000 | micro time: 1.813 | step time: 0.000
train | epoch   6 | Iter:   7917/ 13000 | global iter:   7917/ 13000 | loss: 1.0236 | ds_loss: 0.0000 | lr: 1.6803e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   6 | Iter:   7918/ 13000 | global iter:   7918/ 13000 | loss: 0.7797 | ds_loss: 0.0000 | lr: 1.6797e-05 | scale: 32768.0000 | micro time: 1.727 | step time: 0.000
train | epoch   6 | Iter:   7919/ 13000 | global iter:   7919/ 13000 | loss: 1.4574 | ds_loss: 0.0000 | lr: 1.6792e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   6 | Iter:   7920/ 13000 | global iter:   7920/ 13000 | loss: 1.0341 | ds_loss: 0.0000 | lr: 1.6786e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   7920/ 13000 | global iter:   7920/ 13000 | loss: 1.0740 | ds_loss: 0.0000 | lr: 1.6786e-05 | scale: 32768.0000 | micro time: 1.815 | step time: 1.802
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   7921/ 13000 | global iter:   7921/ 13000 | loss: 1.2171 | ds_loss: 0.0000 | lr: 1.6780e-05 | scale: 32768.0000 | micro time: 1.709 | step time: 0.000
train | epoch   6 | Iter:   7922/ 13000 | global iter:   7922/ 13000 | loss: 1.0772 | ds_loss: 0.0000 | lr: 1.6775e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   6 | Iter:   7923/ 13000 | global iter:   7923/ 13000 | loss: 1.2968 | ds_loss: 0.0000 | lr: 1.6769e-05 | scale: 32768.0000 | micro time: 1.739 | step time: 0.000
train | epoch   6 | Iter:   7924/ 13000 | global iter:   7924/ 13000 | loss: 0.3836 | ds_loss: 0.0000 | lr: 1.6763e-05 | scale: 32768.0000 | micro time: 1.899 | step time: 0.000
train | epoch   6 | Iter:   7925/ 13000 | global iter:   7925/ 13000 | loss: 0.4598 | ds_loss: 0.0000 | lr: 1.6758e-05 | scale: 32768.0000 | micro time: 1.764 | step time: 0.000
train | epoch   6 | Iter:   7926/ 13000 | global iter:   7926/ 13000 | loss: 1.2388 | ds_loss: 0.0000 | lr: 1.6752e-05 | scale: 32768.0000 | micro time: 1.856 | step time: 0.000
train | epoch   6 | Iter:   7927/ 13000 | global iter:   7927/ 13000 | loss: 0.9196 | ds_loss: 0.0000 | lr: 1.6746e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   6 | Iter:   7928/ 13000 | global iter:   7928/ 13000 | loss: 0.7408 | ds_loss: 0.0000 | lr: 1.6740e-05 | scale: 32768.0000 | micro time: 1.795 | step time: 0.000
train | epoch   6 | Iter:   7929/ 13000 | global iter:   7929/ 13000 | loss: 1.2289 | ds_loss: 0.0000 | lr: 1.6735e-05 | scale: 32768.0000 | micro time: 1.763 | step time: 0.000
train | epoch   6 | Iter:   7930/ 13000 | global iter:   7930/ 13000 | loss: 1.0790 | ds_loss: 0.0000 | lr: 1.6729e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   7930/ 13000 | global iter:   7930/ 13000 | loss: 0.9642 | ds_loss: 0.0000 | lr: 1.6729e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 1.808
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   7931/ 13000 | global iter:   7931/ 13000 | loss: 0.9392 | ds_loss: 0.0000 | lr: 1.6723e-05 | scale: 32768.0000 | micro time: 1.831 | step time: 0.000
train | epoch   6 | Iter:   7932/ 13000 | global iter:   7932/ 13000 | loss: 1.4053 | ds_loss: 0.0000 | lr: 1.6718e-05 | scale: 32768.0000 | micro time: 1.852 | step time: 0.000
train | epoch   6 | Iter:   7933/ 13000 | global iter:   7933/ 13000 | loss: 0.9618 | ds_loss: 0.0000 | lr: 1.6712e-05 | scale: 32768.0000 | micro time: 1.839 | step time: 0.000
train | epoch   6 | Iter:   7934/ 13000 | global iter:   7934/ 13000 | loss: 1.2313 | ds_loss: 0.0000 | lr: 1.6706e-05 | scale: 32768.0000 | micro time: 1.879 | step time: 0.000
train | epoch   6 | Iter:   7935/ 13000 | global iter:   7935/ 13000 | loss: 0.5471 | ds_loss: 0.0000 | lr: 1.6701e-05 | scale: 32768.0000 | micro time: 1.841 | step time: 0.000
train | epoch   6 | Iter:   7936/ 13000 | global iter:   7936/ 13000 | loss: 0.3460 | ds_loss: 0.0000 | lr: 1.6695e-05 | scale: 32768.0000 | micro time: 1.873 | step time: 0.000
train | epoch   6 | Iter:   7937/ 13000 | global iter:   7937/ 13000 | loss: 1.0810 | ds_loss: 0.0000 | lr: 1.6689e-05 | scale: 32768.0000 | micro time: 1.903 | step time: 0.000
train | epoch   6 | Iter:   7938/ 13000 | global iter:   7938/ 13000 | loss: 1.4320 | ds_loss: 0.0000 | lr: 1.6684e-05 | scale: 32768.0000 | micro time: 1.822 | step time: 0.000
train | epoch   6 | Iter:   7939/ 13000 | global iter:   7939/ 13000 | loss: 1.0715 | ds_loss: 0.0000 | lr: 1.6678e-05 | scale: 32768.0000 | micro time: 1.828 | step time: 0.000
train | epoch   6 | Iter:   7940/ 13000 | global iter:   7940/ 13000 | loss: 1.1125 | ds_loss: 0.0000 | lr: 1.6672e-05 | scale: 32768.0000 | micro time: 1.744 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   7940/ 13000 | global iter:   7940/ 13000 | loss: 1.0128 | ds_loss: 0.0000 | lr: 1.6672e-05 | scale: 32768.0000 | micro time: 1.744 | step time: 1.841
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   7941/ 13000 | global iter:   7941/ 13000 | loss: 1.2343 | ds_loss: 0.0000 | lr: 1.6667e-05 | scale: 32768.0000 | micro time: 1.896 | step time: 0.000
train | epoch   6 | Iter:   7942/ 13000 | global iter:   7942/ 13000 | loss: 1.0903 | ds_loss: 0.0000 | lr: 1.6661e-05 | scale: 32768.0000 | micro time: 1.759 | step time: 0.000
train | epoch   6 | Iter:   7943/ 13000 | global iter:   7943/ 13000 | loss: 0.9764 | ds_loss: 0.0000 | lr: 1.6655e-05 | scale: 32768.0000 | micro time: 1.915 | step time: 0.000
train | epoch   6 | Iter:   7944/ 13000 | global iter:   7944/ 13000 | loss: 1.2608 | ds_loss: 0.0000 | lr: 1.6650e-05 | scale: 32768.0000 | micro time: 1.782 | step time: 0.000
train | epoch   6 | Iter:   7945/ 13000 | global iter:   7945/ 13000 | loss: 0.8212 | ds_loss: 0.0000 | lr: 1.6644e-05 | scale: 32768.0000 | micro time: 1.785 | step time: 0.000
train | epoch   6 | Iter:   7946/ 13000 | global iter:   7946/ 13000 | loss: 1.0628 | ds_loss: 0.0000 | lr: 1.6638e-05 | scale: 32768.0000 | micro time: 1.858 | step time: 0.000
train | epoch   6 | Iter:   7947/ 13000 | global iter:   7947/ 13000 | loss: 1.2193 | ds_loss: 0.0000 | lr: 1.6633e-05 | scale: 32768.0000 | micro time: 1.679 | step time: 0.000
train | epoch   6 | Iter:   7948/ 13000 | global iter:   7948/ 13000 | loss: 0.9515 | ds_loss: 0.0000 | lr: 1.6627e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   6 | Iter:   7949/ 13000 | global iter:   7949/ 13000 | loss: 0.7608 | ds_loss: 0.0000 | lr: 1.6621e-05 | scale: 32768.0000 | micro time: 1.699 | step time: 0.000
train | epoch   6 | Iter:   7950/ 13000 | global iter:   7950/ 13000 | loss: 0.5253 | ds_loss: 0.0000 | lr: 1.6616e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   7950/ 13000 | global iter:   7950/ 13000 | loss: 0.9903 | ds_loss: 0.0000 | lr: 1.6616e-05 | scale: 32768.0000 | micro time: 1.787 | step time: 1.802
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   7951/ 13000 | global iter:   7951/ 13000 | loss: 1.0265 | ds_loss: 0.0000 | lr: 1.6610e-05 | scale: 32768.0000 | micro time: 1.726 | step time: 0.000
train | epoch   6 | Iter:   7952/ 13000 | global iter:   7952/ 13000 | loss: 1.3723 | ds_loss: 0.0000 | lr: 1.6604e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
train | epoch   6 | Iter:   7953/ 13000 | global iter:   7953/ 13000 | loss: 0.9839 | ds_loss: 0.0000 | lr: 1.6598e-05 | scale: 32768.0000 | micro time: 1.869 | step time: 0.000
train | epoch   6 | Iter:   7954/ 13000 | global iter:   7954/ 13000 | loss: 1.4559 | ds_loss: 0.0000 | lr: 1.6593e-05 | scale: 32768.0000 | micro time: 1.797 | step time: 0.000
train | epoch   6 | Iter:   7955/ 13000 | global iter:   7955/ 13000 | loss: 1.2141 | ds_loss: 0.0000 | lr: 1.6587e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   6 | Iter:   7956/ 13000 | global iter:   7956/ 13000 | loss: 1.0362 | ds_loss: 0.0000 | lr: 1.6581e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   6 | Iter:   7957/ 13000 | global iter:   7957/ 13000 | loss: 0.9884 | ds_loss: 0.0000 | lr: 1.6576e-05 | scale: 32768.0000 | micro time: 1.817 | step time: 0.000
train | epoch   6 | Iter:   7958/ 13000 | global iter:   7958/ 13000 | loss: 0.8247 | ds_loss: 0.0000 | lr: 1.6570e-05 | scale: 32768.0000 | micro time: 1.877 | step time: 0.000
train | epoch   6 | Iter:   7959/ 13000 | global iter:   7959/ 13000 | loss: 0.8876 | ds_loss: 0.0000 | lr: 1.6564e-05 | scale: 32768.0000 | micro time: 1.867 | step time: 0.000
train | epoch   6 | Iter:   7960/ 13000 | global iter:   7960/ 13000 | loss: 0.6446 | ds_loss: 0.0000 | lr: 1.6559e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   7960/ 13000 | global iter:   7960/ 13000 | loss: 1.0434 | ds_loss: 0.0000 | lr: 1.6559e-05 | scale: 32768.0000 | micro time: 1.847 | step time: 1.832
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   7961/ 13000 | global iter:   7961/ 13000 | loss: 1.4423 | ds_loss: 0.0000 | lr: 1.6553e-05 | scale: 32768.0000 | micro time: 1.853 | step time: 0.000
train | epoch   6 | Iter:   7962/ 13000 | global iter:   7962/ 13000 | loss: 0.6703 | ds_loss: 0.0000 | lr: 1.6547e-05 | scale: 32768.0000 | micro time: 1.835 | step time: 0.000
train | epoch   6 | Iter:   7963/ 13000 | global iter:   7963/ 13000 | loss: 0.7157 | ds_loss: 0.0000 | lr: 1.6542e-05 | scale: 32768.0000 | micro time: 1.791 | step time: 0.000
train | epoch   6 | Iter:   7964/ 13000 | global iter:   7964/ 13000 | loss: 0.5943 | ds_loss: 0.0000 | lr: 1.6536e-05 | scale: 32768.0000 | micro time: 1.841 | step time: 0.000
train | epoch   6 | Iter:   7965/ 13000 | global iter:   7965/ 13000 | loss: 0.9948 | ds_loss: 0.0000 | lr: 1.6530e-05 | scale: 32768.0000 | micro time: 1.731 | step time: 0.000
train | epoch   6 | Iter:   7966/ 13000 | global iter:   7966/ 13000 | loss: 1.4714 | ds_loss: 0.0000 | lr: 1.6525e-05 | scale: 32768.0000 | micro time: 1.775 | step time: 0.000
train | epoch   6 | Iter:   7967/ 13000 | global iter:   7967/ 13000 | loss: 0.6469 | ds_loss: 0.0000 | lr: 1.6519e-05 | scale: 32768.0000 | micro time: 1.889 | step time: 0.000
train | epoch   6 | Iter:   7968/ 13000 | global iter:   7968/ 13000 | loss: 1.2753 | ds_loss: 0.0000 | lr: 1.6513e-05 | scale: 32768.0000 | micro time: 1.784 | step time: 0.000
train | epoch   6 | Iter:   7969/ 13000 | global iter:   7969/ 13000 | loss: 0.6150 | ds_loss: 0.0000 | lr: 1.6508e-05 | scale: 32768.0000 | micro time: 1.743 | step time: 0.000
train | epoch   6 | Iter:   7970/ 13000 | global iter:   7970/ 13000 | loss: 1.2408 | ds_loss: 0.0000 | lr: 1.6502e-05 | scale: 32768.0000 | micro time: 1.785 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   7970/ 13000 | global iter:   7970/ 13000 | loss: 0.9667 | ds_loss: 0.0000 | lr: 1.6502e-05 | scale: 32768.0000 | micro time: 1.785 | step time: 1.803
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   7971/ 13000 | global iter:   7971/ 13000 | loss: 0.7319 | ds_loss: 0.0000 | lr: 1.6496e-05 | scale: 32768.0000 | micro time: 1.824 | step time: 0.000
train | epoch   6 | Iter:   7972/ 13000 | global iter:   7972/ 13000 | loss: 0.4629 | ds_loss: 0.0000 | lr: 1.6491e-05 | scale: 32768.0000 | micro time: 1.879 | step time: 0.000
train | epoch   6 | Iter:   7973/ 13000 | global iter:   7973/ 13000 | loss: 0.6797 | ds_loss: 0.0000 | lr: 1.6485e-05 | scale: 32768.0000 | micro time: 1.853 | step time: 0.000
train | epoch   6 | Iter:   7974/ 13000 | global iter:   7974/ 13000 | loss: 1.3871 | ds_loss: 0.0000 | lr: 1.6479e-05 | scale: 32768.0000 | micro time: 1.826 | step time: 0.000
train | epoch   6 | Iter:   7975/ 13000 | global iter:   7975/ 13000 | loss: 1.2894 | ds_loss: 0.0000 | lr: 1.6474e-05 | scale: 32768.0000 | micro time: 1.829 | step time: 0.000
train | epoch   6 | Iter:   7976/ 13000 | global iter:   7976/ 13000 | loss: 1.0103 | ds_loss: 0.0000 | lr: 1.6468e-05 | scale: 32768.0000 | micro time: 1.810 | step time: 0.000
train | epoch   6 | Iter:   7977/ 13000 | global iter:   7977/ 13000 | loss: 0.6483 | ds_loss: 0.0000 | lr: 1.6462e-05 | scale: 32768.0000 | micro time: 1.780 | step time: 0.000
train | epoch   6 | Iter:   7978/ 13000 | global iter:   7978/ 13000 | loss: 0.7516 | ds_loss: 0.0000 | lr: 1.6457e-05 | scale: 32768.0000 | micro time: 1.915 | step time: 0.000
train | epoch   6 | Iter:   7979/ 13000 | global iter:   7979/ 13000 | loss: 0.7752 | ds_loss: 0.0000 | lr: 1.6451e-05 | scale: 32768.0000 | micro time: 1.863 | step time: 0.000
train | epoch   6 | Iter:   7980/ 13000 | global iter:   7980/ 13000 | loss: 0.7305 | ds_loss: 0.0000 | lr: 1.6445e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   7980/ 13000 | global iter:   7980/ 13000 | loss: 0.8467 | ds_loss: 0.0000 | lr: 1.6445e-05 | scale: 32768.0000 | micro time: 1.843 | step time: 1.842
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   7981/ 13000 | global iter:   7981/ 13000 | loss: 1.1118 | ds_loss: 0.0000 | lr: 1.6440e-05 | scale: 32768.0000 | micro time: 1.827 | step time: 0.000
train | epoch   6 | Iter:   7982/ 13000 | global iter:   7982/ 13000 | loss: 1.5226 | ds_loss: 0.0000 | lr: 1.6434e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
train | epoch   6 | Iter:   7983/ 13000 | global iter:   7983/ 13000 | loss: 1.1433 | ds_loss: 0.0000 | lr: 1.6429e-05 | scale: 32768.0000 | micro time: 1.795 | step time: 0.000
train | epoch   6 | Iter:   7984/ 13000 | global iter:   7984/ 13000 | loss: 1.4726 | ds_loss: 0.0000 | lr: 1.6423e-05 | scale: 32768.0000 | micro time: 1.742 | step time: 0.000
train | epoch   6 | Iter:   7985/ 13000 | global iter:   7985/ 13000 | loss: 1.0067 | ds_loss: 0.0000 | lr: 1.6417e-05 | scale: 32768.0000 | micro time: 1.916 | step time: 0.000
train | epoch   6 | Iter:   7986/ 13000 | global iter:   7986/ 13000 | loss: 0.6163 | ds_loss: 0.0000 | lr: 1.6412e-05 | scale: 32768.0000 | micro time: 1.743 | step time: 0.000
train | epoch   6 | Iter:   7987/ 13000 | global iter:   7987/ 13000 | loss: 1.1541 | ds_loss: 0.0000 | lr: 1.6406e-05 | scale: 32768.0000 | micro time: 1.866 | step time: 0.000
train | epoch   6 | Iter:   7988/ 13000 | global iter:   7988/ 13000 | loss: 0.8371 | ds_loss: 0.0000 | lr: 1.6400e-05 | scale: 32768.0000 | micro time: 1.828 | step time: 0.000
train | epoch   6 | Iter:   7989/ 13000 | global iter:   7989/ 13000 | loss: 0.8701 | ds_loss: 0.0000 | lr: 1.6395e-05 | scale: 32768.0000 | micro time: 1.875 | step time: 0.000
train | epoch   6 | Iter:   7990/ 13000 | global iter:   7990/ 13000 | loss: 0.7192 | ds_loss: 0.0000 | lr: 1.6389e-05 | scale: 32768.0000 | micro time: 1.739 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   7990/ 13000 | global iter:   7990/ 13000 | loss: 1.0454 | ds_loss: 0.0000 | lr: 1.6389e-05 | scale: 32768.0000 | micro time: 1.739 | step time: 1.813
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   7991/ 13000 | global iter:   7991/ 13000 | loss: 0.8879 | ds_loss: 0.0000 | lr: 1.6383e-05 | scale: 32768.0000 | micro time: 1.818 | step time: 0.000
train | epoch   6 | Iter:   7992/ 13000 | global iter:   7992/ 13000 | loss: 1.2658 | ds_loss: 0.0000 | lr: 1.6378e-05 | scale: 32768.0000 | micro time: 1.870 | step time: 0.000
train | epoch   6 | Iter:   7993/ 13000 | global iter:   7993/ 13000 | loss: 0.4589 | ds_loss: 0.0000 | lr: 1.6372e-05 | scale: 32768.0000 | micro time: 1.807 | step time: 0.000
train | epoch   6 | Iter:   7994/ 13000 | global iter:   7994/ 13000 | loss: 0.8113 | ds_loss: 0.0000 | lr: 1.6366e-05 | scale: 32768.0000 | micro time: 1.792 | step time: 0.000
train | epoch   6 | Iter:   7995/ 13000 | global iter:   7995/ 13000 | loss: 1.3805 | ds_loss: 0.0000 | lr: 1.6361e-05 | scale: 32768.0000 | micro time: 1.802 | step time: 0.000
train | epoch   6 | Iter:   7996/ 13000 | global iter:   7996/ 13000 | loss: 0.4533 | ds_loss: 0.0000 | lr: 1.6355e-05 | scale: 32768.0000 | micro time: 1.808 | step time: 0.000
train | epoch   6 | Iter:   7997/ 13000 | global iter:   7997/ 13000 | loss: 0.7111 | ds_loss: 0.0000 | lr: 1.6349e-05 | scale: 32768.0000 | micro time: 1.790 | step time: 0.000
train | epoch   6 | Iter:   7998/ 13000 | global iter:   7998/ 13000 | loss: 1.1045 | ds_loss: 0.0000 | lr: 1.6344e-05 | scale: 32768.0000 | micro time: 1.853 | step time: 0.000
train | epoch   6 | Iter:   7999/ 13000 | global iter:   7999/ 13000 | loss: 0.8911 | ds_loss: 0.0000 | lr: 1.6338e-05 | scale: 32768.0000 | micro time: 1.853 | step time: 0.000
train | epoch   6 | Iter:   8000/ 13000 | global iter:   8000/ 13000 | loss: 1.0282 | ds_loss: 0.0000 | lr: 1.6332e-05 | scale: 32768.0000 | micro time: 1.821 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8000/ 13000 | global iter:   8000/ 13000 | loss: 0.8993 | ds_loss: 0.0000 | lr: 1.6332e-05 | scale: 32768.0000 | micro time: 1.821 | step time: 1.821
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
Model save to ./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1/8000
dp size 4
0/63
1/63
2/63
3/63
4/63
5/63
6/63
7/63
8/63
9/63
10/63
11/63
12/63
13/63
14/63
15/63
Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]Evaluating:   2%|         | 1/63 [00:10<10:34, 10.24s/it]Evaluating:   3%|         | 2/63 [00:28<14:55, 14.67s/it]Evaluating:   5%|         | 3/63 [00:43<15:13, 15.22s/it]Evaluating:   6%|         | 4/63 [00:51<12:11, 12.40s/it]Evaluating:   8%|         | 5/63 [01:05<12:30, 12.93s/it]Evaluating:  10%|         | 6/63 [01:22<13:29, 14.20s/it]Evaluating:  11%|         | 7/63 [01:40<14:29, 15.53s/it]Evaluating:  13%|        | 8/63 [01:58<14:55, 16.29s/it]Evaluating:  14%|        | 9/63 [02:15<14:50, 16.49s/it]Evaluating:  16%|        | 10/63 [02:27<13:12, 14.95s/it]Evaluating:  17%|        | 11/63 [02:44<13:28, 15.54s/it]Evaluating:  19%|        | 12/63 [03:02<13:52, 16.32s/it]Evaluating:  21%|        | 13/63 [03:18<13:37, 16.34s/it]Evaluating:  22%|       | 14/63 [03:33<13:08, 16.09s/it]Evaluating:  24%|       | 15/63 [03:51<13:16, 16.60s/it]Evaluating:  25%|     16/63
17/63
18/63
19/63
20/63
21/63
22/63
23/63
24/63
25/63
26/63
27/63
28/63
29/63
30/63
  | 16/63 [04:09<13:10, 16.83s/it]Evaluating:  27%|       | 17/63 [04:25<12:49, 16.74s/it]Evaluating:  29%|       | 18/63 [04:43<12:46, 17.03s/it]Evaluating:  30%|       | 19/63 [05:00<12:26, 16.98s/it]Evaluating:  32%|      | 20/63 [05:18<12:24, 17.31s/it]Evaluating:  33%|      | 21/63 [05:36<12:16, 17.53s/it]Evaluating:  35%|      | 22/63 [05:53<11:53, 17.40s/it]Evaluating:  37%|      | 23/63 [06:10<11:32, 17.32s/it]Evaluating:  38%|      | 24/63 [06:26<10:57, 16.87s/it]Evaluating:  40%|      | 25/63 [06:42<10:34, 16.69s/it]Evaluating:  41%|     | 26/63 [06:59<10:14, 16.61s/it]Evaluating:  43%|     | 27/63 [07:15<09:55, 16.55s/it]Evaluating:  44%|     | 28/63 [07:31<09:33, 16.39s/it]Evaluating:  46%|     | 29/63 [07:49<09:38, 17.00s/it]Evaluating:  48%|     | 30/63 [08:05<09:03, 16.48s/it]Evaluating:  49%| 31/63
32/63
33/63
34/63
35/63
36/63
37/63
38/63
39/63
40/63
41/63
42/63
43/63
44/63
    | 31/63 [08:20<08:32, 16.01s/it]Evaluating:  51%|     | 32/63 [08:35<08:09, 15.80s/it]Evaluating:  52%|    | 33/63 [08:53<08:12, 16.42s/it]Evaluating:  54%|    | 34/63 [09:08<07:47, 16.12s/it]Evaluating:  56%|    | 35/63 [09:26<07:47, 16.70s/it]Evaluating:  57%|    | 36/63 [09:44<07:36, 16.89s/it]Evaluating:  59%|    | 37/63 [10:01<07:24, 17.11s/it]Evaluating:  60%|    | 38/63 [10:20<07:17, 17.51s/it]Evaluating:  62%|   | 39/63 [10:37<06:58, 17.45s/it]Evaluating:  63%|   | 40/63 [10:54<06:41, 17.45s/it]Evaluating:  65%|   | 41/63 [11:11<06:15, 17.07s/it]Evaluating:  67%|   | 42/63 [11:28<05:58, 17.05s/it]Evaluating:  68%|   | 43/63 [11:45<05:41, 17.06s/it]Evaluating:  70%|   | 44/63 [12:01<05:19, 16.80s/it]Evaluating:  71%|45/63
46/63
47/63
Distributed index stop interation. Idx: 779 Total_length: 777
Distributed index stop interation. Idx: 777 Total_length: 777
Distributed index stop interation. Idx: 778 Total_length: 777
Distributed index stop interation. Idx: 780 Total_length: 777
  | 45/63 [12:14<04:44, 15.80s/it]Evaluating:  73%|  | 46/63 [12:32<04:37, 16.32s/it]Evaluating:  75%|  | 47/63 [12:49<04:24, 16.52s/it]Evaluating:  76%|  | 48/63 [13:07<04:16, 17.13s/it]Evaluating:  76%|  | 48/63 [13:07<04:06, 16.42s/it]
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1/eval/6
dev | avg_loss: 2.3100179036458335 | {'exact_match': 1.1719, 'rougeL': 20.5669}
train | epoch   6 | Iter:   8001/ 13000 | global iter:   8001/ 13000 | loss: 0.9837 | ds_loss: 0.0000 | lr: 1.6327e-05 | scale: 32768.0000 | micro time: 1.861 | step time: 0.000
train | epoch   6 | Iter:   8002/ 13000 | global iter:   8002/ 13000 | loss: 0.8266 | ds_loss: 0.0000 | lr: 1.6321e-05 | scale: 32768.0000 | micro time: 1.803 | step time: 0.000
[2025-04-19 15:35:44,847] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
train | epoch   6 | Iter:   8003/ 13000 | global iter:   8003/ 13000 | loss: 0.9714 | ds_loss: 0.0000 | lr: 1.6321e-05 | scale: 16384.0000 | micro time: 1.499 | step time: 0.000
train | epoch   6 | Iter:   8004/ 13000 | global iter:   8004/ 13000 | loss: 1.1926 | ds_loss: 0.0000 | lr: 1.6315e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   6 | Iter:   8005/ 13000 | global iter:   8005/ 13000 | loss: 1.2189 | ds_loss: 0.0000 | lr: 1.6310e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   6 | Iter:   8006/ 13000 | global iter:   8006/ 13000 | loss: 0.9774 | ds_loss: 0.0000 | lr: 1.6304e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   6 | Iter:   8007/ 13000 | global iter:   8007/ 13000 | loss: 1.0283 | ds_loss: 0.0000 | lr: 1.6299e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   6 | Iter:   8008/ 13000 | global iter:   8008/ 13000 | loss: 0.6863 | ds_loss: 0.0000 | lr: 1.6293e-05 | scale: 16384.0000 | micro time: 1.923 | step time: 0.000
train | epoch   6 | Iter:   8009/ 13000 | global iter:   8009/ 13000 | loss: 1.1696 | ds_loss: 0.0000 | lr: 1.6287e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   6 | Iter:   8010/ 13000 | global iter:   8010/ 13000 | loss: 0.8305 | ds_loss: 0.0000 | lr: 1.6282e-05 | scale: 16384.0000 | micro time: 1.693 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8010/ 13000 | global iter:   8010/ 13000 | loss: 0.9885 | ds_loss: 0.0000 | lr: 1.6282e-05 | scale: 16384.0000 | micro time: 1.693 | step time: 1.797
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8011/ 13000 | global iter:   8011/ 13000 | loss: 1.3093 | ds_loss: 0.0000 | lr: 1.6276e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   6 | Iter:   8012/ 13000 | global iter:   8012/ 13000 | loss: 0.9565 | ds_loss: 0.0000 | lr: 1.6270e-05 | scale: 16384.0000 | micro time: 1.903 | step time: 0.000
train | epoch   6 | Iter:   8013/ 13000 | global iter:   8013/ 13000 | loss: 1.4841 | ds_loss: 0.0000 | lr: 1.6265e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   6 | Iter:   8014/ 13000 | global iter:   8014/ 13000 | loss: 1.1086 | ds_loss: 0.0000 | lr: 1.6259e-05 | scale: 16384.0000 | micro time: 1.895 | step time: 0.000
train | epoch   6 | Iter:   8015/ 13000 | global iter:   8015/ 13000 | loss: 0.4510 | ds_loss: 0.0000 | lr: 1.6253e-05 | scale: 16384.0000 | micro time: 1.715 | step time: 0.000
train | epoch   6 | Iter:   8016/ 13000 | global iter:   8016/ 13000 | loss: 1.2496 | ds_loss: 0.0000 | lr: 1.6248e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   6 | Iter:   8017/ 13000 | global iter:   8017/ 13000 | loss: 0.9690 | ds_loss: 0.0000 | lr: 1.6242e-05 | scale: 16384.0000 | micro time: 1.761 | step time: 0.000
train | epoch   6 | Iter:   8018/ 13000 | global iter:   8018/ 13000 | loss: 1.2286 | ds_loss: 0.0000 | lr: 1.6236e-05 | scale: 16384.0000 | micro time: 1.752 | step time: 0.000
train | epoch   6 | Iter:   8019/ 13000 | global iter:   8019/ 13000 | loss: 0.5317 | ds_loss: 0.0000 | lr: 1.6231e-05 | scale: 16384.0000 | micro time: 1.840 | step time: 0.000
train | epoch   6 | Iter:   8020/ 13000 | global iter:   8020/ 13000 | loss: 1.1885 | ds_loss: 0.0000 | lr: 1.6225e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8020/ 13000 | global iter:   8020/ 13000 | loss: 1.0477 | ds_loss: 0.0000 | lr: 1.6225e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 1.825
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8021/ 13000 | global iter:   8021/ 13000 | loss: 1.1435 | ds_loss: 0.0000 | lr: 1.6220e-05 | scale: 16384.0000 | micro time: 1.808 | step time: 0.000
train | epoch   6 | Iter:   8022/ 13000 | global iter:   8022/ 13000 | loss: 0.7832 | ds_loss: 0.0000 | lr: 1.6214e-05 | scale: 16384.0000 | micro time: 1.841 | step time: 0.000
train | epoch   6 | Iter:   8023/ 13000 | global iter:   8023/ 13000 | loss: 1.2037 | ds_loss: 0.0000 | lr: 1.6208e-05 | scale: 16384.0000 | micro time: 1.796 | step time: 0.000
train | epoch   6 | Iter:   8024/ 13000 | global iter:   8024/ 13000 | loss: 1.4399 | ds_loss: 0.0000 | lr: 1.6203e-05 | scale: 16384.0000 | micro time: 1.846 | step time: 0.000
train | epoch   6 | Iter:   8025/ 13000 | global iter:   8025/ 13000 | loss: 0.8039 | ds_loss: 0.0000 | lr: 1.6197e-05 | scale: 16384.0000 | micro time: 1.718 | step time: 0.000
train | epoch   6 | Iter:   8026/ 13000 | global iter:   8026/ 13000 | loss: 0.8075 | ds_loss: 0.0000 | lr: 1.6191e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   6 | Iter:   8027/ 13000 | global iter:   8027/ 13000 | loss: 1.2724 | ds_loss: 0.0000 | lr: 1.6186e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   6 | Iter:   8028/ 13000 | global iter:   8028/ 13000 | loss: 1.4863 | ds_loss: 0.0000 | lr: 1.6180e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   6 | Iter:   8029/ 13000 | global iter:   8029/ 13000 | loss: 1.0802 | ds_loss: 0.0000 | lr: 1.6174e-05 | scale: 16384.0000 | micro time: 1.757 | step time: 0.000
train | epoch   6 | Iter:   8030/ 13000 | global iter:   8030/ 13000 | loss: 0.8691 | ds_loss: 0.0000 | lr: 1.6169e-05 | scale: 16384.0000 | micro time: 1.909 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8030/ 13000 | global iter:   8030/ 13000 | loss: 1.0890 | ds_loss: 0.0000 | lr: 1.6169e-05 | scale: 16384.0000 | micro time: 1.909 | step time: 1.808
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8031/ 13000 | global iter:   8031/ 13000 | loss: 0.7624 | ds_loss: 0.0000 | lr: 1.6163e-05 | scale: 16384.0000 | micro time: 1.695 | step time: 0.000
train | epoch   6 | Iter:   8032/ 13000 | global iter:   8032/ 13000 | loss: 1.4681 | ds_loss: 0.0000 | lr: 1.6158e-05 | scale: 16384.0000 | micro time: 1.833 | step time: 0.000
train | epoch   6 | Iter:   8033/ 13000 | global iter:   8033/ 13000 | loss: 0.6654 | ds_loss: 0.0000 | lr: 1.6152e-05 | scale: 16384.0000 | micro time: 1.842 | step time: 0.000
train | epoch   6 | Iter:   8034/ 13000 | global iter:   8034/ 13000 | loss: 0.9237 | ds_loss: 0.0000 | lr: 1.6146e-05 | scale: 16384.0000 | micro time: 1.828 | step time: 0.000
train | epoch   6 | Iter:   8035/ 13000 | global iter:   8035/ 13000 | loss: 0.5523 | ds_loss: 0.0000 | lr: 1.6141e-05 | scale: 16384.0000 | micro time: 1.857 | step time: 0.000
train | epoch   6 | Iter:   8036/ 13000 | global iter:   8036/ 13000 | loss: 1.2720 | ds_loss: 0.0000 | lr: 1.6135e-05 | scale: 16384.0000 | micro time: 1.879 | step time: 0.000
train | epoch   6 | Iter:   8037/ 13000 | global iter:   8037/ 13000 | loss: 0.7241 | ds_loss: 0.0000 | lr: 1.6129e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   6 | Iter:   8038/ 13000 | global iter:   8038/ 13000 | loss: 1.0855 | ds_loss: 0.0000 | lr: 1.6124e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 0.000
train | epoch   6 | Iter:   8039/ 13000 | global iter:   8039/ 13000 | loss: 0.3052 | ds_loss: 0.0000 | lr: 1.6118e-05 | scale: 16384.0000 | micro time: 1.732 | step time: 0.000
train | epoch   6 | Iter:   8040/ 13000 | global iter:   8040/ 13000 | loss: 0.8210 | ds_loss: 0.0000 | lr: 1.6112e-05 | scale: 16384.0000 | micro time: 1.813 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8040/ 13000 | global iter:   8040/ 13000 | loss: 0.8580 | ds_loss: 0.0000 | lr: 1.6112e-05 | scale: 16384.0000 | micro time: 1.813 | step time: 1.799
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8041/ 13000 | global iter:   8041/ 13000 | loss: 0.7820 | ds_loss: 0.0000 | lr: 1.6107e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   6 | Iter:   8042/ 13000 | global iter:   8042/ 13000 | loss: 1.4510 | ds_loss: 0.0000 | lr: 1.6101e-05 | scale: 16384.0000 | micro time: 1.722 | step time: 0.000
train | epoch   6 | Iter:   8043/ 13000 | global iter:   8043/ 13000 | loss: 1.0663 | ds_loss: 0.0000 | lr: 1.6096e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   6 | Iter:   8044/ 13000 | global iter:   8044/ 13000 | loss: 0.7164 | ds_loss: 0.0000 | lr: 1.6090e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   6 | Iter:   8045/ 13000 | global iter:   8045/ 13000 | loss: 0.6179 | ds_loss: 0.0000 | lr: 1.6084e-05 | scale: 16384.0000 | micro time: 1.842 | step time: 0.000
train | epoch   6 | Iter:   8046/ 13000 | global iter:   8046/ 13000 | loss: 0.9979 | ds_loss: 0.0000 | lr: 1.6079e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   6 | Iter:   8047/ 13000 | global iter:   8047/ 13000 | loss: 1.3722 | ds_loss: 0.0000 | lr: 1.6073e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   6 | Iter:   8048/ 13000 | global iter:   8048/ 13000 | loss: 0.9019 | ds_loss: 0.0000 | lr: 1.6067e-05 | scale: 16384.0000 | micro time: 1.808 | step time: 0.000
train | epoch   6 | Iter:   8049/ 13000 | global iter:   8049/ 13000 | loss: 1.0931 | ds_loss: 0.0000 | lr: 1.6062e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   6 | Iter:   8050/ 13000 | global iter:   8050/ 13000 | loss: 0.6398 | ds_loss: 0.0000 | lr: 1.6056e-05 | scale: 16384.0000 | micro time: 1.895 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8050/ 13000 | global iter:   8050/ 13000 | loss: 0.9638 | ds_loss: 0.0000 | lr: 1.6056e-05 | scale: 16384.0000 | micro time: 1.895 | step time: 1.821
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8051/ 13000 | global iter:   8051/ 13000 | loss: 0.6173 | ds_loss: 0.0000 | lr: 1.6051e-05 | scale: 16384.0000 | micro time: 1.824 | step time: 0.000
train | epoch   6 | Iter:   8052/ 13000 | global iter:   8052/ 13000 | loss: 0.7430 | ds_loss: 0.0000 | lr: 1.6045e-05 | scale: 16384.0000 | micro time: 1.784 | step time: 0.000
train | epoch   6 | Iter:   8053/ 13000 | global iter:   8053/ 13000 | loss: 1.0629 | ds_loss: 0.0000 | lr: 1.6039e-05 | scale: 16384.0000 | micro time: 1.759 | step time: 0.000
train | epoch   6 | Iter:   8054/ 13000 | global iter:   8054/ 13000 | loss: 1.3756 | ds_loss: 0.0000 | lr: 1.6034e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   6 | Iter:   8055/ 13000 | global iter:   8055/ 13000 | loss: 1.0029 | ds_loss: 0.0000 | lr: 1.6028e-05 | scale: 16384.0000 | micro time: 1.707 | step time: 0.000
train | epoch   6 | Iter:   8056/ 13000 | global iter:   8056/ 13000 | loss: 0.7994 | ds_loss: 0.0000 | lr: 1.6022e-05 | scale: 16384.0000 | micro time: 1.838 | step time: 0.000
train | epoch   6 | Iter:   8057/ 13000 | global iter:   8057/ 13000 | loss: 0.7335 | ds_loss: 0.0000 | lr: 1.6017e-05 | scale: 16384.0000 | micro time: 1.740 | step time: 0.000
train | epoch   6 | Iter:   8058/ 13000 | global iter:   8058/ 13000 | loss: 1.0811 | ds_loss: 0.0000 | lr: 1.6011e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   6 | Iter:   8059/ 13000 | global iter:   8059/ 13000 | loss: 1.2527 | ds_loss: 0.0000 | lr: 1.6006e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   6 | Iter:   8060/ 13000 | global iter:   8060/ 13000 | loss: 1.1281 | ds_loss: 0.0000 | lr: 1.6000e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8060/ 13000 | global iter:   8060/ 13000 | loss: 0.9796 | ds_loss: 0.0000 | lr: 1.6000e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 1.800
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8061/ 13000 | global iter:   8061/ 13000 | loss: 1.0505 | ds_loss: 0.0000 | lr: 1.5994e-05 | scale: 16384.0000 | micro time: 1.838 | step time: 0.000
train | epoch   6 | Iter:   8062/ 13000 | global iter:   8062/ 13000 | loss: 0.7818 | ds_loss: 0.0000 | lr: 1.5989e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   6 | Iter:   8063/ 13000 | global iter:   8063/ 13000 | loss: 1.1480 | ds_loss: 0.0000 | lr: 1.5983e-05 | scale: 16384.0000 | micro time: 1.878 | step time: 0.000
train | epoch   6 | Iter:   8064/ 13000 | global iter:   8064/ 13000 | loss: 1.2256 | ds_loss: 0.0000 | lr: 1.5978e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   6 | Iter:   8065/ 13000 | global iter:   8065/ 13000 | loss: 0.7274 | ds_loss: 0.0000 | lr: 1.5972e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   6 | Iter:   8066/ 13000 | global iter:   8066/ 13000 | loss: 0.4503 | ds_loss: 0.0000 | lr: 1.5966e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   6 | Iter:   8067/ 13000 | global iter:   8067/ 13000 | loss: 0.6642 | ds_loss: 0.0000 | lr: 1.5961e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   6 | Iter:   8068/ 13000 | global iter:   8068/ 13000 | loss: 1.0799 | ds_loss: 0.0000 | lr: 1.5955e-05 | scale: 16384.0000 | micro time: 1.935 | step time: 0.000
train | epoch   6 | Iter:   8069/ 13000 | global iter:   8069/ 13000 | loss: 1.0268 | ds_loss: 0.0000 | lr: 1.5949e-05 | scale: 16384.0000 | micro time: 1.909 | step time: 0.000
train | epoch   6 | Iter:   8070/ 13000 | global iter:   8070/ 13000 | loss: 0.8711 | ds_loss: 0.0000 | lr: 1.5944e-05 | scale: 16384.0000 | micro time: 1.909 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8070/ 13000 | global iter:   8070/ 13000 | loss: 0.9026 | ds_loss: 0.0000 | lr: 1.5944e-05 | scale: 16384.0000 | micro time: 1.909 | step time: 1.861
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8071/ 13000 | global iter:   8071/ 13000 | loss: 0.6452 | ds_loss: 0.0000 | lr: 1.5938e-05 | scale: 16384.0000 | micro time: 1.758 | step time: 0.000
train | epoch   6 | Iter:   8072/ 13000 | global iter:   8072/ 13000 | loss: 1.1650 | ds_loss: 0.0000 | lr: 1.5933e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   6 | Iter:   8073/ 13000 | global iter:   8073/ 13000 | loss: 1.0601 | ds_loss: 0.0000 | lr: 1.5927e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   6 | Iter:   8074/ 13000 | global iter:   8074/ 13000 | loss: 0.8596 | ds_loss: 0.0000 | lr: 1.5921e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   6 | Iter:   8075/ 13000 | global iter:   8075/ 13000 | loss: 0.8999 | ds_loss: 0.0000 | lr: 1.5916e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 0.000
train | epoch   6 | Iter:   8076/ 13000 | global iter:   8076/ 13000 | loss: 0.9085 | ds_loss: 0.0000 | lr: 1.5910e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   6 | Iter:   8077/ 13000 | global iter:   8077/ 13000 | loss: 0.7159 | ds_loss: 0.0000 | lr: 1.5905e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   6 | Iter:   8078/ 13000 | global iter:   8078/ 13000 | loss: 1.0460 | ds_loss: 0.0000 | lr: 1.5899e-05 | scale: 16384.0000 | micro time: 1.935 | step time: 0.000
train | epoch   6 | Iter:   8079/ 13000 | global iter:   8079/ 13000 | loss: 0.9166 | ds_loss: 0.0000 | lr: 1.5893e-05 | scale: 16384.0000 | micro time: 1.848 | step time: 0.000
train | epoch   6 | Iter:   8080/ 13000 | global iter:   8080/ 13000 | loss: 1.0206 | ds_loss: 0.0000 | lr: 1.5888e-05 | scale: 16384.0000 | micro time: 1.918 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8080/ 13000 | global iter:   8080/ 13000 | loss: 0.9237 | ds_loss: 0.0000 | lr: 1.5888e-05 | scale: 16384.0000 | micro time: 1.918 | step time: 1.829
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8081/ 13000 | global iter:   8081/ 13000 | loss: 0.9370 | ds_loss: 0.0000 | lr: 1.5882e-05 | scale: 16384.0000 | micro time: 1.853 | step time: 0.000
train | epoch   6 | Iter:   8082/ 13000 | global iter:   8082/ 13000 | loss: 0.9465 | ds_loss: 0.0000 | lr: 1.5877e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   6 | Iter:   8083/ 13000 | global iter:   8083/ 13000 | loss: 0.7133 | ds_loss: 0.0000 | lr: 1.5871e-05 | scale: 16384.0000 | micro time: 1.826 | step time: 0.000
train | epoch   6 | Iter:   8084/ 13000 | global iter:   8084/ 13000 | loss: 0.8549 | ds_loss: 0.0000 | lr: 1.5865e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   6 | Iter:   8085/ 13000 | global iter:   8085/ 13000 | loss: 0.9833 | ds_loss: 0.0000 | lr: 1.5860e-05 | scale: 16384.0000 | micro time: 1.703 | step time: 0.000
train | epoch   6 | Iter:   8086/ 13000 | global iter:   8086/ 13000 | loss: 0.9688 | ds_loss: 0.0000 | lr: 1.5854e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   6 | Iter:   8087/ 13000 | global iter:   8087/ 13000 | loss: 0.5835 | ds_loss: 0.0000 | lr: 1.5848e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   6 | Iter:   8088/ 13000 | global iter:   8088/ 13000 | loss: 0.7966 | ds_loss: 0.0000 | lr: 1.5843e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   6 | Iter:   8089/ 13000 | global iter:   8089/ 13000 | loss: 0.9410 | ds_loss: 0.0000 | lr: 1.5837e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   6 | Iter:   8090/ 13000 | global iter:   8090/ 13000 | loss: 1.1995 | ds_loss: 0.0000 | lr: 1.5832e-05 | scale: 16384.0000 | micro time: 1.853 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8090/ 13000 | global iter:   8090/ 13000 | loss: 0.8924 | ds_loss: 0.0000 | lr: 1.5832e-05 | scale: 16384.0000 | micro time: 1.853 | step time: 1.810
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8091/ 13000 | global iter:   8091/ 13000 | loss: 0.9375 | ds_loss: 0.0000 | lr: 1.5826e-05 | scale: 16384.0000 | micro time: 1.699 | step time: 0.000
train | epoch   6 | Iter:   8092/ 13000 | global iter:   8092/ 13000 | loss: 0.9666 | ds_loss: 0.0000 | lr: 1.5820e-05 | scale: 16384.0000 | micro time: 1.800 | step time: 0.000
train | epoch   6 | Iter:   8093/ 13000 | global iter:   8093/ 13000 | loss: 0.6025 | ds_loss: 0.0000 | lr: 1.5815e-05 | scale: 16384.0000 | micro time: 1.801 | step time: 0.000
train | epoch   6 | Iter:   8094/ 13000 | global iter:   8094/ 13000 | loss: 0.9456 | ds_loss: 0.0000 | lr: 1.5809e-05 | scale: 16384.0000 | micro time: 1.793 | step time: 0.000
train | epoch   6 | Iter:   8095/ 13000 | global iter:   8095/ 13000 | loss: 0.9066 | ds_loss: 0.0000 | lr: 1.5804e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   6 | Iter:   8096/ 13000 | global iter:   8096/ 13000 | loss: 0.4931 | ds_loss: 0.0000 | lr: 1.5798e-05 | scale: 16384.0000 | micro time: 1.679 | step time: 0.000
train | epoch   6 | Iter:   8097/ 13000 | global iter:   8097/ 13000 | loss: 1.1194 | ds_loss: 0.0000 | lr: 1.5792e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   6 | Iter:   8098/ 13000 | global iter:   8098/ 13000 | loss: 0.6842 | ds_loss: 0.0000 | lr: 1.5787e-05 | scale: 16384.0000 | micro time: 1.802 | step time: 0.000
train | epoch   6 | Iter:   8099/ 13000 | global iter:   8099/ 13000 | loss: 1.1818 | ds_loss: 0.0000 | lr: 1.5781e-05 | scale: 16384.0000 | micro time: 1.900 | step time: 0.000
train | epoch   6 | Iter:   8100/ 13000 | global iter:   8100/ 13000 | loss: 0.7162 | ds_loss: 0.0000 | lr: 1.5776e-05 | scale: 16384.0000 | micro time: 1.887 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8100/ 13000 | global iter:   8100/ 13000 | loss: 0.8554 | ds_loss: 0.0000 | lr: 1.5776e-05 | scale: 16384.0000 | micro time: 1.887 | step time: 1.792
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8101/ 13000 | global iter:   8101/ 13000 | loss: 0.8038 | ds_loss: 0.0000 | lr: 1.5770e-05 | scale: 16384.0000 | micro time: 1.898 | step time: 0.000
train | epoch   6 | Iter:   8102/ 13000 | global iter:   8102/ 13000 | loss: 1.0103 | ds_loss: 0.0000 | lr: 1.5764e-05 | scale: 16384.0000 | micro time: 1.813 | step time: 0.000
train | epoch   6 | Iter:   8103/ 13000 | global iter:   8103/ 13000 | loss: 0.7013 | ds_loss: 0.0000 | lr: 1.5759e-05 | scale: 16384.0000 | micro time: 1.911 | step time: 0.000
train | epoch   6 | Iter:   8104/ 13000 | global iter:   8104/ 13000 | loss: 0.8535 | ds_loss: 0.0000 | lr: 1.5753e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   6 | Iter:   8105/ 13000 | global iter:   8105/ 13000 | loss: 0.7140 | ds_loss: 0.0000 | lr: 1.5748e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   6 | Iter:   8106/ 13000 | global iter:   8106/ 13000 | loss: 1.3117 | ds_loss: 0.0000 | lr: 1.5742e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   6 | Iter:   8107/ 13000 | global iter:   8107/ 13000 | loss: 0.7547 | ds_loss: 0.0000 | lr: 1.5736e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   6 | Iter:   8108/ 13000 | global iter:   8108/ 13000 | loss: 0.9882 | ds_loss: 0.0000 | lr: 1.5731e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   6 | Iter:   8109/ 13000 | global iter:   8109/ 13000 | loss: 1.4355 | ds_loss: 0.0000 | lr: 1.5725e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   6 | Iter:   8110/ 13000 | global iter:   8110/ 13000 | loss: 1.0874 | ds_loss: 0.0000 | lr: 1.5720e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8110/ 13000 | global iter:   8110/ 13000 | loss: 0.9661 | ds_loss: 0.0000 | lr: 1.5720e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 1.843
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8111/ 13000 | global iter:   8111/ 13000 | loss: 1.0727 | ds_loss: 0.0000 | lr: 1.5714e-05 | scale: 16384.0000 | micro time: 1.653 | step time: 0.000
train | epoch   6 | Iter:   8112/ 13000 | global iter:   8112/ 13000 | loss: 0.8446 | ds_loss: 0.0000 | lr: 1.5709e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   6 | Iter:   8113/ 13000 | global iter:   8113/ 13000 | loss: 1.6845 | ds_loss: 0.0000 | lr: 1.5703e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   6 | Iter:   8114/ 13000 | global iter:   8114/ 13000 | loss: 0.7297 | ds_loss: 0.0000 | lr: 1.5697e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   6 | Iter:   8115/ 13000 | global iter:   8115/ 13000 | loss: 0.6791 | ds_loss: 0.0000 | lr: 1.5692e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   6 | Iter:   8116/ 13000 | global iter:   8116/ 13000 | loss: 0.7318 | ds_loss: 0.0000 | lr: 1.5686e-05 | scale: 16384.0000 | micro time: 1.753 | step time: 0.000
train | epoch   6 | Iter:   8117/ 13000 | global iter:   8117/ 13000 | loss: 0.6801 | ds_loss: 0.0000 | lr: 1.5681e-05 | scale: 16384.0000 | micro time: 1.828 | step time: 0.000
train | epoch   6 | Iter:   8118/ 13000 | global iter:   8118/ 13000 | loss: 1.2412 | ds_loss: 0.0000 | lr: 1.5675e-05 | scale: 16384.0000 | micro time: 1.832 | step time: 0.000
train | epoch   6 | Iter:   8119/ 13000 | global iter:   8119/ 13000 | loss: 1.3011 | ds_loss: 0.0000 | lr: 1.5669e-05 | scale: 16384.0000 | micro time: 1.877 | step time: 0.000
train | epoch   6 | Iter:   8120/ 13000 | global iter:   8120/ 13000 | loss: 1.3456 | ds_loss: 0.0000 | lr: 1.5664e-05 | scale: 16384.0000 | micro time: 1.801 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8120/ 13000 | global iter:   8120/ 13000 | loss: 1.0310 | ds_loss: 0.0000 | lr: 1.5664e-05 | scale: 16384.0000 | micro time: 1.801 | step time: 1.791
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8121/ 13000 | global iter:   8121/ 13000 | loss: 0.9394 | ds_loss: 0.0000 | lr: 1.5658e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   6 | Iter:   8122/ 13000 | global iter:   8122/ 13000 | loss: 0.7683 | ds_loss: 0.0000 | lr: 1.5653e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   6 | Iter:   8123/ 13000 | global iter:   8123/ 13000 | loss: 1.1408 | ds_loss: 0.0000 | lr: 1.5647e-05 | scale: 16384.0000 | micro time: 1.833 | step time: 0.000
train | epoch   6 | Iter:   8124/ 13000 | global iter:   8124/ 13000 | loss: 1.1537 | ds_loss: 0.0000 | lr: 1.5641e-05 | scale: 16384.0000 | micro time: 1.873 | step time: 0.000
train | epoch   6 | Iter:   8125/ 13000 | global iter:   8125/ 13000 | loss: 0.6012 | ds_loss: 0.0000 | lr: 1.5636e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   6 | Iter:   8126/ 13000 | global iter:   8126/ 13000 | loss: 1.2311 | ds_loss: 0.0000 | lr: 1.5630e-05 | scale: 16384.0000 | micro time: 1.900 | step time: 0.000
train | epoch   6 | Iter:   8127/ 13000 | global iter:   8127/ 13000 | loss: 0.5542 | ds_loss: 0.0000 | lr: 1.5625e-05 | scale: 16384.0000 | micro time: 1.743 | step time: 0.000
train | epoch   6 | Iter:   8128/ 13000 | global iter:   8128/ 13000 | loss: 0.7614 | ds_loss: 0.0000 | lr: 1.5619e-05 | scale: 16384.0000 | micro time: 1.919 | step time: 0.000
train | epoch   6 | Iter:   8129/ 13000 | global iter:   8129/ 13000 | loss: 0.7232 | ds_loss: 0.0000 | lr: 1.5614e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   6 | Iter:   8130/ 13000 | global iter:   8130/ 13000 | loss: 0.7789 | ds_loss: 0.0000 | lr: 1.5608e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8130/ 13000 | global iter:   8130/ 13000 | loss: 0.8652 | ds_loss: 0.0000 | lr: 1.5608e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 1.826
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8131/ 13000 | global iter:   8131/ 13000 | loss: 0.7810 | ds_loss: 0.0000 | lr: 1.5602e-05 | scale: 16384.0000 | micro time: 1.882 | step time: 0.000
train | epoch   6 | Iter:   8132/ 13000 | global iter:   8132/ 13000 | loss: 1.2884 | ds_loss: 0.0000 | lr: 1.5597e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   6 | Iter:   8133/ 13000 | global iter:   8133/ 13000 | loss: 1.0622 | ds_loss: 0.0000 | lr: 1.5591e-05 | scale: 16384.0000 | micro time: 1.684 | step time: 0.000
train | epoch   6 | Iter:   8134/ 13000 | global iter:   8134/ 13000 | loss: 1.0012 | ds_loss: 0.0000 | lr: 1.5586e-05 | scale: 16384.0000 | micro time: 1.802 | step time: 0.000
train | epoch   6 | Iter:   8135/ 13000 | global iter:   8135/ 13000 | loss: 0.9479 | ds_loss: 0.0000 | lr: 1.5580e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   6 | Iter:   8136/ 13000 | global iter:   8136/ 13000 | loss: 0.4990 | ds_loss: 0.0000 | lr: 1.5575e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   6 | Iter:   8137/ 13000 | global iter:   8137/ 13000 | loss: 1.0343 | ds_loss: 0.0000 | lr: 1.5569e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   6 | Iter:   8138/ 13000 | global iter:   8138/ 13000 | loss: 1.1637 | ds_loss: 0.0000 | lr: 1.5563e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   6 | Iter:   8139/ 13000 | global iter:   8139/ 13000 | loss: 0.9797 | ds_loss: 0.0000 | lr: 1.5558e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   6 | Iter:   8140/ 13000 | global iter:   8140/ 13000 | loss: 1.3464 | ds_loss: 0.0000 | lr: 1.5552e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8140/ 13000 | global iter:   8140/ 13000 | loss: 1.0104 | ds_loss: 0.0000 | lr: 1.5552e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 1.812
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8141/ 13000 | global iter:   8141/ 13000 | loss: 0.9187 | ds_loss: 0.0000 | lr: 1.5547e-05 | scale: 16384.0000 | micro time: 1.738 | step time: 0.000
train | epoch   6 | Iter:   8142/ 13000 | global iter:   8142/ 13000 | loss: 1.0555 | ds_loss: 0.0000 | lr: 1.5541e-05 | scale: 16384.0000 | micro time: 1.727 | step time: 0.000
train | epoch   6 | Iter:   8143/ 13000 | global iter:   8143/ 13000 | loss: 0.8224 | ds_loss: 0.0000 | lr: 1.5535e-05 | scale: 16384.0000 | micro time: 1.745 | step time: 0.000
train | epoch   6 | Iter:   8144/ 13000 | global iter:   8144/ 13000 | loss: 0.3226 | ds_loss: 0.0000 | lr: 1.5530e-05 | scale: 16384.0000 | micro time: 1.929 | step time: 0.000
train | epoch   6 | Iter:   8145/ 13000 | global iter:   8145/ 13000 | loss: 1.4746 | ds_loss: 0.0000 | lr: 1.5524e-05 | scale: 16384.0000 | micro time: 1.872 | step time: 0.000
train | epoch   6 | Iter:   8146/ 13000 | global iter:   8146/ 13000 | loss: 0.6245 | ds_loss: 0.0000 | lr: 1.5519e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   6 | Iter:   8147/ 13000 | global iter:   8147/ 13000 | loss: 0.6391 | ds_loss: 0.0000 | lr: 1.5513e-05 | scale: 16384.0000 | micro time: 1.864 | step time: 0.000
train | epoch   6 | Iter:   8148/ 13000 | global iter:   8148/ 13000 | loss: 1.0142 | ds_loss: 0.0000 | lr: 1.5508e-05 | scale: 16384.0000 | micro time: 1.857 | step time: 0.000
train | epoch   6 | Iter:   8149/ 13000 | global iter:   8149/ 13000 | loss: 1.2072 | ds_loss: 0.0000 | lr: 1.5502e-05 | scale: 16384.0000 | micro time: 1.911 | step time: 0.000
train | epoch   6 | Iter:   8150/ 13000 | global iter:   8150/ 13000 | loss: 0.9718 | ds_loss: 0.0000 | lr: 1.5496e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8150/ 13000 | global iter:   8150/ 13000 | loss: 0.9051 | ds_loss: 0.0000 | lr: 1.5496e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 1.825
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8151/ 13000 | global iter:   8151/ 13000 | loss: 0.9931 | ds_loss: 0.0000 | lr: 1.5491e-05 | scale: 16384.0000 | micro time: 1.862 | step time: 0.000
train | epoch   6 | Iter:   8152/ 13000 | global iter:   8152/ 13000 | loss: 0.8305 | ds_loss: 0.0000 | lr: 1.5485e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   6 | Iter:   8153/ 13000 | global iter:   8153/ 13000 | loss: 0.8820 | ds_loss: 0.0000 | lr: 1.5480e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   6 | Iter:   8154/ 13000 | global iter:   8154/ 13000 | loss: 0.9041 | ds_loss: 0.0000 | lr: 1.5474e-05 | scale: 16384.0000 | micro time: 1.872 | step time: 0.000
train | epoch   6 | Iter:   8155/ 13000 | global iter:   8155/ 13000 | loss: 0.9032 | ds_loss: 0.0000 | lr: 1.5469e-05 | scale: 16384.0000 | micro time: 1.802 | step time: 0.000
train | epoch   6 | Iter:   8156/ 13000 | global iter:   8156/ 13000 | loss: 1.0342 | ds_loss: 0.0000 | lr: 1.5463e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   6 | Iter:   8157/ 13000 | global iter:   8157/ 13000 | loss: 0.8212 | ds_loss: 0.0000 | lr: 1.5458e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   6 | Iter:   8158/ 13000 | global iter:   8158/ 13000 | loss: 0.9970 | ds_loss: 0.0000 | lr: 1.5452e-05 | scale: 16384.0000 | micro time: 1.713 | step time: 0.000
train | epoch   6 | Iter:   8159/ 13000 | global iter:   8159/ 13000 | loss: 1.1138 | ds_loss: 0.0000 | lr: 1.5446e-05 | scale: 16384.0000 | micro time: 1.753 | step time: 0.000
train | epoch   6 | Iter:   8160/ 13000 | global iter:   8160/ 13000 | loss: 0.8122 | ds_loss: 0.0000 | lr: 1.5441e-05 | scale: 16384.0000 | micro time: 1.802 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8160/ 13000 | global iter:   8160/ 13000 | loss: 0.9291 | ds_loss: 0.0000 | lr: 1.5441e-05 | scale: 16384.0000 | micro time: 1.802 | step time: 1.811
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8161/ 13000 | global iter:   8161/ 13000 | loss: 0.6442 | ds_loss: 0.0000 | lr: 1.5435e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   6 | Iter:   8162/ 13000 | global iter:   8162/ 13000 | loss: 1.0148 | ds_loss: 0.0000 | lr: 1.5430e-05 | scale: 16384.0000 | micro time: 1.728 | step time: 0.000
train | epoch   6 | Iter:   8163/ 13000 | global iter:   8163/ 13000 | loss: 1.0308 | ds_loss: 0.0000 | lr: 1.5424e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   6 | Iter:   8164/ 13000 | global iter:   8164/ 13000 | loss: 0.8656 | ds_loss: 0.0000 | lr: 1.5419e-05 | scale: 16384.0000 | micro time: 1.792 | step time: 0.000
train | epoch   6 | Iter:   8165/ 13000 | global iter:   8165/ 13000 | loss: 0.9434 | ds_loss: 0.0000 | lr: 1.5413e-05 | scale: 16384.0000 | micro time: 1.857 | step time: 0.000
train | epoch   6 | Iter:   8166/ 13000 | global iter:   8166/ 13000 | loss: 1.0738 | ds_loss: 0.0000 | lr: 1.5407e-05 | scale: 16384.0000 | micro time: 1.757 | step time: 0.000
train | epoch   6 | Iter:   8167/ 13000 | global iter:   8167/ 13000 | loss: 1.1442 | ds_loss: 0.0000 | lr: 1.5402e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   6 | Iter:   8168/ 13000 | global iter:   8168/ 13000 | loss: 1.0869 | ds_loss: 0.0000 | lr: 1.5396e-05 | scale: 16384.0000 | micro time: 1.899 | step time: 0.000
train | epoch   6 | Iter:   8169/ 13000 | global iter:   8169/ 13000 | loss: 0.7710 | ds_loss: 0.0000 | lr: 1.5391e-05 | scale: 16384.0000 | micro time: 1.681 | step time: 0.000
train | epoch   6 | Iter:   8170/ 13000 | global iter:   8170/ 13000 | loss: 0.8808 | ds_loss: 0.0000 | lr: 1.5385e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8170/ 13000 | global iter:   8170/ 13000 | loss: 0.9455 | ds_loss: 0.0000 | lr: 1.5385e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 1.800
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8171/ 13000 | global iter:   8171/ 13000 | loss: 0.6693 | ds_loss: 0.0000 | lr: 1.5380e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   6 | Iter:   8172/ 13000 | global iter:   8172/ 13000 | loss: 0.9988 | ds_loss: 0.0000 | lr: 1.5374e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   6 | Iter:   8173/ 13000 | global iter:   8173/ 13000 | loss: 0.6310 | ds_loss: 0.0000 | lr: 1.5369e-05 | scale: 16384.0000 | micro time: 1.915 | step time: 0.000
train | epoch   6 | Iter:   8174/ 13000 | global iter:   8174/ 13000 | loss: 0.5702 | ds_loss: 0.0000 | lr: 1.5363e-05 | scale: 16384.0000 | micro time: 1.817 | step time: 0.000
train | epoch   6 | Iter:   8175/ 13000 | global iter:   8175/ 13000 | loss: 0.9893 | ds_loss: 0.0000 | lr: 1.5357e-05 | scale: 16384.0000 | micro time: 1.793 | step time: 0.000
train | epoch   6 | Iter:   8176/ 13000 | global iter:   8176/ 13000 | loss: 1.2610 | ds_loss: 0.0000 | lr: 1.5352e-05 | scale: 16384.0000 | micro time: 1.854 | step time: 0.000
train | epoch   6 | Iter:   8177/ 13000 | global iter:   8177/ 13000 | loss: 1.2854 | ds_loss: 0.0000 | lr: 1.5346e-05 | scale: 16384.0000 | micro time: 1.769 | step time: 0.000
train | epoch   6 | Iter:   8178/ 13000 | global iter:   8178/ 13000 | loss: 0.9683 | ds_loss: 0.0000 | lr: 1.5341e-05 | scale: 16384.0000 | micro time: 1.723 | step time: 0.000
train | epoch   6 | Iter:   8179/ 13000 | global iter:   8179/ 13000 | loss: 1.7597 | ds_loss: 0.0000 | lr: 1.5335e-05 | scale: 16384.0000 | micro time: 1.808 | step time: 0.000
train | epoch   6 | Iter:   8180/ 13000 | global iter:   8180/ 13000 | loss: 0.8026 | ds_loss: 0.0000 | lr: 1.5330e-05 | scale: 16384.0000 | micro time: 1.812 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8180/ 13000 | global iter:   8180/ 13000 | loss: 0.9936 | ds_loss: 0.0000 | lr: 1.5330e-05 | scale: 16384.0000 | micro time: 1.812 | step time: 1.816
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8181/ 13000 | global iter:   8181/ 13000 | loss: 1.2950 | ds_loss: 0.0000 | lr: 1.5324e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   6 | Iter:   8182/ 13000 | global iter:   8182/ 13000 | loss: 0.5717 | ds_loss: 0.0000 | lr: 1.5319e-05 | scale: 16384.0000 | micro time: 1.818 | step time: 0.000
train | epoch   6 | Iter:   8183/ 13000 | global iter:   8183/ 13000 | loss: 1.1290 | ds_loss: 0.0000 | lr: 1.5313e-05 | scale: 16384.0000 | micro time: 1.808 | step time: 0.000
train | epoch   6 | Iter:   8184/ 13000 | global iter:   8184/ 13000 | loss: 1.2584 | ds_loss: 0.0000 | lr: 1.5307e-05 | scale: 16384.0000 | micro time: 1.774 | step time: 0.000
train | epoch   6 | Iter:   8185/ 13000 | global iter:   8185/ 13000 | loss: 1.2077 | ds_loss: 0.0000 | lr: 1.5302e-05 | scale: 16384.0000 | micro time: 1.882 | step time: 0.000
train | epoch   6 | Iter:   8186/ 13000 | global iter:   8186/ 13000 | loss: 0.7823 | ds_loss: 0.0000 | lr: 1.5296e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   6 | Iter:   8187/ 13000 | global iter:   8187/ 13000 | loss: 0.9956 | ds_loss: 0.0000 | lr: 1.5291e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   6 | Iter:   8188/ 13000 | global iter:   8188/ 13000 | loss: 0.9507 | ds_loss: 0.0000 | lr: 1.5285e-05 | scale: 16384.0000 | micro time: 1.854 | step time: 0.000
train | epoch   6 | Iter:   8189/ 13000 | global iter:   8189/ 13000 | loss: 1.1049 | ds_loss: 0.0000 | lr: 1.5280e-05 | scale: 16384.0000 | micro time: 1.776 | step time: 0.000
train | epoch   6 | Iter:   8190/ 13000 | global iter:   8190/ 13000 | loss: 0.5215 | ds_loss: 0.0000 | lr: 1.5274e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8190/ 13000 | global iter:   8190/ 13000 | loss: 0.9817 | ds_loss: 0.0000 | lr: 1.5274e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 1.826
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8191/ 13000 | global iter:   8191/ 13000 | loss: 0.9572 | ds_loss: 0.0000 | lr: 1.5269e-05 | scale: 16384.0000 | micro time: 1.737 | step time: 0.000
train | epoch   6 | Iter:   8192/ 13000 | global iter:   8192/ 13000 | loss: 0.9737 | ds_loss: 0.0000 | lr: 1.5263e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   6 | Iter:   8193/ 13000 | global iter:   8193/ 13000 | loss: 1.2311 | ds_loss: 0.0000 | lr: 1.5257e-05 | scale: 16384.0000 | micro time: 1.789 | step time: 0.000
train | epoch   6 | Iter:   8194/ 13000 | global iter:   8194/ 13000 | loss: 0.8893 | ds_loss: 0.0000 | lr: 1.5252e-05 | scale: 16384.0000 | micro time: 1.813 | step time: 0.000
train | epoch   6 | Iter:   8195/ 13000 | global iter:   8195/ 13000 | loss: 0.5984 | ds_loss: 0.0000 | lr: 1.5246e-05 | scale: 16384.0000 | micro time: 1.818 | step time: 0.000
train | epoch   6 | Iter:   8196/ 13000 | global iter:   8196/ 13000 | loss: 0.9633 | ds_loss: 0.0000 | lr: 1.5241e-05 | scale: 16384.0000 | micro time: 1.781 | step time: 0.000
train | epoch   6 | Iter:   8197/ 13000 | global iter:   8197/ 13000 | loss: 1.0137 | ds_loss: 0.0000 | lr: 1.5235e-05 | scale: 16384.0000 | micro time: 1.810 | step time: 0.000
train | epoch   6 | Iter:   8198/ 13000 | global iter:   8198/ 13000 | loss: 0.9834 | ds_loss: 0.0000 | lr: 1.5230e-05 | scale: 16384.0000 | micro time: 1.885 | step time: 0.000
train | epoch   6 | Iter:   8199/ 13000 | global iter:   8199/ 13000 | loss: 0.8908 | ds_loss: 0.0000 | lr: 1.5224e-05 | scale: 16384.0000 | micro time: 1.832 | step time: 0.000
train | epoch   6 | Iter:   8200/ 13000 | global iter:   8200/ 13000 | loss: 1.1485 | ds_loss: 0.0000 | lr: 1.5219e-05 | scale: 16384.0000 | micro time: 1.662 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8200/ 13000 | global iter:   8200/ 13000 | loss: 0.9649 | ds_loss: 0.0000 | lr: 1.5219e-05 | scale: 16384.0000 | micro time: 1.662 | step time: 1.793
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8201/ 13000 | global iter:   8201/ 13000 | loss: 1.3199 | ds_loss: 0.0000 | lr: 1.5213e-05 | scale: 16384.0000 | micro time: 1.914 | step time: 0.000
train | epoch   6 | Iter:   8202/ 13000 | global iter:   8202/ 13000 | loss: 0.8127 | ds_loss: 0.0000 | lr: 1.5208e-05 | scale: 16384.0000 | micro time: 1.822 | step time: 0.000
train | epoch   6 | Iter:   8203/ 13000 | global iter:   8203/ 13000 | loss: 0.9415 | ds_loss: 0.0000 | lr: 1.5202e-05 | scale: 16384.0000 | micro time: 1.856 | step time: 0.000
train | epoch   6 | Iter:   8204/ 13000 | global iter:   8204/ 13000 | loss: 0.8912 | ds_loss: 0.0000 | lr: 1.5197e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   6 | Iter:   8205/ 13000 | global iter:   8205/ 13000 | loss: 0.6215 | ds_loss: 0.0000 | lr: 1.5191e-05 | scale: 16384.0000 | micro time: 1.806 | step time: 0.000
train | epoch   6 | Iter:   8206/ 13000 | global iter:   8206/ 13000 | loss: 1.0508 | ds_loss: 0.0000 | lr: 1.5185e-05 | scale: 16384.0000 | micro time: 1.820 | step time: 0.000
train | epoch   6 | Iter:   8207/ 13000 | global iter:   8207/ 13000 | loss: 0.5555 | ds_loss: 0.0000 | lr: 1.5180e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   6 | Iter:   8208/ 13000 | global iter:   8208/ 13000 | loss: 0.7104 | ds_loss: 0.0000 | lr: 1.5174e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   6 | Iter:   8209/ 13000 | global iter:   8209/ 13000 | loss: 1.2807 | ds_loss: 0.0000 | lr: 1.5169e-05 | scale: 16384.0000 | micro time: 1.743 | step time: 0.000
train | epoch   6 | Iter:   8210/ 13000 | global iter:   8210/ 13000 | loss: 0.8489 | ds_loss: 0.0000 | lr: 1.5163e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8210/ 13000 | global iter:   8210/ 13000 | loss: 0.9033 | ds_loss: 0.0000 | lr: 1.5163e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 1.821
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8211/ 13000 | global iter:   8211/ 13000 | loss: 1.1112 | ds_loss: 0.0000 | lr: 1.5158e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 0.000
train | epoch   6 | Iter:   8212/ 13000 | global iter:   8212/ 13000 | loss: 0.9272 | ds_loss: 0.0000 | lr: 1.5152e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   6 | Iter:   8213/ 13000 | global iter:   8213/ 13000 | loss: 1.1303 | ds_loss: 0.0000 | lr: 1.5147e-05 | scale: 16384.0000 | micro time: 1.760 | step time: 0.000
train | epoch   6 | Iter:   8214/ 13000 | global iter:   8214/ 13000 | loss: 0.2962 | ds_loss: 0.0000 | lr: 1.5141e-05 | scale: 16384.0000 | micro time: 1.858 | step time: 0.000
train | epoch   6 | Iter:   8215/ 13000 | global iter:   8215/ 13000 | loss: 0.5368 | ds_loss: 0.0000 | lr: 1.5136e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   6 | Iter:   8216/ 13000 | global iter:   8216/ 13000 | loss: 0.2931 | ds_loss: 0.0000 | lr: 1.5130e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   6 | Iter:   8217/ 13000 | global iter:   8217/ 13000 | loss: 1.2105 | ds_loss: 0.0000 | lr: 1.5125e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   6 | Iter:   8218/ 13000 | global iter:   8218/ 13000 | loss: 0.6314 | ds_loss: 0.0000 | lr: 1.5119e-05 | scale: 16384.0000 | micro time: 1.975 | step time: 0.000
train | epoch   6 | Iter:   8219/ 13000 | global iter:   8219/ 13000 | loss: 1.1747 | ds_loss: 0.0000 | lr: 1.5113e-05 | scale: 16384.0000 | micro time: 1.903 | step time: 0.000
train | epoch   6 | Iter:   8220/ 13000 | global iter:   8220/ 13000 | loss: 1.0122 | ds_loss: 0.0000 | lr: 1.5108e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8220/ 13000 | global iter:   8220/ 13000 | loss: 0.8324 | ds_loss: 0.0000 | lr: 1.5108e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 1.840
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8221/ 13000 | global iter:   8221/ 13000 | loss: 0.8052 | ds_loss: 0.0000 | lr: 1.5102e-05 | scale: 16384.0000 | micro time: 1.801 | step time: 0.000
train | epoch   6 | Iter:   8222/ 13000 | global iter:   8222/ 13000 | loss: 0.7822 | ds_loss: 0.0000 | lr: 1.5097e-05 | scale: 16384.0000 | micro time: 1.800 | step time: 0.000
train | epoch   6 | Iter:   8223/ 13000 | global iter:   8223/ 13000 | loss: 0.3810 | ds_loss: 0.0000 | lr: 1.5091e-05 | scale: 16384.0000 | micro time: 1.710 | step time: 0.000
train | epoch   6 | Iter:   8224/ 13000 | global iter:   8224/ 13000 | loss: 1.0199 | ds_loss: 0.0000 | lr: 1.5086e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   6 | Iter:   8225/ 13000 | global iter:   8225/ 13000 | loss: 0.7868 | ds_loss: 0.0000 | lr: 1.5080e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   6 | Iter:   8226/ 13000 | global iter:   8226/ 13000 | loss: 0.8848 | ds_loss: 0.0000 | lr: 1.5075e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   6 | Iter:   8227/ 13000 | global iter:   8227/ 13000 | loss: 0.7382 | ds_loss: 0.0000 | lr: 1.5069e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   6 | Iter:   8228/ 13000 | global iter:   8228/ 13000 | loss: 0.9480 | ds_loss: 0.0000 | lr: 1.5064e-05 | scale: 16384.0000 | micro time: 1.825 | step time: 0.000
train | epoch   6 | Iter:   8229/ 13000 | global iter:   8229/ 13000 | loss: 0.7560 | ds_loss: 0.0000 | lr: 1.5058e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   6 | Iter:   8230/ 13000 | global iter:   8230/ 13000 | loss: 0.5029 | ds_loss: 0.0000 | lr: 1.5053e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8230/ 13000 | global iter:   8230/ 13000 | loss: 0.7605 | ds_loss: 0.0000 | lr: 1.5053e-05 | scale: 16384.0000 | micro time: 1.805 | step time: 1.798
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8231/ 13000 | global iter:   8231/ 13000 | loss: 0.8072 | ds_loss: 0.0000 | lr: 1.5047e-05 | scale: 16384.0000 | micro time: 1.822 | step time: 0.000
train | epoch   6 | Iter:   8232/ 13000 | global iter:   8232/ 13000 | loss: 0.8732 | ds_loss: 0.0000 | lr: 1.5042e-05 | scale: 16384.0000 | micro time: 1.861 | step time: 0.000
train | epoch   6 | Iter:   8233/ 13000 | global iter:   8233/ 13000 | loss: 1.4366 | ds_loss: 0.0000 | lr: 1.5036e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   6 | Iter:   8234/ 13000 | global iter:   8234/ 13000 | loss: 1.3402 | ds_loss: 0.0000 | lr: 1.5031e-05 | scale: 16384.0000 | micro time: 1.731 | step time: 0.000
train | epoch   6 | Iter:   8235/ 13000 | global iter:   8235/ 13000 | loss: 0.5690 | ds_loss: 0.0000 | lr: 1.5025e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   6 | Iter:   8236/ 13000 | global iter:   8236/ 13000 | loss: 0.7038 | ds_loss: 0.0000 | lr: 1.5020e-05 | scale: 16384.0000 | micro time: 1.903 | step time: 0.000
train | epoch   6 | Iter:   8237/ 13000 | global iter:   8237/ 13000 | loss: 0.5731 | ds_loss: 0.0000 | lr: 1.5014e-05 | scale: 16384.0000 | micro time: 1.735 | step time: 0.000
train | epoch   6 | Iter:   8238/ 13000 | global iter:   8238/ 13000 | loss: 0.9447 | ds_loss: 0.0000 | lr: 1.5009e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   6 | Iter:   8239/ 13000 | global iter:   8239/ 13000 | loss: 1.2667 | ds_loss: 0.0000 | lr: 1.5003e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   6 | Iter:   8240/ 13000 | global iter:   8240/ 13000 | loss: 1.0047 | ds_loss: 0.0000 | lr: 1.4997e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8240/ 13000 | global iter:   8240/ 13000 | loss: 0.9519 | ds_loss: 0.0000 | lr: 1.4997e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 1.808
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8241/ 13000 | global iter:   8241/ 13000 | loss: 1.0537 | ds_loss: 0.0000 | lr: 1.4992e-05 | scale: 16384.0000 | micro time: 1.777 | step time: 0.000
train | epoch   6 | Iter:   8242/ 13000 | global iter:   8242/ 13000 | loss: 0.8833 | ds_loss: 0.0000 | lr: 1.4986e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   6 | Iter:   8243/ 13000 | global iter:   8243/ 13000 | loss: 1.2216 | ds_loss: 0.0000 | lr: 1.4981e-05 | scale: 16384.0000 | micro time: 1.867 | step time: 0.000
train | epoch   6 | Iter:   8244/ 13000 | global iter:   8244/ 13000 | loss: 1.1754 | ds_loss: 0.0000 | lr: 1.4975e-05 | scale: 16384.0000 | micro time: 1.927 | step time: 0.000
train | epoch   6 | Iter:   8245/ 13000 | global iter:   8245/ 13000 | loss: 0.5830 | ds_loss: 0.0000 | lr: 1.4970e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   6 | Iter:   8246/ 13000 | global iter:   8246/ 13000 | loss: 1.1763 | ds_loss: 0.0000 | lr: 1.4964e-05 | scale: 16384.0000 | micro time: 1.757 | step time: 0.000
train | epoch   6 | Iter:   8247/ 13000 | global iter:   8247/ 13000 | loss: 0.9171 | ds_loss: 0.0000 | lr: 1.4959e-05 | scale: 16384.0000 | micro time: 1.885 | step time: 0.000
train | epoch   6 | Iter:   8248/ 13000 | global iter:   8248/ 13000 | loss: 1.1574 | ds_loss: 0.0000 | lr: 1.4953e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   6 | Iter:   8249/ 13000 | global iter:   8249/ 13000 | loss: 1.3806 | ds_loss: 0.0000 | lr: 1.4948e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   6 | Iter:   8250/ 13000 | global iter:   8250/ 13000 | loss: 1.2004 | ds_loss: 0.0000 | lr: 1.4942e-05 | scale: 16384.0000 | micro time: 1.788 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8250/ 13000 | global iter:   8250/ 13000 | loss: 1.0749 | ds_loss: 0.0000 | lr: 1.4942e-05 | scale: 16384.0000 | micro time: 1.788 | step time: 1.821
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8251/ 13000 | global iter:   8251/ 13000 | loss: 0.5561 | ds_loss: 0.0000 | lr: 1.4937e-05 | scale: 16384.0000 | micro time: 1.890 | step time: 0.000
train | epoch   6 | Iter:   8252/ 13000 | global iter:   8252/ 13000 | loss: 1.3670 | ds_loss: 0.0000 | lr: 1.4931e-05 | scale: 16384.0000 | micro time: 1.727 | step time: 0.000
train | epoch   6 | Iter:   8253/ 13000 | global iter:   8253/ 13000 | loss: 1.2058 | ds_loss: 0.0000 | lr: 1.4926e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   6 | Iter:   8254/ 13000 | global iter:   8254/ 13000 | loss: 1.0346 | ds_loss: 0.0000 | lr: 1.4920e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   6 | Iter:   8255/ 13000 | global iter:   8255/ 13000 | loss: 1.5327 | ds_loss: 0.0000 | lr: 1.4915e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   6 | Iter:   8256/ 13000 | global iter:   8256/ 13000 | loss: 0.6572 | ds_loss: 0.0000 | lr: 1.4909e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   6 | Iter:   8257/ 13000 | global iter:   8257/ 13000 | loss: 1.0319 | ds_loss: 0.0000 | lr: 1.4904e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   6 | Iter:   8258/ 13000 | global iter:   8258/ 13000 | loss: 0.6647 | ds_loss: 0.0000 | lr: 1.4898e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   6 | Iter:   8259/ 13000 | global iter:   8259/ 13000 | loss: 0.9564 | ds_loss: 0.0000 | lr: 1.4893e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   6 | Iter:   8260/ 13000 | global iter:   8260/ 13000 | loss: 1.0400 | ds_loss: 0.0000 | lr: 1.4887e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8260/ 13000 | global iter:   8260/ 13000 | loss: 1.0046 | ds_loss: 0.0000 | lr: 1.4887e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 1.805
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8261/ 13000 | global iter:   8261/ 13000 | loss: 0.7642 | ds_loss: 0.0000 | lr: 1.4882e-05 | scale: 16384.0000 | micro time: 1.800 | step time: 0.000
train | epoch   6 | Iter:   8262/ 13000 | global iter:   8262/ 13000 | loss: 0.9942 | ds_loss: 0.0000 | lr: 1.4876e-05 | scale: 16384.0000 | micro time: 1.844 | step time: 0.000
train | epoch   6 | Iter:   8263/ 13000 | global iter:   8263/ 13000 | loss: 0.8408 | ds_loss: 0.0000 | lr: 1.4871e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   6 | Iter:   8264/ 13000 | global iter:   8264/ 13000 | loss: 1.4056 | ds_loss: 0.0000 | lr: 1.4865e-05 | scale: 16384.0000 | micro time: 1.830 | step time: 0.000
train | epoch   6 | Iter:   8265/ 13000 | global iter:   8265/ 13000 | loss: 1.1966 | ds_loss: 0.0000 | lr: 1.4860e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   6 | Iter:   8266/ 13000 | global iter:   8266/ 13000 | loss: 1.0745 | ds_loss: 0.0000 | lr: 1.4854e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   6 | Iter:   8267/ 13000 | global iter:   8267/ 13000 | loss: 1.0099 | ds_loss: 0.0000 | lr: 1.4849e-05 | scale: 16384.0000 | micro time: 1.911 | step time: 0.000
train | epoch   6 | Iter:   8268/ 13000 | global iter:   8268/ 13000 | loss: 1.2902 | ds_loss: 0.0000 | lr: 1.4843e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   6 | Iter:   8269/ 13000 | global iter:   8269/ 13000 | loss: 0.3434 | ds_loss: 0.0000 | lr: 1.4838e-05 | scale: 16384.0000 | micro time: 1.848 | step time: 0.000
train | epoch   6 | Iter:   8270/ 13000 | global iter:   8270/ 13000 | loss: 0.9076 | ds_loss: 0.0000 | lr: 1.4832e-05 | scale: 16384.0000 | micro time: 1.858 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8270/ 13000 | global iter:   8270/ 13000 | loss: 0.9827 | ds_loss: 0.0000 | lr: 1.4832e-05 | scale: 16384.0000 | micro time: 1.858 | step time: 1.839
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8271/ 13000 | global iter:   8271/ 13000 | loss: 0.8057 | ds_loss: 0.0000 | lr: 1.4827e-05 | scale: 16384.0000 | micro time: 1.857 | step time: 0.000
train | epoch   6 | Iter:   8272/ 13000 | global iter:   8272/ 13000 | loss: 1.6078 | ds_loss: 0.0000 | lr: 1.4821e-05 | scale: 16384.0000 | micro time: 1.793 | step time: 0.000
train | epoch   6 | Iter:   8273/ 13000 | global iter:   8273/ 13000 | loss: 0.9049 | ds_loss: 0.0000 | lr: 1.4816e-05 | scale: 16384.0000 | micro time: 1.845 | step time: 0.000
train | epoch   6 | Iter:   8274/ 13000 | global iter:   8274/ 13000 | loss: 0.7607 | ds_loss: 0.0000 | lr: 1.4810e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   6 | Iter:   8275/ 13000 | global iter:   8275/ 13000 | loss: 1.0580 | ds_loss: 0.0000 | lr: 1.4805e-05 | scale: 16384.0000 | micro time: 1.762 | step time: 0.000
train | epoch   6 | Iter:   8276/ 13000 | global iter:   8276/ 13000 | loss: 0.9307 | ds_loss: 0.0000 | lr: 1.4799e-05 | scale: 16384.0000 | micro time: 1.800 | step time: 0.000
train | epoch   6 | Iter:   8277/ 13000 | global iter:   8277/ 13000 | loss: 1.0696 | ds_loss: 0.0000 | lr: 1.4794e-05 | scale: 16384.0000 | micro time: 2.116 | step time: 0.000
train | epoch   6 | Iter:   8278/ 13000 | global iter:   8278/ 13000 | loss: 1.1294 | ds_loss: 0.0000 | lr: 1.4788e-05 | scale: 16384.0000 | micro time: 1.838 | step time: 0.000
train | epoch   6 | Iter:   8279/ 13000 | global iter:   8279/ 13000 | loss: 1.0450 | ds_loss: 0.0000 | lr: 1.4783e-05 | scale: 16384.0000 | micro time: 1.794 | step time: 0.000
train | epoch   6 | Iter:   8280/ 13000 | global iter:   8280/ 13000 | loss: 0.9319 | ds_loss: 0.0000 | lr: 1.4777e-05 | scale: 16384.0000 | micro time: 1.788 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8280/ 13000 | global iter:   8280/ 13000 | loss: 1.0244 | ds_loss: 0.0000 | lr: 1.4777e-05 | scale: 16384.0000 | micro time: 1.788 | step time: 1.843
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8281/ 13000 | global iter:   8281/ 13000 | loss: 1.1057 | ds_loss: 0.0000 | lr: 1.4772e-05 | scale: 16384.0000 | micro time: 1.865 | step time: 0.000
train | epoch   6 | Iter:   8282/ 13000 | global iter:   8282/ 13000 | loss: 0.5904 | ds_loss: 0.0000 | lr: 1.4766e-05 | scale: 16384.0000 | micro time: 1.737 | step time: 0.000
train | epoch   6 | Iter:   8283/ 13000 | global iter:   8283/ 13000 | loss: 0.8435 | ds_loss: 0.0000 | lr: 1.4761e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   6 | Iter:   8284/ 13000 | global iter:   8284/ 13000 | loss: 0.6235 | ds_loss: 0.0000 | lr: 1.4755e-05 | scale: 16384.0000 | micro time: 1.924 | step time: 0.000
train | epoch   6 | Iter:   8285/ 13000 | global iter:   8285/ 13000 | loss: 1.0163 | ds_loss: 0.0000 | lr: 1.4750e-05 | scale: 16384.0000 | micro time: 1.769 | step time: 0.000
train | epoch   6 | Iter:   8286/ 13000 | global iter:   8286/ 13000 | loss: 1.4728 | ds_loss: 0.0000 | lr: 1.4744e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   6 | Iter:   8287/ 13000 | global iter:   8287/ 13000 | loss: 1.1748 | ds_loss: 0.0000 | lr: 1.4739e-05 | scale: 16384.0000 | micro time: 1.741 | step time: 0.000
train | epoch   6 | Iter:   8288/ 13000 | global iter:   8288/ 13000 | loss: 1.0374 | ds_loss: 0.0000 | lr: 1.4733e-05 | scale: 16384.0000 | micro time: 1.841 | step time: 0.000
train | epoch   6 | Iter:   8289/ 13000 | global iter:   8289/ 13000 | loss: 1.0752 | ds_loss: 0.0000 | lr: 1.4728e-05 | scale: 16384.0000 | micro time: 1.765 | step time: 0.000
train | epoch   6 | Iter:   8290/ 13000 | global iter:   8290/ 13000 | loss: 0.8174 | ds_loss: 0.0000 | lr: 1.4722e-05 | scale: 16384.0000 | micro time: 1.833 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8290/ 13000 | global iter:   8290/ 13000 | loss: 0.9757 | ds_loss: 0.0000 | lr: 1.4722e-05 | scale: 16384.0000 | micro time: 1.833 | step time: 1.803
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8291/ 13000 | global iter:   8291/ 13000 | loss: 1.0001 | ds_loss: 0.0000 | lr: 1.4717e-05 | scale: 16384.0000 | micro time: 1.882 | step time: 0.000
train | epoch   6 | Iter:   8292/ 13000 | global iter:   8292/ 13000 | loss: 1.1788 | ds_loss: 0.0000 | lr: 1.4711e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   6 | Iter:   8293/ 13000 | global iter:   8293/ 13000 | loss: 0.6939 | ds_loss: 0.0000 | lr: 1.4706e-05 | scale: 16384.0000 | micro time: 1.818 | step time: 0.000
train | epoch   6 | Iter:   8294/ 13000 | global iter:   8294/ 13000 | loss: 0.9121 | ds_loss: 0.0000 | lr: 1.4700e-05 | scale: 16384.0000 | micro time: 1.731 | step time: 0.000
train | epoch   6 | Iter:   8295/ 13000 | global iter:   8295/ 13000 | loss: 0.7391 | ds_loss: 0.0000 | lr: 1.4695e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   6 | Iter:   8296/ 13000 | global iter:   8296/ 13000 | loss: 1.3202 | ds_loss: 0.0000 | lr: 1.4689e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   6 | Iter:   8297/ 13000 | global iter:   8297/ 13000 | loss: 1.2406 | ds_loss: 0.0000 | lr: 1.4684e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   6 | Iter:   8298/ 13000 | global iter:   8298/ 13000 | loss: 1.4268 | ds_loss: 0.0000 | lr: 1.4678e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   6 | Iter:   8299/ 13000 | global iter:   8299/ 13000 | loss: 0.8236 | ds_loss: 0.0000 | lr: 1.4673e-05 | scale: 16384.0000 | micro time: 1.791 | step time: 0.000
train | epoch   6 | Iter:   8300/ 13000 | global iter:   8300/ 13000 | loss: 1.1305 | ds_loss: 0.0000 | lr: 1.4667e-05 | scale: 16384.0000 | micro time: 1.715 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8300/ 13000 | global iter:   8300/ 13000 | loss: 1.0466 | ds_loss: 0.0000 | lr: 1.4667e-05 | scale: 16384.0000 | micro time: 1.715 | step time: 1.795
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8301/ 13000 | global iter:   8301/ 13000 | loss: 0.6332 | ds_loss: 0.0000 | lr: 1.4662e-05 | scale: 16384.0000 | micro time: 1.778 | step time: 0.000
train | epoch   6 | Iter:   8302/ 13000 | global iter:   8302/ 13000 | loss: 1.0774 | ds_loss: 0.0000 | lr: 1.4656e-05 | scale: 16384.0000 | micro time: 1.800 | step time: 0.000
train | epoch   6 | Iter:   8303/ 13000 | global iter:   8303/ 13000 | loss: 0.9863 | ds_loss: 0.0000 | lr: 1.4651e-05 | scale: 16384.0000 | micro time: 1.872 | step time: 0.000
train | epoch   6 | Iter:   8304/ 13000 | global iter:   8304/ 13000 | loss: 1.1012 | ds_loss: 0.0000 | lr: 1.4646e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   6 | Iter:   8305/ 13000 | global iter:   8305/ 13000 | loss: 0.8202 | ds_loss: 0.0000 | lr: 1.4640e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   6 | Iter:   8306/ 13000 | global iter:   8306/ 13000 | loss: 1.2100 | ds_loss: 0.0000 | lr: 1.4635e-05 | scale: 16384.0000 | micro time: 1.895 | step time: 0.000
train | epoch   6 | Iter:   8307/ 13000 | global iter:   8307/ 13000 | loss: 0.7486 | ds_loss: 0.0000 | lr: 1.4629e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   6 | Iter:   8308/ 13000 | global iter:   8308/ 13000 | loss: 0.8673 | ds_loss: 0.0000 | lr: 1.4624e-05 | scale: 16384.0000 | micro time: 2.131 | step time: 0.000
train | epoch   6 | Iter:   8309/ 13000 | global iter:   8309/ 13000 | loss: 0.8695 | ds_loss: 0.0000 | lr: 1.4618e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   6 | Iter:   8310/ 13000 | global iter:   8310/ 13000 | loss: 0.8707 | ds_loss: 0.0000 | lr: 1.4613e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8310/ 13000 | global iter:   8310/ 13000 | loss: 0.9184 | ds_loss: 0.0000 | lr: 1.4613e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 1.852
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8311/ 13000 | global iter:   8311/ 13000 | loss: 1.1445 | ds_loss: 0.0000 | lr: 1.4607e-05 | scale: 16384.0000 | micro time: 1.822 | step time: 0.000
train | epoch   6 | Iter:   8312/ 13000 | global iter:   8312/ 13000 | loss: 0.5282 | ds_loss: 0.0000 | lr: 1.4602e-05 | scale: 16384.0000 | micro time: 1.826 | step time: 0.000
train | epoch   6 | Iter:   8313/ 13000 | global iter:   8313/ 13000 | loss: 0.5555 | ds_loss: 0.0000 | lr: 1.4596e-05 | scale: 16384.0000 | micro time: 2.039 | step time: 0.000
train | epoch   6 | Iter:   8314/ 13000 | global iter:   8314/ 13000 | loss: 0.8286 | ds_loss: 0.0000 | lr: 1.4591e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   6 | Iter:   8315/ 13000 | global iter:   8315/ 13000 | loss: 0.8084 | ds_loss: 0.0000 | lr: 1.4585e-05 | scale: 16384.0000 | micro time: 1.788 | step time: 0.000
train | epoch   6 | Iter:   8316/ 13000 | global iter:   8316/ 13000 | loss: 1.3577 | ds_loss: 0.0000 | lr: 1.4580e-05 | scale: 16384.0000 | micro time: 1.785 | step time: 0.000
train | epoch   6 | Iter:   8317/ 13000 | global iter:   8317/ 13000 | loss: 0.6671 | ds_loss: 0.0000 | lr: 1.4574e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   6 | Iter:   8318/ 13000 | global iter:   8318/ 13000 | loss: 1.2029 | ds_loss: 0.0000 | lr: 1.4569e-05 | scale: 16384.0000 | micro time: 2.234 | step time: 0.000
train | epoch   6 | Iter:   8319/ 13000 | global iter:   8319/ 13000 | loss: 0.8805 | ds_loss: 0.0000 | lr: 1.4563e-05 | scale: 16384.0000 | micro time: 1.851 | step time: 0.000
train | epoch   6 | Iter:   8320/ 13000 | global iter:   8320/ 13000 | loss: 0.8331 | ds_loss: 0.0000 | lr: 1.4558e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8320/ 13000 | global iter:   8320/ 13000 | loss: 0.8806 | ds_loss: 0.0000 | lr: 1.4558e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 1.873
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8321/ 13000 | global iter:   8321/ 13000 | loss: 1.1465 | ds_loss: 0.0000 | lr: 1.4552e-05 | scale: 16384.0000 | micro time: 1.878 | step time: 0.000
train | epoch   6 | Iter:   8322/ 13000 | global iter:   8322/ 13000 | loss: 0.9181 | ds_loss: 0.0000 | lr: 1.4547e-05 | scale: 16384.0000 | micro time: 1.751 | step time: 0.000
train | epoch   6 | Iter:   8323/ 13000 | global iter:   8323/ 13000 | loss: 0.8408 | ds_loss: 0.0000 | lr: 1.4542e-05 | scale: 16384.0000 | micro time: 1.685 | step time: 0.000
train | epoch   6 | Iter:   8324/ 13000 | global iter:   8324/ 13000 | loss: 0.6430 | ds_loss: 0.0000 | lr: 1.4536e-05 | scale: 16384.0000 | micro time: 1.766 | step time: 0.000
train | epoch   6 | Iter:   8325/ 13000 | global iter:   8325/ 13000 | loss: 0.9952 | ds_loss: 0.0000 | lr: 1.4531e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
train | epoch   6 | Iter:   8326/ 13000 | global iter:   8326/ 13000 | loss: 0.9009 | ds_loss: 0.0000 | lr: 1.4525e-05 | scale: 16384.0000 | micro time: 1.849 | step time: 0.000
train | epoch   6 | Iter:   8327/ 13000 | global iter:   8327/ 13000 | loss: 1.0931 | ds_loss: 0.0000 | lr: 1.4520e-05 | scale: 16384.0000 | micro time: 1.797 | step time: 0.000
train | epoch   6 | Iter:   8328/ 13000 | global iter:   8328/ 13000 | loss: 0.5052 | ds_loss: 0.0000 | lr: 1.4514e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   6 | Iter:   8329/ 13000 | global iter:   8329/ 13000 | loss: 1.2209 | ds_loss: 0.0000 | lr: 1.4509e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   6 | Iter:   8330/ 13000 | global iter:   8330/ 13000 | loss: 0.5221 | ds_loss: 0.0000 | lr: 1.4503e-05 | scale: 16384.0000 | micro time: 1.745 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8330/ 13000 | global iter:   8330/ 13000 | loss: 0.8786 | ds_loss: 0.0000 | lr: 1.4503e-05 | scale: 16384.0000 | micro time: 1.745 | step time: 1.792
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8331/ 13000 | global iter:   8331/ 13000 | loss: 1.1329 | ds_loss: 0.0000 | lr: 1.4498e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   6 | Iter:   8332/ 13000 | global iter:   8332/ 13000 | loss: 1.2352 | ds_loss: 0.0000 | lr: 1.4492e-05 | scale: 16384.0000 | micro time: 1.801 | step time: 0.000
train | epoch   6 | Iter:   8333/ 13000 | global iter:   8333/ 13000 | loss: 0.9593 | ds_loss: 0.0000 | lr: 1.4487e-05 | scale: 16384.0000 | micro time: 1.741 | step time: 0.000
train | epoch   6 | Iter:   8334/ 13000 | global iter:   8334/ 13000 | loss: 0.5840 | ds_loss: 0.0000 | lr: 1.4481e-05 | scale: 16384.0000 | micro time: 1.862 | step time: 0.000
train | epoch   6 | Iter:   8335/ 13000 | global iter:   8335/ 13000 | loss: 1.1238 | ds_loss: 0.0000 | lr: 1.4476e-05 | scale: 16384.0000 | micro time: 1.804 | step time: 0.000
train | epoch   6 | Iter:   8336/ 13000 | global iter:   8336/ 13000 | loss: 1.1655 | ds_loss: 0.0000 | lr: 1.4470e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   6 | Iter:   8337/ 13000 | global iter:   8337/ 13000 | loss: 0.7963 | ds_loss: 0.0000 | lr: 1.4465e-05 | scale: 16384.0000 | micro time: 1.726 | step time: 0.000
train | epoch   6 | Iter:   8338/ 13000 | global iter:   8338/ 13000 | loss: 1.4599 | ds_loss: 0.0000 | lr: 1.4460e-05 | scale: 16384.0000 | micro time: 1.793 | step time: 0.000
train | epoch   6 | Iter:   8339/ 13000 | global iter:   8339/ 13000 | loss: 1.3527 | ds_loss: 0.0000 | lr: 1.4454e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   6 | Iter:   8340/ 13000 | global iter:   8340/ 13000 | loss: 0.9527 | ds_loss: 0.0000 | lr: 1.4449e-05 | scale: 16384.0000 | micro time: 1.743 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8340/ 13000 | global iter:   8340/ 13000 | loss: 1.0762 | ds_loss: 0.0000 | lr: 1.4449e-05 | scale: 16384.0000 | micro time: 1.743 | step time: 1.793
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8341/ 13000 | global iter:   8341/ 13000 | loss: 0.9779 | ds_loss: 0.0000 | lr: 1.4443e-05 | scale: 16384.0000 | micro time: 1.838 | step time: 0.000
train | epoch   6 | Iter:   8342/ 13000 | global iter:   8342/ 13000 | loss: 0.7560 | ds_loss: 0.0000 | lr: 1.4438e-05 | scale: 16384.0000 | micro time: 1.756 | step time: 0.000
train | epoch   6 | Iter:   8343/ 13000 | global iter:   8343/ 13000 | loss: 0.5193 | ds_loss: 0.0000 | lr: 1.4432e-05 | scale: 16384.0000 | micro time: 1.733 | step time: 0.000
train | epoch   6 | Iter:   8344/ 13000 | global iter:   8344/ 13000 | loss: 0.9815 | ds_loss: 0.0000 | lr: 1.4427e-05 | scale: 16384.0000 | micro time: 1.760 | step time: 0.000
train | epoch   6 | Iter:   8345/ 13000 | global iter:   8345/ 13000 | loss: 0.5872 | ds_loss: 0.0000 | lr: 1.4421e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   6 | Iter:   8346/ 13000 | global iter:   8346/ 13000 | loss: 0.8438 | ds_loss: 0.0000 | lr: 1.4416e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   6 | Iter:   8347/ 13000 | global iter:   8347/ 13000 | loss: 0.7027 | ds_loss: 0.0000 | lr: 1.4410e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   6 | Iter:   8348/ 13000 | global iter:   8348/ 13000 | loss: 0.9919 | ds_loss: 0.0000 | lr: 1.4405e-05 | scale: 16384.0000 | micro time: 1.897 | step time: 0.000
train | epoch   6 | Iter:   8349/ 13000 | global iter:   8349/ 13000 | loss: 0.6922 | ds_loss: 0.0000 | lr: 1.4400e-05 | scale: 16384.0000 | micro time: 1.873 | step time: 0.000
train | epoch   6 | Iter:   8350/ 13000 | global iter:   8350/ 13000 | loss: 0.9103 | ds_loss: 0.0000 | lr: 1.4394e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8350/ 13000 | global iter:   8350/ 13000 | loss: 0.7963 | ds_loss: 0.0000 | lr: 1.4394e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 1.812
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8351/ 13000 | global iter:   8351/ 13000 | loss: 1.5216 | ds_loss: 0.0000 | lr: 1.4389e-05 | scale: 16384.0000 | micro time: 1.782 | step time: 0.000
train | epoch   6 | Iter:   8352/ 13000 | global iter:   8352/ 13000 | loss: 0.6776 | ds_loss: 0.0000 | lr: 1.4383e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   6 | Iter:   8353/ 13000 | global iter:   8353/ 13000 | loss: 1.3522 | ds_loss: 0.0000 | lr: 1.4378e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   6 | Iter:   8354/ 13000 | global iter:   8354/ 13000 | loss: 1.0988 | ds_loss: 0.0000 | lr: 1.4372e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   6 | Iter:   8355/ 13000 | global iter:   8355/ 13000 | loss: 1.1634 | ds_loss: 0.0000 | lr: 1.4367e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   6 | Iter:   8356/ 13000 | global iter:   8356/ 13000 | loss: 0.9145 | ds_loss: 0.0000 | lr: 1.4361e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   6 | Iter:   8357/ 13000 | global iter:   8357/ 13000 | loss: 1.0443 | ds_loss: 0.0000 | lr: 1.4356e-05 | scale: 16384.0000 | micro time: 1.756 | step time: 0.000
train | epoch   6 | Iter:   8358/ 13000 | global iter:   8358/ 13000 | loss: 1.1225 | ds_loss: 0.0000 | lr: 1.4350e-05 | scale: 16384.0000 | micro time: 1.716 | step time: 0.000
train | epoch   6 | Iter:   8359/ 13000 | global iter:   8359/ 13000 | loss: 0.9934 | ds_loss: 0.0000 | lr: 1.4345e-05 | scale: 16384.0000 | micro time: 1.825 | step time: 0.000
train | epoch   6 | Iter:   8360/ 13000 | global iter:   8360/ 13000 | loss: 0.7681 | ds_loss: 0.0000 | lr: 1.4340e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8360/ 13000 | global iter:   8360/ 13000 | loss: 1.0656 | ds_loss: 0.0000 | lr: 1.4340e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 1.797
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8361/ 13000 | global iter:   8361/ 13000 | loss: 1.4958 | ds_loss: 0.0000 | lr: 1.4334e-05 | scale: 16384.0000 | micro time: 1.806 | step time: 0.000
train | epoch   6 | Iter:   8362/ 13000 | global iter:   8362/ 13000 | loss: 1.2097 | ds_loss: 0.0000 | lr: 1.4329e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   6 | Iter:   8363/ 13000 | global iter:   8363/ 13000 | loss: 1.0956 | ds_loss: 0.0000 | lr: 1.4323e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   6 | Iter:   8364/ 13000 | global iter:   8364/ 13000 | loss: 0.7994 | ds_loss: 0.0000 | lr: 1.4318e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   6 | Iter:   8365/ 13000 | global iter:   8365/ 13000 | loss: 0.6683 | ds_loss: 0.0000 | lr: 1.4312e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   6 | Iter:   8366/ 13000 | global iter:   8366/ 13000 | loss: 0.7591 | ds_loss: 0.0000 | lr: 1.4307e-05 | scale: 16384.0000 | micro time: 1.907 | step time: 0.000
train | epoch   6 | Iter:   8367/ 13000 | global iter:   8367/ 13000 | loss: 1.2982 | ds_loss: 0.0000 | lr: 1.4301e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
train | epoch   6 | Iter:   8368/ 13000 | global iter:   8368/ 13000 | loss: 1.0383 | ds_loss: 0.0000 | lr: 1.4296e-05 | scale: 16384.0000 | micro time: 1.863 | step time: 0.000
train | epoch   6 | Iter:   8369/ 13000 | global iter:   8369/ 13000 | loss: 0.5478 | ds_loss: 0.0000 | lr: 1.4291e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   6 | Iter:   8370/ 13000 | global iter:   8370/ 13000 | loss: 0.8945 | ds_loss: 0.0000 | lr: 1.4285e-05 | scale: 16384.0000 | micro time: 1.885 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8370/ 13000 | global iter:   8370/ 13000 | loss: 0.9807 | ds_loss: 0.0000 | lr: 1.4285e-05 | scale: 16384.0000 | micro time: 1.885 | step time: 1.832
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8371/ 13000 | global iter:   8371/ 13000 | loss: 0.9725 | ds_loss: 0.0000 | lr: 1.4280e-05 | scale: 16384.0000 | micro time: 1.733 | step time: 0.000
train | epoch   6 | Iter:   8372/ 13000 | global iter:   8372/ 13000 | loss: 0.8733 | ds_loss: 0.0000 | lr: 1.4274e-05 | scale: 16384.0000 | micro time: 1.889 | step time: 0.000
train | epoch   6 | Iter:   8373/ 13000 | global iter:   8373/ 13000 | loss: 1.0993 | ds_loss: 0.0000 | lr: 1.4269e-05 | scale: 16384.0000 | micro time: 1.781 | step time: 0.000
train | epoch   6 | Iter:   8374/ 13000 | global iter:   8374/ 13000 | loss: 0.6711 | ds_loss: 0.0000 | lr: 1.4263e-05 | scale: 16384.0000 | micro time: 1.743 | step time: 0.000
train | epoch   6 | Iter:   8375/ 13000 | global iter:   8375/ 13000 | loss: 0.6822 | ds_loss: 0.0000 | lr: 1.4258e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   6 | Iter:   8376/ 13000 | global iter:   8376/ 13000 | loss: 1.0992 | ds_loss: 0.0000 | lr: 1.4253e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
train | epoch   6 | Iter:   8377/ 13000 | global iter:   8377/ 13000 | loss: 1.0133 | ds_loss: 0.0000 | lr: 1.4247e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   6 | Iter:   8378/ 13000 | global iter:   8378/ 13000 | loss: 1.1351 | ds_loss: 0.0000 | lr: 1.4242e-05 | scale: 16384.0000 | micro time: 1.741 | step time: 0.000
train | epoch   6 | Iter:   8379/ 13000 | global iter:   8379/ 13000 | loss: 0.8423 | ds_loss: 0.0000 | lr: 1.4236e-05 | scale: 16384.0000 | micro time: 1.772 | step time: 0.000
train | epoch   6 | Iter:   8380/ 13000 | global iter:   8380/ 13000 | loss: 1.4469 | ds_loss: 0.0000 | lr: 1.4231e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8380/ 13000 | global iter:   8380/ 13000 | loss: 0.9835 | ds_loss: 0.0000 | lr: 1.4231e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 1.792
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8381/ 13000 | global iter:   8381/ 13000 | loss: 0.8773 | ds_loss: 0.0000 | lr: 1.4225e-05 | scale: 16384.0000 | micro time: 1.732 | step time: 0.000
train | epoch   6 | Iter:   8382/ 13000 | global iter:   8382/ 13000 | loss: 1.0490 | ds_loss: 0.0000 | lr: 1.4220e-05 | scale: 16384.0000 | micro time: 1.793 | step time: 0.000
train | epoch   6 | Iter:   8383/ 13000 | global iter:   8383/ 13000 | loss: 1.3730 | ds_loss: 0.0000 | lr: 1.4215e-05 | scale: 16384.0000 | micro time: 1.739 | step time: 0.000
train | epoch   6 | Iter:   8384/ 13000 | global iter:   8384/ 13000 | loss: 1.3424 | ds_loss: 0.0000 | lr: 1.4209e-05 | scale: 16384.0000 | micro time: 1.953 | step time: 0.000
train | epoch   6 | Iter:   8385/ 13000 | global iter:   8385/ 13000 | loss: 0.7133 | ds_loss: 0.0000 | lr: 1.4204e-05 | scale: 16384.0000 | micro time: 1.686 | step time: 0.000
train | epoch   6 | Iter:   8386/ 13000 | global iter:   8386/ 13000 | loss: 1.5045 | ds_loss: 0.0000 | lr: 1.4198e-05 | scale: 16384.0000 | micro time: 1.893 | step time: 0.000
train | epoch   6 | Iter:   8387/ 13000 | global iter:   8387/ 13000 | loss: 0.9811 | ds_loss: 0.0000 | lr: 1.4193e-05 | scale: 16384.0000 | micro time: 1.822 | step time: 0.000
train | epoch   6 | Iter:   8388/ 13000 | global iter:   8388/ 13000 | loss: 0.7518 | ds_loss: 0.0000 | lr: 1.4187e-05 | scale: 16384.0000 | micro time: 1.729 | step time: 0.000
train | epoch   6 | Iter:   8389/ 13000 | global iter:   8389/ 13000 | loss: 1.3578 | ds_loss: 0.0000 | lr: 1.4182e-05 | scale: 16384.0000 | micro time: 1.789 | step time: 0.000
train | epoch   6 | Iter:   8390/ 13000 | global iter:   8390/ 13000 | loss: 0.5709 | ds_loss: 0.0000 | lr: 1.4177e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8390/ 13000 | global iter:   8390/ 13000 | loss: 1.0521 | ds_loss: 0.0000 | lr: 1.4177e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 1.798
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8391/ 13000 | global iter:   8391/ 13000 | loss: 0.5806 | ds_loss: 0.0000 | lr: 1.4171e-05 | scale: 16384.0000 | micro time: 1.826 | step time: 0.000
train | epoch   6 | Iter:   8392/ 13000 | global iter:   8392/ 13000 | loss: 0.9707 | ds_loss: 0.0000 | lr: 1.4166e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   6 | Iter:   8393/ 13000 | global iter:   8393/ 13000 | loss: 0.6011 | ds_loss: 0.0000 | lr: 1.4160e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   6 | Iter:   8394/ 13000 | global iter:   8394/ 13000 | loss: 0.5424 | ds_loss: 0.0000 | lr: 1.4155e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   6 | Iter:   8395/ 13000 | global iter:   8395/ 13000 | loss: 0.8753 | ds_loss: 0.0000 | lr: 1.4149e-05 | scale: 16384.0000 | micro time: 1.818 | step time: 0.000
train | epoch   6 | Iter:   8396/ 13000 | global iter:   8396/ 13000 | loss: 0.6563 | ds_loss: 0.0000 | lr: 1.4144e-05 | scale: 16384.0000 | micro time: 1.960 | step time: 0.000
train | epoch   6 | Iter:   8397/ 13000 | global iter:   8397/ 13000 | loss: 1.0038 | ds_loss: 0.0000 | lr: 1.4139e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   6 | Iter:   8398/ 13000 | global iter:   8398/ 13000 | loss: 0.7574 | ds_loss: 0.0000 | lr: 1.4133e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   6 | Iter:   8399/ 13000 | global iter:   8399/ 13000 | loss: 0.8432 | ds_loss: 0.0000 | lr: 1.4128e-05 | scale: 16384.0000 | micro time: 1.786 | step time: 0.000
train | epoch   6 | Iter:   8400/ 13000 | global iter:   8400/ 13000 | loss: 0.3553 | ds_loss: 0.0000 | lr: 1.4122e-05 | scale: 16384.0000 | micro time: 1.729 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8400/ 13000 | global iter:   8400/ 13000 | loss: 0.7186 | ds_loss: 0.0000 | lr: 1.4122e-05 | scale: 16384.0000 | micro time: 1.729 | step time: 1.806
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8401/ 13000 | global iter:   8401/ 13000 | loss: 1.2532 | ds_loss: 0.0000 | lr: 1.4117e-05 | scale: 16384.0000 | micro time: 1.785 | step time: 0.000
train | epoch   6 | Iter:   8402/ 13000 | global iter:   8402/ 13000 | loss: 1.4725 | ds_loss: 0.0000 | lr: 1.4111e-05 | scale: 16384.0000 | micro time: 1.732 | step time: 0.000
train | epoch   6 | Iter:   8403/ 13000 | global iter:   8403/ 13000 | loss: 1.3438 | ds_loss: 0.0000 | lr: 1.4106e-05 | scale: 16384.0000 | micro time: 1.722 | step time: 0.000
train | epoch   6 | Iter:   8404/ 13000 | global iter:   8404/ 13000 | loss: 0.6377 | ds_loss: 0.0000 | lr: 1.4101e-05 | scale: 16384.0000 | micro time: 1.875 | step time: 0.000
train | epoch   6 | Iter:   8405/ 13000 | global iter:   8405/ 13000 | loss: 0.9514 | ds_loss: 0.0000 | lr: 1.4095e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   6 | Iter:   8406/ 13000 | global iter:   8406/ 13000 | loss: 1.0015 | ds_loss: 0.0000 | lr: 1.4090e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   6 | Iter:   8407/ 13000 | global iter:   8407/ 13000 | loss: 0.8566 | ds_loss: 0.0000 | lr: 1.4084e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   6 | Iter:   8408/ 13000 | global iter:   8408/ 13000 | loss: 0.9029 | ds_loss: 0.0000 | lr: 1.4079e-05 | scale: 16384.0000 | micro time: 1.811 | step time: 0.000
train | epoch   6 | Iter:   8409/ 13000 | global iter:   8409/ 13000 | loss: 0.4582 | ds_loss: 0.0000 | lr: 1.4074e-05 | scale: 16384.0000 | micro time: 1.887 | step time: 0.000
train | epoch   6 | Iter:   8410/ 13000 | global iter:   8410/ 13000 | loss: 0.9021 | ds_loss: 0.0000 | lr: 1.4068e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8410/ 13000 | global iter:   8410/ 13000 | loss: 0.9780 | ds_loss: 0.0000 | lr: 1.4068e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 1.789
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8411/ 13000 | global iter:   8411/ 13000 | loss: 1.0370 | ds_loss: 0.0000 | lr: 1.4063e-05 | scale: 16384.0000 | micro time: 1.874 | step time: 0.000
train | epoch   6 | Iter:   8412/ 13000 | global iter:   8412/ 13000 | loss: 0.4164 | ds_loss: 0.0000 | lr: 1.4057e-05 | scale: 16384.0000 | micro time: 1.869 | step time: 0.000
train | epoch   6 | Iter:   8413/ 13000 | global iter:   8413/ 13000 | loss: 0.6414 | ds_loss: 0.0000 | lr: 1.4052e-05 | scale: 16384.0000 | micro time: 1.793 | step time: 0.000
train | epoch   6 | Iter:   8414/ 13000 | global iter:   8414/ 13000 | loss: 0.9695 | ds_loss: 0.0000 | lr: 1.4046e-05 | scale: 16384.0000 | micro time: 1.792 | step time: 0.000
train | epoch   6 | Iter:   8415/ 13000 | global iter:   8415/ 13000 | loss: 1.0212 | ds_loss: 0.0000 | lr: 1.4041e-05 | scale: 16384.0000 | micro time: 1.854 | step time: 0.000
train | epoch   6 | Iter:   8416/ 13000 | global iter:   8416/ 13000 | loss: 1.3566 | ds_loss: 0.0000 | lr: 1.4036e-05 | scale: 16384.0000 | micro time: 1.727 | step time: 0.000
train | epoch   6 | Iter:   8417/ 13000 | global iter:   8417/ 13000 | loss: 0.2829 | ds_loss: 0.0000 | lr: 1.4030e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   6 | Iter:   8418/ 13000 | global iter:   8418/ 13000 | loss: 0.5810 | ds_loss: 0.0000 | lr: 1.4025e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   6 | Iter:   8419/ 13000 | global iter:   8419/ 13000 | loss: 1.2033 | ds_loss: 0.0000 | lr: 1.4019e-05 | scale: 16384.0000 | micro time: 1.667 | step time: 0.000
train | epoch   6 | Iter:   8420/ 13000 | global iter:   8420/ 13000 | loss: 0.7365 | ds_loss: 0.0000 | lr: 1.4014e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8420/ 13000 | global iter:   8420/ 13000 | loss: 0.8246 | ds_loss: 0.0000 | lr: 1.4014e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 1.800
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8421/ 13000 | global iter:   8421/ 13000 | loss: 0.8622 | ds_loss: 0.0000 | lr: 1.4009e-05 | scale: 16384.0000 | micro time: 1.750 | step time: 0.000
train | epoch   6 | Iter:   8422/ 13000 | global iter:   8422/ 13000 | loss: 0.7770 | ds_loss: 0.0000 | lr: 1.4003e-05 | scale: 16384.0000 | micro time: 1.891 | step time: 0.000
train | epoch   6 | Iter:   8423/ 13000 | global iter:   8423/ 13000 | loss: 0.9752 | ds_loss: 0.0000 | lr: 1.3998e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   6 | Iter:   8424/ 13000 | global iter:   8424/ 13000 | loss: 0.1542 | ds_loss: 0.0000 | lr: 1.3992e-05 | scale: 16384.0000 | micro time: 1.813 | step time: 0.000
train | epoch   6 | Iter:   8425/ 13000 | global iter:   8425/ 13000 | loss: 0.8590 | ds_loss: 0.0000 | lr: 1.3987e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   6 | Iter:   8426/ 13000 | global iter:   8426/ 13000 | loss: 1.2076 | ds_loss: 0.0000 | lr: 1.3982e-05 | scale: 16384.0000 | micro time: 1.746 | step time: 0.000
train | epoch   6 | Iter:   8427/ 13000 | global iter:   8427/ 13000 | loss: 1.2361 | ds_loss: 0.0000 | lr: 1.3976e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   6 | Iter:   8428/ 13000 | global iter:   8428/ 13000 | loss: 0.8573 | ds_loss: 0.0000 | lr: 1.3971e-05 | scale: 16384.0000 | micro time: 1.766 | step time: 0.000
train | epoch   6 | Iter:   8429/ 13000 | global iter:   8429/ 13000 | loss: 0.9765 | ds_loss: 0.0000 | lr: 1.3965e-05 | scale: 16384.0000 | micro time: 1.908 | step time: 0.000
train | epoch   6 | Iter:   8430/ 13000 | global iter:   8430/ 13000 | loss: 1.3462 | ds_loss: 0.0000 | lr: 1.3960e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8430/ 13000 | global iter:   8430/ 13000 | loss: 0.9251 | ds_loss: 0.0000 | lr: 1.3960e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 1.814
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8431/ 13000 | global iter:   8431/ 13000 | loss: 0.6075 | ds_loss: 0.0000 | lr: 1.3955e-05 | scale: 16384.0000 | micro time: 1.678 | step time: 0.000
train | epoch   6 | Iter:   8432/ 13000 | global iter:   8432/ 13000 | loss: 1.0026 | ds_loss: 0.0000 | lr: 1.3949e-05 | scale: 16384.0000 | micro time: 1.795 | step time: 0.000
train | epoch   6 | Iter:   8433/ 13000 | global iter:   8433/ 13000 | loss: 1.2518 | ds_loss: 0.0000 | lr: 1.3944e-05 | scale: 16384.0000 | micro time: 1.731 | step time: 0.000
train | epoch   6 | Iter:   8434/ 13000 | global iter:   8434/ 13000 | loss: 1.4009 | ds_loss: 0.0000 | lr: 1.3938e-05 | scale: 16384.0000 | micro time: 1.822 | step time: 0.000
train | epoch   6 | Iter:   8435/ 13000 | global iter:   8435/ 13000 | loss: 0.2742 | ds_loss: 0.0000 | lr: 1.3933e-05 | scale: 16384.0000 | micro time: 1.757 | step time: 0.000
train | epoch   6 | Iter:   8436/ 13000 | global iter:   8436/ 13000 | loss: 0.6752 | ds_loss: 0.0000 | lr: 1.3928e-05 | scale: 16384.0000 | micro time: 1.696 | step time: 0.000
train | epoch   6 | Iter:   8437/ 13000 | global iter:   8437/ 13000 | loss: 0.8281 | ds_loss: 0.0000 | lr: 1.3922e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   6 | Iter:   8438/ 13000 | global iter:   8438/ 13000 | loss: 0.9991 | ds_loss: 0.0000 | lr: 1.3917e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   6 | Iter:   8439/ 13000 | global iter:   8439/ 13000 | loss: 1.1437 | ds_loss: 0.0000 | lr: 1.3911e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
train | epoch   6 | Iter:   8440/ 13000 | global iter:   8440/ 13000 | loss: 0.5882 | ds_loss: 0.0000 | lr: 1.3906e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8440/ 13000 | global iter:   8440/ 13000 | loss: 0.8771 | ds_loss: 0.0000 | lr: 1.3906e-05 | scale: 16384.0000 | micro time: 1.871 | step time: 1.783
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8441/ 13000 | global iter:   8441/ 13000 | loss: 1.1322 | ds_loss: 0.0000 | lr: 1.3901e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   6 | Iter:   8442/ 13000 | global iter:   8442/ 13000 | loss: 0.6532 | ds_loss: 0.0000 | lr: 1.3895e-05 | scale: 16384.0000 | micro time: 1.784 | step time: 0.000
train | epoch   6 | Iter:   8443/ 13000 | global iter:   8443/ 13000 | loss: 1.0641 | ds_loss: 0.0000 | lr: 1.3890e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   6 | Iter:   8444/ 13000 | global iter:   8444/ 13000 | loss: 1.2098 | ds_loss: 0.0000 | lr: 1.3884e-05 | scale: 16384.0000 | micro time: 1.895 | step time: 0.000
train | epoch   6 | Iter:   8445/ 13000 | global iter:   8445/ 13000 | loss: 1.3259 | ds_loss: 0.0000 | lr: 1.3879e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   6 | Iter:   8446/ 13000 | global iter:   8446/ 13000 | loss: 1.0305 | ds_loss: 0.0000 | lr: 1.3874e-05 | scale: 16384.0000 | micro time: 1.915 | step time: 0.000
train | epoch   6 | Iter:   8447/ 13000 | global iter:   8447/ 13000 | loss: 0.6842 | ds_loss: 0.0000 | lr: 1.3868e-05 | scale: 16384.0000 | micro time: 1.717 | step time: 0.000
train | epoch   6 | Iter:   8448/ 13000 | global iter:   8448/ 13000 | loss: 1.1022 | ds_loss: 0.0000 | lr: 1.3863e-05 | scale: 16384.0000 | micro time: 1.865 | step time: 0.000
train | epoch   6 | Iter:   8449/ 13000 | global iter:   8449/ 13000 | loss: 1.3153 | ds_loss: 0.0000 | lr: 1.3857e-05 | scale: 16384.0000 | micro time: 1.743 | step time: 0.000
train | epoch   6 | Iter:   8450/ 13000 | global iter:   8450/ 13000 | loss: 1.0684 | ds_loss: 0.0000 | lr: 1.3852e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8450/ 13000 | global iter:   8450/ 13000 | loss: 1.0586 | ds_loss: 0.0000 | lr: 1.3852e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 1.823
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8451/ 13000 | global iter:   8451/ 13000 | loss: 0.5592 | ds_loss: 0.0000 | lr: 1.3847e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
train | epoch   6 | Iter:   8452/ 13000 | global iter:   8452/ 13000 | loss: 0.7786 | ds_loss: 0.0000 | lr: 1.3841e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   6 | Iter:   8453/ 13000 | global iter:   8453/ 13000 | loss: 0.9258 | ds_loss: 0.0000 | lr: 1.3836e-05 | scale: 16384.0000 | micro time: 1.868 | step time: 0.000
train | epoch   6 | Iter:   8454/ 13000 | global iter:   8454/ 13000 | loss: 1.0794 | ds_loss: 0.0000 | lr: 1.3831e-05 | scale: 16384.0000 | micro time: 1.804 | step time: 0.000
train | epoch   6 | Iter:   8455/ 13000 | global iter:   8455/ 13000 | loss: 0.7405 | ds_loss: 0.0000 | lr: 1.3825e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   6 | Iter:   8456/ 13000 | global iter:   8456/ 13000 | loss: 1.1505 | ds_loss: 0.0000 | lr: 1.3820e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   6 | Iter:   8457/ 13000 | global iter:   8457/ 13000 | loss: 0.7550 | ds_loss: 0.0000 | lr: 1.3814e-05 | scale: 16384.0000 | micro time: 1.746 | step time: 0.000
train | epoch   6 | Iter:   8458/ 13000 | global iter:   8458/ 13000 | loss: 1.2273 | ds_loss: 0.0000 | lr: 1.3809e-05 | scale: 16384.0000 | micro time: 1.880 | step time: 0.000
train | epoch   6 | Iter:   8459/ 13000 | global iter:   8459/ 13000 | loss: 1.1493 | ds_loss: 0.0000 | lr: 1.3804e-05 | scale: 16384.0000 | micro time: 1.923 | step time: 0.000
train | epoch   6 | Iter:   8460/ 13000 | global iter:   8460/ 13000 | loss: 1.0086 | ds_loss: 0.0000 | lr: 1.3798e-05 | scale: 16384.0000 | micro time: 1.699 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8460/ 13000 | global iter:   8460/ 13000 | loss: 0.9374 | ds_loss: 0.0000 | lr: 1.3798e-05 | scale: 16384.0000 | micro time: 1.699 | step time: 1.826
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8461/ 13000 | global iter:   8461/ 13000 | loss: 0.8708 | ds_loss: 0.0000 | lr: 1.3793e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   6 | Iter:   8462/ 13000 | global iter:   8462/ 13000 | loss: 1.0376 | ds_loss: 0.0000 | lr: 1.3787e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   6 | Iter:   8463/ 13000 | global iter:   8463/ 13000 | loss: 1.0596 | ds_loss: 0.0000 | lr: 1.3782e-05 | scale: 16384.0000 | micro time: 1.935 | step time: 0.000
train | epoch   6 | Iter:   8464/ 13000 | global iter:   8464/ 13000 | loss: 1.1824 | ds_loss: 0.0000 | lr: 1.3777e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   6 | Iter:   8465/ 13000 | global iter:   8465/ 13000 | loss: 0.8588 | ds_loss: 0.0000 | lr: 1.3771e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   6 | Iter:   8466/ 13000 | global iter:   8466/ 13000 | loss: 1.7323 | ds_loss: 0.0000 | lr: 1.3766e-05 | scale: 16384.0000 | micro time: 1.887 | step time: 0.000
train | epoch   6 | Iter:   8467/ 13000 | global iter:   8467/ 13000 | loss: 0.5568 | ds_loss: 0.0000 | lr: 1.3761e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   6 | Iter:   8468/ 13000 | global iter:   8468/ 13000 | loss: 0.3059 | ds_loss: 0.0000 | lr: 1.3755e-05 | scale: 16384.0000 | micro time: 1.775 | step time: 0.000
train | epoch   6 | Iter:   8469/ 13000 | global iter:   8469/ 13000 | loss: 0.9600 | ds_loss: 0.0000 | lr: 1.3750e-05 | scale: 16384.0000 | micro time: 1.769 | step time: 0.000
train | epoch   6 | Iter:   8470/ 13000 | global iter:   8470/ 13000 | loss: 1.3338 | ds_loss: 0.0000 | lr: 1.3744e-05 | scale: 16384.0000 | micro time: 1.877 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8470/ 13000 | global iter:   8470/ 13000 | loss: 0.9898 | ds_loss: 0.0000 | lr: 1.3744e-05 | scale: 16384.0000 | micro time: 1.877 | step time: 1.828
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8471/ 13000 | global iter:   8471/ 13000 | loss: 0.9643 | ds_loss: 0.0000 | lr: 1.3739e-05 | scale: 16384.0000 | micro time: 1.678 | step time: 0.000
train | epoch   6 | Iter:   8472/ 13000 | global iter:   8472/ 13000 | loss: 1.1403 | ds_loss: 0.0000 | lr: 1.3734e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
train | epoch   6 | Iter:   8473/ 13000 | global iter:   8473/ 13000 | loss: 0.4352 | ds_loss: 0.0000 | lr: 1.3728e-05 | scale: 16384.0000 | micro time: 1.832 | step time: 0.000
train | epoch   6 | Iter:   8474/ 13000 | global iter:   8474/ 13000 | loss: 1.2838 | ds_loss: 0.0000 | lr: 1.3723e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   6 | Iter:   8475/ 13000 | global iter:   8475/ 13000 | loss: 0.5358 | ds_loss: 0.0000 | lr: 1.3718e-05 | scale: 16384.0000 | micro time: 1.745 | step time: 0.000
train | epoch   6 | Iter:   8476/ 13000 | global iter:   8476/ 13000 | loss: 1.3772 | ds_loss: 0.0000 | lr: 1.3712e-05 | scale: 16384.0000 | micro time: 1.765 | step time: 0.000
train | epoch   6 | Iter:   8477/ 13000 | global iter:   8477/ 13000 | loss: 0.5229 | ds_loss: 0.0000 | lr: 1.3707e-05 | scale: 16384.0000 | micro time: 1.718 | step time: 0.000
train | epoch   6 | Iter:   8478/ 13000 | global iter:   8478/ 13000 | loss: 1.3017 | ds_loss: 0.0000 | lr: 1.3701e-05 | scale: 16384.0000 | micro time: 1.864 | step time: 0.000
train | epoch   6 | Iter:   8479/ 13000 | global iter:   8479/ 13000 | loss: 0.7791 | ds_loss: 0.0000 | lr: 1.3696e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   6 | Iter:   8480/ 13000 | global iter:   8480/ 13000 | loss: 0.8526 | ds_loss: 0.0000 | lr: 1.3691e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8480/ 13000 | global iter:   8480/ 13000 | loss: 0.9193 | ds_loss: 0.0000 | lr: 1.3691e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 1.794
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8481/ 13000 | global iter:   8481/ 13000 | loss: 0.7423 | ds_loss: 0.0000 | lr: 1.3685e-05 | scale: 16384.0000 | micro time: 1.809 | step time: 0.000
train | epoch   6 | Iter:   8482/ 13000 | global iter:   8482/ 13000 | loss: 1.1181 | ds_loss: 0.0000 | lr: 1.3680e-05 | scale: 16384.0000 | micro time: 1.891 | step time: 0.000
train | epoch   6 | Iter:   8483/ 13000 | global iter:   8483/ 13000 | loss: 1.2086 | ds_loss: 0.0000 | lr: 1.3675e-05 | scale: 16384.0000 | micro time: 1.787 | step time: 0.000
train | epoch   6 | Iter:   8484/ 13000 | global iter:   8484/ 13000 | loss: 1.3682 | ds_loss: 0.0000 | lr: 1.3669e-05 | scale: 16384.0000 | micro time: 1.754 | step time: 0.000
train | epoch   6 | Iter:   8485/ 13000 | global iter:   8485/ 13000 | loss: 0.9801 | ds_loss: 0.0000 | lr: 1.3664e-05 | scale: 16384.0000 | micro time: 1.779 | step time: 0.000
train | epoch   6 | Iter:   8486/ 13000 | global iter:   8486/ 13000 | loss: 0.7512 | ds_loss: 0.0000 | lr: 1.3659e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   6 | Iter:   8487/ 13000 | global iter:   8487/ 13000 | loss: 0.6328 | ds_loss: 0.0000 | lr: 1.3653e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
train | epoch   6 | Iter:   8488/ 13000 | global iter:   8488/ 13000 | loss: 0.8989 | ds_loss: 0.0000 | lr: 1.3648e-05 | scale: 16384.0000 | micro time: 1.747 | step time: 0.000
train | epoch   6 | Iter:   8489/ 13000 | global iter:   8489/ 13000 | loss: 0.7855 | ds_loss: 0.0000 | lr: 1.3642e-05 | scale: 16384.0000 | micro time: 1.816 | step time: 0.000
train | epoch   6 | Iter:   8490/ 13000 | global iter:   8490/ 13000 | loss: 1.1135 | ds_loss: 0.0000 | lr: 1.3637e-05 | scale: 16384.0000 | micro time: 1.846 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8490/ 13000 | global iter:   8490/ 13000 | loss: 0.9599 | ds_loss: 0.0000 | lr: 1.3637e-05 | scale: 16384.0000 | micro time: 1.846 | step time: 1.815
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8491/ 13000 | global iter:   8491/ 13000 | loss: 0.5562 | ds_loss: 0.0000 | lr: 1.3632e-05 | scale: 16384.0000 | micro time: 1.837 | step time: 0.000
train | epoch   6 | Iter:   8492/ 13000 | global iter:   8492/ 13000 | loss: 0.6924 | ds_loss: 0.0000 | lr: 1.3626e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   6 | Iter:   8493/ 13000 | global iter:   8493/ 13000 | loss: 0.9882 | ds_loss: 0.0000 | lr: 1.3621e-05 | scale: 16384.0000 | micro time: 1.847 | step time: 0.000
train | epoch   6 | Iter:   8494/ 13000 | global iter:   8494/ 13000 | loss: 0.6863 | ds_loss: 0.0000 | lr: 1.3616e-05 | scale: 16384.0000 | micro time: 1.801 | step time: 0.000
train | epoch   6 | Iter:   8495/ 13000 | global iter:   8495/ 13000 | loss: 0.6003 | ds_loss: 0.0000 | lr: 1.3610e-05 | scale: 16384.0000 | micro time: 1.718 | step time: 0.000
train | epoch   6 | Iter:   8496/ 13000 | global iter:   8496/ 13000 | loss: 1.4556 | ds_loss: 0.0000 | lr: 1.3605e-05 | scale: 16384.0000 | micro time: 1.817 | step time: 0.000
train | epoch   6 | Iter:   8497/ 13000 | global iter:   8497/ 13000 | loss: 1.2601 | ds_loss: 0.0000 | lr: 1.3600e-05 | scale: 16384.0000 | micro time: 1.855 | step time: 0.000
train | epoch   6 | Iter:   8498/ 13000 | global iter:   8498/ 13000 | loss: 1.4752 | ds_loss: 0.0000 | lr: 1.3594e-05 | scale: 16384.0000 | micro time: 1.712 | step time: 0.000
train | epoch   6 | Iter:   8499/ 13000 | global iter:   8499/ 13000 | loss: 1.4810 | ds_loss: 0.0000 | lr: 1.3589e-05 | scale: 16384.0000 | micro time: 1.870 | step time: 0.000
train | epoch   6 | Iter:   8500/ 13000 | global iter:   8500/ 13000 | loss: 1.3748 | ds_loss: 0.0000 | lr: 1.3583e-05 | scale: 16384.0000 | micro time: 1.759 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8500/ 13000 | global iter:   8500/ 13000 | loss: 1.0570 | ds_loss: 0.0000 | lr: 1.3583e-05 | scale: 16384.0000 | micro time: 1.759 | step time: 1.806
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8501/ 13000 | global iter:   8501/ 13000 | loss: 0.6692 | ds_loss: 0.0000 | lr: 1.3578e-05 | scale: 16384.0000 | micro time: 1.826 | step time: 0.000
train | epoch   6 | Iter:   8502/ 13000 | global iter:   8502/ 13000 | loss: 1.1296 | ds_loss: 0.0000 | lr: 1.3573e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   6 | Iter:   8503/ 13000 | global iter:   8503/ 13000 | loss: 1.2143 | ds_loss: 0.0000 | lr: 1.3567e-05 | scale: 16384.0000 | micro time: 1.833 | step time: 0.000
train | epoch   6 | Iter:   8504/ 13000 | global iter:   8504/ 13000 | loss: 0.4839 | ds_loss: 0.0000 | lr: 1.3562e-05 | scale: 16384.0000 | micro time: 1.856 | step time: 0.000
train | epoch   6 | Iter:   8505/ 13000 | global iter:   8505/ 13000 | loss: 0.8181 | ds_loss: 0.0000 | lr: 1.3557e-05 | scale: 16384.0000 | micro time: 1.738 | step time: 0.000
train | epoch   6 | Iter:   8506/ 13000 | global iter:   8506/ 13000 | loss: 1.0546 | ds_loss: 0.0000 | lr: 1.3551e-05 | scale: 16384.0000 | micro time: 1.800 | step time: 0.000
train | epoch   6 | Iter:   8507/ 13000 | global iter:   8507/ 13000 | loss: 0.9209 | ds_loss: 0.0000 | lr: 1.3546e-05 | scale: 16384.0000 | micro time: 1.727 | step time: 0.000
train | epoch   6 | Iter:   8508/ 13000 | global iter:   8508/ 13000 | loss: 1.0138 | ds_loss: 0.0000 | lr: 1.3541e-05 | scale: 16384.0000 | micro time: 1.859 | step time: 0.000
train | epoch   6 | Iter:   8509/ 13000 | global iter:   8509/ 13000 | loss: 0.9170 | ds_loss: 0.0000 | lr: 1.3535e-05 | scale: 16384.0000 | micro time: 1.835 | step time: 0.000
train | epoch   6 | Iter:   8510/ 13000 | global iter:   8510/ 13000 | loss: 0.3533 | ds_loss: 0.0000 | lr: 1.3530e-05 | scale: 16384.0000 | micro time: 1.770 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8510/ 13000 | global iter:   8510/ 13000 | loss: 0.8575 | ds_loss: 0.0000 | lr: 1.3530e-05 | scale: 16384.0000 | micro time: 1.770 | step time: 1.805
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8511/ 13000 | global iter:   8511/ 13000 | loss: 0.8965 | ds_loss: 0.0000 | lr: 1.3525e-05 | scale: 16384.0000 | micro time: 1.826 | step time: 0.000
train | epoch   6 | Iter:   8512/ 13000 | global iter:   8512/ 13000 | loss: 0.9564 | ds_loss: 0.0000 | lr: 1.3519e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   6 | Iter:   8513/ 13000 | global iter:   8513/ 13000 | loss: 0.6484 | ds_loss: 0.0000 | lr: 1.3514e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   6 | Iter:   8514/ 13000 | global iter:   8514/ 13000 | loss: 1.3390 | ds_loss: 0.0000 | lr: 1.3509e-05 | scale: 16384.0000 | micro time: 1.858 | step time: 0.000
train | epoch   6 | Iter:   8515/ 13000 | global iter:   8515/ 13000 | loss: 1.0008 | ds_loss: 0.0000 | lr: 1.3503e-05 | scale: 16384.0000 | micro time: 1.824 | step time: 0.000
train | epoch   6 | Iter:   8516/ 13000 | global iter:   8516/ 13000 | loss: 0.8147 | ds_loss: 0.0000 | lr: 1.3498e-05 | scale: 16384.0000 | micro time: 1.807 | step time: 0.000
train | epoch   6 | Iter:   8517/ 13000 | global iter:   8517/ 13000 | loss: 1.2431 | ds_loss: 0.0000 | lr: 1.3493e-05 | scale: 16384.0000 | micro time: 1.843 | step time: 0.000
train | epoch   6 | Iter:   8518/ 13000 | global iter:   8518/ 13000 | loss: 0.4159 | ds_loss: 0.0000 | lr: 1.3487e-05 | scale: 16384.0000 | micro time: 1.783 | step time: 0.000
train | epoch   6 | Iter:   8519/ 13000 | global iter:   8519/ 13000 | loss: 1.7892 | ds_loss: 0.0000 | lr: 1.3482e-05 | scale: 16384.0000 | micro time: 1.909 | step time: 0.000
train | epoch   6 | Iter:   8520/ 13000 | global iter:   8520/ 13000 | loss: 1.4629 | ds_loss: 0.0000 | lr: 1.3477e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8520/ 13000 | global iter:   8520/ 13000 | loss: 1.0567 | ds_loss: 0.0000 | lr: 1.3477e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 1.825
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8521/ 13000 | global iter:   8521/ 13000 | loss: 0.4732 | ds_loss: 0.0000 | lr: 1.3471e-05 | scale: 16384.0000 | micro time: 1.797 | step time: 0.000
train | epoch   6 | Iter:   8522/ 13000 | global iter:   8522/ 13000 | loss: 1.1373 | ds_loss: 0.0000 | lr: 1.3466e-05 | scale: 16384.0000 | micro time: 1.883 | step time: 0.000
train | epoch   6 | Iter:   8523/ 13000 | global iter:   8523/ 13000 | loss: 0.8601 | ds_loss: 0.0000 | lr: 1.3460e-05 | scale: 16384.0000 | micro time: 1.707 | step time: 0.000
train | epoch   6 | Iter:   8524/ 13000 | global iter:   8524/ 13000 | loss: 0.9017 | ds_loss: 0.0000 | lr: 1.3455e-05 | scale: 16384.0000 | micro time: 1.803 | step time: 0.000
train | epoch   6 | Iter:   8525/ 13000 | global iter:   8525/ 13000 | loss: 0.8998 | ds_loss: 0.0000 | lr: 1.3450e-05 | scale: 16384.0000 | micro time: 1.687 | step time: 0.000
train | epoch   6 | Iter:   8526/ 13000 | global iter:   8526/ 13000 | loss: 0.6132 | ds_loss: 0.0000 | lr: 1.3444e-05 | scale: 16384.0000 | micro time: 1.818 | step time: 0.000
train | epoch   6 | Iter:   8527/ 13000 | global iter:   8527/ 13000 | loss: 1.1077 | ds_loss: 0.0000 | lr: 1.3439e-05 | scale: 16384.0000 | micro time: 1.860 | step time: 0.000
train | epoch   6 | Iter:   8528/ 13000 | global iter:   8528/ 13000 | loss: 0.3569 | ds_loss: 0.0000 | lr: 1.3434e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
train | epoch   6 | Iter:   8529/ 13000 | global iter:   8529/ 13000 | loss: 0.3393 | ds_loss: 0.0000 | lr: 1.3428e-05 | scale: 16384.0000 | micro time: 1.683 | step time: 0.000
train | epoch   6 | Iter:   8530/ 13000 | global iter:   8530/ 13000 | loss: 1.1022 | ds_loss: 0.0000 | lr: 1.3423e-05 | scale: 16384.0000 | micro time: 1.887 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8530/ 13000 | global iter:   8530/ 13000 | loss: 0.7791 | ds_loss: 0.0000 | lr: 1.3423e-05 | scale: 16384.0000 | micro time: 1.887 | step time: 1.792
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8531/ 13000 | global iter:   8531/ 13000 | loss: 1.4150 | ds_loss: 0.0000 | lr: 1.3418e-05 | scale: 16384.0000 | micro time: 1.832 | step time: 0.000
train | epoch   6 | Iter:   8532/ 13000 | global iter:   8532/ 13000 | loss: 1.1495 | ds_loss: 0.0000 | lr: 1.3412e-05 | scale: 16384.0000 | micro time: 1.888 | step time: 0.000
train | epoch   6 | Iter:   8533/ 13000 | global iter:   8533/ 13000 | loss: 0.9448 | ds_loss: 0.0000 | lr: 1.3407e-05 | scale: 16384.0000 | micro time: 1.772 | step time: 0.000
train | epoch   6 | Iter:   8534/ 13000 | global iter:   8534/ 13000 | loss: 1.4031 | ds_loss: 0.0000 | lr: 1.3402e-05 | scale: 16384.0000 | micro time: 1.831 | step time: 0.000
train | epoch   6 | Iter:   8535/ 13000 | global iter:   8535/ 13000 | loss: 1.0180 | ds_loss: 0.0000 | lr: 1.3396e-05 | scale: 16384.0000 | micro time: 1.903 | step time: 0.000
train | epoch   6 | Iter:   8536/ 13000 | global iter:   8536/ 13000 | loss: 0.9576 | ds_loss: 0.0000 | lr: 1.3391e-05 | scale: 16384.0000 | micro time: 1.753 | step time: 0.000
train | epoch   6 | Iter:   8537/ 13000 | global iter:   8537/ 13000 | loss: 1.0685 | ds_loss: 0.0000 | lr: 1.3386e-05 | scale: 16384.0000 | micro time: 1.877 | step time: 0.000
train | epoch   6 | Iter:   8538/ 13000 | global iter:   8538/ 13000 | loss: 0.7890 | ds_loss: 0.0000 | lr: 1.3380e-05 | scale: 16384.0000 | micro time: 1.903 | step time: 0.000
train | epoch   6 | Iter:   8539/ 13000 | global iter:   8539/ 13000 | loss: 1.6027 | ds_loss: 0.0000 | lr: 1.3375e-05 | scale: 16384.0000 | micro time: 1.823 | step time: 0.000
train | epoch   6 | Iter:   8540/ 13000 | global iter:   8540/ 13000 | loss: 0.8744 | ds_loss: 0.0000 | lr: 1.3370e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8540/ 13000 | global iter:   8540/ 13000 | loss: 1.1223 | ds_loss: 0.0000 | lr: 1.3370e-05 | scale: 16384.0000 | micro time: 1.799 | step time: 1.838
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8541/ 13000 | global iter:   8541/ 13000 | loss: 0.7657 | ds_loss: 0.0000 | lr: 1.3364e-05 | scale: 16384.0000 | micro time: 1.738 | step time: 0.000
train | epoch   6 | Iter:   8542/ 13000 | global iter:   8542/ 13000 | loss: 0.4973 | ds_loss: 0.0000 | lr: 1.3359e-05 | scale: 16384.0000 | micro time: 1.763 | step time: 0.000
train | epoch   6 | Iter:   8543/ 13000 | global iter:   8543/ 13000 | loss: 0.8615 | ds_loss: 0.0000 | lr: 1.3354e-05 | scale: 16384.0000 | micro time: 1.771 | step time: 0.000
train | epoch   6 | Iter:   8544/ 13000 | global iter:   8544/ 13000 | loss: 0.9507 | ds_loss: 0.0000 | lr: 1.3349e-05 | scale: 16384.0000 | micro time: 1.814 | step time: 0.000
train | epoch   6 | Iter:   8545/ 13000 | global iter:   8545/ 13000 | loss: 1.5423 | ds_loss: 0.0000 | lr: 1.3343e-05 | scale: 16384.0000 | micro time: 1.740 | step time: 0.000
train | epoch   6 | Iter:   8546/ 13000 | global iter:   8546/ 13000 | loss: 0.9548 | ds_loss: 0.0000 | lr: 1.3338e-05 | scale: 16384.0000 | micro time: 1.767 | step time: 0.000
train | epoch   6 | Iter:   8547/ 13000 | global iter:   8547/ 13000 | loss: 0.9789 | ds_loss: 0.0000 | lr: 1.3333e-05 | scale: 16384.0000 | micro time: 1.897 | step time: 0.000
train | epoch   6 | Iter:   8548/ 13000 | global iter:   8548/ 13000 | loss: 0.7165 | ds_loss: 0.0000 | lr: 1.3327e-05 | scale: 16384.0000 | micro time: 1.829 | step time: 0.000
train | epoch   6 | Iter:   8549/ 13000 | global iter:   8549/ 13000 | loss: 1.0823 | ds_loss: 0.0000 | lr: 1.3322e-05 | scale: 16384.0000 | micro time: 1.861 | step time: 0.000
train | epoch   6 | Iter:   8550/ 13000 | global iter:   8550/ 13000 | loss: 0.9826 | ds_loss: 0.0000 | lr: 1.3317e-05 | scale: 16384.0000 | micro time: 1.757 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8550/ 13000 | global iter:   8550/ 13000 | loss: 0.9333 | ds_loss: 0.0000 | lr: 1.3317e-05 | scale: 16384.0000 | micro time: 1.757 | step time: 1.794
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8551/ 13000 | global iter:   8551/ 13000 | loss: 0.9060 | ds_loss: 0.0000 | lr: 1.3311e-05 | scale: 16384.0000 | micro time: 1.774 | step time: 0.000
train | epoch   6 | Iter:   8552/ 13000 | global iter:   8552/ 13000 | loss: 1.1016 | ds_loss: 0.0000 | lr: 1.3306e-05 | scale: 16384.0000 | micro time: 1.815 | step time: 0.000
train | epoch   6 | Iter:   8553/ 13000 | global iter:   8553/ 13000 | loss: 0.6131 | ds_loss: 0.0000 | lr: 1.3301e-05 | scale: 16384.0000 | micro time: 1.895 | step time: 0.000
train | epoch   6 | Iter:   8554/ 13000 | global iter:   8554/ 13000 | loss: 0.5645 | ds_loss: 0.0000 | lr: 1.3295e-05 | scale: 16384.0000 | micro time: 1.839 | step time: 0.000
train | epoch   6 | Iter:   8555/ 13000 | global iter:   8555/ 13000 | loss: 1.1937 | ds_loss: 0.0000 | lr: 1.3290e-05 | scale: 16384.0000 | micro time: 1.785 | step time: 0.000
train | epoch   6 | Iter:   8556/ 13000 | global iter:   8556/ 13000 | loss: 0.9763 | ds_loss: 0.0000 | lr: 1.3285e-05 | scale: 16384.0000 | micro time: 1.777 | step time: 0.000
train | epoch   6 | Iter:   8557/ 13000 | global iter:   8557/ 13000 | loss: 0.3744 | ds_loss: 0.0000 | lr: 1.3279e-05 | scale: 16384.0000 | micro time: 1.755 | step time: 0.000
train | epoch   6 | Iter:   8558/ 13000 | global iter:   8558/ 13000 | loss: 1.0468 | ds_loss: 0.0000 | lr: 1.3274e-05 | scale: 16384.0000 | micro time: 1.711 | step time: 0.000
train | epoch   6 | Iter:   8559/ 13000 | global iter:   8559/ 13000 | loss: 0.9488 | ds_loss: 0.0000 | lr: 1.3269e-05 | scale: 16384.0000 | micro time: 1.868 | step time: 0.000
train | epoch   6 | Iter:   8560/ 13000 | global iter:   8560/ 13000 | loss: 1.1464 | ds_loss: 0.0000 | lr: 1.3263e-05 | scale: 16384.0000 | micro time: 1.858 | step time: 0.000
****************************************************************************************************
train | epoch   6 | Iter:   8560/ 13000 | global iter:   8560/ 13000 | loss: 0.8872 | ds_loss: 0.0000 | lr: 1.3263e-05 | scale: 16384.0000 | micro time: 1.858 | step time: 1.808
./results/gpt2/train/sft/gpt2-xlarge-spanish/e10-bs2-lr5e-05-G1-N4-NN1
****************************************************************************************************
train | epoch   6 | Iter:   8561/ 13000 | global iter:   8561/ 13000 | loss: 0.8785 | ds_loss: 0.0000 | lr: 1.3258e-05 | scale: 16384.0000 | micro time: 1.733 | step time: 0.000
train | epoch   6 | Iter:   8562/ 13000 | global iter:   8562/ 13000 | loss: 1.4599 | ds_loss: 0.0000 | lr: 1.3253e-05 | scale: 16384.0000 | micro time: 1.827 | step time: 0.000
train | epoch   6 | Iter:   8563/ 13000 | global iter:   8563/ 13000 | loss: 1.4572 | ds_loss: 0.0000 | lr: 1.3247e-05 | scale: 16384.0000 | micro time: 1.819 | step time: 0.000
slurmstepd: error: *** STEP 8864256.2 ON compsci-cluster-fitz-34 CANCELLED AT 2025-04-19T15:52:43 DUE TO TIME LIMIT ***
slurmstepd: error: *** JOB 8864256 ON compsci-cluster-fitz-34 CANCELLED AT 2025-04-19T15:52:43 DUE TO TIME LIMIT ***
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
